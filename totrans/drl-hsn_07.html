<html><head></head><body>
<div id="sbo-rt-content"><section class="level2 chapterHead" id="chapter-7-higher-level-rl-libraries">
<h1 class="chapterNumber">7</h1>
<h1 class="chapterTitle" id="sigil_toc_id_410">
<span id="x1-1070007"/>Higher-Level RL Libraries
    </h1>
<p>In <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109"> </span><a href="#"><span class="cmti-10x-x-109">6</span></a>, we implemented the <span class="cmbx-10x-x-109">deep Q-network </span>(<span class="cmbx-10x-x-109">DQN</span>) model published <span id="dx1-107001"/>by DeepMind in 2015 [<span id="x1-107002"/><a href="#">Mni+15</a>]. This paper had a significant effect on the RL field by demonstrating that, despite common belief, it’s possible to use nonlinear approximators in RL. This proof of concept stimulated great interest in the deep Q-learning field and in deep RL in general.</p>
<p>In this chapter, we will take another step toward a practical RL by discussing higher-level RL libraries, which will allow you to build your code from higher-level blocks and focus on the details of the method that you are implementing, avoiding reimplementing the same logic multiple times. Most of the chapter will describe the <span class="cmbx-10x-x-109">PyTorch AgentNet (PTAN) </span>library, which <span id="dx1-107003"/>will be used in the rest of the book to prevent code repetition, so will be covered in detail.</p>
<p>We will cover:</p>
<ul>
<li>
<p>The motivation for using high-level libraries, rather than reimplementing everything from scratch</p>
</li>
<li>
<p>The PTAN library, including coverage of the most important parts, which will be illustrated with code examples</p>
</li>
<li>
<p>DQN on CartPole, implemented using the PTAN library</p>
</li>
<li>
<p>Other RL libraries that you might consider</p>
</li>
</ul>
<section class="level3 sectionHead" id="why-rl-libraries">
<h1 class="heading-1" id="sigil_toc_id_96"> <span id="x1-1080007.1"/>Why RL libraries?</h1>
<p>Our implementation of basic DQN in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109"> </span><a href="#"><span class="cmti-10x-x-109">6</span></a> wasn’t very, long and<span id="dx1-108001"/> complicated—about 200 lines of training code plus 50 lines in environment wrappers. When you are becoming familiar with RL methods, it is very useful to implement everything yourself to understand how things actually work. However, the more involved you become in the field, the more often you will realize that you are writing the same code over and over again.</p>
<p>This repetition comes from the generality of RL methods. As we discussed in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109"> </span><a href="ch005.xhtml#x1-190001"><span class="cmti-10x-x-109">1</span></a>, RL is quite flexible, and many real-life problems fall into the environment-agent interaction scheme. RL methods don’t make many assumptions about the specifics of observations and actions, so code implemented for the CartPole environment will be applicable to Atari games (maybe with some minor tweaks).</p>
<p>Writing the same code over and over again is not very efficient, as bugs might be introduced every time, which will cost you time for debugging and understanding. In addition, carefully designed code that has been used in several projects usually has a higher quality in terms of performance, unit tests, readability, and documentation.</p>
<p>The practical applications of RL are quite young by computer science standards, so in comparison to other more mature domains, you might not have that rich a choice of approaches. For example, in web development, even if you limit yourself to just Python, you have hundreds of very good libraries of all sorts: Django for heavyweight, fully functional websites; Flask for light <span class="cmbx-10x-x-109">Web</span> <span class="cmbx-10x-x-109">Server Gateway Interface </span>(<span class="cmbx-10x-x-109">WSGI</span>) apps; and much more, large and small.</p>
<p>RL is not as mature as web frameworks, but still, you can choose from several projects that are trying to <span id="dx1-108002"/>simplify RL practitioners’ lives. In addition, you can always write your own set of tools, as I did several years ago. The tool I created is a library called PTAN, and, as mentioned, it will be used in the rest of the book to illustrate examples.</p>
</section>
<section class="level3 sectionHead" id="the-ptan-library">
<h1 class="heading-1" id="sigil_toc_id_97"> <span id="x1-1090007.2"/>The PTAN library</h1>
<p>This library <span id="dx1-109001"/>is available on GitHub: <a class="url" href="https://github.com/Shmuma/ptan"><span class="cmtt-10x-x-109">https://github.com/Shmuma/ptan</span></a>. All the subsequent examples were implemented using version 0.8 of PTAN, which can be installed in your virtual environment by running the following:</p>
<pre class="lstlisting" id="listing-160"><code>$ pip install ptan==0.8</code></pre>
<p>The original goal of PTAN was to simplify my RL experiments, and it tries to keep the balance between two extremes:</p>
<ul>
<li>
<p>Import the library and then write just a couple of lines with tons of parameters to train one of the provided methods, like DQN (a very vivid example is the OpenAI Baselines and Stable Baselines3 projects). This first approach is very inflexible. It works well when you are using the library the way it is supposed to be used. But if you want to do something fancy, you will quickly find yourself hacking the library and fighting with the constraints it imposes, rather than solving the problem you want to solve.</p>
</li>
<li>
<p>Implement all the method’s logic from scratch. This second extreme gives too much freedom and requires implementing replay buffers and trajectory handling over and over again, which is error-prone, boring, and inefficient.</p>
</li>
</ul>
<p>PTAN tries to balance those extremes, providing high-quality building blocks to simplify your RL code, but at the same time being flexible and not limiting<span id="dx1-109003"/> your creativity.</p>
<p>At a high level, PTAN provides<span id="dx1-109004"/> the following entities:</p>
<ul>
<li>
<p><span class="cmtt-10x-x-109">Agent</span>: A class that knows how to convert a batch of observations to a batch of actions to be executed. It can contain an optional state, in case you need to track some information between consequent actions in one episode. (We will use this approach in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109"> </span><a href="ch019.xhtml#x1-27200015"><span class="cmti-10x-x-109">15</span></a>, in the <span class="cmbx-10x-x-109">deep</span> <span class="cmbx-10x-x-109">deterministic policy gradient (DDPG) </span>method, which <span id="dx1-109005"/>includes the Ornstein–Uhlenbeck random process for exploration.) The library provides several agents for the most common RL cases, but you always can write your own subclass of <span class="cmtt-10x-x-109">BaseAgent </span>if none of the predefined classes are meeting your needs.</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">ActionSelector</span>: A small piece of logic that knows how to choose the action from some output of the network. It works in tandem with the <span class="cmtt-10x-x-109">Agent </span>class.</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">ExperienceSource </span>and subclasses: The <span class="cmtt-10x-x-109">Agent </span>instance and a Gym environment object can provide information about the trajectory of the agent during the episodes. In its simplest form, it is one single (<span class="cmmi-10x-x-109">a</span>, <span class="cmmi-10x-x-109">r</span>, <span class="cmmi-10x-x-109">s</span><span class="cmsy-10x-x-109">′</span>) transition at a time, but its functionality goes beyond this.</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">ExperienceSourceBuffer </span>and subclasses: Replay buffers with various characteristics. They include a simple replay buffer and two versions of prioritized replay buffers.</p>
</li>
<li>
<p>Various utility classes: An example is <span class="cmtt-10x-x-109">TargetNet </span>and wrappers for time-series preprocessing (used for tracking training progress in TensorBoard).</p>
</li>
<li>
<p>PyTorch Ignite helpers: These can be used to integrate PTAN into the Ignite framework.</p>
</li>
<li>
<p>Wrappers for Gym environments: For example, wrappers for Atari games (very similar to the wrappers we described in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109"> </span><a href="#"><span class="cmti-10x-x-109">6</span></a>).</p>
</li>
</ul>
<p>And that’s basically it. In the following sections, we will take a look at these entities in detail.</p>
<section class="level4 subsectionHead" id="action-selectors">
<h2 class="heading-2" id="sigil_toc_id_98"> <span id="x1-1100007.2.1"/>Action selectors</h2>
<p>In PTAN <span id="dx1-110001"/>terminology, an <span class="cmti-10x-x-109">action selector </span>is<span id="dx1-110002"/> an object that helps with going from network output to concrete action values. The most common cases include:</p>
<ul>
<li>
<p><span class="cmbx-10x-x-109">Greedy (or argmax)</span>: Commonly used by Q-value methods when the network predicts Q-values for a set of actions and the desired action is the action with the largest <span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s,a</span>).</p>
</li>
<li>
<p><span class="cmbx-10x-x-109">Policy-based</span>: The network outputs the probability distribution (in the form of logits or normalized distribution), and an action needs to be sampled from this distribution. You have already seen this in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109"> </span><a href="ch008.xhtml#x1-740004"><span class="cmti-10x-x-109">4</span></a>, when we discussed the cross-entropy method.</p>
</li>
</ul>
<p>An action selector is used by the <span class="cmtt-10x-x-109">Agent </span>and rarely needs to be customized (but you have the option). The concrete classes provided by the library are:</p>
<ul>
<li>
<p><span class="cmtt-10x-x-109">ArgmaxActionSelector</span>: Applies <span class="cmtt-10x-x-109">argmax </span>on the second axis of a passed tensor. It assumes a matrix with batch dimension along the first axis.</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">ProbabilityActionSelector</span>: Samples from the probability distribution of a discrete set of actions.</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">EpsilonGreedyActionSelector</span>: Has the <span class="cmtt-10x-x-109">epsilon </span>parameter, which specifies the probability of a random action being taken. It also holds another <span class="cmtt-10x-x-109">ActionSelector </span>instance, which is used when we’re not sampling random actions.</p>
</li>
</ul>
<p>All the classes assume that NumPy arrays will be passed to them. The complete example from this section can be found in <span class="cmtt-10x-x-109">Chapter07/01</span><span class="cmtt-10x-x-109">_actions.py</span>. Here, I’m going to show you how these classes are supposed to be used:</p>
<pre class="lstlisting" id="listing-161"><code>&gt;&gt;&gt; import numpy as np 
&gt;&gt;&gt; import ptan 
&gt;&gt;&gt; q_vals = np.array([[1, 2, 3], [1, -1, 0]]) 
&gt;&gt;&gt; q_vals 
array([[ 1,  2,  3], 
      [ 1, -1,  0]]) 
&gt;&gt;&gt; selector = ptan.actions.ArgmaxActionSelector() 
&gt;&gt;&gt; selector(q_vals) 
array([2, 0])</code></pre>
<p>As you can <span id="dx1-110012"/>see, the selector returns indices of actions with the largest values.</p>
<p>The next action selector is <span class="cmtt-10x-x-109">EpisilonGreedyActionSelector</span>, which ”wraps” another action <span id="dx1-110013"/>selector and, depending on the <span class="cmtt-10x-x-109">epsilon </span>parameter, either uses the wrapped action selector or takes the random action. This action selector is used during training to introduce randomness to the agent’s actions. If <span class="cmtt-10x-x-109">epsilon </span>is <span class="cmtt-10x-x-109">0.0</span>, no random actions are taken:</p>
<pre class="lstlisting" id="listing-162"><code>&gt;&gt;&gt; selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=0.0, selector=ptan.actions.ArgmaxActionSelector()) 
&gt;&gt;&gt; selector(q_vals) 
array([2, 0])</code></pre>
<p>If we change <span class="cmtt-10x-x-109">epsilon </span>to <span class="cmtt-10x-x-109">1</span>, actions will be random:</p>
<pre class="lstlisting" id="listing-163"><code>&gt;&gt;&gt; selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=1.0) 
&gt;&gt;&gt; selector(q_vals) 
array([0, 1])</code></pre>
<p>You can also change the value of epsilon by assigning the action selector’s attribute:</p>
<pre class="lstlisting" id="listing-164"><code>&gt;&gt;&gt; selector.epsilon 
1.0 
&gt;&gt;&gt; selector.epsilon = 0.0 
&gt;&gt;&gt; selector(q_vals) 
array([2, 0])</code></pre>
<p>Working with <span class="cmtt-10x-x-109">ProbabilityActionSelector </span>is the same, but the input needs to be a normalized probability distribution:</p>
<pre class="lstlisting" id="listing-165"><code>&gt;&gt;&gt; selector = ptan.actions.ProbabilityActionSelector() 
&gt;&gt;&gt; for _ in range(10): 
...    acts = selector(np.array([ 
...        [0.1, 0.8, 0.1], 
...        [0.0, 0.0, 1.0], 
...        [0.5, 0.5, 0.0] 
...    ])) 
...    print(acts) 
... 
[0 2 1] 
[1 2 1] 
[1 2 1] 
[0 2 1] 
[2 2 0] 
[0 2 0] 
[1 2 1] 
[1 2 0] 
[1 2 1] 
[1 2 0]</code></pre>
<p>In the preceding example, we sample from three distributions (as we have three rows in the passed matrix):</p>
<ul>
<li>
<p>The first is defined by the vector <span class="cmtt-10x-x-109">[0.1, 0.8, 0.1]</span>; as a result, the action with index <span class="cmtt-10x-x-109">1 </span>is chosen with probability 80%</p>
</li>
<li>
<p>The vector <span class="cmtt-10x-x-109">[0.0, 0.0, 1.0] </span>always gives us an action with index <span class="cmtt-10x-x-109">2</span></p>
</li>
<li>
<p>The <span id="dx1-110044"/>distribution <span class="cmtt-10x-x-109">[0.5, 0.5, 0.0] </span>produces <span id="dx1-110045"/>actions <span class="cmtt-10x-x-109">0 </span>and <span class="cmtt-10x-x-109">1 </span>with 50% chance</p>
</li>
</ul>
</section>
<section class="level4 subsectionHead" id="the-agent-1">
<h2 class="heading-2" id="sigil_toc_id_99"> <span id="x1-1110007.2.2"/>The agent</h2>
<p>The agent <span id="dx1-111001"/>entity provides an unified way of <span id="dx1-111002"/>bridging observations from the environment and the actions that we want to execute. So far, you have seen only a simple, stateless DQN agent that uses a <span class="cmbx-10x-x-109">neural network </span>(<span class="cmbx-10x-x-109">NN</span>) to obtain action values from the current observation and behaves greedily on those values. We have used epsilon-greedy behavior to explore the environment, but this doesn’t change the picture much.</p>
<p>In the RL field, this could be more complicated. For example, instead of predicting the values of the actions, our agent could predict a probability distribution over the actions. Such agents are called <span class="cmti-10x-x-109">policy agents</span>, and we will talk about those methods in <span class="cmti-10x-x-109">Part 3 </span>of the book.</p>
<p>In some situations, it might be neccesary for the agent to keep state between observations. For example, very often, one observation (or even the <span class="cmmi-10x-x-109">k </span>last observations) is not enough to make a decision about the action, and we want to keep some memory in the agent to capture the necessary information. There is a whole subdomain of RL that tries to address this complication with <span class="cmbx-10x-x-109">partially</span> <span class="cmbx-10x-x-109">observable Markov decision process </span>(<span class="cmbx-10x-x-109">POMDP</span>) formalism, which we briefly mentioned in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109"> </span><a href="#"><span class="cmti-10x-x-109">6</span></a> but is not covered extensively in the book.</p>
<p>The third variant of the agent is very common in <span class="cmti-10x-x-109">continuous control problems</span>, which will be discussed in <span class="cmti-10x-x-109">Part 4 </span>of the book. For now, it suffices to say that in such cases, actions are not discrete anymore but continuous values, and the agent needs to predict them from the observations.</p>
<p>To capture all those variants and make the code flexible, the agent in PTAN is implemented as an extensible hierarchy of classes with the <span class="cmtt-10x-x-109">ptan.agent.BaseAgent </span>abstract class at the top. From a high level, the agent needs to accept the batch of observations (in the form of a NumPy array or a list of NumPy arrays) and return the batch of actions that it wants to take. The batch is used to make the processing more efficient, as processing several observations in one pass in a <span class="cmbx-10x-x-109">graphics</span> <span class="cmbx-10x-x-109">processing unit </span>(<span class="cmbx-10x-x-109">GPU</span>) is frequently much faster than processing them individually.</p>
<p>The <span id="dx1-111003"/>abstract base class doesn’t define the types of input and output, which makes it very flexible and easy to extend. For example, in the continuous domain, our actions will no longer be indices of discrete actions, but float values. In any case, the agent can be seen as something that knows how to convert observations into actions, and it’s up to the <span id="dx1-111004"/>agent how to do this. In general, there are no assumptions made on observation and action types, but the concrete implementation of agents is more limiting. PTAN provides two of the most common ways to convert observations into actions: <span class="cmtt-10x-x-109">DQNAgent </span>and <span class="cmtt-10x-x-109">PolicyAgent</span>. We will explore these in subsequent sections.</p>
<p>However, in real problems, a custom agent is often needed. These are some of the reasons:</p>
<ul>
<li>
<p>The architecture of the NN is fancy—its action space is a mixture of continuous and discrete and it has multimodal observations (text and pixels, for example), or something like that.</p>
</li>
<li>
<p>You want to use non-standard exploration strategies, for example, the Ornstein–Uhlenbeck process (a very popular exploration strategy in the continuous control domain).</p>
</li>
<li>
<p>You have a POMDP environment, and the agent’s decision is not fully defined by observations, but by some internal agent state (which is also the case for Ornstein–Uhlenbeck exploration).</p>
</li>
</ul>
<p>All those cases are easily supported by subclassing the <span class="cmtt-10x-x-109">BaseAgent </span>class, and in the rest of the book, several examples of such redefinition will be given.</p>
<p>Let’s now check <span id="dx1-111005"/>the standard agents provided<span id="dx1-111006"/> by the library: <span class="cmtt-10x-x-109">DQNAgent </span>and <span class="cmtt-10x-x-109">PolicyAgent</span>. The complete example is in <span class="cmtt-10x-x-109">Chapter07/02</span><span class="cmtt-10x-x-109">_agents.py</span>.</p>
<section class="level5 subsubsectionHead" id="dqnagent">
<h3 class="heading-3" id="sigil_toc_id_392"><span id="x1-112000"/>DQNAgent</h3>
<p>This class<span id="dx1-112001"/> is applicable in Q-learning when the action<span id="dx1-112002"/> space is not very large, which covers Atari games and lots of classical problems. This representation is not universal and, later in the book, you will see ways of dealing with that. <span class="cmtt-10x-x-109">DQNAgent </span>takes a batch of observations as input (as a NumPy array), applies the network to them to get Q-values, and then uses the provided <span class="cmtt-10x-x-109">ActionSelector </span>to convert Q-values to indices of actions.</p>
<p>Let’s consider a small example. For simplicity, our network always produces the same output for the input batch.</p>
<p>First, we define the NN class, which is supposed to convert observations to actions. In our example, it doesn’t use NNs at all and always produces the same output:</p>
<div class="tcolorbox" id="tcolobox-133">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-166"><code>class DQNNet(nn.Module): 
    def __init__(self, actions: int): 
        super(DQNNet, self).__init__() 
        self.actions = actions 
 
    def forward(self, x): 
        # we always produce diagonal tensor of shape 
        # (batch_size, actions) 
        return torch.eye(x.size()[0], self.actions)</code></pre>
</div>
</div>
<p>Once we have defined the model class, we can use it as a DQN model:</p>
<pre class="lstlisting" id="listing-167"><code>&gt;&gt;&gt; net = DQNNet(actions=3) 
&gt;&gt;&gt; net(torch.zeros(2, 10)) 
tensor([[1., 0., 0.], 
       [0., 1., 0.]])</code></pre>
<p>We start with the simple argmax policy (which returns the action with the largest value), so the agent will always return actions corresponding to ones in the network output:</p>
<pre class="lstlisting" id="listing-168"><code>&gt;&gt;&gt; selector = ptan.actions.ArgmaxActionSelector() 
&gt;&gt;&gt; agent = ptan.agent.DQNAgent(model=net, action_selector=selector) 
&gt;&gt;&gt; agent(torch.zeros(2, 5)) 
(array([0, 1]), [None, None])</code></pre>
<p>In the input, a batch of two observations, each having five values, was given, and in the output, the agent returned a tuple of two objects:</p>
<ul>
<li>
<p>An array with actions to be executed for our batch. In our case, this is action <span class="cmtt-10x-x-109">0 </span>for the first batch sample and action <span class="cmtt-10x-x-109">1 </span>for the second.</p>
</li>
<li>
<p>A list with the agent’s internal state. This is used for stateful agents and is a list of <span class="cmtt-10x-x-109">None </span>in our case. As our agent is stateless, you can ignore it.</p>
</li>
</ul>
<p>Now, let’s make the agent with an epsilon-greedy exploration strategy. To do this, we just need to pass a different action selector:</p>
<pre class="lstlisting" id="listing-169"><code>&gt;&gt;&gt; selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=1.0) 
&gt;&gt;&gt; agent = ptan.agent.DQNAgent(model=net, action_selector=selector) 
&gt;&gt;&gt; agent(torch.zeros(10, 5))[0] 
array([2, 0, 0, 0, 1, 2, 1, 2, 2, 1])</code></pre>
<p>As <span class="cmtt-10x-x-109">epsilon </span>is <span class="cmtt-10x-x-109">1.0</span>, all the actions will be random, regardless of the network’s output. But we can change the epsilon value on the fly, which is very handy during the <span id="dx1-112024"/>training<span id="dx1-112025"/> when we anneal <span class="cmtt-10x-x-109">epsilon </span>over time:</p>
<pre class="lstlisting" id="listing-170"><code>&gt;&gt;&gt; selector.epsilon = 0.5 
&gt;&gt;&gt; agent(torch.zeros(10, 5))[0] 
array([0, 1, 2, 2, 0, 0, 1, 2, 0, 2]) 
&gt;&gt;&gt; selector.epsilon = 0.1 
&gt;&gt;&gt; agent(torch.zeros(10, 5))[0] 
array([0, 1, 2, 0, 0, 0, 0, 0, 0, 0])</code></pre>
</section>
<section class="level5 subsubsectionHead" id="policyagent">
<h3 class="heading-3" id="sigil_toc_id_393"><span id="x1-113000"/>PolicyAgent</h3>
<p><span class="cmtt-10x-x-109">PolicyAgent </span>expects the network to produce <span id="dx1-113001"/>a policy distribution<span id="dx1-113002"/> over a discrete set of actions. The policy distribution could be either logits (unnormalized) or a normalized distribution. In practice, you should always use logits to improve the numeric stability of the training process.</p>
<p>Let’s reimplement our previous example, but now, the network will produce a probability.</p>
<p>We begin by defining the following class:</p>
<div class="tcolorbox" id="tcolobox-134">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-171"><code>class PolicyNet(nn.Module): 
    def __init__(self, actions: int): 
        super(PolicyNet, self).__init__() 
        self.actions = actions 
 
    def forward(self, x): 
        # Now we produce the tensor with first two actions 
        # having the same logit scores 
        shape = (x.size()[0], self.actions) 
        res = torch.zeros(shape, dtype=torch.float32) 
        res[:, 0] = 1 
        res[:, 1] = 1 
        return res</code></pre>
</div>
</div>
<p>The class above could be used to get the action logits for a batch of observations:</p>
<pre class="lstlisting" id="listing-172"><code>&gt;&gt;&gt; net = PolicyNet(actions=5) 
&gt;&gt;&gt; net(torch.zeros(6, 10)) 
tensor([[1., 1., 0., 0., 0.], 
       [1., 1., 0., 0., 0.], 
       [1., 1., 0., 0., 0.], 
       [1., 1., 0., 0., 0.], 
       [1., 1., 0., 0., 0.], 
       [1., 1., 0., 0., 0.]])</code></pre>
<p>Now, we can use <span class="cmtt-10x-x-109">PolicyAgent </span>in combination with <span class="cmtt-10x-x-109">ProbabilityActionSelector</span>. As the latter expects normalized probabilities, we need to ask <span class="cmtt-10x-x-109">PolicyAgent </span>to <span id="dx1-113024"/>apply softmax to the network’s output:</p>
<pre class="lstlisting" id="listing-173"><code>&gt;&gt;&gt; selector = ptan.actions.ProbabilityActionSelector() 
&gt;&gt;&gt; agent = ptan.agent.PolicyAgent(model=net, action_selector=selector, apply_softmax=True) 
&gt;&gt;&gt; agent(torch.zeros(6, 5))[0] 
array([2, 1, 2, 0, 2, 3])</code></pre>
<p>Please note that the softmax operation produces non-zero probabilities for zero logits, so our agent can still select actions <span id="dx1-113029"/>with zero logit values:</p>
<pre class="lstlisting" id="listing-174"><code>&gt;&gt;&gt; torch.nn.functional.softmax(torch.tensor([1., 1., 0., 0., 0.])) 
tensor([0.3222, 0.3222, 0.1185, 0.1185, 0.1185])</code></pre>
</section>
</section>
<section class="level4 subsectionHead" id="experience-source">
<h2 class="heading-2" id="sigil_toc_id_100"> <span id="x1-1140007.2.3"/>Experience source</h2>
<p>The agent<span id="dx1-114001"/> abstraction described in the<span id="dx1-114002"/> previous section allows us to implement environment communications in a generic way. These communications happen in the form of trajectories, produced by applying the agent’s actions to the Gym environment.</p>
<p>At a high level, experience source classes take the agent instance and environment and provide you with step-by-step data from the trajectories. The functionality of those classes includes:</p>
<ul>
<li>
<p>Support for multiple environments being communicated at the same time. This allows efficient GPU utilization as a batch of observations is being processed by the agent at once.</p>
</li>
<li>
<p>A trajectory can be preprocessed and presented in a convenient form for further training. For example, there is an implementation of subtrajectory rollouts with accumulation of the reward. That preprocessing is convenient for DQN and n-step DQN, when we are not interested in individual intermediate steps in subtrajectories, so they can be dropped. This saves memory and reduces the amount of code we need to write.</p>
</li>
<li>
<p>Support for vectorized environments from Gymnasium (classes <span class="cmtt-10x-x-109">AsyncVectorEnv </span>and <span class="cmtt-10x-x-109">SyncVectorEnv</span>). We will cover this in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109"> </span><a href="ch021.xhtml#x1-31100017"><span class="cmti-10x-x-109">17</span></a>.</p>
</li>
</ul>
<p>So, the experience source classes act as a ”magic black box” to hide the environment interaction and trajectory handling complexities from the library user. But the overall PTAN philosophy is to be flexible and extensible, so if you want, you can subclass one of the existing classes or implement your own version as needed.</p>
<p>Three classes are provided by the system:</p>
<ul>
<li>
<p><span class="cmtt-10x-x-109">ExperienceSource</span>: Using the agent and the set of environments, it produces n-step subtrajectories with all intermediate steps.</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">ExperienceSourceFirstLast</span>: This is the same as <span class="cmtt-10x-x-109">ExperienceSource</span>, but instead of the full subtrajectory (with all steps), it keeps only the first and last steps, with proper reward accumulation in between. This can save a lot of memory in the case of n-step DQN or <span class="cmbx-10x-x-109">advantage actor-critic </span>(<span class="cmbx-10x-x-109">A2C</span>) rollouts.</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">ExperienceSourceRollouts</span>: This follows the <span class="cmbx-10x-x-109">asynchronous</span> <span class="cmbx-10x-x-109">advantage actor-critic </span>(<span class="cmbx-10x-x-109">A3C</span>) rollouts scheme described in Mnih’s paper about Atari games (we will discuss this topic in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109"> </span><a href="ch016.xhtml#x1-20300012"><span class="cmti-10x-x-109">12</span></a>).</p>
</li>
</ul>
<p>All the classes are written to be efficient both in terms of <span class="cmbx-10x-x-109">central processing</span> <span class="cmbx-10x-x-109">unit </span>(<span class="cmbx-10x-x-109">CPU</span>) and memory, which is not very important for toy problems, but will become relevant in the next chapter when we get to Atari games with much larger amounts of data to be<span id="dx1-114003"/> stored and processed.</p>
<section class="level5 subsubsectionHead" id="toy-environment">
<h3 class="heading-3" id="sigil_toc_id_394"><span id="x1-115000"/>Toy environment</h3>
<p>For demonstration, we will implement a very simple<span id="dx1-115001"/> Gym environment with a small predictable observation state to show how <span class="cmtt-10x-x-109">ExperienceSource </span>classes work. This environment has integer observation, which increases from 0 to 4, integer action, and a reward equal to the action given. All episodes produced by the environment always have 10 steps:</p>
<div class="tcolorbox" id="tcolobox-135">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-175"><code>class ToyEnv(gym.Env): 
    def __init__(self): 
        super(ToyEnv, self).__init__() 
        self.observation_space = gym.spaces.Discrete(n=5) 
        self.action_space = gym.spaces.Discrete(n=3) 
        self.step_index = 0 
 
    def reset(self): 
        self.step_index = 0 
        return self.step_index, {} 
 
    def step(self, action: int): 
        is_done = self.step_index == 10 
        if is_done: 
            return self.step_index % self.observation_space.n, 0.0, is_done, False, {} 
        self.step_index += 1 
        return self.step_index % self.observation_space.n, float(action), \ 
            self.step_index == 10, False, {}</code></pre>
</div>
</div>
<p>In addition to this environment, we will use an agent that always generates fixed actions regardless of observations:</p>
<div class="tcolorbox" id="tcolobox-136">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-176"><code>class DullAgent(ptan.agent.BaseAgent): 
    def __init__(self, action: int): 
        self.action = action 
 
    def __call__(self, observations: tt.List[int], state: tt.Optional[list] = None) -&gt; \ 
            tt.Tuple[tt.List[int], tt.Optional[list]]: 
        return [self.action for _ in observations], state</code></pre>
</div>
</div>
<p>Both classes are defined in the <span class="cmtt-10x-x-109">Chapter07/lib.py </span>module. Now that we have defined the agent, let’s talk about the <span id="dx1-115027"/>data it produces.</p>
</section>
<section class="level5 subsubsectionHead" id="the-experiencesource-class">
<h3 class="heading-3" id="sigil_toc_id_395"><span id="x1-116000"/>The ExperienceSource class</h3>
<p>The first <span id="dx1-116001"/>class we will discuss is <span class="cmtt-10x-x-109">ptan.experience.ExperienceSource</span>, which generates chunks of agent trajectories of the given length. The implementation automatically handles the end-of-episode situation (when the <span class="cmtt-10x-x-109">step() </span>method in the environment returns <span class="cmtt-10x-x-109">is</span><span class="cmtt-10x-x-109">_done=True</span>) and resets the environment. The constructor accepts several arguments:</p>
<ul>
<li>
<p>The Gym environment to be used. Alternatively, it could be the list of environments.</p>
</li>
<li>
<p>The agent instance.</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">steps</span><span class="cmtt-10x-x-109">_count=2</span>: The length of subtrajectories to be generated.</p>
</li>
</ul>
<p>The class instance provides the standard Python iterator interface, so you can just iterate over it to get subtrajectories:</p>
<pre class="lstlisting" id="listing-177"><code>&gt;&gt;&gt; from lib import * 
&gt;&gt;&gt; env = ToyEnv() 
&gt;&gt;&gt; agent = DullAgent(action=1) 
&gt;&gt;&gt; exp_source = ptan.experience.ExperienceSource(env=env, agent=agent, steps_count=2) 
&gt;&gt;&gt; for idx, exp in zip(range(3), exp_source): 
...    print(exp) 
... 
(Experience(state=0, action=1, reward=1.0, done_trunc=False), Experience(state=1, action=1, reward=1.0, done_trunc=False)) 
(Experience(state=1, action=1, reward=1.0, done_trunc=False), Experience(state=2, action=1, reward=1.0, done_trunc=False)) 
(Experience(state=2, action=1, reward=1.0, done_trunc=False), Experience(state=3, action=1, reward=1.0, done_trunc=False))</code></pre>
<p>On every iteration, <span class="cmtt-10x-x-109">ExperienceSource </span>returns a piece of the agent’s trajectory in environment communication. It might look simple, but there are<span id="dx1-116012"/> several things happening under the hood of our example:</p>
<ol>
<li>
<div id="x1-116014x1">
<p><span class="cmtt-10x-x-109">reset() </span>was called in the environment to get the initial state.</p>
</div>
</li>
<li>
<div id="x1-116016x2">
<p>The agent was asked to select the action to execute from the state returned.</p>
</div>
</li>
<li>
<div id="x1-116018x3">
<p>The <span class="cmtt-10x-x-109">step() </span>method was executed to get the reward and the next state.</p>
</div>
</li>
<li>
<div id="x1-116020x4">
<p>This next state was passed to the agent for the next action.</p>
</div>
</li>
<li>
<div id="x1-116022x5">
<p>Information about the transition from one state to the next state was returned.</p>
</div>
</li>
<li>
<div id="x1-116024x6">
<p>If the environment returns the end-of-episode flag, we emit the rest of the trajectory and reset the environment to start over.</p>
</div>
</li>
<li>
<div id="x1-116026x7">
<p>The process continues (from step 3) during the iteration over the experience source.</p>
</div>
</li>
</ol>
<p>If the agent changes the way it generates actions (we can get this by updating the network weights, decreasing epsilon, or by some other means), it will immediately affect the experience trajectories that we get.</p>
<p>The <span class="cmtt-10x-x-109">ExperienceSource </span>instance returns tuples with lengths equal to or less than the <span class="cmtt-10x-x-109">step</span><span class="cmtt-10x-x-109">_count </span>argument passed on construction. In our case, we asked for two-step subtrajectories, so tuples will be of length <span class="cmtt-10x-x-109">2 </span>or <span class="cmtt-10x-x-109">1</span> (at the end of episodes). Every object in a tuple is an instance of the <span class="cmtt-10x-x-109">ptan.experience.Experience </span>class, which is a dataclass with the following fields:</p>
<ul>
<li>
<p><span class="cmtt-10x-x-109">state</span>: The state we observed before taking the action</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">action</span>: The action we completed</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">reward</span>: The immediate reward we got from <span class="cmtt-10x-x-109">env</span></p>
</li>
<li>
<p><span class="cmtt-10x-x-109">done</span><span class="cmtt-10x-x-109">_trunc</span>: Whether the episode was done or truncated</p>
</li>
</ul>
<p>If the episode reaches the end, the subtrajectory will be shorter and the underlying environment will be reset automatically, so we don’t need to bother with this and can just keep iterating:</p>
<pre class="lstlisting" id="listing-178"><code>&gt;&gt;&gt; for idx, exp in zip(range(15), exp_source): 
...    print(exp) 
... 
(Experience(state=0, action=1, reward=1.0, done_trunc=False), Experience(state=1, action=1, reward=1.0, done_trunc=False)) 
....... 
(Experience(state=3, action=1, reward=1.0, done_trunc=False), Experience(state=4, action=1, reward=1.0, done_trunc=True)) 
(Experience(state=4, action=1, reward=1.0, done_trunc=True),) 
(Experience(state=0, action=1, reward=1.0, done_trunc=False), Experience(state=1, action=1, reward=1.0, done_trunc=False)) 
(Experience(state=1, action=1, reward=1.0, done_trunc=False), Experience(state=2, action=1, reward=1.0, done_trunc=False))</code></pre>
<p>We can ask <span id="dx1-116036"/><span class="cmtt-10x-x-109">ExperienceSource </span>for subtrajectories of any length:</p>
<pre class="lstlisting" id="listing-179"><code>&gt;&gt;&gt; exp_source = ptan.experience.ExperienceSource(env=env, agent=agent, steps_count=4) 
&gt;&gt;&gt; next(iter(exp_source)) 
(Experience(state=0, action=1, reward=1.0, done_trunc=False), Experience(state=1, action=1, reward=1.0, done_trunc=False), Experience(state=2, action=1, reward=1.0, done_trunc=False), Experience(state=3, action=1, reward=1.0, done_trunc=False))</code></pre>
<p>We can pass it several instances of <span class="cmtt-10x-x-109">gym.Env</span>. In that case, they will be used in a round-robin fashion:</p>
<pre class="lstlisting" id="listing-180"><code>&gt;&gt;&gt; exp_source = ptan.experience.ExperienceSource(env=[ToyEnv(), ToyEnv()], agent=agent, steps_count=4) 
&gt;&gt;&gt; for idx, exp in zip(range(5), exp_source): 
...    print(exp) 
... 
(Experience(state=0, action=1, reward=1.0, done_trunc=False), Experience(state=1, action=1, reward=1.0, done_trunc=False), Experience(state=2, action=1, reward=1.0, done_trunc=False), Experience(state=3, action=1, reward=1.0, done_trunc=False)) 
(Experience(state=0, action=1, reward=1.0, done_trunc=False), Experience(state=1, action=1, reward=1.0, done_trunc=False), Experience(state=2, action=1, reward=1.0, done_trunc=False), Experience(state=3, action=1, reward=1.0, done_trunc=False)) 
(Experience(state=1, action=1, reward=1.0, done_trunc=False), Experience(state=2, action=1, reward=1.0, done_trunc=False), Experience(state=3, action=1, reward=1.0, done_trunc=False), Experience(state=4, action=1, reward=1.0, done_trunc=False)) 
(Experience(state=1, action=1, reward=1.0, done_trunc=False), Experience(state=2, action=1, reward=1.0, done_trunc=False), Experience(state=3, action=1, reward=1.0, done_trunc=False), Experience(state=4, action=1, reward=1.0, done_trunc=False)) 
(Experience(state=2, action=1, reward=1.0, done_trunc=False), Experience(state=3, action=1, reward=1.0, done_trunc=False), Experience(state=4, action=1, reward=1.0, done_trunc=False), Experience(state=0, action=1, reward=1.0, done_trunc=False))</code></pre>
<div class="tcolorbox tipbox" id="tcolobox-137">
<div class="tcolorbox-content">
<p>Please note that when you’re passing several environments to the <span class="cmtt-10x-x-109">ExperienceSource</span>, they have to be independent instances and not a single environment instance, otherwise your observations will become a mess.</p>
</div>
</div>
</section>
<section class="level5 subsubsectionHead" id="the-experiencesourcefirstlast-class">
<h3 class="heading-3" id="sigil_toc_id_396"><span id="x1-117000"/>The ExperienceSourceFirstLast Class</h3>
<p>The <span class="cmtt-10x-x-109">ExperienceSource </span>class<span id="dx1-117001"/> provides<span id="dx1-117002"/> us with full subtrajectories of the given length as a list of (<span class="cmmi-10x-x-109">s</span>, <span class="cmmi-10x-x-109">a</span>, <span class="cmmi-10x-x-109">r</span>) objects. The next state, <span class="cmmi-10x-x-109">s</span><span class="cmsy-10x-x-109">′</span>, is returned in the next tuple, which is not always convenient. For example, in DQN training, we want to have tuples (<span class="cmmi-10x-x-109">s</span>, <span class="cmmi-10x-x-109">a</span>, <span class="cmmi-10x-x-109">r</span>, <span class="cmmi-10x-x-109">s</span><span class="cmsy-10x-x-109">′</span>) at once to do one-step Bellman approximation during the training. In addition, some extension of DQN, like n-step DQN, might want to collapse longer sequences of observations into (<span class="cmti-10x-x-109">first-state</span>, <span class="cmti-10x-x-109">action</span>, <span class="cmti-10x-x-109">total-reward-for-n-steps</span>, <span class="cmti-10x-x-109">state-after-step-n</span>).</p>
<p>To support this in a generic way, a simple subclass of <span class="cmtt-10x-x-109">ExperienceSource </span>is implemented: <span class="cmtt-10x-x-109">ExperienceSourceFirstLast</span>. It accepts almost the same arguments in the constructor, but returns different data:</p>
<pre class="lstlisting" id="listing-181"><code>&gt;&gt;&gt; exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=1.0, steps_count=1) 
&gt;&gt;&gt; for idx, exp in zip(range(11), exp_source): 
...    print(exp) 
... 
ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1) 
ExperienceFirstLast(state=1, action=1, reward=1.0, last_state=2) 
ExperienceFirstLast(state=2, action=1, reward=1.0, last_state=3) 
ExperienceFirstLast(state=3, action=1, reward=1.0, last_state=4) 
ExperienceFirstLast(state=4, action=1, reward=1.0, last_state=0) 
ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1) 
ExperienceFirstLast(state=1, action=1, reward=1.0, last_state=2) 
ExperienceFirstLast(state=2, action=1, reward=1.0, last_state=3) 
ExperienceFirstLast(state=3, action=1, reward=1.0, last_state=4) 
ExperienceFirstLast(state=4, action=1, reward=1.0, last_state=None) 
ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1)</code></pre>
<p>Now, instead of the tuple, it returns a single object on every iteration, which is again a dataclass with the following fields:</p>
<ul>
<li>
<p><span class="cmtt-10x-x-109">state</span>: The state we used to decide on the action to take.</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">action</span>: The action we took at this step.</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">reward</span>: The partial accumulated reward for <span class="cmtt-10x-x-109">steps</span><span class="cmtt-10x-x-109">_count </span>(in our case, <span class="cmtt-10x-x-109">steps</span><span class="cmtt-10x-x-109">_count=1</span>, so it is equal to the immediate reward).</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">last</span><span class="cmtt-10x-x-109">_state</span>: The state we got after executing the action. If our episode ends, we have <span class="cmtt-10x-x-109">None </span>here.</p>
</li>
</ul>
<p>This data is much more convenient for DQN training, as we can apply Bellman approximation directly to it.</p>
<p>Let’s check the result with a larger number of steps:</p>
<pre class="lstlisting" id="listing-182"><code>&gt;&gt;&gt; exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=1.0, steps_count=2) 
&gt;&gt;&gt; for idx, exp in zip(range(11), exp_source): 
...    print(exp) 
... 
ExperienceFirstLast(state=0, action=1, reward=2.0, last_state=2) 
ExperienceFirstLast(state=1, action=1, reward=2.0, last_state=3) 
ExperienceFirstLast(state=2, action=1, reward=2.0, last_state=4) 
ExperienceFirstLast(state=3, action=1, reward=2.0, last_state=0) 
ExperienceFirstLast(state=4, action=1, reward=2.0, last_state=1) 
ExperienceFirstLast(state=0, action=1, reward=2.0, last_state=2) 
ExperienceFirstLast(state=1, action=1, reward=2.0, last_state=3) 
ExperienceFirstLast(state=2, action=1, reward=2.0, last_state=4) 
ExperienceFirstLast(state=3, action=1, reward=2.0, last_state=None) 
ExperienceFirstLast(state=4, action=1, reward=1.0, last_state=None) 
ExperienceFirstLast(state=0, action=1, reward=2.0, last_state=2)</code></pre>
<p>So, now we are<span id="dx1-117033"/> collapsing two steps on every iteration <span id="dx1-117034"/>and calculating the immediate reward (that’s why <span class="cmtt-10x-x-109">reward=2.0 </span>for most of the samples). More interesting samples are at the end of the episode:</p>
<pre class="lstlisting" id="listing-183"><code>ExperienceFirstLast(state=3, action=1, reward=2.0, last_state=None) 
ExperienceFirstLast(state=4, action=1, reward=1.0, last_state=None)</code></pre>
<p>As the episode ends, we have <span class="cmtt-10x-x-109">last</span><span class="cmtt-10x-x-109">_state=None </span>in those samples, but additionally, we calculate the reward for the tail of the episode. Those tiny details are very easy to implement wrongly if you are doing all<span id="dx1-117037"/> the trajectory<span id="dx1-117038"/> handling yourself.</p>
</section>
</section>
<section class="level4 subsectionHead" id="experience-replay-buffers">
<h2 class="heading-2" id="sigil_toc_id_101"> <span id="x1-1180007.2.4"/>Experience replay buffers</h2>
<p>In DQN, we <span id="dx1-118001"/>rarely deal with <span id="dx1-118002"/>immediate experience samples, as they are heavily correlated, which leads to instability in the training. Normally, we have large replay buffers, which are populated with experience pieces. Then the buffer is sampled (randomly or with priority weights) to get the training batch. The replay buffer normally has a maximum capacity, so old samples are pushed out when the replay buffer reaches the limit.</p>
<p>There are several implementation tricks here, which become extremely important when you need to deal with large problems:</p>
<ul>
<li>
<p>How to efficiently sample from a large buffer</p>
</li>
<li>
<p>How to push old samples from the buffer</p>
</li>
<li>
<p>In the case of a prioritized buffer, how priorities need to be maintained and handled in the most efficient way</p>
</li>
</ul>
<p>All this becomes a quite non-trivial task if you want to deal with Atari games, keeping 10-100M samples, where every sample is an image from the game. A small mistake can lead to a 10-100x memory increase and major slowdowns in the training process.</p>
<p>PTAN provides several variants of replay buffers, which integrate simply with the <span class="cmtt-10x-x-109">ExperienceSource </span>and <span class="cmtt-10x-x-109">Agent </span>machinery. Normally, what you need to do is ask the buffer to pull a new sample from the source and sample the training batch. The provided classes are:</p>
<ul>
<li>
<p><span class="cmtt-10x-x-109">ExperienceReplayBuffer</span>: A simple replay buffer of a predefined size with uniform sampling.</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">PrioReplayBufferNaive</span>: A simple, but not very efficient, prioritized replay buffer implementation. The complexity of sampling is <span class="cmmi-10x-x-109">O</span>(<span class="cmmi-10x-x-109">n</span>), which might become an issue with large buffers. This version has the advantage over the optimized class, having much easier code. For medium-sized buffers the performance is still acceptable, so we will use it in some examples.</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">PrioritizedReplayBuffer</span>: Uses segment trees for sampling, which makes the code cryptic, but with <span class="cmmi-10x-x-109">O</span>(<span class="cmmi-10x-x-109">log</span>(<span class="cmmi-10x-x-109">n</span>)) sampling complexity.</p>
</li>
</ul>
<p>The following shows how the replay buffer could be used:</p>
<pre class="lstlisting" id="listing-184"><code>&gt;&gt;&gt; exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=1.0, steps_count=1) 
&gt;&gt;&gt; buffer = ptan.experience.ExperienceReplayBuffer(exp_source, buffer_size=100) 
&gt;&gt;&gt; len(buffer) 
0 
&gt;&gt;&gt; buffer.populate(1) 
&gt;&gt;&gt; len(buffer) 
1</code></pre>
<p>All replay buffers provide the following interface:</p>
<ul>
<li>
<p>A Python iterator interface to walk over all the samples in the buffer</p>
</li>
<li>
<p>The <span class="cmtt-10x-x-109">populate(N) </span>method to get <span class="cmmi-10x-x-109">N </span>samples from the experience source and put them into the buffer</p>
</li>
<li>
<p>The method <span class="cmtt-10x-x-109">sample(N) </span>to get the batch of <span class="cmmi-10x-x-109">N </span>experience objects</p>
</li>
</ul>
<p>So, the normal training loop for DQN looks like an infinite repetition of the following steps:</p>
<ol>
<li>
<div id="x1-118011x1">
<p>Call <span class="cmtt-10x-x-109">buffer.populate(1) </span>to get a fresh sample from the environment.</p>
</div>
</li>
<li>
<div id="x1-118013x2">
<p>Call <span class="cmtt-10x-x-109">batch = buffer.sample(BATCH</span><span class="cmtt-10x-x-109">_SIZE) </span>to get the batch from the buffer.</p>
</div>
</li>
<li>
<div id="x1-118015x3">
<p>Calculate the loss on the sampled batch.</p>
</div>
</li>
<li>
<div id="x1-118017x4">
<p>Backpropagate.</p>
</div>
</li>
<li>
<div id="x1-118019x5">
<p>Repeat until convergence (hopefully).</p>
</div>
</li>
</ol>
<p>All the rest <span id="dx1-118020"/>happens automatically-—resetting<span id="dx1-118021"/> the environment, handling subtrajectories, buffer size maintenance, and so on:</p>
<pre class="lstlisting" id="listing-185"><code>&gt;&gt;&gt; for step in range(6): 
...    buffer.populate(1) 
...    if len(buffer) &lt; 5: 
...        continue 
...    batch = buffer.sample(4) 
...    print(f"Train time, {len(batch)} batch samples") 
...    for s in batch: 
...        print(s) 
... 
Train time, 4 batch samples 
ExperienceFirstLast(state=1, action=1, reward=1.0, last_state=2) 
ExperienceFirstLast(state=2, action=1, reward=1.0, last_state=3) 
ExperienceFirstLast(state=2, action=1, reward=1.0, last_state=3) 
ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1) 
Train time, 4 batch samples 
ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1) 
ExperienceFirstLast(state=4, action=1, reward=1.0, last_state=0) 
ExperienceFirstLast(state=3, action=1, reward=1.0, last_state=4) 
ExperienceFirstLast(state=3, action=1, reward=1.0, last_state=4)</code></pre>
</section>
<section class="level4 subsectionHead" id="the-targetnet-class">
<h2 class="heading-2" id="sigil_toc_id_102"> <span id="x1-1190007.2.5"/>The TargetNet class</h2>
<p>We mentioned<span id="dx1-119001"/> the bootstrapping problem in the<span id="dx1-119002"/> previous chapter, when the network used for the next state evaluation becomes influenced by our training process. This was solved by disentangling the currently trained network from the network used for next-state Q-values prediction.</p>
<p><span class="cmtt-10x-x-109">TargetNet </span>is a small but useful class that allows us to synchronize two NNs of the same architecture. This class supports two modes of such synchronization:</p>
<ul>
<li>
<p><span class="cmtt-10x-x-109">sync()</span>: Weights from the source network are copied into the target network.</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">alpha</span><span class="cmtt-10x-x-109">_sync()</span>: The source network’s weights are blended into the target network with some alpha weight (between 0 and 1).</p>
</li>
</ul>
<p>The first mode is the standard way to perform a target network sync in discrete action space problems, like Atari and CartPole, as we did in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109"> </span><a href="#"><span class="cmti-10x-x-109">6</span></a>. The latter mode is used in continuous control problems, which will be described in <span class="cmti-10x-x-109">Part 4 </span>of the book. In such problems, the transition between two networks’ parameters should be smooth, so alpha blending is used, given by the formula <span class="cmmi-10x-x-109">w</span><sub><span class="cmmi-8">i</span></sub> = <span class="cmmi-10x-x-109">w</span><sub><span class="cmmi-8">i</span></sub><span class="cmmi-10x-x-109">α </span>+ <span class="cmmi-10x-x-109">s</span><sub><span class="cmmi-8">i</span></sub>(1 <span class="cmsy-10x-x-109">−</span><span class="cmmi-10x-x-109">α</span>), where <span class="cmmi-10x-x-109">w</span><sub><span class="cmmi-8">i</span></sub> is the target network’s <span class="cmmi-10x-x-109">i</span>-th parameter and <span class="cmmi-10x-x-109">s</span><sub><span class="cmmi-8">i</span></sub> is the source network’s weight. The following is a small example of how <span class="cmtt-10x-x-109">TargetNet </span>should be used in code. Let’s assume we have the following network:</p>
<div class="tcolorbox" id="tcolobox-138">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-186"><code>class DQNNet(nn.Module): 
    def __init__(self): 
        super(DQNNet, self).__init__() 
        self.ff = nn.Linear(5, 3) 
    def forward(self, x): 
        return self.ff(x)</code></pre>
</div>
</div>
<p>The target network could be created as follows:</p>
<pre class="lstlisting" id="listing-187"><code>&gt;&gt;&gt; net = DQNNet() 
&gt;&gt;&gt; net 
DQNNet( 
  (ff): Linear(in_features=5, out_features=3, bias=True) 
) 
&gt;&gt;&gt; tgt_net = ptan.agent.TargetNet(net)</code></pre>
<p>The target network contains two fields: <span class="cmtt-10x-x-109">model</span>, which is the reference to the original network, and <span class="cmtt-10x-x-109">target</span><span class="cmtt-10x-x-109">_model</span>, which is a deep copy of it. If we examine both networks’ weights, they will be the same:</p>
<pre class="lstlisting" id="listing-188"><code>&gt;&gt;&gt; net.ff.weight 
Parameter containing: 
tensor([[ 0.2039,  0.1487,  0.4420, -0.0210, -0.2726], 
       [-0.2020, -0.0787,  0.2852, -0.1565,  0.4012], 
       [-0.0569, -0.4184, -0.3658,  0.4212,  0.3647]], requires_grad=True) 
&gt;&gt;&gt; tgt_net.target_model.ff.weight 
Parameter containing: 
tensor([[ 0.2039,  0.1487,  0.4420, -0.0210, -0.2726], 
       [-0.2020, -0.0787,  0.2852, -0.1565,  0.4012], 
       [-0.0569, -0.4184, -0.3658,  0.4212,  0.3647]], requires_grad=True)</code></pre>
<p>They are independent of each other, however, just having the same architecture:</p>
<pre class="lstlisting" id="listing-189"><code>&gt;&gt;&gt; net.ff.weight.data += 1.0 
&gt;&gt;&gt; net.ff.weight 
Parameter containing: 
tensor([[1.2039, 1.1487, 1.4420, 0.9790, 0.7274], 
       [0.7980, 0.9213, 1.2852, 0.8435, 1.4012], 
       [0.9431, 0.5816, 0.6342, 1.4212, 1.3647]], requires_grad=True) 
&gt;&gt;&gt; tgt_net.target_model.ff.weight 
Parameter containing: 
tensor([[ 0.2039,  0.1487,  0.4420, -0.0210, -0.2726], 
       [-0.2020, -0.0787,  0.2852, -0.1565,  0.4012], 
       [-0.0569, -0.4184, -0.3658,  0.4212,  0.3647]], requires_grad=True)</code></pre>
<p>To synchronize<span id="dx1-119036"/> them <span id="dx1-119037"/>again, the <span class="cmtt-10x-x-109">sync() </span>method can be used:</p>
<pre class="lstlisting" id="listing-190"><code>&gt;&gt;&gt; tgt_net.sync() 
&gt;&gt;&gt; tgt_net.target_model.ff.weight 
Parameter containing: 
tensor([[1.2039, 1.1487, 1.4420, 0.9790, 0.7274], 
       [0.7980, 0.9213, 1.2852, 0.8435, 1.4012], 
       [0.9431, 0.5816, 0.6342, 1.4212, 1.3647]], requires_grad=True)</code></pre>
<p>For the <span id="dx1-119044"/>blended sync, you can use the <span class="cmtt-10x-x-109">alpha</span><span class="cmtt-10x-x-109">_sync() </span>method:</p>
<pre class="lstlisting" id="listing-191"><code>&gt;&gt;&gt; net.ff.weight.data += 1.0 
&gt;&gt;&gt; net.ff.weight 
Parameter containing: 
tensor([[2.2039, 2.1487, 2.4420, 1.9790, 1.7274], 
       [1.7980, 1.9213, 2.2852, 1.8435, 2.4012], 
       [1.9431, 1.5816, 1.6342, 2.4212, 2.3647]], requires_grad=True) 
&gt;&gt;&gt; tgt_net.target_model.ff.weight 
Parameter containing: 
tensor([[1.2039, 1.1487, 1.4420, 0.9790, 0.7274], 
       [0.7980, 0.9213, 1.2852, 0.8435, 1.4012], 
       [0.9431, 0.5816, 0.6342, 1.4212, 1.3647]], requires_grad=True) 
&gt;&gt;&gt; tgt_net.alpha_sync(0.1) 
&gt;&gt;&gt; tgt_net.target_model.ff.weight 
Parameter containing: 
tensor([[2.1039, 2.0487, 2.3420, 1.8790, 1.6274], 
       [1.6980, 1.8213, 2.1852, 1.7435, 2.3012], 
       [1.8431, 1.4816, 1.5342, 2.3212, 2.2647]], requires_grad=True)</code></pre>
</section>
<section class="level4 subsectionHead" id="ignite-helpers">
<h2 class="heading-2" id="sigil_toc_id_103"> <span id="x1-1200007.2.6"/>Ignite helpers</h2>
<p>PyTorch Ignite <span id="dx1-120001"/>was briefly discussed in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109"> </span><a href="ch007.xhtml#x1-530003"><span class="cmti-10x-x-109">3</span></a>, and it will be used in the<span id="dx1-120002"/> rest of the book to reduce the amount of training loop code. PTAN provides several small helpers to simplify integration with Ignite, which reside in the <span class="cmtt-10x-x-109">ptan.ignite </span>package:</p>
<ul>
<li>
<p><span class="cmtt-10x-x-109">EndOfEpisodeHandler</span>: Attached to <span class="cmtt-10x-x-109">ignite.Engine</span>, it emits an <span class="cmtt-10x-x-109">EPISODE</span><span class="cmtt-10x-x-109">_COMPLETED </span>event and tracks the reward and number of steps in the event in the engine’s metrics. It also can emit an event when the average reward for the last episodes reaches the predefined boundary, which is supposed to be used to stop the training when some goal reward has been reached.</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">EpisodeFPSHandler</span>: Tracks the number of interactions between the agent and environment that are performed and calculates performance metrics as frames per second. It also tracks the number of seconds passed since the start of the training.</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">PeriodicEvents</span>: Emits corresponding events every 10, 100, or 1,000 training iterations. It is useful for reducing the amount of data being written into TensorBoard.</p>
</li>
</ul>
<p>A detailed illustration of how these classes can be used will be given in the next chapter, when we will use them to reimplement the DQN training from <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109"> </span><a href="#"><span class="cmti-10x-x-109">6</span></a>, and then check several DQN extensions and tweaks to improve basic DQN convergence.</p>
</section>
</section>
<section class="level3 sectionHead" id="the-ptan-cartpole-solver">
<h1 class="heading-1" id="sigil_toc_id_104"> <span id="x1-1210007.3"/>The PTAN CartPole solver</h1>
<p>Let’s now take<span id="dx1-121001"/> the PTAN classes (without Ignite so far) and try to combine everything to solve our first environment: CartPole. The complete code is in <span class="cmtt-10x-x-109">Chapter07/06</span><span class="cmtt-10x-x-109">_cartpole.py</span>. I will show only the important parts of the code related to the material that we have just covered.</p>
<p>First, we create the NN (the simple two-layer feed-forward NN that we used for CartPole before) and target the NN epsilon-greedy action selector and <span class="cmtt-10x-x-109">DQNAgent</span>. Then, the experience source and replay buffer are created:</p>
<div class="tcolorbox" id="tcolobox-139">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-192"><code>    net = Net(obs_size, HIDDEN_SIZE, n_actions) 
    tgt_net = ptan.agent.TargetNet(net) 
    selector = ptan.actions.ArgmaxActionSelector() 
    selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=1, selector=selector) 
    agent = ptan.agent.DQNAgent(net, selector) 
    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=GAMMA) 
    buffer = ptan.experience.ExperienceReplayBuffer(exp_source, buffer_size=REPLAY_SIZE)</code></pre>
</div>
</div>
<p>With these few lines, we have finished with our data pipeline.</p>
<p>Now, we just need to call <span class="cmtt-10x-x-109">populate() </span>on the buffer and sample training batches from it:</p>
<div class="tcolorbox" id="tcolobox-140">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-193"><code>    while True: 
        step += 1 
        buffer.populate(1) 
 
        for reward, steps in exp_source.pop_rewards_steps(): 
            episode += 1 
            print(f"{step}: episode {episode} done, reward={reward:.2f}, " 
                  f"epsilon={selector.epsilon:.2f}") 
            solved = reward &gt; 150 
        if solved: 
            print("Whee!") 
            break 
        if len(buffer) &lt; 2*BATCH_SIZE: 
            continue 
        batch = buffer.sample(BATCH_SIZE)</code></pre>
</div>
</div>
<p>At the beginning <span id="dx1-121024"/>of every training loop iteration, we ask the buffer to fetch one sample from the experience source and then check for the finished episode. The <span class="cmtt-10x-x-109">pop</span><span class="cmtt-10x-x-109">_rewards</span><span class="cmtt-10x-x-109">_steps() </span>method in the <span class="cmtt-10x-x-109">ExperienceSource </span>class returns the list of tuples with information about episodes completed since the last call to the method.</p>
<p>Later in the training loop, we convert a batch of <span class="cmtt-10x-x-109">ExperienceFirstLast </span>objects into tensors suitable for DQN training:</p>
<div class="tcolorbox" id="tcolobox-141">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-194"><code>        batch = buffer.sample(BATCH_SIZE) 
        states_v, actions_v, tgt_q_v = unpack_batch(batch, tgt_net.target_model, GAMMA) 
        optimizer.zero_grad() 
        q_v = net(states_v) 
        q_v = q_v.gather(1, actions_v.unsqueeze(-1)).squeeze(-1) 
        loss_v = F.mse_loss(q_v, tgt_q_v) 
        loss_v.backward() 
        optimizer.step() 
        selector.epsilon *= EPS_DECAY 
 
        if step % TGT_NET_SYNC == 0: 
            tgt_net.sync()</code></pre>
</div>
</div>
<p>We calculate the loss and do a backpropagation step. Finally, we decay epsilon in our action selector (with the hyperparameters used, epsilon <span id="dx1-121037"/>decays to zero at training step 500) and ask the target network to sync every 10 training iterations.</p>
<p>The <span class="cmtt-10x-x-109">unpack</span><span class="cmtt-10x-x-109">_batch </span>method is the last piece of our implementation:</p>
<div class="tcolorbox" id="tcolobox-142">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-195"><code>@torch.no_grad() 
def unpack_batch(batch: tt.List[ExperienceFirstLast], net: Net, gamma: float): 
    states = [] 
    actions = [] 
    rewards = [] 
    done_masks = [] 
    last_states = [] 
    for exp in batch: 
        states.append(exp.state) 
        actions.append(exp.action) 
        rewards.append(exp.reward) 
        done_masks.append(exp.last_state is None) 
        if exp.last_state is None: 
            last_states.append(exp.state) 
        else: 
            last_states.append(exp.last_state) 
 
    states_v = torch.as_tensor(np.stack(states)) 
    actions_v = torch.tensor(actions) 
    rewards_v = torch.tensor(rewards) 
    last_states_v = torch.as_tensor(np.stack(last_states)) 
    last_state_q_v = net(last_states_v) 
    best_last_q_v = torch.max(last_state_q_v, dim=1)[0] 
    best_last_q_v[done_masks] = 0.0 
    return states_v, actions_v, best_last_q_v * gamma + rewards_v</code></pre>
</div>
</div>
<p>It takes a sampled batch of <span class="cmtt-10x-x-109">ExperienceFirstLast </span>objects and converts them into three tensors: states, actions, and target Q-values. The code <span id="dx1-121063"/>should converge in 2,000-3,000 training iterations:</p>
<pre class="lstlisting" id="listing-196"><code>Chapter07$ python 06_cartpole.py 
26: episode 1 done, reward=25.00, epsilon=1.00 
52: episode 2 done, reward=26.00, epsilon=0.82 
67: episode 3 done, reward=15.00, epsilon=0.70 
80: episode 4 done, reward=13.00, epsilon=0.62 
112: episode 5 done, reward=32.00, epsilon=0.45 
123: episode 6 done, reward=11.00, epsilon=0.40 
139: episode 7 done, reward=16.00, epsilon=0.34 
148: episode 8 done, reward=9.00, epsilon=0.31 
156: episode 9 done, reward=8.00, epsilon=0.29 
... 
2481: episode 113 done, reward=58.00, epsilon=0.00 
2544: episode 114 done, reward=63.00, epsilon=0.00 
2594: episode 115 done, reward=50.00, epsilon=0.00 
2786: episode 116 done, reward=192.00, epsilon=0.00 
Whee!</code></pre>
</section>
<section class="level3 sectionHead" id="other-rl-libraries">
<h1 class="heading-1" id="sigil_toc_id_105"> <span id="x1-1220007.4"/>Other RL libraries</h1>
<p>As we discussed earlier, there are several RL-specific libraries available. A <span id="dx1-122001"/>few years ago, TensorFlow was more popular than PyTorch, but nowadays, PyTorch is dominating the field, and there is a recent trend of JAX being used as it provides better performance. The following is my recommended list of libraries you might want to take into consideration for your projects:</p>
<ul>
<li>
<p><span class="cmbx-10x-x-109">stable-baselines3</span>: We mentioned this <span id="dx1-122002"/>library when we discussed Atari wrappers. This is a fork of the OpenAI Stable Baselines repository, and the main idea is to have an optimized and reproducible set of RL algorithms that you can use to check your methods ( <a class="url" href="https://github.com/DLR-RM/stable-baselines3"><span class="cmtt-10x-x-109">https://github.com/DLR-RM/stable-baselines3</span></a>).</p>
</li>
<li>
<p><span class="cmbx-10x-x-109">TorchRL</span>: RL extensions<span id="dx1-122003"/> for PyTorch. This library is relatively young-—the first release was at the end of 2022—but provides rich set of helper classes for RL. Its design philosophy is very close to PTAN—a Python-first set of flexible classes that you can combine and extend to build your system—so I highly recommend that you learn this library. In the rest of the book, we’ll use this library’s classes. Most likely, examples in the next edition of this book (unless we reach ”AI Singularity” and books become obsolete, like clay tablets) will not be based on PTAN but on TorchRL, which is better maintained. Documentation: <a class="url" href="https://pytorch.org/rl/"><span class="cmtt-10x-x-109">https://pytorch.org/rl/</span></a>, source code: <a class="url" href="https://github.com/pytorch/rl"><span class="cmtt-10x-x-109">https://github.com/pytorch/rl</span></a>.</p>
</li>
<li>
<p><span class="cmbx-10x-x-109">Spinning Up</span>: Another repo from OpenAI, but <span id="dx1-122004"/>with a different goal in mind: providing valuable and clean education materials about state-of-the-art methods. This repo hasn’t been updated for several years (the last commit was in 2020), but still provides very valuable materials about the methods. Documentation: <a class="url" href="https://spinningup.openai.com/"><span class="cmtt-10x-x-109">https://spinningup.openai.com/</span></a>. Code: <a class="url" href="https://github.com/openai/spinningup"><span class="cmtt-10x-x-109">https://github.com/openai/spinningup</span></a>.</p>
</li>
<li>
<p><span class="cmbx-10x-x-109">Keras-RL</span>: Started<span id="dx1-122005"/> by Matthias Plappert in 2016, this includes basic deep RL methods. As suggested by the name, this library was implemented using Keras, which is a high-level wrapper around TensorFlow (<a class="url" href="https://github.com/keras-rl/keras-rl"><span class="cmtt-10x-x-109">https://github.com/keras-rl/keras-rl</span></a>). Unfortunately, the last commit was in 2019, so the project has been abandoned.</p>
</li>
<li>
<p><span class="cmbx-10x-x-109">Dopamine</span>: A library<span id="dx1-122006"/> from Google published in 2018. It is TensorFlow-specific, which is not surprising for a library from Google (<a class="url" href="https://github.com/google/dopamine"><span class="cmtt-10x-x-109">https://github.com/google/dopamine</span></a>).</p>
</li>
<li>
<p><span class="cmbx-10x-x-109">Ray</span>: A library <span id="dx1-122007"/>for distributed execution of machine learning code. It includes RL utilities as part of the library ( <a class="url" href="https://github.com/ray-project/ray"><span class="cmtt-10x-x-109">https://github.com/ray-project/ray</span></a>).</p>
</li>
<li>
<p><span class="cmbx-10x-x-109">TF-Agents</span>: Another <span id="dx1-122008"/>library from Google published in 2018 ( <a class="url" href="https://github.com/tensorflow/agents"><span class="cmtt-10x-x-109">https://github.com/tensorflow/agents</span></a>).</p>
</li>
<li>
<p><span class="cmbx-10x-x-109">ReAgent</span>: A library from Facebook Research. It uses <span id="dx1-122009"/>PyTorch internally and uses a declarative style of configuration (when you are creating a JSON file to describe your problem), which limits extensibility. But, of course, as it is open source, you can always extend the functionality (<a class="url" href="https://github.com/facebookresearch/ReAgent"><span class="cmtt-10x-x-109">https://github.com/facebookresearch/ReAgent</span></a>). Recently, ReAgent was<span id="dx1-122010"/> archived and replaced by the <span class="cmbx-10x-x-109">Pearl </span>library from the same team: <a class="url" href="https://github.com/facebookresearch/Pearl/"><span class="cmtt-10x-x-109">https://github.com/facebookresearch/Pearl/</span></a>.</p>
</li>
</ul>
</section>
<section class="level3 sectionHead" id="summary-6">
<h1 class="heading-1" id="sigil_toc_id_106"> <span id="x1-1230007.5"/>Summary</h1>
<p>In this chapter, we talked about higher-level RL libraries, their motivation, and their requirements. Then, we took a deep look at the PTAN library, which will be used in the rest of the book to simplify example code. This focus on the details of the methods rather than implementation will be extremely useful for you in later chapters of this book, as you progress further with RL.</p>
<p>In the next chapter, we will return to DQN methods by exploring extensions that researchers and practitioners have discovered since the classic DQN introduction to improve the stability and performance of the method.</p>
</section>
</section>
</div></body></html>