- en: Deep Learning with R
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter will build a foundation for neural networks followed by deep learning
    foundation and trends. We will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Starting with logistic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing logistic regression using H2O
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing logistic regression using TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing TensorFlow graphs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Starting with multilayer perceptrons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up a neural network using H2O
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tuning hyper-parameters using grid searches in H2O
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up a neural network using MXNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up a neural network using TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Starting with logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we delve into neural networks and deep learning models, let's take a
    look at logistic regression, which can be viewed as a single layer neural network.
    Even the **sigmoid** function commonly used in logistic regression is used as
    an activation function in neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Logistic regression is a supervised machine learning approach for the classification
    of dichotomous/ordinal (order discrete) categories.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Logistic regression serves as a building block for complex neural network models
    using sigmoid as an activation function. The logistic function (or sigmoid) can
    be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00003.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding sigmoid function forms a continuous curve with a value bound
    between [0, 1], as illustrated in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00005.gif)'
  prefs: []
  type: TYPE_IMG
- en: Sigmoid functional form
  prefs: []
  type: TYPE_NORMAL
- en: 'The formulation of a logistic regression model can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00008.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *W* is the weight associated with features *X***=** [*x[1], x[2], ...,
    x[m]*] and *b* is the model intercept, also known as the model bias. The whole
    objective is to optimize *W* for a given loss function such as cross entropy.
    Another view of the logistic regression model to attain *Pr*(*y=1|***X)** is shown
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00009.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Logistic regression architecture with the sigmoid activation function
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe shows how to prepare a dataset to be used to demonstrate different
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As logistic regression is a linear classifier, it assumes linearity in independent
    variables and log odds. Thus, in scenarios where independent features are linear-dependent
    on log odds, the model performs very well. Higher-order features can be included
    in the model to capture nonlinear behavior. Let's see how to build logistic regression
    models using major deep learning packages as discussed in the previous chapter.
    Internet connectivity will be required to download the dataset from the UCI repository.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, the Occupancy Detection dataset from the **UC Irivine ML repository**
    is used to build models on logistic regression and neural networks. It is an experimental
    dataset primarily used for binary classification to determine whether a room is
    occupied (1) or not occupied (0) based on multivariate predictors as described
    in the following table. The contributor of the dataset is *Luis Candanedo* from
    UMONS.
  prefs: []
  type: TYPE_NORMAL
- en: Download the dataset at [https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+](https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+).
  prefs: []
  type: TYPE_NORMAL
- en: There are three datasets tobe downloaded; however, we will use `datatraining.txt`
    for training/cross validation purposes and `datatest.txt` for testing purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset has seven attributes (including response occupancy) with 20,560
    instances. The following table summarizes the attribute information:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Attribute** | **Description** | **Characteristic** |'
  prefs: []
  type: TYPE_TB
- en: '| Date time | Year-month-day hour:minute:second format | Date |'
  prefs: []
  type: TYPE_TB
- en: '| Temperature | In Celsius | Real |'
  prefs: []
  type: TYPE_TB
- en: '| Relative Humidity | In % | Real |'
  prefs: []
  type: TYPE_TB
- en: '| Light | In Lux | Real |'
  prefs: []
  type: TYPE_TB
- en: '| CO2 | In ppm | Real |'
  prefs: []
  type: TYPE_TB
- en: '| Humidity ratio | Derived quantity from temperature and relative humidity,
    in kg water-vapor/kg-air | Real |'
  prefs: []
  type: TYPE_TB
- en: '| Occupancy | 0 for not occupied;1 for occupied | Binary class |'
  prefs: []
  type: TYPE_TB
- en: Performing logistic regression using H2O
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Generalized linear models** (**GLM**) are widely used in both regression-
    and classification-based predictive analysis. These models optimize using maximum
    likelihood and scale well with larger datasets. In H2O, GLM has the flexibility
    to handle both L1 and L2 penalties (including elastic net). It supports Gaussian,
    Binomial, Poisson, and Gamma distributions of dependent variables. It is efficient
    in handling categorical variables, computing full regularizations, and performing
    distributed *n-fold* cross validations to control for model overfitting. It has
    a feature to optimize hyperparameters such as elastic net (α) using distributed
    grid searches along with handling upper and lower bounds for predictor attribute
    coefficients. It can also handle automatic missing value imputation. It uses the
    Hogwild method for optimization, a parallel version of stochastic gradient descent.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The previous chapter provided the details for the installation of H2O in R
    along with a working example using its web interface. To start modeling, load
    the `h20` package in the R environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, initialize a single-node H2O instance using the `h2o.init()` function
    on eight cores and instantiate the corresponding client module on the IP address
    `localhost` and port number `54321`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The H2O package has dependency on the Java JRE. Thus, it should be pre-installed
    before executing the initialization command.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The section will demonstrate steps to build the GLM model using H2O.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, load the occupancy train and test datasets in R:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The following independent (`x`) and dependent (`y`) variables will be used
    to model GLM:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the requirement for H2O, convert the dependent variables into factors
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, convert the datasets to H2OParsedData objects:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the data is loaded and converted to H2OParsedData objects, run a GLM model
    using the `h2o.glm` function. In the current setup, we intend to train for parameters
    such as five-fold cross validation, elastic net regularization (*α = 5*), and
    optimal regularization strength (with `lamda_search = TRUE`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In addition to the preceding command, you can also define other parameters to
    fine-tune the model performance. The following list does not cover all the functional
    parameters, but covers some based on importance. The complete list of parameters
    can be found in the documentation of the `h2o` package.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Specify the strategy of generating cross-validation samples such as random sampling,
    stratified sampling, modulo sampling, and auto (select) using fold_assignment.
    The sampling can also be performed on a particular attribute by specifying the
    column name (fold_column).
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: Option to handle skewed outcomes (imbalanced data) by specifying weights to
    each observation using weights_column or performing over/under sampling using
    balance_classes.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Option to handle missing values by mean imputation or observation skip using
    missing_values_handling.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Option to restrict the coefficients to be non-negative using non_negative and
    constrain their values using beta_constraints.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Option to provide prior probability for y==1(logistic regression) in the case
    of sampled data if its mean of response does not reflect the reality (prior).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Specify the variables to be considered for interactions (interactions).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The performance of the model can be assessed using many metrics such as accuracy,
    **Area under curve** (**AUC**), misclassification error (%), misclassification
    error count, F1-score, precision, recall, specificity, and so on. However, in
    this chapter, the assessment of model performance is based on AUC.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the training and cross validation accuracy of the trained
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s assess the performance of the model on test data. The following
    code helps in predicting the outcome of the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, evaluate the `AUC` value based on the actual test outcome as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In H2O, one can also compute variable importance from the GLM model, as shown
    in the figure following this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00130.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Variable importance using H2O
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: More functional parameters for `h2o.glm` can be found at [https://www.rdocumentation.org/packages/h2o/versions/3.10.3.6/topics/h2o.gbm](https://www.rdocumentation.org/packages/h2o/versions/3.10.3.6/topics/h2o.gbm).
  prefs: []
  type: TYPE_NORMAL
- en: Performing logistic regression using TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will cover the application of TensorFlow in setting up a
    logistic regression model. The example will use a similar dataset to that used
    in the H2O model setup.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The previous chapter provided details for the installation of TensorFlow. The
    code for this section is created on Linux but can be run on any operating system.
    To start modeling, load the `tensorflow` package in the environment. R loads the
    default TensorFlow environment variable and also the NumPy library from Python
    in the `np` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The data is imported using a standard function from R, as shown in the following
    code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data is imported using the `read.csv` file and transformed into the matrix
    format followed by selecting the features used to model as defined in `xFeatures`
    and `yFeatures`*.* The next step in TensorFlow is to set up a graph to run optimization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Before setting up the graph, let''s reset the graph using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Additionally, let''s start an interactive session as it will allow us to execute
    variables without referring to the session-to-session object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the logistic regression model in TensorFlow:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The input feature `x` is defined as a constant as it will be an input to the
    system. The weight `W` and bias `b` are defined as variables that will be optimized
    during the optimization process. The y is set up as a symbolic representation
    between `x`, `W`, and `b`. The weight `W` is set up to initialize random uniform
    distribution and `b` is assigned the value zero.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The next step is to set up the cost function for logistic regression:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The variable `y_` is the response variable. Logistic regression is set up using
    cross entropy as the loss function. The loss function is passed to the gradient
    descent optimizer with a learning rate of 0.15\. Before running the optimization,
    initialize the global variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the gradient descent algorithm for the optimization of weights using
    cross entropy as the loss function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The performance of the model can be evaluated using AUC:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: AUC can be visualized using the `plot.auc` function from the `pROC` package,
    as shown in the screenshot following this command. The performance for training
    and testing (hold-out) is very similar.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00010.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Performance of logistic regression using TensorFlow
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing TensorFlow graphs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow graphs can be visualized using TensorBoard. It is a service that
    utilizes TensorFlow event files to visualize TensorFlow models as graphs. Graph
    model visualization in TensorBoard is also used to debug TensorFlow models.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'TensorBoard can be started using the following command in the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The following are the major parameters for TensorBoard:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--logdir` : To map to the directory to load TensorFlow events'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--debug`: To increase log verbosity'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--host`: To define the host to listen to its localhost (`127.0.0.1`) by default'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--port`: To define the port to which TensorBoard will serve'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The preceding command will launch the TensorFlow service on localhost at port
    `6006`, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00132.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: TensorBoard
  prefs: []
  type: TYPE_NORMAL
- en: The tabs on the TensorBoard capture relevant data generated during graph execution.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The section covers how to visualize TensorFlow models and output in TernsorBoard.
  prefs: []
  type: TYPE_NORMAL
- en: 'To visualize summaries and graphs, data from TensorFlow can be exported using
    the `FileWriter` command from the summary module. A default session graph can
    be added using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The graph for logistic regression developed using the preceding code is shown
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00067.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Visualization of the logistic regression graph in TensorBoard
  prefs: []
  type: TYPE_NORMAL
- en: Details about symbol descriptions on TensorBoard can be found at [https://www.tensorflow.org/get_started/graph_viz](https://www.tensorflow.org/get_started/graph_viz).
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, other variable summaries can be added to the TensorBoard using correct
    summaries, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The summaries can be a very useful way to determine how the model is performing.
    For example, for the preceding case, the cost function for test and train can
    be studied to understand optimization performance and convergence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a cross entropy evaluation for test. An example script to generate the
    cross entropy cost function for test and train is shown in the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code is similar to training cross entropy calculations with a
    different dataset. The effort can be minimized by setting up a function to return
    tensor objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add summary variables to be collected:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The script defines the summary events to be logged in the file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the writing object, `log_writer`. It writes the default graph to the location,
    `c:/log`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the optimization and collect the summaries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Collect all the summaries to a single tensor using the`merge_all` command from
    the summary module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Write the summaries to the log file using the `log_writer` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The section covers model performance visualization using TensorBoard. The cross
    entropy for train and test are recorded in the SCALARS tab, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00011.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Cross entropy for train and test data
  prefs: []
  type: TYPE_NORMAL
- en: The objective function shows similar behaviors for train and test cost function;
    thus, the model seems to be stable for the given case with convergence attaining
    around 1,600 iterations.
  prefs: []
  type: TYPE_NORMAL
- en: Starting with multilayer perceptrons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section will focus on extending the logistic regression concept to neural
    networks.
  prefs: []
  type: TYPE_NORMAL
- en: The neural network, also known as **Artificial neural network** (**ANN**), is
    a computational paradigm that is inspired by the neuronal structure of the biological
    brain.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The ANN is a collection of artificial neurons that perform simple operations
    on the data; the output from this is passed to another neuron. The output generated
    at each neuron is called its **activation function**. An example of a multilayer
    perceptron model can be seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00012.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: An example of a multilayer neural network
  prefs: []
  type: TYPE_NORMAL
- en: 'Each link in the preceding figure is associated to weights processed by a neuron.
    Each neuron can be looked at as a processing unit that takes input processing
    and the output is passed to the next layer, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00101.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: An example of a neuron getting three inputs and one output
  prefs: []
  type: TYPE_NORMAL
- en: The preceding figure demonstrates three inputs combined at neuron to give an
    output that may be further passed to another neuron. The processing conducted
    at the neuron could be a very simple operation such as the input multiplied by
    weights followed by summation or a transformation operation such as the sigmoid
    activation function.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section covers type activation functions in multilayer perceptrons. Activation
    is one of the critical component of ANN as it defines the output of that node
    based on the given input. There are many different activation functions used while
    building a neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sigmoid**: The sigmoid activation function is a continuous function also
    known as a logistic function and has the form, *1/(1+exp(-x))*. The sigmoid function
    has a tendency to zero out the backpropagation terms during training leading to
    saturation in response. In TensorFlow, the sigmoid activation function is defined
    using the `tf.nn.sigmoid` function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ReLU**: Rectified linear unit (ReLU) is one of the most famous continuous,
    but not smooth, activation functions used in neural networks to capture non-linearity.
    The ReLU function is defined as *max(0,x)*. In TensorFlow, the ReLU activation
    function is defined as `tf.nn.relu`*.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ReLU6**: It caps the ReLU function at 6 and is defined as *min(max(0,x),
    6)*, thus the value does not become very small or large. The function is defined
    in TensorFlow as `tf.nn.relu6`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**tanh**: Hypertangent is another smooth function used as an activation function
    in neural networks and is bound [ -1 to 1] and implemented as `tf.nn.tanh`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**softplus**: It is a continuous version of ReLU, so the differential exists
    and is defined as *log(exp(x)+1)*. In TensorFlow the softplus is defined as `tf.nn.softplus`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are three main neural network architectures in neural networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feedforward ANN**: This is a class of neural network models where the flow
    of information is unidirectional from input to output; thus, the architecture
    does not form any cycle. An example of a Feedforward network is shown in the following
    image:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00013.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Feedforward architecture of neural networks
  prefs: []
  type: TYPE_NORMAL
- en: '**Feedback ANN**: This is also known as the Elman recurrent network and is
    a class of neural networks where the error at the output node used as feedback
    to update iteratively to minimize errors. An example of a one layer Feedback neural
    network architecture is shown in the following image:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00014.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: xFeedback architecture of neural networks
  prefs: []
  type: TYPE_NORMAL
- en: '**Lateral ANN**: This is a class of neural networks between Feedback and Feedforward
    neural networks with neurons interacting within layers. An example lateral neural
    network architecture is shown in the following image:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00144.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Lateral neural network architecture
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: More activation functions supported in TensorFlow can be found at [https://www.tensorflow.org/versions/r0.10/api_docs/python/nn/activation_functions_.](https://www.tensorflow.org/versions/r0.10/api_docs/python/nn/activation_functions_)
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a neural network using H2O
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will cover the application of H2O in setting up a neural
    network. The example will use a similar dataset as used in logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We first load all the required packages with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, initialize a single-node H2O instance using the `h2o.init()` function
    on eight cores and instantiate the corresponding client module on the IP address
    `localhost` and port number `54321`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The section shows how to build neural network using H20.
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the occupancy train and test datasets in R:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The following independent (`x`) and dependent (`y`) variables will be used
    to model GLM:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the requirement by H2O, convert dependent variables to factors as
    follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Then convert the datasets to H2OParsedData objects:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the data is loaded and converted to H2OParsedData objects, build a multilayer
    Feedforward neural network using the `h2o.deeplearning` function. In the current
    setup, the following parameters are used to build the NN model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Single hidden layer with five neurons using `hidden`
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: 50 iterations using `epochs`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Adaptive learning rate (`adaptive_rate`) instead of a fixed learning rate (rate)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Rectifier` activation function based on ReLU'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Five-fold cross validation using `nfold`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: In addition to the command described in the recipe *Performing logistic regression
    using H2O*, you can also define other parameters to fine-tune the model performance.
    The following list does not cover all the functional parameters, but covers some
    based on importance. The complete list of parameters is available in the documentation
    of the `h2o` package.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Option to initialize a model using a pretrained autoencoder model.
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: Provision to fine-tune the adaptive learning rate via an option to modify the
    time decay factor (*rho*) and smoothing factor (*epsilon*). In the case of a fixed
    learning rate (*rate*), an option to modify the annealing rate (*rate_annealing*)
    and decay factor between layers (*rate_decay*).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Option to initialize weights and biases along with weight distribution and scaling.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Stopping criteria based on the error fraction in the case of classification
    and mean squared errors with regard to regression (*classification_stop* and *regression_stop*).
    An option to also perform early stopping.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Option to improve distributed model convergence using the elastic averaging
    method with parameters such as moving rate and regularization strength.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The performance of the model can be assessed using many metrics such as accuracy,
    AUC, misclassification error (%), misclassification error count, F1-score, precision,
    recall, specificity, and so on. However, in this chapter, the assessment of the
    model performance is based on AUC.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the training and cross validation accuracy for the trained
    model. The training and cross validation AUC is `0.984` and `0.982` respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: As we have already provided test data in the model (as a validation dataset),
    the following is its performance. The AUC on the test data is `0.991`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Tuning hyper-parameters using grid searches in H2O
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: H2O packages also allow you to perform hyper-parameter tuning using grid search
    (`h2o.grid`).
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We first load and initialize the H2O package with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The occupancy dataset is loaded, converted to hex format, and named *occupancy_train.hex*.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The section will focus on optimizing hyper parameters in H2O using grid searches.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, we will optimize for the activation function, the number of hidden
    layers (along with the number of neurons in each layer), `epochs`, and regularization
    lambda (`l1` and `l2`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The following search criteria have been set to perform a grid search. Adding
    to the following list, one can also specify the type of stopping metric, the minimum
    tolerance for stopping, and the maximum number of rounds for stopping:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s perform a grid search on the training data as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the grid search is complete (here, there are 216 different models), the
    best model can be selected based on multiple metrics such as logloss, residual
    deviance, mean squared error, AUC, accuracy, precision, recall, f1, and so on.
    In our scenario, let''s select the best model with the highest AUC:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following is the performance of the grid-searched model on both the training
    and cross-validation datasets. We can observe that the AUC has increased by one
    unit in both training and cross-validation scenarios, after performing a grid
    search. The training and cross validation AUC after the grid search is `0.996`
    and `0.997` respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's assess the performance of the best grid-searched model on the test
    dataset. We can observe that the AUC has increased by 0.25 units after performing
    the grid search. The AUC on the test data is `0.993`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Setting up a neural network using MXNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous chapter provided the details for the installation of MXNet in R
    along with a working example using its web interface. To start modeling, load
    the MXNet package in the R environment.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Load the required packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Load the occupancy train and test datasets in R:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The following independent (`x`) and dependent (`y`) variables will be used
    to model GLM:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the requirement by MXNet, convert the train and test datasets to a
    matrix and ensure that the class of the outcome variable is numeric (instead of
    factor as in the case of H2O):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's configure a neural network manually. First, configure a symbolic
    variable with a specific name. Then configure a symbolic fully connected network
    with five neurons in a single hidden layer followed with the softmax activation
    function with logit loss (or cross entropy loss). One can also create additional
    (fully connected) hidden layers with different activation functions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the neural network is configured, let''s create (or train) the (Feedforward)
    neural network model using the `mx.model.FeedForward.create` function. The model
    is fine-tuned for parameters such as the number of iterations or epochs (*100*),
    the metric for evaluation (classification accuracy), the size of each iteration
    or epoch (100 observations), and the learning rate (*0.01*):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s assess the performance of the model on train and test datasets.
    The AUC on the train data is `0.978` and on the test data is `0.982`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Setting up a neural network using TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will cover an application of TensorFlow in setting up a
    two-layer neural network model.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To start modeling, load the `tensorflow` package in the environment. R loads
    the default tf environment variable and also the NumPy library from Python in
    the `np` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The data is imported using the standard function from R, as shown in the following
    code. The data is imported using the `read.csv` file and transformed into the
    matrix format followed by selecting the features used for the modeling as defined
    in `xFeatures` and `yFeatures`*:*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Now load both the network and model parameters. The network parameters define
    the structure of the neural network and the model parameters define its tuning
    criteria. As stated earlier, the neural network is built using two hidden layers,
    each with five neurons. The `n_input` parameter defines the number of independent
    variables and `n_classes` defines one fewer than the number of output classes.
    In cases where the output variable is one-hot encoded (one attribute with yes
    occupancy and a second attribute with no occupancy), then `n_classes` will be
    2L (equal to the number of one-hot encoded attributes). Among model parameters,
    the learning rate is `0.001` and the number of epochs (or iterations) for model
    building is `10000`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step in TensorFlow is to set up a graph to run the optimization. Before
    setting up the graph, let''s reset the graph using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Additionally, let''s start an interactive session as it will allow us to execute
    variables without referring to the session-to-session object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'The following script defines the graph input (x for independent variables and
    y for dependent variable). The input feature `x` is defined as a constant as it
    will be input to the system. Similarly, the output feature `y` is also defined
    as a constant with the `float32` type:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s create a multilayer perceptron with two hidden layers. Both the
    hidden layers are built using the ReLU activation function and the output layer
    is built using the linear activation function. The weights and biases are defined
    as variables that will be optimized during the optimization process. The initial
    values are randomly selected from a normal distribution. The following script
    is used to initialize and store a hidden layer''s weights and biases along with
    a mulitilayer perceptron model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, construct the model using the initialized `weights` and `biases`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to define the `cost` and `optimizer` functions of the neural
    network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'The neural network is set up using sigmoid cross entropy as the cost function.
    The cost function is then passed to a gradient descent optimizer (Adam) with a
    learning rate of 0.001\. Before running the optimization, initialize the global
    variables as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the global variables are initialized along with the cost and optimizer
    functions, let''s begin training on the train dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The performance of the model can be evaluated using AUC:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: AUC can be visualized using the `plot.auc` function from the `pROC` package,
    as shown in the image following the next command. The performance of train and
    test (hold out) is very similar.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00026.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Performance of multilayer perceptron using TensorFlow
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Neural networks are based on the philosophy from the brain; however, the brain
    consists of around 100 billion neurons with each neuron connected to 10,000 other
    neurons. Neural networks developed in the early 90s faced a lot of challenges
    in building deeper neural networks due to computation and algorithmic limitations.
  prefs: []
  type: TYPE_NORMAL
- en: With advances in big data, computational resources (such as GPUs), and better
    algorithms, the concept of deep learning has emerged and allows us to capture
    a deeper representation from all kinds of data such as text, image, audio, and
    so on.
  prefs: []
  type: TYPE_NORMAL
- en: '**Trends in Deep Learning**: Deep learning is an advance on neural networks,
    which are very much driven by technology enhancement. The main factors that have
    impacted the development of deep learning as a dominant area in artificial intelligence
    are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Computational power**: The consistency of Moore''s law, which states that
    the acceleration power of hardware will double every two years, helped in training
    more layers and bigger data within time limitations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Storage and better compression algorithms**: The ability to store big models
    due to cheaper storage and better compression algorithms have pushed this area
    with practitioners focusing on capturing real-time data feeds in the form of image,
    text, audio, and video formats'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: The ability to scale from a simple computer to a farm or using
    GPU devices has given a great boost to training deep learning models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deep learning architectures**: With new architectures such as Convolution
    Neural network, re-enforcement learning has provided a boost to what we can learn
    and also helped expedite learning rates'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cross-platform programming:** The ability to program and build models in
    a cross-platform architecture significantly helped increase the user base and
    in drastic development in the domain'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transfer learning:** This allows reusing pretrained models and further helps
    in significantly reducing training times'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
