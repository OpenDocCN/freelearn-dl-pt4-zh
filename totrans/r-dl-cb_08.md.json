["```py\nload_packages=c(\"janeaustenr\",\"tidytext\",\"dplyr\",\"stringr\",\"ggplot2\",\"wordcloud\",\"reshape2\",\"igraph\",\"ggraph\",\"widyr\",\"tidyr\") \nlapply(load_packages, require, character.only = TRUE) \n\n```", "```py\nPride_Prejudice <- data.frame(\"text\" = prideprejudice, \n                              \"book\" = \"Pride and Prejudice\", \n                              \"line_num\" = 1:length(prideprejudice), \n                              stringsAsFactors=F) \n\n```", "```py\nPride_Prejudice <- Pride_Prejudice %>% unnest_tokens(word,text) \n\n```", "```py\ndata(stop_words) \nPride_Prejudice <- Pride_Prejudice %>% anti_join(stop_words,\nby=\"word\") \n\n```", "```py\nmost.common <- Pride_Prejudice %>% dplyr::count(word, sort = TRUE)\n\n```", "```py\nmost.common$word  <- factor(most.common$word , levels = most.common$word) \nggplot(data=most.common[1:10,], aes(x=word, y=n, fill=word)) + \n  geom_bar(colour=\"black\", stat=\"identity\")+ \n  xlab(\"Common Words\") + ylab(\"N Count\")+ \n  ggtitle(\"Top 10 common words\")+ \n  guides(fill=FALSE)+ \n  theme(plot.title = element_text(hjust = 0.5))+ \n  theme(text = element_text(size = 10))+ \n  theme(panel.background = element_blank(), panel.grid.major = element_blank(),panel.grid.minor = element_blank()) \n\n```", "```py\nPride_Prejudice_POS_NEG_sentiment <- Pride_Prejudice %>% inner_join(get_sentiments(\"bing\"), by=\"word\") %>% dplyr::count(book, index = line_num %/% 150, sentiment) %>% spread(sentiment, n, fill = 0) %>% mutate(net_sentiment = positive - negative)\n\n```", "```py\nggplot(Pride_Prejudice_POS_NEG_sentiment, aes(index, net_sentiment))+ \n  geom_col(show.legend = FALSE) + \n  geom_line(aes(y=mean(net_sentiment)),color=\"blue\")+ \n  xlab(\"Section (150 words each)\") + ylab(\"Values\")+ \n  ggtitle(\"Net Sentiment (POS - NEG) of Pride and Prejudice\")+ \n  theme(plot.title = element_text(hjust = 0.5))+ \n  theme(text = element_text(size = 10))+ \n  theme(panel.background = element_blank(), panel.grid.major = element_blank(),panel.grid.minor = element_blank())\n\n```", "```py\nPride_Prejudice_GRAN_sentiment <- Pride_Prejudice %>% inner_join(get_sentiments(\"nrc\"), by=\"word\") %>% dplyr::count(book, index = line_num %/% 150, sentiment) %>% spread(sentiment, n, fill = 0)\n\n```", "```py\nggplot(stack(Pride_Prejudice_GRAN_sentiment[,3:12]), aes(x = ind, y = values)) + \n  geom_boxplot()+ \n  xlab(\"Sentiment types\") + ylab(\"Sections (150 words) of text\")+ \n  ggtitle(\"Variation across different sentiments\")+ \n  theme(plot.title = element_text(hjust = 0.5))+ \n  theme(text = element_text(size = 15))+ \n  theme(panel.background = element_blank(), panel.grid.major = element_blank(),panel.grid.minor = element_blank())\n\n```", "```py\nPOS_NEG_word_counts <- Pride_Prejudice %>% inner_join(get_sentiments(\"bing\"), by=\"word\") %>% dplyr::count(word, sentiment, sort = TRUE) %>% ungroup() POS_NEG_word_counts %>% group_by(sentiment) %>% top_n(10) %>% ungroup() %>% mutate(word = reorder(word, n)) %>% ggplot(aes(word, n, fill = sentiment)) + geom_col(show.legend = FALSE) + facet_wrap(~sentiment, scales = \"free_y\") + ggtitle(\"Top 10 positive and negative words\")+ coord_flip() + theme(plot.title = element_text(hjust = 0.5))+ theme(text = element_text(size = 15))+ labs(y = NULL, x = NULL)+ theme(panel.background = element_blank(),panel.border = element_rect(linetype = \"dashed\", fill = NA))\n\n```", "```py\nPrejudice %>% \ninner_join(get_sentiments(\"bing\"), by = \"word\") %>% dplyr::count(word, sentiment, sort = TRUE) %>% acast(word ~ sentiment, value.var = \"n\", fill = 0) %>% comparison.cloud(colors = c(\"red\", \"green\"), max.words = 100,title.size=2, use.r.layout=TRUE, random.order=TRUE, scale=c(6,0.5)\n\n```", "```py\nausten_books_df <- as.data.frame(austen_books(),stringsAsFactors=F) austen_books_df$book <- as.character(austen_books_df$book) Pride_Prejudice_chapters <- austen_books_df %>% group_by(book) %>% filter(book == \"Pride & Prejudice\") %>% mutate(chapter = cumsum(str_detect(text, regex(\"^chapter [\\\\divxlc]\", ignore_case = TRUE)))) %>% ungroup() %>% unnest_tokens(word, text)\n\n```", "```py\nbingNEG <- get_sentiments(\"bing\") %>%  \n  filter(sentiment == \"negative\")  \nbingPOS <- get_sentiments(\"bing\") %>%  \n  filter(sentiment == \"positive\") \n\n```", "```py\n  wordcounts <- Pride_Prejudice_chapters %>% \n  group_by(book, chapter) %>% \n  dplyr::summarize(words = n()) \n\n```", "```py\nPOS_NEG_chapter_distribution <- merge ( Pride_Prejudice_chapters %>% \nsemi_join(bingNEG, by=\"word\") %>% \ngroup_by(book, chapter) %>% \ndplyr::summarize(neg_words = n()) %>% \nleft_join(wordcounts, by = c(\"book\", \"chapter\")) %>% \nmutate(neg_ratio = round(neg_words*100/words,2)) %>% \nfilter(chapter != 0) %>% \nungroup(), \nPride_Prejudice_chapters %>% \nsemi_join(bingPOS, by=\"word\") %>% \ngroup_by(book, chapter) %>%            dplyr::summarize(pos_words = n()) %>% \nleft_join(wordcounts, by = c(\"book\", \"chapter\")) %>% \nmutate(pos_ratio = round(pos_words*100/words,2)) %>% \nfilter(chapter != 0) %>% \nungroup() ) \n\n```", "```py\nPOS_NEG_chapter_distribution$sentiment_flag <- ifelse(POS_NEG_chapter_distribution$neg_ratio > POS_NEG_chapter_distribution$pos_ratio,\"NEG\",\"POS\") \ntable(POS_NEG_chapter_distribution$sentiment_flag)  \n\n```", "```py\nPride_Prejudice_chapters <- austen_books_df %>% \ngroup_by(book) %>% \nfilter(book == \"Pride & Prejudice\") %>% \nmutate(linenumber = row_number(), \nchapter = cumsum(str_detect(text, regex(\"^chapter [\\\\divxlc]\",\n                                        ignore_case = TRUE)))) %>% \nungroup() %>% \nunnest_tokens(word, text) %>% \ncount(book, chapter, word, sort = TRUE) %>% \nungroup() \n\n```", "```py\nfreq_vs_rank <- Pride_Prejudice_chapters %>%  \nmutate(rank = row_number(),  \n       term_frequency = n/totalwords) \nfreq_vs_rank %>%  \n  ggplot(aes(rank, term_frequency)) +  \n  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +  \n  scale_x_log10() + \n  scale_y_log10()\n\n```", "```py\nPride_Prejudice_chapters <- Pride_Prejudice_chapters %>% \nbind_tf_idf(word, chapter, n)\n\n```", "```py\n\nPride_Prejudice_chapters %>% \n  select(-totalwords) %>% \n  arrange(desc(tf_idf)) \n\nPride_Prejudice_chapters %>% \n  arrange(desc(tf_idf)) %>% \n  mutate(word = factor(word, levels = rev(unique(word)))) %>%  \n  group_by(book) %>%  \n  top_n(15) %>%  \n  ungroup %>% \n  ggplot(aes(word, tf_idf, fill = book)) + \n  geom_col(show.legend = FALSE) + \n  labs(x = NULL, y = \"tf-idf\") + \n  facet_wrap(~book, ncol = 2, scales = \"free\") + \n  coord_flip() \n\n```", "```py\nload_packages=c(\"text2vec\",\"tidytext\",\"tensorflow\") \nlapply(load_packages, require, character.only = TRUE) \ndata(\"movie_review\") \n\n```", "```py\nreviews <- data.frame(\"Sno\" = 1:nrow(movie_review), \n                         \"text\"=movie_review$review, \n                         stringsAsFactors=F) \n\nlabels <- as.matrix(data.frame(\"Positive_flag\" = movie_review$sentiment,\"negative_flag\" = (1\n                    movie_review$sentiment)))\n\n```", "```py\nreviews_sortedWords <- reviews %>% unnest_tokens(word,text) %>% dplyr::count(word, sort = TRUE) \nreviews_sortedWords$orderNo <- 1:nrow(reviews_sortedWords) \nreviews_sortedWords <- as.data.frame(reviews_sortedWords) \n\n```", "```py\nreviews_words <- reviews %>% unnest_tokens(word,text) \nreviews_words <- plyr::join(reviews_words,reviews_sortedWords,by=\"word\") \n\n```", "```py\nreviews_words_sno <- list() \nfor(i in 1:length(reviews$text))\n{ \n  reviews_words_sno[[i]] <- c(subset(reviews_words,Sno==i,orderNo)) \n} \n\n```", "```py\nreviews_words_sno <- lapply(reviews_words_sno,function(x) \n{ \n  x <- x$orderNo \n  if(length(x)>150)\n  { \n    return (x[1:150]) \n  } \n  else \n  { \n  return(c(rep(0,150-length(x)),x)) \n  } \n})\n\n```", "```py\ntrain_samples <- caret::createDataPartition(c(1:length(labels[1,1])),p = 0.7)$Resample1 \n\ntrain_reviews <- reviews_words_sno[train_samples] \ntest_reviews <- reviews_words_sno[-train_samples] \n\ntrain_reviews <- do.call(rbind,train_reviews) \ntest_reviews <- do.call(rbind,test_reviews)  \n\n```", "```py\ntrain_labels <- as.matrix(labels[train_samples,]) \ntest_labels <- as.matrix(labels[-train_samples,]) \n\n```", "```py\ntf$reset_default_graph() \nsess<-tf$InteractiveSession() \n\n```", "```py\nn_input<-15 \nstep_size<-10 \nn.hidden<-2 \nn.class<-2 \n\n```", "```py\nlr<-0.01 \nbatch<-200 \niteration = 500\n\n```", "```py\nsess$run(tf$global_variables_initializer()) \ntrain_error <- c() \nfor(i in 1:iteration){ \n  spls <- sample(1:dim(train_reviews)[1],batch) \n  sample_data<-train_reviews[spls,] \n  sample_y<-train_labels[spls,] \n\n  # Reshape sample into 15 sequence with each of 10 elements \n  sample_data=tf$reshape(sample_data, shape(batch, step_size, n_input)) \n  out<-optimizer$run(feed_dict = dict(x=sample_data$eval(session = sess), y=sample_y)) \n\n  if (i %% 1 == 0){ \n    cat(\"iteration - \", i, \"Training Loss - \",  cost$eval(feed_dict = dict(x=sample_data$eval(), y=sample_y)), \"\\n\") \n  } \n  train_error <-  c(train_error,cost$eval(feed_dict = dict(x=sample_data$eval(), y=sample_y))) \n} \n\n```", "```py\nplot(train_error, main=\"Training sentiment prediction error\", xlab=\"Iterations\", ylab = \"Train Error\")\n\n```", "```py\ntest_data=tf$reshape(test_reviews, shape(-1, step_size, n_input)) \ncost$eval(feed_dict=dict(x= test_data$eval(), y=test_labels))\n\n```", "```py\nlibrary(text2vec) \nlibrary(glmnet) \ndata(\"movie_review\") \n\n```", "```py\nlogistic_model <- function(Xtrain,Ytrain,Xtest,Ytest)\n{ \n  classifier <- cv.glmnet(x=Xtrain, y=Ytrain, \n  family=\"binomial\", alpha=1, type.measure = \"auc\", \n  nfolds = 5, maxit = 1000) \n  plot(classifier) \n  vocab_test_pred <- predict(classifier, Xtest, type = \"response\") \n  return(cat(\"Train AUC : \", round(max(classifier$cvm), 4), \n  \"Test AUC : \",glmnet:::auc(Ytest, vocab_test_pred),\"\\n\")) \n} \n\n```", "```py\ntrain_samples <- caret::createDataPartition(c(1:length(labels[1,1])),p = 0.8)$Resample1 \ntrain_movie <- movie_review[train_samples,] \ntest_movie <- movie_review[-train_samples,] \n\n```", "```py\ntrain_tokens <- train_movie$review %>% tolower %>% word_tokenizer \ntest_tokens <- test_movie$review %>% tolower %>% word_tokenizer \n\nvocab_train <- create_vocabulary(itoken(train_tokens,ids=train$id,progressbar = FALSE)) \n\n# Create train and test DTMs \nvocab_train_dtm <- create_dtm(it = itoken(train_tokens,ids=train$id,progressbar = FALSE), \n                              vectorizer = vocab_vectorizer(vocab_train)) \nvocab_test_dtm <- create_dtm(it = itoken(test_tokens,ids=test$id,progressbar = FALSE), \n                              vectorizer = vocab_vectorizer(vocab_train)) \n\ndim(vocab_train_dtm) \ndim(vocab_test_dtm) \n\n# Run LASSO (L1 norm) Logistic Regression \nlogistic_model(Xtrain = vocab_train_dtm, \n               Ytrain = train_movie$sentiment, \n               Xtest = vocab_test_dtm, \n               Ytest = test_movie$sentiment) \n\n```", "```py\ndata(\"stop_words\") \nvocab_train_prune <- create_vocabulary(itoken(train_tokens,ids=train$id,progressbar = FALSE), \n                                       stopwords = stop_words$word) \n\nvocab_train_prune <- prune_vocabulary(vocab_train_prune,term_count_min = 15, \n                                      doc_proportion_min = 0.0005, \n                                      doc_proportion_max = 0.5) \n\nvocab_train_prune_dtm <- create_dtm(it = itoken(train_tokens,ids=train$id,progressbar = FALSE), \n                              vectorizer = vocab_vectorizer(vocab_train_prune)) \nvocab_test_prune_dtm <- create_dtm(it = itoken(test_tokens,ids=test$id,progressbar = FALSE), \n                             vectorizer = vocab_vectorizer(vocab_train_prune)) \n\nlogistic_model(Xtrain = vocab_train_prune_dtm, \n               Ytrain = train_movie$sentiment, \n               Xtest = vocab_test_prune_dtm, \n               Ytest = test_movie$sentiment) \n\n```", "```py\nvocab_train_ngrams <- create_vocabulary(itoken(train_tokens,ids=train$id,progressbar = FALSE), \n                                        ngram = c(1L, 2L)) \n\nvocab_train_ngrams <- prune_vocabulary(vocab_train_ngrams,term_count_min = 10, \n                                       doc_proportion_min = 0.0005, \n                                       doc_proportion_max = 0.5) \n\nvocab_train_ngrams_dtm <- create_dtm(it = itoken(train_tokens,ids=train$id,progressbar = FALSE), \n                                    vectorizer = vocab_vectorizer(vocab_train_ngrams)) \nvocab_test_ngrams_dtm <- create_dtm(it = itoken(test_tokens,ids=test$id,progressbar = FALSE), \n                                   vectorizer = vocab_vectorizer(vocab_train_ngrams)) \n\nlogistic_model(Xtrain = vocab_train_ngrams_dtm, \n               Ytrain = train_movie$sentiment, \n               Xtest = vocab_test_ngrams_dtm, \n               Ytest = test_movie$sentiment) \n\n```", "```py\nvocab_train_hashing_dtm <- create_dtm(it = itoken(train_tokens,ids=train$id,progressbar = FALSE), \n                                      vectorizer = hash_vectorizer(hash_size = 2^14, ngram = c(1L, 2L))) \nvocab_test_hashing_dtm <- create_dtm(it = itoken(test_tokens,ids=test$id,progressbar = FALSE), \n                                    vectorizer = hash_vectorizer(hash_size = 2^14, ngram = c(1L, 2L))) \n\nlogistic_model(Xtrain = vocab_train_hashing_dtm, \n               Ytrain = train_movie$sentiment, \n               Xtest = vocab_test_hashing_dtm, \n               Ytest = test_movie$sentiment) \n\n```", "```py\nvocab_train_tfidf <- fit_transform(vocab_train_dtm, TfIdf$new()) \nvocab_test_tfidf <- fit_transform(vocab_test_dtm, TfIdf$new()) \n\nlogistic_model(Xtrain = vocab_train_tfidf, \n               Ytrain = train_movie$sentiment, \n               Xtest = vocab_test_tfidf, \n               Ytest = test_movie$sentiment)  \n\n```"]