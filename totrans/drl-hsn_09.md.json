["```py\n>>> import torch \n>>> @torch.no_grad \n... def fun_a(t): \n...    return t*2 \n... \n>>> def fun_b(t): \n...    return t*2 \n...\n```", "```py\n>>> t = torch.ones(3, requires_grad=True) \n>>> t \ntensor([1., 1., 1.], requires_grad=True) \n>>> a = fun_a(t) \n>>> b = fun_b(t) \n>>> b \ntensor([2., 2., 2.], grad_fn=<MulBackward0>) \n>>> a \ntensor([2., 2., 2.])\n```", "```py\n>>> a*t \ntensor([2., 2., 2.], grad_fn=<MulBackward0>)\n```", "```py\n>>> with torch.no_grad(): \n...    c = t*2 \n... \n>>> c \ntensor([2., 2., 2.])\n```", "```py\ndef batch_generator(buffer: ptan.experience.ExperienceReplayBuffer, \n                    initial: int, batch_size: int, steps: int): \n    buffer.populate(initial) \n    while True: \n        buffer.populate(steps) \n        yield buffer.sample(batch_size)\n```", "```py\n envs = [ \n        ptan.common.wrappers.wrap_dqn(gym.make(params.env_name)) \n        for _ in range(args.envs) \n    ] \n    params.batch_size *= args.envs \n    exp_source = ptan.experience.ExperienceSourceFirstLast( \n        envs, agent, gamma=params.gamma, env_seed=common.SEED)\n```", "```py\nimport torch.multiprocessing as mp \n\n@dataclass \nclass EpisodeEnded: \n    reward: float \n    steps: int \n    epsilon: float \n\ndef play_func(params: common.Hyperparams, net: dqn_model.DQN, \n              dev_name: str, exp_queue: mp.Queue): \n    env = gym.make(params.env_name) \n    env = ptan.common.wrappers.wrap_dqn(env) \n    device = torch.device(dev_name) \n\n    selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=params.epsilon_start) \n    epsilon_tracker = common.EpsilonTracker(selector, params) \n    agent = ptan.agent.DQNAgent(net, selector, device=device) \n    exp_source = ptan.experience.ExperienceSourceFirstLast( \n        env, agent, gamma=params.gamma, env_seed=common.SEED) \n\n    for frame_idx, exp in enumerate(exp_source): \n        epsilon_tracker.frame(frame_idx//2) \n        exp_queue.put(exp) \n        for reward, steps in exp_source.pop_rewards_steps(): \n            ee = EpisodeEnded(reward=reward, steps=steps, epsilon=selector.epsilon) \n            exp_queue.put(ee)\n```", "```py\nclass BatchGenerator: \n    def __init__(self, buffer_size: int, exp_queue: mp.Queue, \n                 fps_handler: ptan_ignite.EpisodeFPSHandler, \n                 initial: int, batch_size: int): \n        self.buffer = ptan.experience.ExperienceReplayBuffer( \n            experience_source=None, buffer_size=buffer_size) \n        self.exp_queue = exp_queue \n        self.fps_handler = fps_handler \n        self.initial = initial \n        self.batch_size = batch_size \n        self._rewards_steps = [] \n        self.epsilon = None \n\n    def pop_rewards_steps(self) -> tt.List[tt.Tuple[float, int]]: \n        res = list(self._rewards_steps) \n        self._rewards_steps.clear() \n        return res \n\n    def __iter__(self): \n        while True: \n            while self.exp_queue.qsize() > 0: \n                exp = self.exp_queue.get() \n                if isinstance(exp, EpisodeEnded): \n                    self._rewards_steps.append((exp.reward, exp.steps)) \n                    self.epsilon = exp.epsilon \n                else: \n                    self.buffer._add(exp) \n                    self.fps_handler.step() \n            if len(self.buffer) < self.initial: \n                continue \n            yield self.buffer.sample(self.batch_size)\n```", "```py\nif __name__ == \"__main__\": \n    warnings.simplefilter(\"ignore\", category=UserWarning) \n    mp.set_start_method(’spawn’)\n```", "```py\n exp_queue = mp.Queue(maxsize=2) \n    proc_args = (params, net, args.dev, exp_queue) \n    play_proc = mp.Process(target=play_func, args=proc_args) \n    play_proc.start()\n```"]