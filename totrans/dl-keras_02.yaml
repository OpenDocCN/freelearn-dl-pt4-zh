- en: Keras Installation and API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we discussed the basic principles of neural networks
    and provided a few examples of nets that are able to recognize MNIST handwritten
    numbers.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter explains how to install Keras, Theano, and TensorFlow. Step by
    step, we will look at how to get the environment working and move from intuition
    to working nets in very little time. Then we will discuss how to install on a
    dockerized infrastructure based on containers, and in the cloud with Google GCP,
    Amazon AWS, and Microsoft Azure. In addition to that, we will present an overview
    of Keras APIs, and some commonly useful operations such as loading and saving
    neural networks' architectures and weights, early stopping, history saving, checkpointing,
    and interactions with TensorBoard and Quiver. Let us start.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, we will have covered the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Installing and configuring Keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keras architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the sections to follow, we will show how to install Keras on multiple platforms.
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 — install some useful dependencies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we install the `numpy` package, which provides support for large, multidimensional
    arrays and matrices as well as high-level mathematical functions. Then we install
    `scipy`, a library used for scientific computation. After that, it might be appropriate
    to install `scikit-learn`, a package considered the Python Swiss army knife for
    machine learning. In this case, we will use it for data exploration. Optionally,
    it could be useful to install `pillow`, a library useful for image processing,
    and `h5py`, a library useful for data serialization used by Keras for model saving.
    A single command line is enough for installing what is needed. Alternatively,
    one can install Anaconda Python, which will automatically install `numpy`, `scipy`,
    `scikit-learn`, `h5py`, `pillow`, and a lot of other libraries that are needed
    for scientific computing (for more information, refer to: *Batch Normalization:
    Accelerating Deep Network Training by Reducing Internal Covariate Shift*, by S.
    Ioffe and C. Szegedy, [arXiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167),
    2015). You can find the packages available in Anaconda Python at [https://docs.continuum.io/anaconda/pkg-docs](https://docs.continuum.io/anaconda/pkg-docs). The
    following screenshot shows how to install the packages for our work:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_001.png)'
  prefs: []
  type: TYPE_IMG
- en: Step 2 — install Theano
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can use `pip` to install Theano, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_002.png)'
  prefs: []
  type: TYPE_IMG
- en: Step 3 — install TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we can install TensorFlow using the instructions found on the TensorFlow
    website at [https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#pip-installation](https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#pip-installation).
    Again, we simply use `pip` for installing the correct package, as shown in the
    following screenshot. For instance, if we need to use GPUs, it is important to
    pick the appropriate package:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_003.png)'
  prefs: []
  type: TYPE_IMG
- en: Step 4 — install Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we can simply install Keras and start testing the installed environment.
    Pretty simple; let''s use `pip` again, as shown in this screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_004.png)'
  prefs: []
  type: TYPE_IMG
- en: Step 5 — testing Theano, TensorFlow, and Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now let''s test the environment. First let''s look at how to define the sigmoid
    function in Theano. As you see, it is very simple; we just write the mathematical
    formula and compute the function element-wise on a matrix. Just run the Python
    Shell and write the code as shown in the following screenshot to get the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, Theano works. Let''s test TensorFlow by simply importing the MNIST dataset
    as shown in the following screenshot. We have already seen, in [Chapter 1](c2484fb4-248d-49ed-8166-06aff812e5e9.xhtml),
    *Neural Networks Foundations*, a few working examples of the Keras network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_006.png)'
  prefs: []
  type: TYPE_IMG
- en: Configuring Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Keras has a very minimalist configuration file. Let''s load it with a `vi`
    session. The parameters are very simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Parameters** | **Values** |'
  prefs: []
  type: TYPE_TB
- en: '| `image_dim_ordering` | Can be either `tf` for the TensorFlow image ordering
    or `th` for Theano image ordering |'
  prefs: []
  type: TYPE_TB
- en: '| `epsilon` | The `epsilon` value used during computation |'
  prefs: []
  type: TYPE_TB
- en: '| `floatx` | Can be either `float32` or `float64` |'
  prefs: []
  type: TYPE_TB
- en: '| `backend` | Can be either `tensorflow` or `theano` |'
  prefs: []
  type: TYPE_TB
- en: 'The `image_dim_ordering` of `th` value gives you a somewhat non-intuitive dimension
    ordering for images (depth, width, and height), instead of (width, height, and
    depth), for `tf`. The following are the default parameters in my machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_007.png)'
  prefs: []
  type: TYPE_IMG
- en: If you install a GPU-enabled TensorFlow version, then Keras will automatically
    use your configured GPU when TensorFlow is selected as the backend.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Keras on Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the easiest ways to get started with TensorFlow and Keras is running
    in a Docker container. A convenient solution is to use a predefined Docker image
    for deep learning created by the community that contains all the popular DL frameworks
    (TensorFlow, Theano, Torch, Caffe, and so on). Refer to the GitHub repository
    at [https://github.com/saiprashanths/dl-docker](https://github.com/saiprashanths/dl-docker) for
    the code files. Assuming that you already have Docker up and running (for more
    information, refer to [https://www.docker.com/products/overview](https://www.docker.com/products/overview)),
    installing it is pretty simple and is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_008.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot, says something like, after getting the image from
    Git, we build the Docker image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_009.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this screenshot, we see how to run it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_010.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From within the container, it is possible to activate support for Jupyter Notebooks
    (for more information, refer to [http://jupyter.org/](http://jupyter.org/)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_011.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Access it directly from the host machine on port:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_012.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It is also possible to access TensorBoard (for more information, refer to [https://www.tensorflow.org/how_tos/summaries_and_tensorboard/](https://www.tensorflow.org/how_tos/summaries_and_tensorboard/))
    with the help of the command in the screenshot that follows, which is discussed
    in the next section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_013.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After running the preceding command, you will be redirected to the following
    page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_014.png)'
  prefs: []
  type: TYPE_IMG
- en: Installing Keras on Google Cloud ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Installing Keras on Google Cloud is very simple. First, we can install Google
    Cloud (for the downloadable file, refer to [https://cloud.google.com/sdk/](https://cloud.google.com/sdk/))*,*
    a command-line interface for Google Cloud Platform; then we can use CloudML, a
    managed service that enables us to easily build machine, learning models with
    TensorFlow. Before using Keras, let''s use Google Cloud with TensorFlow to train
    an MNIST example available on GitHub. The code is local and training happens in
    the cloud:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_015.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the following screenshot, you can see how to run a training session:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_016.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can use TensorBoard to show how cross-entropy decreases across iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_017.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the next screenshot, we see the graph of cross-entropy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_018.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, if we want to use Keras on the top of TensorFlow, we simply download the
    Keras source from PyPI (for the downloadable file, refer to [https://pypi.Python.org/pypi/Keras/1.2.0](https://pypi.Python.org/pypi/Keras/1.2.0)
    or later versions) and then directly use Keras as a CloudML package solution,
    as in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_019.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, `trainer.task2.py` is an example script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Installing Keras on Amazon AWS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Installing TensorFlow and Keras on Amazon is very simple. Indeed, it is possible
    to use a prebuilt AMI named `TFAMI.v3` that is open and free (for more information,
    refer to [https://github.com/ritchieng/tensorflow-aws-ami](https://github.com/ritchieng/tensorflow-aws-ami)),
    shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_020.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This AMI runs TensorFlow in less than five minutes and supports TensorFlow,
    Keras, OpenAI Gym, and all dependencies. As of January 2017, it supports the following:'
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow 0.12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keras 1.1.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorLayer 1.2.7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CUDA 8.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CuDNN 5.1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python 2.7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ubuntu 16.04
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In addition, `TFAMI.v3` works on P2 computing instances (for more information,
    refer to [https://aws.amazon.com/ec2/instance-types/#p2](https://aws.amazon.com/ec2/instance-types/#p2)),
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_021.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Some features of P2 instances are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Intel Xeon E5-2686v4 (Broadwell) processors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NVIDIA K80 GPUs, each with 2,496 parallel cores and 12 GB of GPU memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supports peer-to-peer GPU communication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provides enhanced networking (for more information, refer to [https://aws.amazon.com/ec2/faqs/#What_networking_capabilities_are_included_in_this_feature](https://aws.amazon.com/ec2/faqs/#What_networking_capabilities_are_included_in_this_feature))
    with 20 Gbps of aggregate network bandwidth
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `TFAMI.v3` also works on G2 computing instances (for more information,
    refer to [https://aws.amazon.com/ec2/instance-types/#g2](https://aws.amazon.com/ec2/instance-types/#g2)).
    Some features of G2 instances are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Intel Xeon E5-2670 (Sandy Bridge) processors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NVIDIA GPUs, each with 1,536 CUDA cores and 4 GB of video memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing Keras on Microsoft Azure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One way to install Keras on Azure is to install the support for Docker and then
    get a containerized version of TensorFlow plus Keras. Online, it is also possible
    to find a detailed set of instructions on how to install Keras and TensorFlow
    with Docker, but this is essentially what we have seen already in a previous section
    (for more information, refer to [https://blogs.msdn.microsoft.com/uk_faculty_connection/2016/09/26/tensorflow-on-docker-with-microsoft-azure/](https://blogs.msdn.microsoft.com/uk_faculty_connection/2016/09/26/tensorflow-on-docker-with-microsoft-azure/)).
  prefs: []
  type: TYPE_NORMAL
- en: If you use Theano as the only backend, then Keras can run with just a click
    by loading a pre-built package available on Cortana Intelligence Gallery (for
    more information, refer to [https://gallery.cortanaintelligence.com/Experiment/Theano-Keras-1](https://gallery.cortanaintelligence.com/Experiment/Theano-Keras-1)).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following sample shows how to import Theano and Keras into Azure ML directly
    as a ZIP file and use them in the Execute Python Script module. This example is
    due to Hai Ning (for more information, refer to [https://goo.gl/VLR25o](https://goo.gl/VLR25o)),
    and it essentially runs the Keras code within the `azureml_main()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In this screenshot, you see an example use of Microsoft Azure ML to run Theano
    and Keras:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_022.png)'
  prefs: []
  type: TYPE_IMG
- en: Keras API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Keras has a modular, minimalist, and easy extendable architecture. Francois
    Chollet, the author of Keras, says:'
  prefs: []
  type: TYPE_NORMAL
- en: The library was developed with a focus on enabling fast experimentation. Being
    able to go from idea to result with the least possible delay is key to doing good
    research.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keras defines high-level neural networks running on top of either TensorFlow
    (for more information, refer to [https://github.com/tensorflow/tensorflow](https://github.com/tensorflow/tensorflow)) or
    Theano (for more information, refer to [https://github.com/Theano/Theano](https://github.com/Theano/Theano)).
    In details:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Modularity**: A model is either a sequence or a graph of standalone modules
    that can be combined together like LEGO blocks for building neural networks. Namely,
    the library predefines a very large number of modules implementing different types
    of neural layers, cost functions, optimizers, initialization schemes, activation
    functions, and regularization schemes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Minimalism**: The library is implemented in Python and each module is kept
    short and self-describing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Easy extensibility**: The library can be extended with new functionalities,
    as we will describe in [Chapter 7](9384823c-eb58-4a0f-91e7-1a5508eeb520.xhtml), *Additional
    Deep Learning Models*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting started with Keras architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we review the most important Keras components used for defining
    neural networks. First, we define what a tensor is, then we discuss different
    ways of composing predefined modules, and we conclude with an overview of the
    ones most commonly used.
  prefs: []
  type: TYPE_NORMAL
- en: What is a tensor?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Keras uses either Theano or TensorFlow to perform very efficient computations
    on tensors. But what is a tensor anyway? A tensor is nothing but a multidimensional
    array or matrix. Both the backends are capable of efficient symbolic computations
    on tensors, which are the fundamental building blocks for creating neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Composing models in Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two ways of composing models in Keras. They are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Sequential composition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Functional composition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let us take a look at each one in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Sequential composition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first one is the sequential composition, where different predefined models
    are stacked together in a linear pipeline of layers similar to a stack or a queue.
    In [Chapter 1](c2484fb4-248d-49ed-8166-06aff812e5e9.xhtml), *Neural Networks Foundations*,
    we saw a few examples of sequential pipelines. For instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Functional composition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The second way of composing modules is via the functional API, where it is possible
    to define complex models, such as directed acyclic graphs, models with shared
    layers, or multi-output models. We will see such examples in [Chapter 7](9384823c-eb58-4a0f-91e7-1a5508eeb520.xhtml),
    *Additional Deep Learning Models*.
  prefs: []
  type: TYPE_NORMAL
- en: An overview of predefined neural network layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Keras has a number of prebuilt layers. Let us review the most commonly used
    ones and highlight in which chapter these layers are mostly used.
  prefs: []
  type: TYPE_NORMAL
- en: Regular dense
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A dense model is a fully connected neural network layer. We have already seen
    examples of usage in [Chapter 1](c2484fb4-248d-49ed-8166-06aff812e5e9.xhtml),
    *Neural Networks Foundations*. Here is the prototype with a definition of the
    parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Recurrent neural networks — simple, LSTM, and GRU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recurrent neural networks are a class of neural networks that exploit the sequential
    nature of their input. Such inputs could be a text, a speech, time series, and
    anything else where the occurrence of an element in the sequence is dependent
    on the elements that appeared before it. We will discuss simple, LSTM, and GRU
    recurrent neural networks in [Chapter 6](57a694a6-93f4-4eec-9fbf-e4eafd2d6824.xhtml),
    *Recurrent Neural Network — RNN*. Here you can see some prototypes with a definition
    of the parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Convolutional and pooling layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'ConvNets are a class of neural networks using convolutional and pooling operations
    for progressively learning rather sophisticated models based on progressive levels
    of abstraction. This learning via progressive abstraction resembles vision models
    that have evolved over millions of years inside the human brain. People called
    it *deep* with 3-5 layers a few years ago, and now it has gone up to 100-200\.
    We will discuss convolutional neural networks in [Chapter 3](4be2a04a-4545-4051-bcd9-32764d21f0f2.xhtml),
    *Deep Learning with ConvNets*. Here are some prototypes with a definition of the
    parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Regularization is a way to prevent overfitting. We have already seen examples
    of usage in [Chapter 1](c2484fb4-248d-49ed-8166-06aff812e5e9.xhtml), *Neural Networks
    Foundations*. Multiple layers have parameters for regularization.  The following
    is the list of regularization parameters commonly used for dense, and convolutional
    modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kernel_regularizer`: Regularizer function applied to the weight matrix'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bias_regularizer`: Regularizer function applied to the bias vector'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`activity_regularizer`: Regularizer function applied to the output of the layer
    (its activation)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition is possible to use Dropout for regularization and that is frequently
    a very effective choice
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: '`rate`: It is a float between 0 and 1 which represents the fraction of the
    input units to drop'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`noise_shape`: It is a 1D integer tensor which represents the shape of the
    binary dropout mask that will be multiplied with the input'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`seed`: It is a integer which is used use as random seed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Batch normalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Batch normalization (for more information, refer to [https://www.colwiz.com/cite-in-google-docs/cid=f20f9683aaf69ce](https://www.colwiz.com/cite-in-google-docs/cid=f20f9683aaf69ce)) is
    a way to accelerate learning and generally achieve better accuracy. We will look
    at examples of usage in [Chapter 4](a67ea944-b1a6-48a3-b8aa-4e698166c0eb.xhtml),
    *Generative Adversarial Networks and WaveNet*, when we discuss GANs. Here is the
    prototype with a definition of the parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: An overview of predefined activation functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Activation includes commonly used functions such as sigmoid, linear, hyperbolic
    tangent, and ReLU. We have seen a few examples of activation functions in [Chapter
    1](c2484fb4-248d-49ed-8166-06aff812e5e9.xhtml), *Neural Networks Foundations*,
    and more examples will be presented in the next chapters. The following diagrams
    are examples of sigmoid, linear, hyperbolic tangent, and ReLU activation functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Sigmoid**![](img/image_02_023.png) | **Linear**![](img/image_02_024.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Hyperbolic tangent**![](img/image_02_025.png) | **ReLU**![](img/image_02_026.png)
    |'
  prefs: []
  type: TYPE_TB
- en: An overview of losses functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Losses functions (or objective functions, or optimization score function; for
    more information, refer to [https://keras.io/losses/](https://keras.io/losses/)) can
    be classified into four categories:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Accuracy which is used for classification problems. There are multiple choices:
    `binary_accuracy` (mean accuracy rate across all predictions for binary classification
    problems), `categorical_accuracy` (mean accuracy rate across all predictions for
    multiclass classification problems), `sparse_categorical_accuracy` (useful for
    sparse targets), and `top_k_categorical_accuracy` (success when the target class
    is within the `top_k` predictions provided).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Error loss, which measures the difference between the values predicted and
    the values actually observed. There are multiple choices: `mse` (mean square error
    between predicted and target values), `rmse` (root square error between predicted
    and target values), `mae` (mean absolute error between predicted and target values),
    `mape` (mean percentage error between predicted and target values), and `msle`
    (mean squared logarithmic error between predicted and target values).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinge loss, which is generally used for training classifiers. There are two
    versions: *hinge* defined as ![](img/image_02_027.png) and *squared hinge* defined
    as the the squared value of the hinge loss.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Class loss is used to calculate the cross-entropy for classification problems.
    There are multiple versions, including binary cross-entropy (for more information,
    refer to [https://en.wikipedia.org/wiki/Cross_entropy](https://en.wikipedia.org/wiki/Cross_entropy)),
    and categorical cross-entropy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have seen a few examples of objective functions in [Chapter 1](c2484fb4-248d-49ed-8166-06aff812e5e9.xhtml),
    *Neural Networks Foundations*, and more examples will be presented in the next
    chapters.
  prefs: []
  type: TYPE_NORMAL
- en: An overview of metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A metric function (for more information, refer to [https://keras.io/metrics/](https://keras.io/metrics/))
    is similar to an objective function. The only difference is that the results from
    evaluating a metric are not used when training the model. We have seen a few examples
    of metrics in [Chapter 1](c2484fb4-248d-49ed-8166-06aff812e5e9.xhtml), *Neural
    Networks Foundations*, and more examples will be presented in the next chapters.
  prefs: []
  type: TYPE_NORMAL
- en: An overview of optimizers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Optimizers include SGD, RMSprop, and Adam. We have seen a few examples of optimizers
    in [Chapter 1](c2484fb4-248d-49ed-8166-06aff812e5e9.xhtml), *Neural Networks Foundations*,
    and more examples (Adagrad and Adadelta; for more information, refer to [https://keras.io/optimizers/](https://keras.io/optimizers/))
    will be presented in the next chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Some useful operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here we report some utility operations that can be carried out with Keras APIs.
    The goal is to facilitate the creation of networks, the training process, and
    the saving of intermediate results.
  prefs: []
  type: TYPE_NORMAL
- en: Saving and loading the weights and the architecture of a model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Model architectures can be easily saved and loaded as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Model parameters (weights) can be easily saved and loaded as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Callbacks for customizing the training process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The training process can be stopped when a metric has stopped improving by
    using an appropriate `callback`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Loss history can be saved by defining a `callback` like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Checkpointing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Checkpointing is a process that saves a snapshot of the application's state
    at regular intervals, so the application can be restarted from the last saved
    state in case of failure. This is useful during training of deep learning models,
    which can often be a time-consuming task. The state of a deep learning model at
    any point in time is the weights of the model at that time. Keras saves these
    weights in HDF5 format (for more information, refer to [https://www.hdfgroup.org/](https://www.hdfgroup.org/)) and
    provides checkpointing using its callback API.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some scenarios where checkpointing can be useful include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: If you want the ability to restart from your last checkpoint after your AWS
    Spot instance (for more information, refer to [http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/how-spot-instances-work.html](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/how-spot-instances-work.html)) or
    Google preemptible virtual machine (for more information, refer to [https://cloud.google.com/compute/docs/instances/preemptible](https://cloud.google.com/compute/docs/instances/preemptible)) is
    unexpectedly terminated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to stop training, perhaps to test your model on test data, then
    continue training from the last checkpoint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to retain the best version (by some metric such as validation loss)
    as it trains over multiple epochs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The first and second scenarios can be handled by saving a checkpoint after
    each epoch, which is handled by the default usage of the `ModelCheckpoint` callback.
    The following code illustrates how to add checkpointing during training of your
    deep learning model in Keras:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The third scenario involves monitoring a metric, such as validation accuracy
    or loss, and only saving a checkpoint if the current metric is better than the
    previously saved checkpoint. Keras provides an additional parameter, `save_best_only`,
    which needs to be set to `true` when instantiating the checkpoint object in order
    to support this functionality.
  prefs: []
  type: TYPE_NORMAL
- en: Using TensorBoard and Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Keras provides a callback for saving your training and test metrics, as well
    as activation histograms for the different layers in your model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Saved data can then be visualized with TensorBoad launched at the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Using Quiver and Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 3](4be2a04a-4545-4051-bcd9-32764d21f0f2.xhtml), *Deep Learning
    with ConvNets*, we will discuss ConvNets, which are an advanced deep learning
    technique for dealing with images. Here we give a preview of Quiver (for more
    information, refer to [https://github.com/jakebian/quiver](https://github.com/jakebian/quiver)),
    a tool useful for visualizing ConvNets features in an interactive way. The installation
    is pretty simple, and after that Quiver can be used with one single line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This will launch the visualization at `localhost:5000`. Quiver allows you to
    visually inspect a neural network, as in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_028.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we discussed how to install Theano, TensorFlow, and Keras
    on the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Your local machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A dockerized infrastructure based on containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the cloud with Google GCP, Amazon AWS, and Microsoft Azure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to that, we looked at a few modules defining Keras APIs and some
    commonly useful operations such as loading and saving neural networks' architectures
    and weights, early stopping, history saving, checkpointing, interactions with
    TensorBoard, and interactions with Quiver.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will introduce the concept of convolutional networks
    a fundamental innovation in deep learning which has been used with success in
    multiple domains from text, to video, to speech going well beyond the initial
    image processing domain where they were originally conceived.
  prefs: []
  type: TYPE_NORMAL
