- en: Building Your First GAN with PyTorch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous chapters, we covered the idea of using adversarial learning to generate
    simple signals with NumPy and learned about the new features and capabilities
    of PyTorch 1.3\. It's time for us to use PyTorch to train a GAN model for generating
    interesting samples.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will introduce you to a classic and well-performing GAN
    model, called DCGAN, to generate 2D images. You will learn the following:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of DCGANs
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The training and evaluation of DCGANs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a DCGAN to generate handwritten digits, human faces
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Having fun with the generator network by performing image interpolation and
    arithmetic calculation on the latent vectors to change the image attributes
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have grasped the core architecture design
    of GAN models for generating image data and have a better understanding of the
    relationship between latent vectors and generated samples.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Deep Convolutional GANs
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**DCAGN** (**Deep Convolutional Generative Adversarial ****Network**) is one
    of the early well-performing and stable approaches to generate images with adversarial
    training. Let''s take a look back at the simple example in [Chapter 1](66a945c3-9fd3-4d27-a6ec-b47d2e299e84.xhtml), *Generative
    Adversarial Networks Fundamentals*.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, even when we only train a GAN to manipulate 1D data, we have to use multiple
    techniques to ensure a stable training. A lot of things could go wrong in the
    training of GANs. For example, either a generator or a discriminator could overfit
    if one or the other does not converge. Sometimes, the generator only generates
    a handful of sample varieties. This is called **mode collapse**. The following
    is an example of mode collapse, where we want to train a GAN with some popular
    meme images in China called **Baozou**. We can see that our GAN is only capable
    of generating one or two memes at a time. Problems that commonly occur in other
    machine learning algorithms such as gradient vanishing/explosion and underfitting
    can also look familiar in the training of GANs. Therefore, just replacing 1D data
    with 2D images won''t easily guarantee successful training:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6e458f91-d634-486e-abc7-1b48a8d0150c.png)![](img/668363a6-4abe-4656-9aff-65bb9c3a0a9c.png)![](img/949bf1ca-dc4d-461d-9c55-1a5134f91129.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
- en: 'Mode collapse in GAN training (left: some training samples; middle: results
    at 492nd iteration; right: results at 500th iteration)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 'To ensure the stable training of GANs on image data like this, a DCGAN uses
    three techniques:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Getting rid of fully connected layers and only using convolution layers
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using strided convolution layers to perform downsampling, instead of using pooling
    layers
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using ReLU/leakyReLU activation functions instead of Tanh between hidden layers
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we will introduce the architectures of the generator and discriminator
    of the DCGAN and learn how to generate images with it. We'll use MNIST ([http://yann.lecun.com/exdb/mnist](http://yann.lecun.com/exdb/mnist))
    samples to illustrate the architecture of a DCGAN and use it to train the model
    in the next two sections.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将介绍 DCGAN 的生成器和判别器架构，并学习如何使用它生成图像。我们将使用 MNIST ([http://yann.lecun.com/exdb/mnist](http://yann.lecun.com/exdb/mnist))
    样本来说明 DCGAN 的架构，并在接下来的两节中使用它来训练模型。
- en: The architecture of generator
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成器的架构
- en: 'The generator network of a DCGAN contains 4 hidden layers (we treat the input
    layer as the 1^(st) hidden layer for simplicity) and 1 output layer. Transposed
    convolution layers are used in hidden layers, which are followed by batch normalization
    layers and ReLU activation functions. The output layer is also a transposed convolution
    layer and Tanh is used as the activation function. The architecture of the generator
    is shown in the following diagram:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: DCGAN 的生成器网络包含 4 个隐藏层（为简化起见，我们将输入层视为第 1 个隐藏层）和 1 个输出层。隐藏层中使用转置卷积层，后面跟随批量归一化层和
    ReLU 激活函数。输出层也是一个转置卷积层，使用 Tanh 作为激活函数。生成器的架构如下图所示：
- en: '![](img/383259d3-c12d-4c77-91b4-7286a0fbe20d.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/383259d3-c12d-4c77-91b4-7286a0fbe20d.png)'
- en: Generator architecture in DCGAN
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: DCGAN 中的生成器架构
- en: The 2^(nd), 3^(rd),and 4^(th) hidden layers and the output layer have a stride
    value of 2\. The 1^(st) layer has a padding value of 0 and the other layers have
    a padding value of 1\. As the image (feature map) sizes increase by two in deeper
    layers, the numbers of channels are decreasing by half. This is a common convention
    in the architecture design of neural networks. All kernel sizes of transposed
    convolution layers are set to 4 x 4\. The output channel can be either 1 or 3,
    depending on whether you want to generate grayscale images or color images.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 第 2、3、4 个隐藏层和输出层的步幅值为 2。第 1 层的填充值为 0，其余层的填充值为 1。随着图像（特征图）尺寸在更深的层中增加一倍，通道数减少一半。这是神经网络架构设计中的一种常见约定。所有转置卷积层的卷积核大小都设置为
    4 x 4。输出通道可以是 1 或 3，具体取决于你是想生成灰度图像还是彩色图像。
- en: The transposed convolution layer can be considered as the **reverse process**
    of a normal convolution. It was once called by some a deconvolution layer, which
    is misleading because the transposed convolution is not the **inverse** of convolution.
    Most convolution layers are not invertible, because they are ill-conditioned (have
    extremely large condition numbers) from the linear algebra perspective, which
    makes their pseudoinverse matrices unfit for representing the inverse process.
    If you are interested in finding the inverse of a convolution kernel, you can
    search for numerical deconvolution methods on the internet.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 转置卷积层可以被看作是普通卷积的**逆过程**。它曾一度被一些人称为反卷积层，这种叫法具有误导性，因为转置卷积并不是卷积的**逆**操作。从线性代数的角度来看，大多数卷积层是不可逆的，因为它们是病态的（具有极大的条件数），这使得它们的伪逆矩阵不适合表示逆过程。如果你有兴趣寻找卷积核的逆运算方法，可以在互联网上搜索数值反卷积方法。
- en: The architecture of a discriminator
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 判别器的架构
- en: 'The discriminator network of a DCGAN consists of 4 hidden layers (again, we
    treat the input layer as the 1^(st) hidden layer) and 1 output layer. Convolution
    layers are used in all layers, which are followed by batch normalization layers
    except that the first layer does not have batch normalization. LeakyReLU activation
    functions are used in the hidden layers and Sigmoid is used for the output layer.
    The architecture of the discriminator is shown in the following:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: DCGAN 的判别器网络由 4 个隐藏层（同样，我们将输入层视为第 1 个隐藏层）和 1 个输出层组成。所有层中都使用卷积层，后面跟随批量归一化层，除了第一层没有批量归一化。隐藏层中使用
    LeakyReLU 激活函数，输出层使用 Sigmoid 激活函数。判别器的架构如下所示：
- en: '![](img/0a1cf07f-9051-42ec-ad32-e29f8eae7b55.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0a1cf07f-9051-42ec-ad32-e29f8eae7b55.png)'
- en: Discriminator architecture in DCGAN
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: DCGAN 中的判别器架构
- en: The input channel can be either 1 or 3, depending on whether you are dealing
    with grayscale images or color images. All hidden layers have a stride value of
    2 and a padding value of 1 so that their output image sizes will be half the input
    images. As image sizes increase in deeper layers, the numbers of channels are
    increasing by twice. All kernels in convolution layers are of a size of 4 x 4. The
    output layer has a stride value of 1 and a padding value of 0\. It maps 4 x 4
    feature maps to single values so that the Sigmoid function can transform the value
    into prediction confidence.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 输入通道可以是 1 或 3，具体取决于你处理的是灰度图像还是彩色图像。所有隐藏层的步幅值为 2，填充值为 1，因此它们的输出图像尺寸将是输入图像的一半。随着图像在更深层次的尺寸增大，通道的数量会翻倍。卷积层中的所有卷积核大小为
    4 x 4。输出层的步幅值为 1，填充值为 0。它将 4 x 4 的特征图映射为单一值，以便 Sigmoid 函数能够将该值转换为预测置信度。
- en: Creating a DCGAN with PyTorch
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 PyTorch 创建 DCGAN
- en: Let's start writing PyTorch code to create a DCGAN model. Here, we assume that
    you are using the Python 3.7 environment in Ubuntu 18.04\. If not, please refer
    to [Chapter 2](4459c703-9610-43e7-9eda-496d63a45924.xhtml), *Getting Started with
    PyTorch 1.3*, to learn how to create an Anaconda environment.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始编写 PyTorch 代码来创建一个 DCGAN 模型。在这里，我们假设你正在使用 Ubuntu 18.04 的 Python 3.7
    环境。如果不是，请参考 [第 2 章](4459c703-9610-43e7-9eda-496d63a45924.xhtml)，*PyTorch 1.3 入门*，了解如何创建
    Anaconda 环境。
- en: 'First, let''s create a Python source file called `dcgan.py` and import the
    packages that we need:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建一个名为 `dcgan.py` 的 Python 源文件，并导入我们需要的包：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, NumPy is only used to initialize a random seed. If you don't have NumPy
    installed, simple replace `np.random` with `random` and insert the `import random`
    line after `import os`. In the last line of code, we import a module called `utils`,
    which is a custom utility package defined in the `utils.py` file. The full source
    code of `utils.py` is available under the code repository for this chapter.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，NumPy 仅用于初始化随机种子。如果你没有安装 NumPy，只需将 `np.random` 替换为 `random`，并在 `import os`
    后插入 `import random`。在代码的最后一行，我们导入了一个名为 `utils` 的模块，它是一个自定义的实用程序包，定义在 `utils.py`
    文件中。`utils.py` 的完整源代码可以在本章节的代码仓库中找到。
- en: In this book, we will put most of the PyTorch-independent helper functions (including
    file organization, learning rate adjustment, logging, tensor visualization, and
    so on) in this `utils.py` file. Therefore, we will also come across this module
    in future chapters. Don't forget to update this file as we move on to later chapters.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将把大部分与 PyTorch 无关的辅助函数（包括文件组织、学习率调整、日志记录、张量可视化等）放在 `utils.py` 文件中。因此，在未来的章节中我们还会遇到这个模块。随着章节的推进，别忘了更新这个文件。
- en: 'Then, we define the output path and hyperparameters. Note that here we set
    the minimal channel size of hidden layers in both the generator and discriminator
    to `64`, because we find that the value of `128` as we previously show could lead
    to the overfitting of the discriminator:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义输出路径和超参数。请注意，这里我们将生成器和判别器中隐藏层的最小通道大小设置为 `64`，因为我们发现之前展示的 `128` 可能导致判别器的过拟合：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If you don't have a CUDA-enabled graphics card and want to train the networks
    on the CPU, you can change `CUDA` to `False`. `DATA_PATH` points to the root directory
    of the MNIST dataset. If you haven't downloaded and properly preprocessed MNIST
    yet, simply point it to any directory (such as `'.'`) and we can download it later. `BATCH_SIZE`
    has a major impact on how much GPU memory your code will consume. If you are not
    sure what batch size is appropriate for your system, you can start at a small
    value, train your model for 1 epoch, and double the batch size until errors pop
    up.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有 CUDA 支持的显卡，并且想在 CPU 上训练网络，你可以将 `CUDA` 设置为 `False`。`DATA_PATH` 指向 MNIST
    数据集的根目录。如果你还没有下载并正确预处理 MNIST，只需将其指向任意目录（例如 `'.'`），稍后我们可以下载数据。`BATCH_SIZE` 会对代码消耗的
    GPU 内存量产生重大影响。如果你不确定哪个批量大小适合你的系统，可以从一个较小的值开始，训练模型 1 个 epoch，然后将批量大小加倍，直到出现错误。
- en: For MNIST, setting `BATCH_SIZE` to 128 should be good enough and it costs less
    than 1 GB of GPU memory. `IMAGE_CHANNEL` describes the number of color channels
    of image samples. Since all images in MNIST are single-channel, we should set
    it to 1. `EPOCH_NUM` has a great impact on the training time of neural networks.
    If you want better results, setting a larger epoch number and small learning rates
    is almost always a good strategy. We set `seed=1` so that your results should
    look exactly the same as what we get in this book.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 MNIST，设置 `BATCH_SIZE` 为 128 应该足够，而且 GPU 内存消耗不到 1GB。`IMAGE_CHANNEL` 描述图像样本的颜色通道数。由于
    MNIST 中的所有图像都是单通道的，因此我们应该将其设置为 1。`EPOCH_NUM` 对神经网络的训练时间有很大的影响。如果你希望得到更好的结果，通常将
    epoch 数量设置大一点，学习率设置小一点是个不错的策略。我们将 `seed=1`，这样你的结果应该和我们在本书中得到的结果完全一致。
- en: 'Next, we need to do some preparation before creating the networks:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在创建网络之前，我们需要做一些准备工作：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here, `utils.clear_folder(OUT_PATH)` will empty the output folder for us and
    create one if it doesn't exist. `sys.stdout = utils.StdOut(LOG_FILE)` will redirect
    all messages from `print` to the log file and show these messages in the console
    at the same time. Refer to the `utils.py` file if you are interested in the implementations. `cudnn.benchmark
    = True` will tell cuDNN to choose the best set of algorithms for your model if
    the size of input data is fixed; otherwise, cuDNN will have to find the best algorithms
    at each iteration.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`utils.clear_folder(OUT_PATH)` 将清空输出文件夹，并在该文件夹不存在时创建一个。`sys.stdout = utils.StdOut(LOG_FILE)`
    将把所有 `print` 的消息重定向到日志文件，并同时在控制台显示这些消息。如果你对实现感兴趣，可以参考 `utils.py` 文件。`cudnn.benchmark
    = True` 将告诉 cuDNN 为你的模型选择最优的算法集，如果输入数据的大小是固定的；否则，cuDNN 将在每次迭代时都寻找最佳算法。
- en: If you have previously done some training tasks on CNNs with PyTorch, you might
    notice that, sometimes, setting `cudnn.benchmark = True` will dramatically increase
    the GPU memory consumption, especially when your model architectures are changed
    during training and you are doing both training and evaluation in your code. Change
    it to `False` if you encounter strange **OOM** (**Out-Of-Memory**) issues.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你之前在使用 PyTorch 进行 CNN 训练时遇到过问题，你可能会注意到，有时候设置 `cudnn.benchmark = True` 会显著增加
    GPU 内存消耗，特别是在模型架构在训练过程中发生变化且你在代码中同时进行训练和评估时。如果遇到奇怪的 **OOM** (**内存溢出**) 问题，请将其改为
    `False`。
- en: Generator network
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成器网络
- en: 'Now, let''s define the generator network with PyTorch:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用 PyTorch 来定义生成器网络：
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that the output layer does not have a batch normalization layer connected
    to it.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，输出层没有连接批归一化层。
- en: 'Let''s create a `helper` function to initialize the network parameters:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个 `helper` 函数来初始化网络参数：
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'There are only two types of layers in the generator network that contain trainable
    parameters: transposed convolution layers and batch normalization layers. Here,
    we initialize the convolution kernels based on the Gaussian distribution (normal
    distribution) with a mean of 0 and a standard deviation of 0.02\. We also need
    to initialize the affine parameters (scaling factors) in batch normalization.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成器网络中，只有两种类型的层包含可训练的参数：转置卷积层和批归一化层。在这里，我们根据高斯分布（正态分布）初始化卷积核，均值为 0，标准差为 0.02。我们还需要初始化批归一化中的仿射参数（缩放因子）。
- en: 'Now, we can create a `Generator` object, as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以按照以下方式创建一个 `Generator` 对象：
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We can check what modules are contained in the generator network by directly
    printing it. We won't show the output of it considering its length.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过直接打印生成器网络来检查其中包含的模块。考虑到输出的长度，我们不会显示它的输出。
- en: Discriminator network
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 判别器网络
- en: 'Now, let''s define the discriminator network:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义判别器网络：
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note that the input layer does not have a batch normalization layer connected
    to it. This is because, when applying batch normalization to all layers, it could
    lead to sample oscillation and model instability, as pointed out in the original
    paper.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，输入层没有连接批归一化层。这是因为，当将批归一化应用到所有层时，可能会导致样本震荡和模型不稳定，正如原始论文中所指出的那样。
- en: 'Similarly, we can create a `Discriminator` object as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以按以下方式创建一个 `Discriminator` 对象：
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Model training and evaluation
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型训练与评估
- en: We will use Adam as the training method for both the generator and discriminator
    networks. If you are interested in the details of gradient descent methods, please
    refer to [Chapter 3](8aa2141f-1f14-405f-a5e6-31daf5f4163a.xhtml), *Best Practices
    for Model Design and Training**,* to learn more about the common training methods.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first define the loss function for the discriminator network and `optimizers` for
    both of the networks:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Here, `nn.BCELoss()` represents the Binary Cross-Entropy loss function, which
    we previously used in [Chapter 1](66a945c3-9fd3-4d27-a6ec-b47d2e299e84.xhtml), *Generative
    Adversarial Networks Fundamentals*.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s load the MNIST dataset to the GPU memory:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You can also add a `pin_memory=True` argument when calling `torch.utils.data.DataLoader()` on
    small datasets, which will make sure data is stored at fixed GPU memory addresses
    and thus increase the data loading speed during training.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Training iteration
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The training procedure is basically the same as in the simple example in [Chapter
    1](66a945c3-9fd3-4d27-a6ec-b47d2e299e84.xhtml), *Generative Adversarial Networks
    Fundamentals*:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Train the discriminator with the real data and recognize it as real.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the discriminator with the fake data and recognize it as fake.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the generator with the fake data and recognize it as real.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The first two steps let the discriminator learn how to tell the difference
    between real data and fake data. The third step teaches the generator how to confuse
    the discriminator with generated samples:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Here, we create the `real_label` and `fake_label` tensors in real time because
    there is no guarantee that all sample batches will have the same size (the last
    batch is often smaller depending on the batch size and the total number of training
    samples).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing generated samples
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It''s better if we can check how well the generator is trained. Therefore,
    we need to export the generated images during training. Add these lines at the
    end of the `if` scope:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, your DCGAN is ready for training. Open the Terminal, `activate` the Anaconda
    environment and start training DCGAN:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The training takes about 13 minutes on a GTX 1080Ti graphics card. If you don't
    like the generated samples even before the training is finished, you can always
    press <q>Ctrl</q> + <q>C</q> to cancel the training. The generated images after
    the 1^(st) and 25^(th) epoch are shown in the following. Note that we only show
    halves of the generated images (that is, 64 samples).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that the DCGAN does a good job at generating handwritten digits:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/301cf81f-ede8-408f-884f-9d4db4c6ac17.png)![](img/9d465db8-37e7-4265-ba85-5008aaa3ce4c.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
- en: Generated images by the DCGAN from MNIST after the 1st and 25th epoch
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'For your reference, here is a list of GPU memory consumption with different
    `BATCH_SIZE` values. Note that no matter how large the batch size is, the total
    training time is almost unchanged, since the total workload of the computation
    is basically the same:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '| Batch size | 128 | 256 | 512 | 1024 | 2048 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
- en: '| GPU memory | 939 MB | 1283 MB | 1969 MB | 3305 MB | 6011 MB |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
- en: Checking GPU usage information
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here, we will talk about how to check GPU usage along with other hardware usage
    information in Windows 10 and Ubuntu 18.04.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: In Windows 10, the easiest way to check hardware usage (including GPU usage)
    is using Task Manager. You can open it by pressing <q>Ctrl</q> + <q>Shift</q>
    + <q>Esc</q>, and switch to the Performance panel. All hardware usage information
    is available to you now.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: In Ubuntu 18.04, you can check CPU, RAM, and drive usage with **GNOME System
    Monitor**, which is shipped with the system. You can search for the system monitor
    in the Application menu, or run `gnome-system-monitor` in a Terminal to open it.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, you can install a GNOME extension to illustrate the usage graphs
    in the status bar. We recommend that you use the** system-monitor extension**
    ([https://extensions.gnome.org/extension/120/system-monitor](https://extensions.gnome.org/extension/120/system-monitor))
    for this purpose. To install it, you first need to install several prerequisites:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Then, open a Firefox browser and navigate to this site, [https://addons.mozilla.org/en-US/firefox/addon/gnome-shell-integration](https://addons.mozilla.org/en-US/firefox/addon/gnome-shell-integration/),
    to install the browser extension for easy installation of GNOME extensions provided
    by [http://gnome.org](http://gnome.org). You also need to run `sudo apt-get install
    chrome-gnome-shell` in Terminal.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Next, open the web page, [https://extensions.gnome.org/extension/120/system-monitor](https://extensions.gnome.org/extension/120/system-monitor),
    with the Firefox browser; you'll see a switch button on the right side of the
    extension title. Click it to switch it to `ON` and you will be prompted to install
    the system-monitor extension.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Finally, press *Alt* + *F2*, type in `r`, and then press <q>Enter</q>. This
    will restart the GNOME shell so that the system-monitor extension will be activated.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: 'To check GPU usage in Ubuntu, you can run this script in the Terminal to show
    it in real time:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: You can also create a `.sh` file in a convenient directory, for example, `~/gpu.sh`: copy
    the script into this file, then run `chmod +x ~/.gpu.sh`. Then, you can simply
    run `./gpu.sh` in the Terminal whenever you need to check GPU usage.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, there are many other tools you can use on Ubuntu, for example,
    NVTOP ([https://github.com/Syllo/nvtop](https://github.com/Syllo/nvtop)).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Moving to larger datasets
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generating digits is fun. We can have way more fun generating other stuff, such
    as human faces and bedroom photos. To generate good complex images like these,
    we need more training samples than the 60,000 samples that MNIST offers. In this
    section, we will download two much larger datasets (CelebA and LSUN) and train
    the DCGAN on them to get more complex generated samples.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Generating human faces from the CelebA dataset
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The CelebFaces Attributes (**CelebA**, [http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html))
    dataset is a large-scale face attributes dataset with more than 200,000 celebrity
    images, each with 40 attribute annotations. We need to download the cropped and
    aligned images. We won't need any attribute annotation here so we only need to
    download the file named `img_align_celeba.zip`, which is no more than 2 GB in
    size.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: If you can't download the CelebA dataset from official links, try these links
    provided by Kaggle and the official PyTorch tutorial: [https://www.kaggle.com/jessicali9530/celeba-dataset](https://www.kaggle.com/jessicali9530/celeba-dataset)
    and [https://drive.google.com/drive/folders/0B7EVK8r0v71pWEZsZE9oNnFzTm8](https://drive.google.com/drive/folders/0B7EVK8r0v71pWEZsZE9oNnFzTm8).
    Note that you only need to download `Img/img_align_celeba.zip` from the Google
    Drive link.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Extract the downloaded images to a directory, for example, `~/Data/CelebA`.
    Make sure all your images are contained in an individual directory inside this
    root directory so that the images are stored at a location such as `~/Data/CelebA/img_align_celeba/000001.png`.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: If you have a **Solid**-**State Drive** (**SSD**) with enough space plugged
    in your machine, we highly recommend you move all of your training samples to
    the SSD, especially when you have a powerful graphics card. Because when you are
    training neural networks on a very large dataset, which cannot fit in the GPU
    memory, the reading speed from physical drives could be the bottleneck of your
    training performance. Sometimes, the speed-up of SSD (reading samples at 50 MB/s)
    over the traditional hard drive (5 MB/s) can save you a big chunk of training
    time.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'We only need to alter 3 different parts of code in the previous section to
    train the DCGAN on the CelebA dataset:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: 'Change the dataset root directory:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: If you are not sure what absolute path you're currently at in the file manager
    on Ubuntu, simply press *Ctrl* + *L* and the full path will show up.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'Change the image channel number:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Redefine the `dataset` object:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, let''s run `python dcgan.py` in the Terminal and wait for a while. It
    takes about 88 minutes to finish 25 epochs of training on a GTX 1080Ti graphics
    card. The generated images after the 1^(st) epoch and the 25^(th) epoch are shown
    in the following. Again, we only show 64 generated samples:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ea55d23e-5e10-419c-a8ca-ddae7ed7a0f6.png)![](img/8ecc9a1e-f556-43f7-924d-87d57677506c.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
- en: Generated images by the DCGAN from CelebA after the 1st and 25th epoch
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a list of GPU memory consumption with different `BATCH_SIZE` values:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '| Batch size | 64 | 128 | 256 | 512 | 1024 | 2048 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
- en: '| GPU memory | 773 MB | 963 MB | 1311 MB | 2029 MB | 3441 MB | 6283 MB |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
- en: Generating bedroom photos from the LSUN dataset
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'LSUN (Large-scale Scene Understanding, [https://www.yf.io/p/lsun](https://www.yf.io/p/lsun))
    is a large image dataset with 10 scene categories and 20 object categories. You
    can get the downloading toolkit from [https://github.com/fyu/lsun](https://github.com/fyu/lsun).
    We will use the `bedroom` category to train our DCGAN, which has more than 3 million
    bedroom photos:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: LSUN（大规模场景理解，[https://www.yf.io/p/lsun](https://www.yf.io/p/lsun)）是一个包含10个场景类别和20个物体类别的大型图像数据集。你可以从[https://github.com/fyu/lsun](https://github.com/fyu/lsun)获取下载工具包。我们将使用`bedroom`类别来训练我们的DCGAN，它包含超过300万张卧室照片：
- en: '[PRE18]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: You can also export the images as individual files with `python data.py export
    bedroom_train_lmdb --out_dir` `bedroom_train_img` so that you can easily use these
    images for other projects. But try not to directly open the image folder with
    your file manager. It will take a lot of RAM and time.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用`python data.py export bedroom_train_lmdb --out_dir` `bedroom_train_img`将图像导出为单独的文件，这样你就可以轻松地将这些图像用于其他项目。但尽量不要直接通过文件管理器打开图像文件夹，因为这会占用大量内存和时间。
- en: The dataset is contained in an **LMDB** (**Lightning Memory-Mapped Database
    Manager**) database file, which is about 54 GB in size. Make sure the database
    files are located in the `bedroom_train_lmdb` directory so that PyTorch's data
    loader can recognize it when the root directory is specified.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集保存在**LMDB**（**Lightning Memory-Mapped Database Manager**）数据库文件中，文件大小约为54GB。确保数据库文件位于`bedroom_train_lmdb`目录下，以便PyTorch的数据加载器在指定根目录时能够识别它。
- en: 'Similarly, we only need to change 3 parts of the code to use the LSUN dataset
    for our model:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们只需要更改代码中的3个部分，就能使用LSUN数据集来训练我们的模型：
- en: 'Change the dataset root directory:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更改数据集根目录：
- en: '[PRE19]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Change the image channel number:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更改图像通道数：
- en: '[PRE20]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Redefine the `dataset` object:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新定义`dataset`对象：
- en: '[PRE21]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'And don''t forget to install the `lmdb` library for Python so that we can read
    the database file:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 别忘了为Python安装`lmdb`库，这样我们才能读取数据库文件：
- en: '[PRE22]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now, let''s save the source file and run `python dcgan.py` in the Terminal.
    Since there are way more samples in the LSUN dataset, we don''t have to train
    the model for 25 epochs. Some of the generated images are already impressive even
    after the 1^(st) epoch of training. It takes about 5 hours to train for 5 epochs
    on a GTX 1080Ti graphics card. The generated images after the 1^(st) epoch and
    the 25^(th) epoch are shown in the following. Here, we only show 64 generated
    samples. We will not show the GPU memory consumption for LSUN because it''s almost
    the same as CelebA since the input images are both 3-channel and the network structure
    is not changed:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，保存源文件并在终端中运行`python dcgan.py`。由于LSUN数据集中有更多的样本，我们不需要训练25个epoch。一些生成的图像在第1个epoch训练后就已经相当出色了。在GTX
    1080Ti显卡上，训练5个epoch大约需要5小时。以下是第1个epoch和第25个epoch生成的图像。这里我们只展示64个生成样本。由于LSUN和CelebA的输入图像都是3通道，并且网络结构没有变化，我们不展示LSUN的GPU内存消耗，它几乎与CelebA相同：
- en: '![](img/9163ef11-96c4-42ca-8a45-89e602f19f80.png)![](img/71e1ad9d-6e46-4777-81a9-62fabc9f9df1.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9163ef11-96c4-42ca-8a45-89e602f19f80.png)![](img/71e1ad9d-6e46-4777-81a9-62fabc9f9df1.png)'
- en: Generated images by the DCGAN from LSUN after the 1st and 5th epochs
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: DCGAN生成的LSUN图像，分别是在第1和第5个epoch后生成的
- en: 'Again, we''d like to point out that if you plan on training GANs on a large
    dataset, always consider using powerful GPUs and putting your dataset on an SSD.
    Here, we give two sets of performance comparisons. In the first configuration,
    we use an NVIDIA GTX 960 graphics card and put the training set on an **HDD**
    (**hard disk drive**). In the second configuration, we use an NVIDIA GTX 1080Ti
    graphics card and put the training set on an SSD. We can see the speedup of the
    powerful platform is life changing:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，如果你计划在大数据集上训练GAN，务必考虑使用强大的GPU，并将数据集放在SSD上。这里，我们提供了两组性能对比。在第一种配置中，我们使用NVIDIA
    GTX 960显卡，并将训练集放在**HDD**（**硬盘驱动器**）上。在第二种配置中，我们使用NVIDIA GTX 1080Ti显卡，并将训练集放在SSD上。我们可以看到强大平台的加速效果堪称改变人生：
- en: '| Dataset | CelebA | LSUN |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | CelebA | LSUN |'
- en: '| GTX 960 + HDD | 2 hours/epoch | 16.6 hours/epoch |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| GTX 960 + HDD | 2小时/epoch | 16.6小时/epoch |'
- en: '| GTX 1080Ti + SSD | 3.5 minutes/epoch | 53 minutes/epoch |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| GTX 1080Ti + SSD | 3.5分钟/epoch | 53分钟/epoch |'
- en: '| Speedup | 34X | 19X |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 加速 | 34X | 19X |'
- en: Having fun with the generator network
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 玩转生成器网络
- en: Now that our first image generator is trained, aren't you curious about what
    it is capable of and how images are generated from random noise vectors? In this
    section, we will have some fun with the generator network. First, we will choose
    two random vectors and calculate the interpolation between them to see what images
    will be generated. Second, we will choose some exemplary vectors and perform arithmetic
    calculations on them to find out what changes appear in the generated samples.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: First, we need a test version of the DCGAN code.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: 'Copy your original `dcgan.py` file to `dcgan_test.py`. Next, we need to make
    some changes to our new file. First, we need to replace these lines of just the
    `Generator` class:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We replace them with the following lines (you can either delete them or simply
    comment them out):'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Next, we need to remove (or comment out) the `weights_init`, `Discriminator`,
    `dataset`, `dataloader`, `criterion`, and `optimizer` objects.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to replace the entire training iteration section with this:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We''re almost done. We need to add the following code at the end:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now, go back to the top of the code file and add a line in the `import` section:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'And finally, we need to add a line to the variable definitions. Just after
    the line that says `CUDA = True`, add this:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The values for `VIZ_MODE` are 0 for random, 1 for interpolation, and 2 for semantic
    calculation. This will be used as we move forward through the three sets of code.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: We need to export the input vector and the generated images to file. The full
    code for the DCGAN testing is available under the code repository for this chapter,
    which is called `dcgan_test.py`. And don't forget to delete or comment out the `utils.clear_folder(OUT_PATH)` line; otherwise,
    all your training results will be deleted, which would be a bad thing.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Image interpolation
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The generator network maps the input random vector (the latent vector) to a
    generated image. If we perform linear interpolation on the latent vectors, the
    corresponding output images also obey the interpolation relation. Let's take the
    trained model on CelebA, for example.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s randomly choose two vectors that generate clean images. Here,
    we set `BATCH_SIZE=10` for simplicity. We''ll also add the beginnings of an `if`
    conditional to allow easy selection of what parts of the code to run:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The generated images may look like the following. And the latent vectors for
    these images are exported to a file (for example, `vec_20190317-223131.txt`):'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dd07d801-4807-49ee-868b-e5d3b3d01a51.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
- en: Randomly generated images
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'Assume that we choose the 3^(rd) and the last images for interpolation. Now,
    let''s perform linear interpolation on their latent vectors with SciPy (replace
    the previous line starting with `viz_tensor = ...` with the following lines).
    Be sure to change the filename to the one that was just generated on your system:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'You will also need to change the `VIZ_MODE` flag from `0` to `1` for interpolation:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now, run your changed source code. The corresponding generated images are as
    follows:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bdb7d87f-3f8b-4aef-99a8-d4d3e5ef1b6a.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
- en: Image interpolation
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the image on the left is smoothly transformed into the one on
    the right. Therefore, we know that the interpolation of the latent vectors leads
    to the interpolation of generated images.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Semantic vector arithmetic
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear interpolation is one of the basic methods in linear algebra. We can do
    a lot more with arithmetic calculations on the latent vectors.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Take the randomly generated images from previous steps. We notice that some
    images are of smiling women (the 1^(st), 7^(th), and 9^(th) images), some women's
    images are not smiling (the 2^(nd), 3^(rd), and 5^(th) images), and none of the
    men in the images are smiling. Man, aren't they serious! How do we put a smile
    on a man's face without regenerating a new set of random vectors?
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, imagine we can solve it with arithmetic calculations:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: <q>[smiling woman] - [woman] = [smile]</q>
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: <q>[smile] + [man] = [smiling man]</q>
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Can we do that? Let's try it!
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'First, set the `VIS_MODE` flag again, this time to `2` for semantic calculations:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Next, continue the `if` conditional with the following code. Once again, use
    the filename that was created earlier:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Here, by performing `z1-z2`, we get a smiling vector. And `z3` gives us a man
    vector. Adding them together will give us the following results. We use the mean
    vector of 3 different latent vectors for more stable results and we add small
    random values to the arithmetic results to introduce a slight randomness:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1f894278-6379-4d9b-b801-301b8781e45d.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
- en: Vector arithmetic on latent vectors
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 'The vector arithmetic calculation process can be described as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/94aa789f-d6b6-4411-a4b2-4452eb8e0740.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
- en: Vector arithmetic
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Out of curiosity, we directly generate images based on *z1-z2*, which gives
    us the sample at the bottom right in the previous screenshot. We can tell it's
    a smiling face, but the rest of the face is rather unnatural. It looks like the
    face of a strange dude.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have unlocked the potential of GANs on manipulating the attributes of
    the generated images. However, the results are not natural and authentic enough.
    In the next chapter, we will learn how to generate samples with the exact attributes
    we desire.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We spent a tremendous amount of time learning about Deep Convolutional GANs
    in this chapter. We dealt with the MNIST dataset as well as two huge datasets
    in the form of the CelebA and LSUN datasets. We also consumed a large number of
    computing cycles. Hopefully, you have a good grasp of DCGANs at this point.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll look at a **Conditional GAN** (**CGAN**) and how to add label information
    during the training process. Let's get going!
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: References and useful reading list
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hui J. (2018, Jun 21). GAN — *Why it is so hard to train Generative Adversarial
    Networks!*. Retrieved from [https://medium.com/@jonathan_hui/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b](https://medium.com/@jonathan_hui/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
