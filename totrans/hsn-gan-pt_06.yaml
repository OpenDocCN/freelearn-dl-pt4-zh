- en: Building Your First GAN with PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous chapters, we covered the idea of using adversarial learning to generate
    simple signals with NumPy and learned about the new features and capabilities
    of PyTorch 1.3\. It's time for us to use PyTorch to train a GAN model for generating
    interesting samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will introduce you to a classic and well-performing GAN
    model, called DCGAN, to generate 2D images. You will learn the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of DCGANs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The training and evaluation of DCGANs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a DCGAN to generate handwritten digits, human faces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Having fun with the generator network by performing image interpolation and
    arithmetic calculation on the latent vectors to change the image attributes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have grasped the core architecture design
    of GAN models for generating image data and have a better understanding of the
    relationship between latent vectors and generated samples.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Deep Convolutional GANs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**DCAGN** (**Deep Convolutional Generative Adversarial ****Network**) is one
    of the early well-performing and stable approaches to generate images with adversarial
    training. Let''s take a look back at the simple example in [Chapter 1](66a945c3-9fd3-4d27-a6ec-b47d2e299e84.xhtml), *Generative
    Adversarial Networks Fundamentals*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, even when we only train a GAN to manipulate 1D data, we have to use multiple
    techniques to ensure a stable training. A lot of things could go wrong in the
    training of GANs. For example, either a generator or a discriminator could overfit
    if one or the other does not converge. Sometimes, the generator only generates
    a handful of sample varieties. This is called **mode collapse**. The following
    is an example of mode collapse, where we want to train a GAN with some popular
    meme images in China called **Baozou**. We can see that our GAN is only capable
    of generating one or two memes at a time. Problems that commonly occur in other
    machine learning algorithms such as gradient vanishing/explosion and underfitting
    can also look familiar in the training of GANs. Therefore, just replacing 1D data
    with 2D images won''t easily guarantee successful training:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6e458f91-d634-486e-abc7-1b48a8d0150c.png)![](img/668363a6-4abe-4656-9aff-65bb9c3a0a9c.png)![](img/949bf1ca-dc4d-461d-9c55-1a5134f91129.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Mode collapse in GAN training (left: some training samples; middle: results
    at 492nd iteration; right: results at 500th iteration)'
  prefs: []
  type: TYPE_NORMAL
- en: 'To ensure the stable training of GANs on image data like this, a DCGAN uses
    three techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting rid of fully connected layers and only using convolution layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using strided convolution layers to perform downsampling, instead of using pooling
    layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using ReLU/leakyReLU activation functions instead of Tanh between hidden layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we will introduce the architectures of the generator and discriminator
    of the DCGAN and learn how to generate images with it. We'll use MNIST ([http://yann.lecun.com/exdb/mnist](http://yann.lecun.com/exdb/mnist))
    samples to illustrate the architecture of a DCGAN and use it to train the model
    in the next two sections.
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of generator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The generator network of a DCGAN contains 4 hidden layers (we treat the input
    layer as the 1^(st) hidden layer for simplicity) and 1 output layer. Transposed
    convolution layers are used in hidden layers, which are followed by batch normalization
    layers and ReLU activation functions. The output layer is also a transposed convolution
    layer and Tanh is used as the activation function. The architecture of the generator
    is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/383259d3-c12d-4c77-91b4-7286a0fbe20d.png)'
  prefs: []
  type: TYPE_IMG
- en: Generator architecture in DCGAN
  prefs: []
  type: TYPE_NORMAL
- en: The 2^(nd), 3^(rd),and 4^(th) hidden layers and the output layer have a stride
    value of 2\. The 1^(st) layer has a padding value of 0 and the other layers have
    a padding value of 1\. As the image (feature map) sizes increase by two in deeper
    layers, the numbers of channels are decreasing by half. This is a common convention
    in the architecture design of neural networks. All kernel sizes of transposed
    convolution layers are set to 4 x 4\. The output channel can be either 1 or 3,
    depending on whether you want to generate grayscale images or color images.
  prefs: []
  type: TYPE_NORMAL
- en: The transposed convolution layer can be considered as the **reverse process**
    of a normal convolution. It was once called by some a deconvolution layer, which
    is misleading because the transposed convolution is not the **inverse** of convolution.
    Most convolution layers are not invertible, because they are ill-conditioned (have
    extremely large condition numbers) from the linear algebra perspective, which
    makes their pseudoinverse matrices unfit for representing the inverse process.
    If you are interested in finding the inverse of a convolution kernel, you can
    search for numerical deconvolution methods on the internet.
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of a discriminator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The discriminator network of a DCGAN consists of 4 hidden layers (again, we
    treat the input layer as the 1^(st) hidden layer) and 1 output layer. Convolution
    layers are used in all layers, which are followed by batch normalization layers
    except that the first layer does not have batch normalization. LeakyReLU activation
    functions are used in the hidden layers and Sigmoid is used for the output layer.
    The architecture of the discriminator is shown in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0a1cf07f-9051-42ec-ad32-e29f8eae7b55.png)'
  prefs: []
  type: TYPE_IMG
- en: Discriminator architecture in DCGAN
  prefs: []
  type: TYPE_NORMAL
- en: The input channel can be either 1 or 3, depending on whether you are dealing
    with grayscale images or color images. All hidden layers have a stride value of
    2 and a padding value of 1 so that their output image sizes will be half the input
    images. As image sizes increase in deeper layers, the numbers of channels are
    increasing by twice. All kernels in convolution layers are of a size of 4 x 4. The
    output layer has a stride value of 1 and a padding value of 0\. It maps 4 x 4
    feature maps to single values so that the Sigmoid function can transform the value
    into prediction confidence.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a DCGAN with PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's start writing PyTorch code to create a DCGAN model. Here, we assume that
    you are using the Python 3.7 environment in Ubuntu 18.04\. If not, please refer
    to [Chapter 2](4459c703-9610-43e7-9eda-496d63a45924.xhtml), *Getting Started with
    PyTorch 1.3*, to learn how to create an Anaconda environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s create a Python source file called `dcgan.py` and import the
    packages that we need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Here, NumPy is only used to initialize a random seed. If you don't have NumPy
    installed, simple replace `np.random` with `random` and insert the `import random`
    line after `import os`. In the last line of code, we import a module called `utils`,
    which is a custom utility package defined in the `utils.py` file. The full source
    code of `utils.py` is available under the code repository for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In this book, we will put most of the PyTorch-independent helper functions (including
    file organization, learning rate adjustment, logging, tensor visualization, and
    so on) in this `utils.py` file. Therefore, we will also come across this module
    in future chapters. Don't forget to update this file as we move on to later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we define the output path and hyperparameters. Note that here we set
    the minimal channel size of hidden layers in both the generator and discriminator
    to `64`, because we find that the value of `128` as we previously show could lead
    to the overfitting of the discriminator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: If you don't have a CUDA-enabled graphics card and want to train the networks
    on the CPU, you can change `CUDA` to `False`. `DATA_PATH` points to the root directory
    of the MNIST dataset. If you haven't downloaded and properly preprocessed MNIST
    yet, simply point it to any directory (such as `'.'`) and we can download it later. `BATCH_SIZE`
    has a major impact on how much GPU memory your code will consume. If you are not
    sure what batch size is appropriate for your system, you can start at a small
    value, train your model for 1 epoch, and double the batch size until errors pop
    up.
  prefs: []
  type: TYPE_NORMAL
- en: For MNIST, setting `BATCH_SIZE` to 128 should be good enough and it costs less
    than 1 GB of GPU memory. `IMAGE_CHANNEL` describes the number of color channels
    of image samples. Since all images in MNIST are single-channel, we should set
    it to 1. `EPOCH_NUM` has a great impact on the training time of neural networks.
    If you want better results, setting a larger epoch number and small learning rates
    is almost always a good strategy. We set `seed=1` so that your results should
    look exactly the same as what we get in this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to do some preparation before creating the networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Here, `utils.clear_folder(OUT_PATH)` will empty the output folder for us and
    create one if it doesn't exist. `sys.stdout = utils.StdOut(LOG_FILE)` will redirect
    all messages from `print` to the log file and show these messages in the console
    at the same time. Refer to the `utils.py` file if you are interested in the implementations. `cudnn.benchmark
    = True` will tell cuDNN to choose the best set of algorithms for your model if
    the size of input data is fixed; otherwise, cuDNN will have to find the best algorithms
    at each iteration.
  prefs: []
  type: TYPE_NORMAL
- en: If you have previously done some training tasks on CNNs with PyTorch, you might
    notice that, sometimes, setting `cudnn.benchmark = True` will dramatically increase
    the GPU memory consumption, especially when your model architectures are changed
    during training and you are doing both training and evaluation in your code. Change
    it to `False` if you encounter strange **OOM** (**Out-Of-Memory**) issues.
  prefs: []
  type: TYPE_NORMAL
- en: Generator network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s define the generator network with PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Note that the output layer does not have a batch normalization layer connected
    to it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a `helper` function to initialize the network parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'There are only two types of layers in the generator network that contain trainable
    parameters: transposed convolution layers and batch normalization layers. Here,
    we initialize the convolution kernels based on the Gaussian distribution (normal
    distribution) with a mean of 0 and a standard deviation of 0.02\. We also need
    to initialize the affine parameters (scaling factors) in batch normalization.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can create a `Generator` object, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We can check what modules are contained in the generator network by directly
    printing it. We won't show the output of it considering its length.
  prefs: []
  type: TYPE_NORMAL
- en: Discriminator network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s define the discriminator network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Note that the input layer does not have a batch normalization layer connected
    to it. This is because, when applying batch normalization to all layers, it could
    lead to sample oscillation and model instability, as pointed out in the original
    paper.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, we can create a `Discriminator` object as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Model training and evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will use Adam as the training method for both the generator and discriminator
    networks. If you are interested in the details of gradient descent methods, please
    refer to [Chapter 3](8aa2141f-1f14-405f-a5e6-31daf5f4163a.xhtml), *Best Practices
    for Model Design and Training**,* to learn more about the common training methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first define the loss function for the discriminator network and `optimizers` for
    both of the networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Here, `nn.BCELoss()` represents the Binary Cross-Entropy loss function, which
    we previously used in [Chapter 1](66a945c3-9fd3-4d27-a6ec-b47d2e299e84.xhtml), *Generative
    Adversarial Networks Fundamentals*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s load the MNIST dataset to the GPU memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: You can also add a `pin_memory=True` argument when calling `torch.utils.data.DataLoader()` on
    small datasets, which will make sure data is stored at fixed GPU memory addresses
    and thus increase the data loading speed during training.
  prefs: []
  type: TYPE_NORMAL
- en: Training iteration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The training procedure is basically the same as in the simple example in [Chapter
    1](66a945c3-9fd3-4d27-a6ec-b47d2e299e84.xhtml), *Generative Adversarial Networks
    Fundamentals*:'
  prefs: []
  type: TYPE_NORMAL
- en: Train the discriminator with the real data and recognize it as real.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the discriminator with the fake data and recognize it as fake.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the generator with the fake data and recognize it as real.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The first two steps let the discriminator learn how to tell the difference
    between real data and fake data. The third step teaches the generator how to confuse
    the discriminator with generated samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Here, we create the `real_label` and `fake_label` tensors in real time because
    there is no guarantee that all sample batches will have the same size (the last
    batch is often smaller depending on the batch size and the total number of training
    samples).
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing generated samples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It''s better if we can check how well the generator is trained. Therefore,
    we need to export the generated images during training. Add these lines at the
    end of the `if` scope:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, your DCGAN is ready for training. Open the Terminal, `activate` the Anaconda
    environment and start training DCGAN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The training takes about 13 minutes on a GTX 1080Ti graphics card. If you don't
    like the generated samples even before the training is finished, you can always
    press <q>Ctrl</q> + <q>C</q> to cancel the training. The generated images after
    the 1^(st) and 25^(th) epoch are shown in the following. Note that we only show
    halves of the generated images (that is, 64 samples).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that the DCGAN does a good job at generating handwritten digits:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/301cf81f-ede8-408f-884f-9d4db4c6ac17.png)![](img/9d465db8-37e7-4265-ba85-5008aaa3ce4c.png)'
  prefs: []
  type: TYPE_IMG
- en: Generated images by the DCGAN from MNIST after the 1st and 25th epoch
  prefs: []
  type: TYPE_NORMAL
- en: 'For your reference, here is a list of GPU memory consumption with different
    `BATCH_SIZE` values. Note that no matter how large the batch size is, the total
    training time is almost unchanged, since the total workload of the computation
    is basically the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Batch size | 128 | 256 | 512 | 1024 | 2048 |'
  prefs: []
  type: TYPE_TB
- en: '| GPU memory | 939 MB | 1283 MB | 1969 MB | 3305 MB | 6011 MB |'
  prefs: []
  type: TYPE_TB
- en: Checking GPU usage information
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here, we will talk about how to check GPU usage along with other hardware usage
    information in Windows 10 and Ubuntu 18.04.
  prefs: []
  type: TYPE_NORMAL
- en: In Windows 10, the easiest way to check hardware usage (including GPU usage)
    is using Task Manager. You can open it by pressing <q>Ctrl</q> + <q>Shift</q>
    + <q>Esc</q>, and switch to the Performance panel. All hardware usage information
    is available to you now.
  prefs: []
  type: TYPE_NORMAL
- en: In Ubuntu 18.04, you can check CPU, RAM, and drive usage with **GNOME System
    Monitor**, which is shipped with the system. You can search for the system monitor
    in the Application menu, or run `gnome-system-monitor` in a Terminal to open it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, you can install a GNOME extension to illustrate the usage graphs
    in the status bar. We recommend that you use the** system-monitor extension**
    ([https://extensions.gnome.org/extension/120/system-monitor](https://extensions.gnome.org/extension/120/system-monitor))
    for this purpose. To install it, you first need to install several prerequisites:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Then, open a Firefox browser and navigate to this site, [https://addons.mozilla.org/en-US/firefox/addon/gnome-shell-integration](https://addons.mozilla.org/en-US/firefox/addon/gnome-shell-integration/),
    to install the browser extension for easy installation of GNOME extensions provided
    by [http://gnome.org](http://gnome.org). You also need to run `sudo apt-get install
    chrome-gnome-shell` in Terminal.
  prefs: []
  type: TYPE_NORMAL
- en: Next, open the web page, [https://extensions.gnome.org/extension/120/system-monitor](https://extensions.gnome.org/extension/120/system-monitor),
    with the Firefox browser; you'll see a switch button on the right side of the
    extension title. Click it to switch it to `ON` and you will be prompted to install
    the system-monitor extension.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, press *Alt* + *F2*, type in `r`, and then press <q>Enter</q>. This
    will restart the GNOME shell so that the system-monitor extension will be activated.
  prefs: []
  type: TYPE_NORMAL
- en: 'To check GPU usage in Ubuntu, you can run this script in the Terminal to show
    it in real time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: You can also create a `.sh` file in a convenient directory, for example, `~/gpu.sh`: copy
    the script into this file, then run `chmod +x ~/.gpu.sh`. Then, you can simply
    run `./gpu.sh` in the Terminal whenever you need to check GPU usage.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, there are many other tools you can use on Ubuntu, for example,
    NVTOP ([https://github.com/Syllo/nvtop](https://github.com/Syllo/nvtop)).
  prefs: []
  type: TYPE_NORMAL
- en: Moving to larger datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generating digits is fun. We can have way more fun generating other stuff, such
    as human faces and bedroom photos. To generate good complex images like these,
    we need more training samples than the 60,000 samples that MNIST offers. In this
    section, we will download two much larger datasets (CelebA and LSUN) and train
    the DCGAN on them to get more complex generated samples.
  prefs: []
  type: TYPE_NORMAL
- en: Generating human faces from the CelebA dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The CelebFaces Attributes (**CelebA**, [http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html))
    dataset is a large-scale face attributes dataset with more than 200,000 celebrity
    images, each with 40 attribute annotations. We need to download the cropped and
    aligned images. We won't need any attribute annotation here so we only need to
    download the file named `img_align_celeba.zip`, which is no more than 2 GB in
    size.
  prefs: []
  type: TYPE_NORMAL
- en: If you can't download the CelebA dataset from official links, try these links
    provided by Kaggle and the official PyTorch tutorial: [https://www.kaggle.com/jessicali9530/celeba-dataset](https://www.kaggle.com/jessicali9530/celeba-dataset)
    and [https://drive.google.com/drive/folders/0B7EVK8r0v71pWEZsZE9oNnFzTm8](https://drive.google.com/drive/folders/0B7EVK8r0v71pWEZsZE9oNnFzTm8).
    Note that you only need to download `Img/img_align_celeba.zip` from the Google
    Drive link.
  prefs: []
  type: TYPE_NORMAL
- en: Extract the downloaded images to a directory, for example, `~/Data/CelebA`.
    Make sure all your images are contained in an individual directory inside this
    root directory so that the images are stored at a location such as `~/Data/CelebA/img_align_celeba/000001.png`.
  prefs: []
  type: TYPE_NORMAL
- en: If you have a **Solid**-**State Drive** (**SSD**) with enough space plugged
    in your machine, we highly recommend you move all of your training samples to
    the SSD, especially when you have a powerful graphics card. Because when you are
    training neural networks on a very large dataset, which cannot fit in the GPU
    memory, the reading speed from physical drives could be the bottleneck of your
    training performance. Sometimes, the speed-up of SSD (reading samples at 50 MB/s)
    over the traditional hard drive (5 MB/s) can save you a big chunk of training
    time.
  prefs: []
  type: TYPE_NORMAL
- en: 'We only need to alter 3 different parts of code in the previous section to
    train the DCGAN on the CelebA dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Change the dataset root directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: If you are not sure what absolute path you're currently at in the file manager
    on Ubuntu, simply press *Ctrl* + *L* and the full path will show up.
  prefs: []
  type: TYPE_NORMAL
- en: 'Change the image channel number:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Redefine the `dataset` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s run `python dcgan.py` in the Terminal and wait for a while. It
    takes about 88 minutes to finish 25 epochs of training on a GTX 1080Ti graphics
    card. The generated images after the 1^(st) epoch and the 25^(th) epoch are shown
    in the following. Again, we only show 64 generated samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ea55d23e-5e10-419c-a8ca-ddae7ed7a0f6.png)![](img/8ecc9a1e-f556-43f7-924d-87d57677506c.png)'
  prefs: []
  type: TYPE_IMG
- en: Generated images by the DCGAN from CelebA after the 1st and 25th epoch
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a list of GPU memory consumption with different `BATCH_SIZE` values:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Batch size | 64 | 128 | 256 | 512 | 1024 | 2048 |'
  prefs: []
  type: TYPE_TB
- en: '| GPU memory | 773 MB | 963 MB | 1311 MB | 2029 MB | 3441 MB | 6283 MB |'
  prefs: []
  type: TYPE_TB
- en: Generating bedroom photos from the LSUN dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'LSUN (Large-scale Scene Understanding, [https://www.yf.io/p/lsun](https://www.yf.io/p/lsun))
    is a large image dataset with 10 scene categories and 20 object categories. You
    can get the downloading toolkit from [https://github.com/fyu/lsun](https://github.com/fyu/lsun).
    We will use the `bedroom` category to train our DCGAN, which has more than 3 million
    bedroom photos:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: You can also export the images as individual files with `python data.py export
    bedroom_train_lmdb --out_dir` `bedroom_train_img` so that you can easily use these
    images for other projects. But try not to directly open the image folder with
    your file manager. It will take a lot of RAM and time.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset is contained in an **LMDB** (**Lightning Memory-Mapped Database
    Manager**) database file, which is about 54 GB in size. Make sure the database
    files are located in the `bedroom_train_lmdb` directory so that PyTorch's data
    loader can recognize it when the root directory is specified.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, we only need to change 3 parts of the code to use the LSUN dataset
    for our model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Change the dataset root directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Change the image channel number:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Redefine the `dataset` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'And don''t forget to install the `lmdb` library for Python so that we can read
    the database file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s save the source file and run `python dcgan.py` in the Terminal.
    Since there are way more samples in the LSUN dataset, we don''t have to train
    the model for 25 epochs. Some of the generated images are already impressive even
    after the 1^(st) epoch of training. It takes about 5 hours to train for 5 epochs
    on a GTX 1080Ti graphics card. The generated images after the 1^(st) epoch and
    the 25^(th) epoch are shown in the following. Here, we only show 64 generated
    samples. We will not show the GPU memory consumption for LSUN because it''s almost
    the same as CelebA since the input images are both 3-channel and the network structure
    is not changed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9163ef11-96c4-42ca-8a45-89e602f19f80.png)![](img/71e1ad9d-6e46-4777-81a9-62fabc9f9df1.png)'
  prefs: []
  type: TYPE_IMG
- en: Generated images by the DCGAN from LSUN after the 1st and 5th epochs
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, we''d like to point out that if you plan on training GANs on a large
    dataset, always consider using powerful GPUs and putting your dataset on an SSD.
    Here, we give two sets of performance comparisons. In the first configuration,
    we use an NVIDIA GTX 960 graphics card and put the training set on an **HDD**
    (**hard disk drive**). In the second configuration, we use an NVIDIA GTX 1080Ti
    graphics card and put the training set on an SSD. We can see the speedup of the
    powerful platform is life changing:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | CelebA | LSUN |'
  prefs: []
  type: TYPE_TB
- en: '| GTX 960 + HDD | 2 hours/epoch | 16.6 hours/epoch |'
  prefs: []
  type: TYPE_TB
- en: '| GTX 1080Ti + SSD | 3.5 minutes/epoch | 53 minutes/epoch |'
  prefs: []
  type: TYPE_TB
- en: '| Speedup | 34X | 19X |'
  prefs: []
  type: TYPE_TB
- en: Having fun with the generator network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that our first image generator is trained, aren't you curious about what
    it is capable of and how images are generated from random noise vectors? In this
    section, we will have some fun with the generator network. First, we will choose
    two random vectors and calculate the interpolation between them to see what images
    will be generated. Second, we will choose some exemplary vectors and perform arithmetic
    calculations on them to find out what changes appear in the generated samples.
  prefs: []
  type: TYPE_NORMAL
- en: First, we need a test version of the DCGAN code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Copy your original `dcgan.py` file to `dcgan_test.py`. Next, we need to make
    some changes to our new file. First, we need to replace these lines of just the
    `Generator` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We replace them with the following lines (you can either delete them or simply
    comment them out):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Next, we need to remove (or comment out) the `weights_init`, `Discriminator`,
    `dataset`, `dataloader`, `criterion`, and `optimizer` objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to replace the entire training iteration section with this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We''re almost done. We need to add the following code at the end:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, go back to the top of the code file and add a line in the `import` section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'And finally, we need to add a line to the variable definitions. Just after
    the line that says `CUDA = True`, add this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The values for `VIZ_MODE` are 0 for random, 1 for interpolation, and 2 for semantic
    calculation. This will be used as we move forward through the three sets of code.
  prefs: []
  type: TYPE_NORMAL
- en: We need to export the input vector and the generated images to file. The full
    code for the DCGAN testing is available under the code repository for this chapter,
    which is called `dcgan_test.py`. And don't forget to delete or comment out the `utils.clear_folder(OUT_PATH)` line; otherwise,
    all your training results will be deleted, which would be a bad thing.
  prefs: []
  type: TYPE_NORMAL
- en: Image interpolation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The generator network maps the input random vector (the latent vector) to a
    generated image. If we perform linear interpolation on the latent vectors, the
    corresponding output images also obey the interpolation relation. Let's take the
    trained model on CelebA, for example.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s randomly choose two vectors that generate clean images. Here,
    we set `BATCH_SIZE=10` for simplicity. We''ll also add the beginnings of an `if`
    conditional to allow easy selection of what parts of the code to run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The generated images may look like the following. And the latent vectors for
    these images are exported to a file (for example, `vec_20190317-223131.txt`):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dd07d801-4807-49ee-868b-e5d3b3d01a51.png)'
  prefs: []
  type: TYPE_IMG
- en: Randomly generated images
  prefs: []
  type: TYPE_NORMAL
- en: 'Assume that we choose the 3^(rd) and the last images for interpolation. Now,
    let''s perform linear interpolation on their latent vectors with SciPy (replace
    the previous line starting with `viz_tensor = ...` with the following lines).
    Be sure to change the filename to the one that was just generated on your system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'You will also need to change the `VIZ_MODE` flag from `0` to `1` for interpolation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, run your changed source code. The corresponding generated images are as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bdb7d87f-3f8b-4aef-99a8-d4d3e5ef1b6a.png)'
  prefs: []
  type: TYPE_IMG
- en: Image interpolation
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the image on the left is smoothly transformed into the one on
    the right. Therefore, we know that the interpolation of the latent vectors leads
    to the interpolation of generated images.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic vector arithmetic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear interpolation is one of the basic methods in linear algebra. We can do
    a lot more with arithmetic calculations on the latent vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Take the randomly generated images from previous steps. We notice that some
    images are of smiling women (the 1^(st), 7^(th), and 9^(th) images), some women's
    images are not smiling (the 2^(nd), 3^(rd), and 5^(th) images), and none of the
    men in the images are smiling. Man, aren't they serious! How do we put a smile
    on a man's face without regenerating a new set of random vectors?
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, imagine we can solve it with arithmetic calculations:'
  prefs: []
  type: TYPE_NORMAL
- en: <q>[smiling woman] - [woman] = [smile]</q>
  prefs: []
  type: TYPE_NORMAL
- en: <q>[smile] + [man] = [smiling man]</q>
  prefs: []
  type: TYPE_NORMAL
- en: Can we do that? Let's try it!
  prefs: []
  type: TYPE_NORMAL
- en: 'First, set the `VIS_MODE` flag again, this time to `2` for semantic calculations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, continue the `if` conditional with the following code. Once again, use
    the filename that was created earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, by performing `z1-z2`, we get a smiling vector. And `z3` gives us a man
    vector. Adding them together will give us the following results. We use the mean
    vector of 3 different latent vectors for more stable results and we add small
    random values to the arithmetic results to introduce a slight randomness:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1f894278-6379-4d9b-b801-301b8781e45d.png)'
  prefs: []
  type: TYPE_IMG
- en: Vector arithmetic on latent vectors
  prefs: []
  type: TYPE_NORMAL
- en: 'The vector arithmetic calculation process can be described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/94aa789f-d6b6-4411-a4b2-4452eb8e0740.png)'
  prefs: []
  type: TYPE_IMG
- en: Vector arithmetic
  prefs: []
  type: TYPE_NORMAL
- en: Out of curiosity, we directly generate images based on *z1-z2*, which gives
    us the sample at the bottom right in the previous screenshot. We can tell it's
    a smiling face, but the rest of the face is rather unnatural. It looks like the
    face of a strange dude.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have unlocked the potential of GANs on manipulating the attributes of
    the generated images. However, the results are not natural and authentic enough.
    In the next chapter, we will learn how to generate samples with the exact attributes
    we desire.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We spent a tremendous amount of time learning about Deep Convolutional GANs
    in this chapter. We dealt with the MNIST dataset as well as two huge datasets
    in the form of the CelebA and LSUN datasets. We also consumed a large number of
    computing cycles. Hopefully, you have a good grasp of DCGANs at this point.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll look at a **Conditional GAN** (**CGAN**) and how to add label information
    during the training process. Let's get going!
  prefs: []
  type: TYPE_NORMAL
- en: References and useful reading list
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hui J. (2018, Jun 21). GAN — *Why it is so hard to train Generative Adversarial
    Networks!*. Retrieved from [https://medium.com/@jonathan_hui/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b](https://medium.com/@jonathan_hui/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b).
  prefs: []
  type: TYPE_NORMAL
