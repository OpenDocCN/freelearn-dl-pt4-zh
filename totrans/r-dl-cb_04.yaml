- en: Data Representation Using Autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter will introduce unsupervised applications of deep learning using
    autoencoders. In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data normalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up a regularized autoencoder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning the parameters of the autoencoder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up stacked autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up denoising autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building and comparing stochastic encoders and decoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning manifolds from autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating the sparse decomposition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Neural networks aim to find a non-linear relationship between input *X* with
    output *y,* as *y=f(x)***.** An autoencoder is a form of unsupervised neural network
    which tries to find a relationship between features in space such that *h*=*f(x)*,
    which helps us learn the relationship between input space and can be used for
    data compression, dimensionality reduction, and feature learning.
  prefs: []
  type: TYPE_NORMAL
- en: An autoencoder consists of an encoder and decoder. The encoder helps encode
    the input *x* in a latent representation *y,* whereas a decoder converts back
    the *y* to *x*. Both the encoder and decoder possess a similar representation
    of form.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a representation of a one layer autoencoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '**![](img/00051.jpeg)**'
  prefs: []
  type: TYPE_NORMAL
- en: The coder encodes input *X* to *h* under a hidden layer contain, whereas the
    decoder helps to attain the original data from encoded output *h*. The matrices
    *W[e]* and *W[d]* represent the weights of the encoder and decoder layers, respectively.
    The function *f* is the activation function.
  prefs: []
  type: TYPE_NORMAL
- en: 'An illustration of an autoencoder is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00053.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The constraints in the form of nodes allows the autoencoder to discover interesting
    structures within the data. For example, in the encoder in the preceding diagram,
    the five-input dataset must pass through three-node compression to get an encoded
    value *h****.*** The **Encoded Output layer** of an encoder can have a dimensionality
    that is the same, lower, or higher than the **input/output Decoded Output layer**.
    The **Encoded Output layer** with a fewer number of nodes than the input layer
    is referred to as an under-complete representation, and can be thought of as data
    compression transforming data into low-dimensional representation.
  prefs: []
  type: TYPE_NORMAL
- en: An **Encoded Output layer** with a larger number of input layers is referred
    to as an over-complete representation and is used in a **sparse autoencoder**
    as a regularization strategy. The objective of an autoencoder is to find *y,*
    capturing the main factors along the variation of data, which is similar to **Principal
    Component Analysis (PCA)**, and thus can be used for compression as well.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There exist a lot of different architectures of autoencoders distinguished by
    cost functions used to capture data representation. The most basic autoencoder
    is known as a vanilla autoencoder. It's a two-layer neural network with one hidden
    layer the same number of nodes at the input and output layers, with an objective
    to minimize the cost function*.* The typical choices, but not limited to, for
    a loss function are **mean square error** (**MSE**) for regression and cross entropy
    for classification. The current approach can be easily extended to multiple layers,
    also known as multilayer autoencoder.
  prefs: []
  type: TYPE_NORMAL
- en: The number of nodes plays a very critical role in autoencoders. If the number
    of nodes in the hidden layer is less than the input layer then an autoencoder
    is known as an **under-complete** autoencoder. A higher number of nodes in the
    hidden layer represents an **over-complete** autoencoder or sparse autoencoder.
  prefs: []
  type: TYPE_NORMAL
- en: The sparse autoencoder aims to impose sparsity in the hidden layer. This sparsity
    can be achieved by introducing a higher number of nodes than the input in the
    hidden layer or by introducing a penalty in the loss function that will move the
    weights for the hidden layer toward zero. Some autoencoders attain the sparsity
    by manually zeroing out the weight for nodes; these are referred to as **K-sparse
    autoencoders***.* We will set up an autoencoder on the occupancy dataset discussed
    in [Chapter 1](part0021.html#K0RQ1-a0a93989f17f4d6cb68b8cfd331bc5ab), *Getting
    Started*. The hidden layer for the current example can be tweaked around.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s use the `Occupancy` dataset to set up an autoencoder:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the `Occupancy` dataset as described in [Chapter 1](part0021.html#K0RQ1-a0a93989f17f4d6cb68b8cfd331bc5ab),
    *Getting Started*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow installation in R and Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The current occupancy dataset as described in [Chapter 1](part0021.html#K0RQ1-a0a93989f17f4d6cb68b8cfd331bc5ab),
    *Getting Started*, is used to demonstrate the autoencoder setup in R using TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: Set up the R TensorFlow environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `load_occupancy_data` function can be used to load the data by setting
    the correct working directory path using `setwd`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The train and test occupancy dataset can be loaded to the R environment with
    the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Data normalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Data normalization** is a critical step in machine learning to bring data
    to a similar scale. It is also known as feature scaling and is performed as data
    preprocessing.'
  prefs: []
  type: TYPE_NORMAL
- en: The correct normalization is very critical in neural networks, else it will
    lead to saturation within the hidden layers, which in turn leads to zero gradient
    and no learning will be possible.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are multiple ways to perform normalization:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Min-max standardization**: The min-max retains the original distribution
    and scales the feature values between *[0, 1],* with *0* as the minimum value
    of the feature and *1* as the maximum value. The standardization is performed
    as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00055.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *x*' is the normalized value of the feature. The method is sensitive to
    outliers in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**Decimal scaling**: This form of scaling is used where values of different
    decimal ranges are present. For example, two features with different bounds can
    be brought to a similar scale using decimal scaling as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x''=x/10^n*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Z-score:** This transformation scales the value toward a normal distribution
    with a zero mean and unit variance. The Z-score is computed as:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Z*=(**x-µ)/σ**'
  prefs: []
  type: TYPE_NORMAL
- en: Here, *µ* is the mean and σ is the standard deviation of the feature. These
    distributions are very efficient for a dataset with a Gaussian distribution.
  prefs: []
  type: TYPE_NORMAL
- en: All the preceding methods are sensitive to outliers; there are other more robust
    approaches for normalization that you can explore, such as **Median Absolute Deviation**
    (**MAD**), tanh-estimator, and double sigmoid.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing dataset distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at the distribution of features for the occupation data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00059.gif)'
  prefs: []
  type: TYPE_IMG
- en: The figure shows that the features have linear correlations and the distributions
    are non-normal. The non-normality can be further validated using the Shapiro-Wilk
    test, using the `shapiro.test` function from R. Let's use min-max standardization
    for the occupation data.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Perform the following operation for data normalization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The `minmax.normalize` function normalizes the data using min-max normalization.
    When the `scaler` variable is `NULL`, it performs normalization using the dataset
    provided, or normalizes using `scaler` values. The normalized data pair plot is
    shown in the following figure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00060.gif)'
  prefs: []
  type: TYPE_IMG
- en: This figure shows min-max normalization bringing the values within bounds *[0,
    1]* and it does not change the distribution and correlations between features.
  prefs: []
  type: TYPE_NORMAL
- en: How to set up an autoencoder model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The next step is to set up the autoencoder model. Let''s set up a vanilla autoencoder
    using TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Reset the `graph` and start `InteractiveSession`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the input parameter where `n` and `m` are the number of samples and
    features, respectively. To build, network `m` is used to set up the input parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: When `n_hidden_1` is low, the autoencoder is compressing the data and is referred
    to as an under-complete autoencoder; whereas, when `n_hidden_1` is large, then
    the autoencoder is sparse and is referred to as an over-complete autoencoder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define graph input parameters that include the input tensor and layer definitions
    for the encoder and decoder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The preceding script designs a single-layer encoder and decoder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define a function to evaluate the response:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The `auto_encoder` function takes the node bias weights and computes the output.
    The same function can be used for `encoder` and `decoder` by passing respective
    weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create `encoder` and `decoder` objects by passing symbolic TensorFlow variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The `y_pred` is the outcome from `decoder`, which takes the `encoder` object
    as input with nodes and bias weights:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding script defines mean square error as the cost function, and uses
    `RMSPropOptimizer` from TensorFlow with 0.1 learning rate for the optimization
    of weights. The TensorFlow graph for the preceding model is shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00063.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Running optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The next step is to run optimizer optimization. Executing this process in TensorFlow
    consists of two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is parameter initialization of the variables defined in the
    graph. The initialization is performed by calling the `global_variables_initializer`
    function from TensorFlow:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Optimization is performed based on optimizing and monitoring the train and
    test performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The cost function from train and test can be observed to understand convergence
    of the model, as shown in the following figure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00133.gif)'
  prefs: []
  type: TYPE_IMG
- en: This graph shows that the model major `convergence` is at around **400** iterations;
    however, it is still converging at a very slow rate even after **1,000** iterations.
    The model is stable in both the train and holdout test datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a regularized autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A regularized autoencoder extends the standard autoencoder by adding a regularization
    parameter to the `cost` function.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The regularized autoencoder is an extension of the standard autoencoder. The
    set-up will require:'
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow installation in R and Python.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implementation of a standard autoencoder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code setup for the autoencoder can directly be converted to a regularized
    autoencoder by replacing the cost definition with the following lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As mentioned earlier, a regularized autoencoder extends the standard autoencoder
    by adding a regularization parameter to the cost function, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00148.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *λ* is the regularization parameter and *i* and *j* are the node indexes
    with *W* representing the hidden layer weights for the autoencoder. The regularization
    autoencoder aims to ensure more robust encoding and prefers a low weight *h* function.
    The concept is further utilized to develop a contractive autoencoder, which utilizes
    the Frobenius norm of the Jacobian matrix on input, represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00029.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'where **J(x)** is the Jacobian matrix and is evaluated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00146.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: For a linear encoder, a contractive encoder and regularized encoder converge
    to L2 weight decay. The regularization helps in making the autoencoder less sensitive
    to the input; however, the minimization of the cost function helps the model to
    capture the variation and remain sensitive to manifolds of high density. These
    autoencoders are also referred to as **contractive autoencoders**.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning the parameters of the autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The autoencoder involves a couple of parameters to tune, depending on the type
    of autoencoder we are working on. The major parameters in an autoencoder include
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Number of nodes in any hidden layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of hidden layers applicable for deep autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Activation unit such as sigmoid, tanh, softmax, and ReLU activation functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization parameters or weight decay terms on hidden unit weights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fraction of the signal to be corrupted in a denoising autoencoder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sparsity parameters in sparse autoencoders that control the expected activation
    of neurons in hidden layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Batch size, if using batch gradient descent learning; learning rate and momentum
    parameter for stochastic gradient descent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maximum iterations to be used for the training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weight initialization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dropout regularization if dropout is used
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These hyperparameters can be trained by setting the problem as a grid search
    problem. However, each hyperparameter combination requires training the neuron
    weights for the hidden layer(s), which results in increasing computational complexity
    with an increase in the number of layers and number of nodes within each layer.
    To deal with these critical parameters and training issues, stacked autoencoder
    concepts have been proposed that train each layer separately to get pretrained
    weights, and then the model is fine-tuned using the obtained weights. This approach
    tremendously improves the training performance over the conventional mode of training.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up stacked autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The stacked autoencoder is an approach to train deep networks consisting of
    multiple layers trained using the greedy approach. An example of a stacked autoencoder
    is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00037.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: An example of a stacked autoencoder
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The preceding diagram demonstrates a stacked autoencoder with two layers. A
    stacked autoencoder can have *n* layers, where each layer is trained using one
    layer at a time. For example, the previous layer will be trained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00039.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Training of a stacked autoencoder
  prefs: []
  type: TYPE_NORMAL
- en: The initial pre-training of layer 1 is obtained by training it over the actual
    input *x[i]* . The first step is to optimize the *We(1)* layer of the encoder
    with respect to output X. The second step in the preceding example is to optimize
    the weights *We(2)* in the second layer, using *We(1)* as input and output. Once
    all the layers of *We(i)* where *i*=1, 2, ...,*n* is number of layers are pretrained,
    model fine-tuning is performed by connecting all the layers together, as shown
    in step 3 of the preceding diagram. The concept can also be applied to denoising
    to train multilayer networks, which is known as a stacked denoising autoencoder.
    The code developed in a denoising autoencoder can be easily tweaked to develop
    a **stacked denoising autoencoder**, which is an extension of the stacked autoencoder.
  prefs: []
  type: TYPE_NORMAL
- en: 'The requirements for this recipe are:'
  prefs: []
  type: TYPE_NORMAL
- en: R should be installed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`SAENET` package, The package can be download from Cran using the command `install.packages("SAENET")`*.*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are other popular libraries in R to develop stacked autoencoders. Let''s
    utilize the `SAENET` package from R to set up a stacked autoencoder. The `SAENET`
    is a stacked autoencoder implementation, using a feedforward neural network using
    the `neuralnet` package from CRAN:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Get the `SAENET` package from the CRAN repository, if not installed already:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Load all library dependencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the train and test occupancy dataset using `load_occupancy_data`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Normalize the dataset using the `minmax.normalize` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The stacked autoencoder model can be built using the `SAENET.train` train function
    from the `SAENET` package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The output of the last node can be extracted using the `SAE_obj[[n]]$X.output`
    command.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up denoising autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Denoising autoencoders are a special kind of autoencoder with a focus on extracting
    robust features from the input dataset. Denoising autoencoders are similar to
    the previous model except with a major difference that the data is corrupted before
    training the network. Different approaches for corruption can be used such as
    masking, which induces random error into the data.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s use the CIFAR-10 image data to set up a denoising dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the CIFAR-10 dataset using the `download_cifar_data` function (covered
    in [Chapter 3](part0093.html#2OM4A1-a0a93989f17f4d6cb68b8cfd331bc5ab), *Convolution
    Neural Network*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow installation in R and Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We first need to read the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Reading the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Load the `CIFAR` dataset using the steps explained in [Chapter 3](part0093.html#2OM4A1-a0a93989f17f4d6cb68b8cfd331bc5ab),
    *Convolution Neural Network*. The data files `data_batch_1` and `data_batch_2`
    are used to train. The `data_batch_5` and `test_batch` files are used for validation
    and testing, respectively. The data can be flattened using the `flat_data` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The `flat_data` function flattens the dataset as *NCOL = (Height * Width *
    number of channels)*, thus the dimension of the dataset is (# of images X NCOL).
    The images in `CIFAR` are 32 x 32 with three RGB channels; thus, we obtain 3,072
    columns after data flattening:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Corrupting data to train
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The next critical function needed to set up a denoising autoencoder is data
    corruption:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The CIFAR-10 data can be corrupted using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'An example image after corruption is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00002.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding figure uses the masking approach to add noise. This method adds
    zero values at random image locations with a defined fraction. Another approach
    to add noise is by using salt & pepper noise. This method selects random locations
    in the image and replaces them, adding min or max values to the image using the
    flip of coin principle. An example of the salt and pepper approach for data corruption
    is shown in the following figure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00045.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The data corruption helps the autoencoder to learn more robust representation.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a denoising autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The next step is to set up the autoencoder model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, reset the graph and start an interactive session as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to define two placeholders for the input signal and corrupt
    signal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The `x_corrupt` will be used as input in the autoencoder, and *x* is the actual
    image that will be used as output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Set up a denoising autoencoder function as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the denoising object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the cost function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Run optimization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The autoencoder keeps learning about the function form for the feature to capture
    the relationship between input and output. An example of how the computer is visualizing
    the image after 1,000 iterations is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00061.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'After 1,000 iterations, the computer can distinguish between a major part of
    the object and environment. As we run the algorithm further to fine-tune the weights,
    the computer keeps learning more features about the object itself, as shown in
    the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00140.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding graph shows that the model is still learning, but the learning
    rate has become smaller over the iterations as it starts learning fine features
    about objects, as shown in the following image. There are instances when the model
    starts ascending instead of descending, due to batch gradient descent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00007.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Illustration of learning using denoising autoencoder
  prefs: []
  type: TYPE_NORMAL
- en: Building and comparing stochastic encoders and decoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Stochastic encoders fall into the domain of generative modeling, where the
    objective is to learn join probability *P(X)* over given data *X* transformed
    into another high-dimensional space. For example, we want to learn about images
    and produce similar, but not exactly the same, images by learning about pixel
    dependencies and distribution. One of the popular approaches in generative modeling
    is **Variational autoencoder** (**VAE**), which combines deep learning with statistical
    inference by making a strong distribution assumption on *h ~ P(h),* such as Gaussian
    or Bernoulli. For a given weight *W***,** the *X* can be sampled from the distribution
    as *Pw(X|h)***.** An example of VAE architecture is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00052.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'The cost function of VAE is based on log likelihood maximization. The cost
    function consists of reconstruction and regularization error terms:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Cost = Reconstruction Error + Regularization Error*'
  prefs: []
  type: TYPE_NORMAL
- en: The **reconstruction error** is how well we could map the outcome with the training
    data and the **regularization error** puts a penalty on the distribution formed
    at the encoder and decoder.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'TensorFlow needs to be installed and loaded in the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Dependencies need to be loaded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The `MNIST` dataset needs to be loaded. The dataset is normalized using the
    following script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00054.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `MNIST` dataset is used to demonstrate the concept of sparse decomposition.
    The `MNIST` dataset uses handwritten digits. It is downloaded from the `tensorflow`
    dataset library. The dataset consists of handwritten images of `28 x 28` pixels.
    It consists of 55,000 training examples, 10,000 test examples, and 5,000 test
    examples. The dataset can be downloaded from the `tensorflow` library using the
    following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'For computational simplicity, the `MNIST` image size is reduced from `28 x
    28` pixels to `16 x 16` pixels using the following function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The following script can be used to prepare the `MNIST` training data with
    a 16 x 16 pixel image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The `plot_mnist` function can be used to visualize the selected `MNIST` image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Setting up a VAE model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Start a new TensorFlow environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Define network parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Start a new TensorFlow environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Define network parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding parameter will form a VAE network as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00056.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Define the model initialization function, defining weights and biases at each
    layer of `encoder` and `decoder`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The `model_init` function returns `weights`, which is a two-dimensional list.
    The first dimension captures the weight''s association and type. For example,
    it describes if the `weights` variable is assigned to the encoder or decoder and
    if it stores the weight of the node or bias. The `xavier_init` function in `model_init`
    is used to assign initial weights for model training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up the encoder evaluation function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The `vae_encoder` computes the mean and variance to sample the layer, using
    the weights and bias from the hidden layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Set up the decoder evaluation function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The `vae_decoder` function computes the mean and standard deviation associated
    with the sampling layer at output and average output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Set up the function for reconstruction estimation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the cost function for optimization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up the model to train:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Run optimization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Output from the VAE autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The outcome can be generated using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The outcome obtained after `20,000` iterations from the preceding VAE autoencoder
    is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00147.gif)'
  prefs: []
  type: TYPE_IMG
- en: Additionally, as VAE is a generative model, the outcome is not an exact replica
    of the input and would vary with runs, as a representative sample is extracted
    from the estimated distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Learning manifolds from autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Manifold learning is an approach in machine learning that assumes that data
    lies on a manifold of a much lower dimension. These manifolds can be linear or
    non-linear. Thus, the area tries to project the data from high-dimension space
    to a low dimension. For example, **principle component analysis** (**PCA**) is
    an example of linear manifold learning whereas an autoencoder is a **non-linear
    dimensionality reduction** (**NDR**) with the ability to learn non-linear manifolds
    in low dimensions. A comparison of linear and non-linear manifold learning is
    shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00062.gif)'
  prefs: []
  type: TYPE_IMG
- en: As you can see from graph **a)**, the data is residing at a linear manifold,
    whereas in graph graph **b)**, the data is residing on a second-order non-linear
    manifold.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's take an output from the stacked autoencoder section and analyze how manifolds
    look when transferred into a different dimension.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up principal component analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before getting into non-linear manifolds, let''s analyze principal component
    analysis on the occupancy data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding function will transform the data into six orthogonal directions
    specified as linear combinations of features. The variance explained by each dimension
    can be viewed using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command will plot the variance across principal components, as
    shown in the following figure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00064.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'For the occupancy dataset, the first two principal components capture the majority
    of the variation, and when the principal component is plotted, it shows separation
    between the positive and negative classes for occupancy, as shown in the following
    figure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00066.gif)'
  prefs: []
  type: TYPE_IMG
- en: Output from the first two principal components
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s visualize the manifold in a low dimension learned by the autoencoder.
    Let''s use only one dimension to visualize the outcome, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The encoder architecture for the preceding script is shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00115.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The hidden layer outcome with one latent node from the stacked autoencoder
    is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00092.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding graph shows that occupancy is true at the peaks of the latent
    variables. However, the peaks are found at different values. Let''s increase the
    latent variables 2, as captured by PCA. The model can be developed and data can
    be plotted using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The encoded values with two layers are shown in the following diagram:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00123.gif)'
  prefs: []
  type: TYPE_IMG
- en: Evaluating the sparse decomposition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The sparse autoencoder is also known as over-complete representation and has
    a higher number of nodes in the hidden layer. The sparse autoencoders are usually
    executed with the sparsity parameter (regularization), which acts as a constraint
    and restricts the node to being active. The sparsity can also be assumed as nodes
    dropout caused due to sparsity constraints. The loss function for a sparse autoencoder
    consists of a reconstruction error, a regularization term to contain the weight
    decay, and KL divergence for sparsity constraint. The following representation
    gives a very good illustration of what we are talking about:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00095.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The dataset is loaded and set up.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Install and load the `autoencoder` package using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The standard autoencoder code of TensorFlow can easily be extended to the sparse
    autoencoder module by updating the cost function. This section will introduce
    the autoencoder package of R, which comes with built-in functionality to run the
    sparse autoencoder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The major parameters in the `autoencode` functions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`nl`: This is the number of layers including the input and output layer (the
    default is three).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`N.hidden`: This is the vector with the number of neurons in each hidden layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unit.type`: This is the type of activation function to be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lambda`: This is the regularization parameter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rho`: This is the sparsity parameter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`beta`: This is the penalty for the sparsity term.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max.iterations`: This is the maximum number of iterations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`epsilon`: This is the parameter for weight initialization. The weights are
    initialized using Gaussian distribution ~N(0, epsilon2).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following figure shows the shapes and orientation of digits from `MNIST`
    captured by the sparse autoencoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00138.gif)'
  prefs: []
  type: TYPE_IMG
- en: Filter generated by the sparse autoencoder to get the digit outcome
  prefs: []
  type: TYPE_NORMAL
- en: The filter learned by the sparse autoencoder can be visualized using the `visualize.hidden.units`
    function from the autoencoder package. The package plots the weight of the final
    layer with respect to the output. In the current scenario, 100 is the number of
    neurons in the hidden layer and 256 is the number of nodes in the output layer.
  prefs: []
  type: TYPE_NORMAL
