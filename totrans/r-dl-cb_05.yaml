- en: Generative Models in Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Comparing principal component analysis with the Restricted Boltzmann machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up a Restricted Boltzmann machine for Bernoulli distribution input
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a Restricted Boltzmann machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Backward or reconstruction phase of RBM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the contrastive divergence of the reconstruction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Initializing and starting a new TensorFlow session
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating the output from an RBM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up a Restricted Boltzmann machine for Collaborative Filtering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing a full run of training an RBM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up a Deep Belief Network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a feed-forward backpropagation Neural Network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up a Deep Restricted Boltzmann Machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing principal component analysis with the Restricted Boltzmann machine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, you will learn about two widely recommended dimensionality
    reduction techniques--**Principal component analysis** (**PCA**) and the **Restricted
    Boltzmann machine** (**RBM**). Consider a vector *v* in *n*-dimensional space.
    The dimensionality reduction technique essentially transforms the vector *v* into
    a relatively smaller (or sometimes equal) vector *v'* with *m*-dimensions (*m*<*n*).
    The transformation can be either linear or nonlinear.
  prefs: []
  type: TYPE_NORMAL
- en: 'PCA performs a linear transformation on features such that orthogonally adjusted
    components are generated that are later ordered based on their relative importance
    of variance capture. These *m* components can be considered as new input features,
    and can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Vector *v'* = ![](img/00103.gif)
  prefs: []
  type: TYPE_NORMAL
- en: Here, *w* and *c* correspond to weights (loading) and transformed components,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike PCA, RBMs (or DBNs/autoencoders) perform non-linear transformations using
    connections between visible and hidden units, as described in [Chapter 4](part0166.html#4U9TC1-a0a93989f17f4d6cb68b8cfd331bc5ab),
    *Data Representation Using Autoencoders*. The nonlinearity helps in better understanding
    the relationship with latent variables. Along with information capture, they also
    tend to remove noise. RBMs are generally based on stochastic distribution (either
    Bernoulli or Gaussian).
  prefs: []
  type: TYPE_NORMAL
- en: 'A large amount of Gibbs sampling is performed to learn and optimize the connection
    weights between visible and hidden layers. The optimization happens in two passes:
    a forward pass where hidden layers are sampled using given visible layers and
    a backward pass where visible layers are resampled using given hidden layers.
    The optimization is performed to minimize the reconstruction error.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image represents a restricted Boltzmann machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00099.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this recipe, you will require R (the `rbm` and `ggplot2` packages) and
    the MNIST dataset. The MNIST dataset can be downloaded from the TensorFlow dataset
    library. The dataset consists of handwritten images of 28 x 28 pixels. It has
    55,000 training examples and 10,000 test examples. It can be downloaded from the
    `tensorflow` library using the following script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Extract the train dataset (`trainX` with all 784 independent variables and
    `trainY` with the respective 10 binary outputs):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Run a PCA on the `trainX` data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Run an RBM on the `trainX` data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Predict on the train data using the generated models. In the case of the RBM
    model, generate probabilities:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert the outcomes into data frames:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert the 10-class binary `trainY` data frame into a numeric vector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the components generated using PCA. Here, the *x*-axis represents component
    1 and the *y*-axis represents component 2\. The following image shows the outcome
    of the PCA model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00058.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Plot the hidden layers generated using PCA. Here, the *x*-axis represents hidden
    1 and *y*-axis represents hidden 2\. The following image shows the outcome of
    the RBM model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00102.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following code and image shows the cumulative variance explained by the
    principal components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00106.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following code and image shows the decrease in the reconstruction training
    error while generating an RBM using multiple epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00107.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Setting up a Restricted Boltzmann machine for Bernoulli distribution input
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, let's set up a restricted Boltzmann machine for Bernoulli distributed
    input data, where each attribute has values ranging from 0 to 1 (equivalent to
    a probability distribution). The dataset (MNIST) used in this recipe has input
    data satisfying a Bernoulli distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'An RBM comprises of two layers: a visible layer and a hidden layer. The visible
    layer is an input layer of nodes equal to the number of input attributes. In our
    case, each image in the MNIST dataset is defined using 784 pixels (28 x 28 size).
    Hence, our visible layer will have 784 nodes.'
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the hidden layer is generally user-defined. The hidden layer
    has a set of binary activated nodes, with each node having a probability of linkage
    with all other visible nodes. In our case, the hidden layer will have 900 nodes.
    As an initial step, all the nodes in the visible layer are connected with all
    the nodes in the hidden layer bidirectionally.
  prefs: []
  type: TYPE_NORMAL
- en: Each connection is defined using a weight, and hence a weight matrix is defined
    where the rows represent the number of input nodes and the columns represent the
    number of hidden nodes. In our case, the weight matrix (*w*) will be a tensor
    of dimensions 784 x 900.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to weights, all the nodes in each layer are assisted by a bias node.
    The bias node of the visible layer will have connections with all the visible
    nodes (that is, the 784 nodes) and is represented with **vb**, whereas the bias
    node of the hidden layer will have connections with all the hidden nodes (that
    is, the 900 nodes) and is represented as **vh**.
  prefs: []
  type: TYPE_NORMAL
- en: A point to remember with RBMs is that there will be no connections among nodes
    within each layer. In other words, the connections will be interlayer, but not
    intralayer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image represents an RBM with the visible layer, hidden layer,
    and interconnections:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00108.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section provides the requirements for setting up an RBM.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow in R is installed and set up
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `mnist` data is downloaded and loaded for setting up RBM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section provides the steps to set up the visible and hidden layers of
    an RBM using TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start a new interactive TensorFlow session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the model parameters. The `num_input` parameter defines the number of
    nodes in the visible layer and `num_hidden` defines the number of nodes in the
    hidden layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a placeholder variable for the weight matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Create placeholder variables of the visible and hidden biases:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Training a Restricted Boltzmann machine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Every training step of an RBM goes through two phases: the forward phase and
    the backward phase (or reconstruction phase). The reconstruction of visible units
    is fine tuned by making several iterations of the forward and backward phases.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training a forward phase**: In the forward phase, the input data is passed
    from the visible layer to the hidden layer and all the computation occurs within
    the nodes of the hidden layer. The computation is essentially to take a stochastic
    decision of each connection from the visible to the hidden layer. In the hidden
    layer, the input data (`X`) is multiplied by the weight matrix (`W`) and added
    to a hidden bias vector (`hb`).'
  prefs: []
  type: TYPE_NORMAL
- en: The resultant vector of a size equal to the number of hidden nodes is then passed
    through a sigmoid function to determine each hidden node's output (or activation
    state). In our case, each input digit will produce a tensor vector of 900 probabilities,
    and as we have 55,000 input digits, we will have an activation matrix of the size
    55,000 x 900\. Using the hidden layer's probability distribution matrix, we can
    generate samples of activation vectors that can be used later to estimate negative
    phase gradients.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section provides the requirements for setting up an RBM.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow in R is installed and set up
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `mnist` data is downloaded and loaded for setting up the RBM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The RBM model is set up as described in the recipe *Setting up a Restricted
    Boltzmann machine for Bernoulli distribution input*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example of a sampling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Consider a constant vector `s1` equivalent to a tensor vector of probabilities.
    Then, create a new random uniformly distributed sample `s2` using the distribution
    of the constant vector `s1`. Then calculate the difference and apply a rectified
    linear activation function.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section provides the steps to set up the script for running the RBM model
    using TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the following code to execute the graph created in TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Backward or reconstruction phase of RBM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the reconstruction phase, the data from the hidden layer is passed back to
    the visible layer. The hidden layer vector of probabilities `h0` is multiplied
    by the transpose of the weight matrix `W` and added to a visible layer bias `vb`,
    which is then passed through a sigmoid function to generate a reconstructed input
    vector `prob_v1`.
  prefs: []
  type: TYPE_NORMAL
- en: A sample input vector is created using the reconstructed input vector, which
    is then multiplied by the weight matrix `W` and added to the hidden bias vector
    `hb` to generate an updated hidden vector of probabilities `h1`.
  prefs: []
  type: TYPE_NORMAL
- en: This is also called Gibbs sampling. In some scenarios, the sample input vector
    is not generated and the reconstructed input vector `prob_v1` is directly used
    to update the hi
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00124.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section provides the requirements for image reconstruction using the input
    probability vector.
  prefs: []
  type: TYPE_NORMAL
- en: '`mnist` data is loaded in the environment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The RBM model is trained using the recipe *Training a Restricted Boltzmann machine*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section covers the steps to perform backward reconstruction and evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The backward image reconstruction can be performed using the input probability
    vector with the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The evaluation can be performed using a defined metric, such as **mean squared
    error** (**MSE**), which is computed between the actual input data (`X`) and the
    reconstructed input data (`v1`). The MSE is computed after each epoch and the
    key objective is to minimize the MSE:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Understanding the contrastive divergence of the reconstruction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As an initial start, the objective function can be defined as the minimization
    of the average negative log-likelihood of reconstructing the visible vector *v*
    where *P(v)* denotes the vector of generated probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00110.gif)'
  prefs: []
  type: TYPE_IMG
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section provides the requirements for image reconstruction using the input
    probability vector.
  prefs: []
  type: TYPE_NORMAL
- en: '`mnist` data is loaded in the environment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The images are reconstructed using the recipe *Backward or reconstruction phase*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This current recipe present the steps for, a **contrastive divergence** (**CD**)
    technique used to speed up the sampling process:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Compute a positive weight gradient by multiplying (outer product) the input
    vector `X` with a sample of the hidden vector `h0` from the given probability
    distribution `prob_h0`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute a negative weight gradient by multiplying (outer product) the sample
    of the reconstructed input data `v1` with the updated hidden activation vector
    `h1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, compute the `CD` matrix by subtracting the negative gradient from the
    positive gradient and dividing by the size of the input data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, update the weight matrix `W` to `update_W` using a learning rate (*alpha*)
    and the CD matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Additionally, update the visible and hidden bias vectors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The objective function can be minimized using stochastic gradient descent by
    indirectly modifying (and optimizing) the weight matrix. The entire gradient can
    be further divided into two forms based on the probability density: positive gradient
    and negative gradient. The positive gradient primarily depends on the input data
    and the negative gradient depends only on the generated model.'
  prefs: []
  type: TYPE_NORMAL
- en: In the positive gradient, the probability toward the reconstructing training
    data increases, and in the negative gradient, the probability of randomly generated
    uniform samples by the model decreases.
  prefs: []
  type: TYPE_NORMAL
- en: 'The CD technique is used to optimize the negative phase. In the CD technique,
    the weight matrix is adjusted in each iteration of reconstruction. The new weight
    matrix is generated using the following formula. The learning rate is defined
    as *alpha*, in our case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00105.gif)'
  prefs: []
  type: TYPE_IMG
- en: Initializing and starting a new TensorFlow session
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A big part of calculating the error metric such as mean square error (MSE) is
    initialization and starting a new TensorFlow session. Here is how we proceed with
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section provides the requirements for starting a new TensorFlow session
    used to compute the error metric.
  prefs: []
  type: TYPE_NORMAL
- en: '`mnist` data is loaded in the environment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The TensorFlow graph for the RBM is loaded
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section provides the steps for optimizing the error using reconstruction
    from an RBM:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize the current and previous vector of biases and matrices of weights:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Start a new TensorFlow session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform a first run with the full input data (**trainX**) and obtain the first
    set of weight matrix and bias vectors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at the error of the first run:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The full model for the RBM can be trained using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot reconstruction using mean squared errors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here, we will run 15 epochs (or iterations) where, in each epoch, a batchwise
    (size = 100) optimization is performed. In each batch, the CD is computed and,
    accordingly, weights and biases are updated. To keep track of the optimization,
    MSE is calculated after every batch of 10,000 rows.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image shows the declining trend of mean squared reconstruction
    errors computed for 90 batches:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00112.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Evaluating the output from an RBM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here, let's plot the weights of the final layer with respect to the output (reconstruction
    input data). In the current scenario, 900 is the number of nodes in the hidden
    layer and 784 is the number of nodes in the output (reconstructed) layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following image, the first 400 nodes in the hidden layer are seen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00113.gif)'
  prefs: []
  type: TYPE_IMG
- en: Here, each tile represents a vector of connections formed between a hidden node
    and all the visible layer nodes. In each tile, the black region represents negative
    weights (weight < 0), the white region represents positive weights (weight > 1),
    and the grey region represents no connection (weight = 0). The higher the positive
    value, the greater the chance of activation in hidden nodes, and vice versa. These
    activations help determine which part of the input image is being determined by
    a given hidden node.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section provides the requirements for running the evaluation recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '`mnist` data is loaded in the environment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The RBM model is executed using TensorFlow and the optimal weights are obtained
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This recipe covers the steps for the evaluation of weights obtained from an
    RBM:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following code to generate the image of 400 hidden nodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Select a sample of four actual input digits from the training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, visualize these sample digits using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, reconstruct these four sample images using the final weights and biases
    obtained:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, visualize the reconstructed sample digits using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following image illustrates a raw image of the four sample digits:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00152.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The reconstructed images seemed to have had their noise removed, especially
    in the case of digits **3** and **6**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image illustrates a reconstructed image of the same four digits:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00069.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Setting up a Restricted Boltzmann machine for Collaborative Filtering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, you will learn how to build a collaborative-filtering-based
    recommendation system using an RBM. Here, for every user, the RBM tries to identify
    similar users based on their past behavior of rating various items, and then tries
    to recommend the next best item.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use the movielens dataset from the Grouplens research
    organization. The datasets (`movies.dat` and `ratings.dat`) can be downloaded
    from the following link. `Movies.dat` contains information of 3,883 movies and
    `Ratings.dat` contains information of 1,000,209 user ratings for these movies.
    The ratings range from 1 to 5, with 5 being the highest.
  prefs: []
  type: TYPE_NORMAL
- en: '[http://files.grouplens.org/datasets/movielens/ml-1m.zip](http://files.grouplens.org/datasets/movielens/ml-1m.zip)'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe covers the steps for setting up collaborative filtering using an
    RBM.
  prefs: []
  type: TYPE_NORMAL
- en: 'Read the `movies.dat` datasets in R:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Add a new column (`id_order`) to the movies dataset, as the current ID column
    (`UserID`) cannot be used to index movies because they range from 1 to 3,952:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Read the `ratings.dat` dataset in R:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Merge the movies and ratings datasets with `all=FALSE`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Remove the non-required columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert the ratings to percentages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Generate a matrix of ratings across all the movies for a sample of 1,000 users:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Look at the distribution of the `trX` training dataset. It seems to follow
    a Bernoulli distribution (values in the range of 0 to 1):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the input model parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Start a new TensorFlow session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the RBM using 500 epoch iterations and a batch size of 100:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot reconstruction mean squared errors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Performing a full run of training an RBM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using the same RBM setup mentioned in the preceding recipe, train the RBM on
    the user ratings dataset (`trX`) using 20 hidden nodes. To keep a track of the
    optimization, the MSE is calculated after every batch of 1,000 rows. The following
    image shows the declining trend of mean squared reconstruction errors computed
    for 500 batches (equal to epochs):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00027.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '**Looking into RBM recommendations**: Let''s now look into the recommendations
    generated by RBM-based collaborative filtering for a given user ID. Here, we will
    look into the top-rated genres and top-recommended genres of this user ID, along
    with the top 10 movie recommendations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image illustrates a list of top-rated genres:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00119.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following image illustrates a list of top-recommended genres:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00145.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section provides the requirements for collaborative filtering the output
    evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow in R is installed and set up
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `movies.dat` and `ratings.dat` datasets are loaded in environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The recipe *Setting up a Restricted Boltzmann machine for Collaborative Filtering*
    has been executed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This recipe covers the steps for evaluating the output from RBM-based collaborative
    filtering:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Select the ratings of a user:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Remove the movies that were not rated by the user (assuming that they have
    yet to be seen):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the top genres seen by the user:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Reconstruct the input vector to obtain the recommendation percentages for all
    the genres/movies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the top-recommended genres:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Find the top 10 recommended movies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The following image shows the top 10 recommended movies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00126.gif)'
  prefs: []
  type: TYPE_IMG
- en: Setting up a Deep Belief Network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep belief networks are a type of **Deep Neural Network** (**DNN**), and are
    composed of multiple hidden layers (or latent variables). Here, the connections
    are present only between the layers and not within the nodes of each layer. The
    DBN can be trained both as an unsupervised and supervised model.
  prefs: []
  type: TYPE_NORMAL
- en: The unsupervised model is used to reconstruct the input with noise removal and
    the supervised model (after pretraining) is used to perform classification. As
    there are no connections within the nodes in each layer, the DBNs can be considered
    as a set of unsupervised RBMs or autoencoders, where each hidden layer serves
    as a visible layer to its subsequent connected hidden layer.
  prefs: []
  type: TYPE_NORMAL
- en: This kind of stacked RBM enhances the performance of input reconstruction where
    CD is applied across all layers, starting from the actual input training layer
    and finishing at the last hidden (or latent) layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'DBNs are a type of graphical model that train the stacked RBMs in a greedy
    manner. Their networks tend to learn the deep hierarchical representation using
    joint distributions between the input feature vector *i* and hidden layers *h[1,2....m]*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00117.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *i* = *h[0]* ; *P(h[k-1]|h[k])* is a conditional distribution of reconstructed
    visible units on the hidden layers of the RBM at level *k*; *P(h[m-1],h[m])* is
    the joint distribution of hidden and visible units (reconstructed) at the final
    RBM layer of the DBN. The following image illustrates a DBN of four hidden layers,
    where **W** represents the weight matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00142.gif)'
  prefs: []
  type: TYPE_IMG
- en: DBNs can also be used to enhance the robustness of DNNs. DNNs face an issue
    of local optimization while implementing backpropagation. This is possible in
    scenarios where an error surface features numerous troughs, and the gradient descent,
    due to backpropagation occurs inside a local deep trough (not a global deep trough).
    DBNs, on the other hand, perform pretraining of the input features, which helps
    the optimization direct toward the global deepest trough, and then use backpropagation,
    to perform a gradient descent to gradually minimize the error rate.
  prefs: []
  type: TYPE_NORMAL
- en: '**Training a stack of three RBMs**: In this recipe, we will train a DBN using
    three stacked RBMs, where the first hidden layer will have 900 nodes, the second
    hidden layer will have 500 nodes, and the third hidden layer will have 300 nodes.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section provides the requirements for TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset is loaded and set up
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Load the `TensorFlow` package using the following script:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This recipe covers the steps for setting up **Deep belief network** (**DBM**):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the number of nodes in each hidden layer as a vector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Generate an RBM function leveraging the codes illustrated in the *Setting up
    a Restricted Boltzmann Machine for Bernoulli distribution input* recipe with the
    following input and output parameters mentioned:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00153.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here is the function for setting up up the RBM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the RBM for all three different types of hidden nodes in a sequence.
    In other words, first train RBM1 with 900 hidden nodes, then use the output of
    RBM1 as an input for RBM2 with 500 hidden nodes and train RBM2, then use the output
    of RBM2 as an input for RBM3 with 300 hidden nodes and train RBM3\. Store the
    outputs of all three RBMs as a list, `RBM_output`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a data frame of batch errors across three hidden layers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot reconstruction mean squared errors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Assessing the performance of training three stacked RBMs**: Here, we will
    run five epochs (or iterations) for each RBM. Each epoch will perform batchwise
    (size = 100) optimization. In each batch, CD is computed and, accordingly, weights
    and biases are updated.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To keep a track of optimization, the MSE is calculated after every batch of
    10,000 rows. The following image shows the declining trend of mean squared reconstruction
    errors computed for 30 batches for three RBMs separately:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00041.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Implementing a feed-forward backpropagation Neural Network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will implement a willow neural network with backpropagation.
    The input of the neural network is the outcome of the third (or last) RBM. In
    other words, the reconstructed raw data (`trainX`) is actually used to train the
    neural network as a supervised classifier of (10) digits. The backpropagation
    technique is used to further fine-tune the performance of classification.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section provides the requirements for TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset is loaded and set up
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `TensorFlow` package is set up and loaded
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section covers the steps for setting up a feed-forward backpropagation
    Neural Network:'
  prefs: []
  type: TYPE_NORMAL
- en: Let's define the input parameters of the neural network as function parameters.
    The following table describes each parameter:![](img/00047.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The neural network function will have a structure as shown in the following
    script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize a weight and bias list of length 4, with the first being a tensor
    of random normal distribution (with a standard deviation of 0.01) of dimensions
    784 x 900, the second being 900 x 500, the third being 500 x 300, and the fourth
    being 300 x 10:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Check whether the outcome of the stacked RBM conforms to the sizes of the hidden
    layers mentioned in the `dbn_sizes` parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, place the weights and biases in suitable positions within `weight_list`
    and `bias_list`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Create placeholders for the input and output data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, use the weights and biases obtained from the stacked RBM to reconstruct
    the input data and store each RBM''s reconstructed data in the list `input_sub`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the cost function--that is, the mean squared error of difference between
    prediction and actual digits:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Implement backpropagation for the purpose of minimizing the cost:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Generate the prediction results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform iterations of training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, return a list of four outcomes, which are train accuracy (`train_accuracy`),
    test accuracy (`test_accuracy`), a list of weight matrices generated in each iteration
    (`weight_list`), and a list of bias vectors generated in each iteration (`bias_list`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the iterations for the defined neural network for training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code is used to plot the train and test accuracy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Assessing the train and test performance of the neural network**: The following
    image shows the increasing trend of train and test accuracy observed while training
    the neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00068.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Setting up a Deep Restricted Boltzmann Machine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unlike DBNs, **Deep Restricted Boltzmann Machines** (**DRBM**) are undirected
    networks of interconnected hidden layers with the capability to learn joint probabilities
    over these connections. In the current setup, centering is performed where visible
    and hidden variables are subtracted from offset bias vectors after every iteration.
    Research has shown that centering optimizes the performance of DRBMs and can reach
    higher log-likelihood values in comparison with traditional RBMs.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section provides the requirements for setting up a DRBM:'
  prefs: []
  type: TYPE_NORMAL
- en: The `MNIST` dataset is loaded and set up
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `tensorflow` package is set up and loaded
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section covers detailed the steps for setting up the DRBM model using
    TensorFlow in R:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the parameters for the DRBM:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a sigmoid function using a hyperbolic arc tangent *[(log(1+x) -log(1-x))/2]*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a sigmoid function using only a hyperbolic tangent *[(e^x-e^(-x))/(e^x+e^(-x))]*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a `binarize` function to return a matrix of binary values (0,1):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a `re_construct` function to return a matrix of pixels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a function to perform `gibbs` activation for a given layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a function to perform the reparameterization of bias vectors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a function to perform the reparameterization of offset bias vectors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a function to initialize weights, biases, offset biases, and input matrices:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the MNIST train data (`trainX`) introduced in the previous recipes. Standardize
    the `trainX` data by dividing it by 255:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'Generate the initial weight matrices, bias vectors, offset bias vectors, and
    input matrices:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'Subset a sample (`minbatch_size`) of the input data `X`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform a set of 1,000 iterations. Within each iteration, update the initial
    weights and biases 100 times and plot the images of the weight matrices:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As the preceding DRBM is trained using two hidden layers, we generate two weight
    matrices. The first weight matrix defines the connection between the visible layer
    and the first hidden layer. The second weight matrix defines the connection between
    the first and second hidden layer. The following image shows pixel images of the
    first weight matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00006.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following image shows the second pixel images of the second weight matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00116.gif)'
  prefs: []
  type: TYPE_IMG
