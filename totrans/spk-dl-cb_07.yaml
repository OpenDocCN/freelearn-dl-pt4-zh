- en: Natural Language Processing with TF-IDF
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, the following recipes will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: Downloading the therapy bot session text dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing the therapy bot session dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing word counts in the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculating sentiment analysis of text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Removing stop words from the text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training the TF-IDF model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating TF-IDF model performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing model performance to a baseline score
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Natural language processing** (**NLP**) is all over the news lately, and
    if you ask five different people, you will get ten different definitions. Recently
    NLP has been used to help identify bots or trolls on the internet trying to spread
    fake news or, even worse, tactics such as cyberbullying. In fact, recently there
    was a case in Spain where a student at a school was getting cyberbullied through
    social media accounts and it was having such a serious effect on the health of
    the student that the teachers started to get involved. The school reached out
    to researchers who were able to help identify several potential sources for the
    trolls using NLP methods such as TF-IDF. Ultimately, the list of potential students
    was presented to the school and when confronted the actual suspect admitted to
    being the perpetrator. The story was published in a paper titled *Supervised Machine
    Learning for the Detection of Troll Profiles in Twitter Social Network: Application
    to a Real Case of Cyberbullying* by Patxi Galan-Garcıa, Jose Gaviria de la Puerta,
    Carlos Laorden Gomez, Igor Santos, and Pablo Garcıa Bringas.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This paper highlights the ability to utilize several varying methods to analyze
    text and develop human-like language processing. It is this methodology that incorporates
    NLP into machine learning, deep learning, and artificial intelligence. Having
    machines able to ingest text data and potentially make decisions from that same
    text data is the core of natural language processing. There are many algorithms
    that are used for NLP, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word2Vec
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: N-grams
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Latent Dirichlet allocation (LDA)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Long short-term memory (LSTM)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter will focus on a dataset that contains conversations between an
    individual and a chatbot from an online therapy website. The purpose of the chatbot
    is to recognize conversations that need to be flagged for immediate attention
    to an individual rather than continued discussion with the chatbot. Ultimately,
    we will focus on using a TF-IDF algorithm to perform text analysis on the dataset
    to determine whether the chat conversation warrants a classification that needs
    to be escalated to an individual or not. **TF-IDF** stands for **Term Frequency-Inverse
    Document Frequency**. This is a technique commonly used in algorithms to identify
    the importance of a word in a document. Additionally, TF-IDF is easy to compute
    especially when dealing with high word counts in documents and has the ability
    to measure the uniqueness of a word. This comes in handy when dealing with a chatbot
    data. The main goal is to quickly identify a unique word that would trigger escalation
    to an individual to provide immediate support.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading the therapy bot session text dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section will focus on downloading and setting up the dataset that will
    be used for NLP in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The dataset that we will use in this chapter is based on interactions between
    a therapy bot and visitors to an online therapy website. It contains 100 interactions
    and each interaction is tagged as either `escalate` or `do_not_escalate`. If the
    discussion warrants a more serious conversation, the bot will tag the discussion
    as `escalate` to an individual. Otherwise, the bot will continue the discussion
    with the user.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section walks through the steps for downloading the chatbot data.
  prefs: []
  type: TYPE_NORMAL
- en: Access the dataset from the following GitHub repository: [https://github.com/asherif844/ApacheSparkDeepLearningCookbook/tree/master/CH07/data](https://github.com/asherif844/ApacheSparkDeepLearningCookbook/tree/master/CH07/data)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once you arrive at the repository, right-click on the file seen in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/5333b70f-7e8a-43ff-a537-87c7392b5af1.png)'
  prefs: []
  type: TYPE_IMG
- en: Download `TherapyBotSession.csv` and save to the same local directory as the
    Jupyter notebook `SparkSession`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Access the dataset through the Jupyter notebook using the following script
    to build the `SparkSession` called `spark`, as well as to assign the dataset to
    a dataframe in Spark, called `df`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section explains how the chatbot data makes its way into our Jupyter notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'The contents of the dataset can be seen by clicking on TherapyBotSession.csv
    on the repository as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/b237f63c-d351-495b-bb66-37dcc5dac0d7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once the dataset is downloaded, it can be uploaded and converted into a dataframe,
    `df`. The dataframe can be viewed by executing `df.show()`, as seen in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/b869c90a-6637-4115-b795-9a9ed5a4af32.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are 3 main fields that are of particular interest to us from the dataframe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`id`: the unique id of each transaction between a visitor to the website and
    the chatbot.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`label`: since this is a supervised modeling approach where we know the outcome
    that we are trying to predict, each transaction has been classified as either
    `escalate` or `do_not_escalate`. This field will be used during the modeling process
    to train the text to identify words that would classify falling under one of these
    two scenarios.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`chat`: lastly we have the `chat` text from the visitor on the website that
    our model will classify.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The dataframe, `df`, has some additional columns, `_c3`, `_c4`, `_c5`, and
    `_c6` that will not be used in the model and therefore, can be excluded from the
    dataset using the following script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the script can be seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ff0dad66-0172-447e-897f-7d74f9138643.png)'
  prefs: []
  type: TYPE_IMG
- en: Analyzing the therapy bot session dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is always important to first analyze any dataset before applying models on
    that same dataset
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section will require importing `functions` from `pyspark.sql` to be performed
    on our dataframe.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following section walks through the steps to profile the text data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute the following script to group the `label` column and to generate a
    count distribution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Add a new column, `word_count`, to the dataframe, `df`, using the following
    script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Aggregate the average word count, `avg_word_count`, by `label` using the following
    script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following section explains the feedback obtained from analyzing the text
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is useful to collect data across multiple rows and group the results by
    a dimension. In this case, the dimension is `label`. A `df.groupby()` function
    is used to measure the count of 100 therapy transactions online distributed by `label`.
    We can see that there is a `65`:`35` distribution of `do_not_escalate` to `escalate`
    as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/9c270c9f-e510-4548-878c-cdd70926792c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A new column, `word_count`, is created to calculate how many words are used
    in each of the 100 transactions between the chatbot and the online visitor. The
    newly created column, `word_count`, can be seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/4d3b4867-b4cc-4174-b011-28f25f2622ac.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since the `word_count` is now added to the dataframe, it can be aggregated
    to calculate the average word count by `label`. Once this is performed, we can
    see that `escalate` conversations on average are more than twice as long as `do_not_escalate`
    conversations, as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/a4bb5dfb-f9cf-4c26-b408-3ef612192169.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualizing word counts in the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A picture is worth a thousand words and this section will set out to prove that.
    Unfortunately, Spark does not have any inherent plotting capabilities as of version
    2.2\. In order to plot values in a dataframe, we must convert to `pandas`.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section will require importing `matplotlib` for plotting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section walks through the steps to convert the Spark dataframe into a visualization
    that can be seen in the Jupyter notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'Convert Spark dataframe to a `pandas` dataframe using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the dataframe using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section explains how the Spark dataframe is converted to `pandas` and then
    plotted.
  prefs: []
  type: TYPE_NORMAL
- en: A subset of the Spark dataframe is collected and converted to `pandas` using
    the `toPandas()` method in Spark.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'That subset of data is then plotted using matplotlib setting the y-values to
    be `word_count` and the x-values to be the `id` as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/07a5e35f-d769-4487-bd3f-84995ef9ace5.png)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are other plotting capabilities in Python other than `matplotlib` such
    as `bokeh`, `plotly`, and `seaborn`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To learn more about `bokeh`, visit the following website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://bokeh.pydata.org/en/latest/](https://bokeh.pydata.org/en/latest/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'To learn more about `plotly`, visit the following website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://plot.ly/](https://plot.ly/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'To learn more about `seaborn`, visit the following website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://seaborn.pydata.org/](https://seaborn.pydata.org/)'
  prefs: []
  type: TYPE_NORMAL
- en: Calculating sentiment analysis of text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sentiment analysis is the ability to derive tone and feeling behind a word or
    series of words. This section will utilize techniques in python to calculate a
    sentiment analysis score from the 100 transactions in our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section will require using functions and data types within PySpark. Additionally,
    we well importing the `TextBlob` library for sentiment analysis. In order to use
    SQL and data type functions within PySpark, the following must be imported:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Additionally, in order to use `TextBlob`, the following library must be imported:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following section walks through the steps to apply sentiment score to the
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a sentiment score function, `sentiment_score`, using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Apply `sentiment_score` to each conversation response in the dataframe using
    the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a `lambda` function, called `sentiment_score_udf`, that maps `sentiment_score`
    into a user-defined function within Spark, `udf`, to each transaction and specifies
    the output type of `FloatType()` as seen in the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Apply the function, `sentiment_score_udf`, to each `chat` column in the dataframe
    as seen in the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Calculate the average sentiment score, `avg_sentiment_score`, by `label` using
    the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section explains how a Python function is converted into a user-defined
    function, `udf`, within Spark to apply a sentiment analysis score to each column
    in the dataframe.
  prefs: []
  type: TYPE_NORMAL
- en: '`Textblob` is a sentiment analysis library in Python. It can calculate the
    sentiment score from a method called `sentiment.polarity` that is scored from
    -1 (very negative) to +1 (very positive) with 0 being neutral. Additionally, `Textblob`
    can measure subjectivity from 0 (very objective) to 1 (very subjective); although,
    we will not be measuring subjectivity in this chapter.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'There are a couple of steps to applying a Python function to Spark dataframe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Textblob` is imported and a function called `sentiment_score` is applied to
    the `chat` column to generate the sentiment polarity of each bot conversation
    in a new column, also called `sentiment_score`.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: A Python function cannot be directly applied to a Spark dataframe without first
    going through a user-defined function transformation, `udf`, within Spark.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Additionally, the output of the function must also be explicitly stated, whether
    it be an integer or float data type. In our situation, we explicitly state that
    the output of the function will be using the `FloatType() from pyspark.sql.types`.
    Finally, the sentiment is applied across each row using a `lambda` function within
    the `udf` sentiment score function, called `sentiment_score_udf`.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The updated dataframe with the newly created field,`sentiment score`, can be
    seen by executing `df.show()`, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/c71a115d-071e-44ba-a103-39370ba0b7d1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that the `sentiment_score` is calculated for each response from the chat
    conversation, we can denote a value range of -1 (very negative polarity) to +1
    (very positive polarity) for each row. Just as we did with counts and average
    word count, we can compare whether `escalate` conversations are more positive
    or negative in sentiment than `do_not_escalate` conversations on average. We can
    calculate an average sentiment score, `avg_sentiment_score`, by `label` as seen
    in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/c0f5585d-bb43-490e-9d4f-b73cb674d47e.png)'
  prefs: []
  type: TYPE_IMG
- en: Initially, it would make sense to assume that `escalate` conversations would
    be more negative from a polarity score than `do_not_escalate`. We actually find
    that `escalate` is slightly more positive in polarity than `do_not_escalate`;
    however, both are pretty neutral as they are close to 0.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about the `TextBlob` library, visit the following website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://textblob.readthedocs.io/en/dev/](http://textblob.readthedocs.io/en/dev/)'
  prefs: []
  type: TYPE_NORMAL
- en: Removing stop words from the text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A stop word is a very common word used in the English language and is often
    removed from common NLP techniques because they can be distracting. Common stop
    word would be words such as *the* or *and*.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section requires importing the following libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section walks through the steps to remove stop words.
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute the following script to extract each word in `chat` into a string within
    an array:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Assign a list of common words to a variable, `stop_words`, that will be considered
    stop words using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following script to import the `StopWordsRemover` function from
    PySpark and configure the input and output columns, `words` and `word without
    stop`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following script to import Pipeline and define the `stages` for
    the stop word transformation process that will be applied to the dataframe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, apply the stop word removal transformation, `pipelineFitRemoveStopWords`,
    to the dataframe, `df`, using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section explains how to remove stop words from the text.
  prefs: []
  type: TYPE_NORMAL
- en: Just as we did by applying some analysis when profiling and exploring the `chat` data,
    we can also tweak the text of the `chat` conversation and break up each word into
    a separate array. This will be used to isolate stop words and remove them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The new column with each word extracted as a string is called `words` and can
    be seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/10f69edb-5e82-4230-9d8d-0356a7b7df3f.png)'
  prefs: []
  type: TYPE_IMG
- en: There are many ways to assign a group of words to a stop word list. Some of
    these words can be automatically downloaded and updated using a proper Python
    library called `nltk`, which stands for natural language toolkit. For our purposes,
    we will utilize a common list of 124 stop words to generate our own list. Additional
    words can be easily added or removed from the list manually.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Stop words do not add any value to the text and will be removed from the newly
    created column by specifying `outputCol="words without stop"`. Additionally, the
    column that will serve as the source for the transformation is set by specifying `inputCol
    = "words"`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We create a pipeline, `stopWordRemovalPipeline`, to define the sequence of steps
    or `stages` that will transform the data. In this situation, the only stage that
    will be used to transform the data is the feature, `stopwordsRemover`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Each stage in a pipeline can have a transforming role and an estimator role.
    The estimator role, `pipeline.fit(df)`, is called on to produce a transformer
    function called `pipelineFitRemoveStopWords`. Finally, the `transform(df)` function
    is called on the dataframe to produce an updated dataframe with a new column called `words
    without stop`. We can compare both columns side by side to examine the differences
    as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/164af41e-b68a-4e6d-aaa4-ba4236e2f1d3.png)'
  prefs: []
  type: TYPE_IMG
- en: The new column, `words without stop`, contains none of the strings that are
    considered stop words from the original column, `words`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about stop words from `nltk`, visit the following website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.nltk.org/data.html](https://www.nltk.org/data.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'To learn more about Spark machine learning pipelines, visit the following website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://spark.apache.org/docs/2.2.0/ml-pipeline.html](https://spark.apache.org/docs/2.2.0/ml-pipeline.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'To learn more about the `StopWordsRemover` feature in PySpark, visit the following
    website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.feature.StopWordsRemover](https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.feature.StopWordsRemover)'
  prefs: []
  type: TYPE_NORMAL
- en: Training the TF-IDF model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are now ready to train our TF-IDF NLP model and see if we can classify these
    transactions as either `escalate` or `do_not_escalate`.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section will require importing from `spark.ml.feature` and `spark.ml.classification`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following section walks through the steps to train the TF-IDF model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new user-defined function, `udf`, to define numerical values for the `label`
    column using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following script to set the TF and IDF columns for the vectorization
    of the words:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up a pipeline, `pipelineTFIDF`, to set the sequence of stages for `TF_` and `IDF_` using
    the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Fit and transform the IDF estimator onto the dataframe, `df`, using the following
    script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Split the dataframe into a 75:25 split for model evaluation purposes using
    the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Import and configure a classification model, `LogisticRegression`, using the
    following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Fit the logistic regression model, `logreg`, onto the training dataframe, `trainingDF.` A
    new dataframe, `predictionDF`, is created based on the `transform()` method from
    the logistic regression model, as seen in the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following section explains to effectively train a TF-IDF NLP model.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is ideal to have labels in a numerical format rather than a categorical
    form as the model is able to interpret numerical values while classifying outputs
    between 0 and 1. Therefore, all labels under the `label` column are converted
    to a numerical `label` of 0.0 or 1.0, as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/41d1c54e-e6c8-43c9-9027-0d4cad7e3fcc.png)'
  prefs: []
  type: TYPE_IMG
- en: TF-IDF models require a two-step approach by importing both `HashingTF` and
    `IDF` from `pyspark.ml.feature` to handle separate tasks. The first task merely
    involves importing both `HashingTF` and `IDF` and assigning values for the input
    and subsequent output columns. The `numfeatures` parameter is set to 100,000 to
    ensure that it is larger than the distinct number of words in the dataframe. If
    `numfeatures` were to be than the distinct word count, the model would be inaccurate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As stated earlier, each step of the pipeline contains a transformation process
    and an estimator process. The pipeline, `pipelineTFIDF`, is configured to order
    the sequence of steps where `IDF` will follow `HashingTF`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`HashingTF` is used to transform the `words without stop` into vectors within
    a new column called `rawFeatures`. Subsequently, `rawFeatures` will then be consumed
    by `IDF` to estimate the size and fit the dataframe to produce a new column called `features`,
    as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/988894fc-443a-47ba-a01b-b3154e088e6a.png)'
  prefs: []
  type: TYPE_IMG
- en: For training purposes, our dataframe will be conservatively split into a `75`:`25`
    ratio with a random seed set at `1234`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Since our main goal is to classify each conversation as either `escalate` for
    escalation or `do_not_escalate` for continued bot chat, we can use a traditional
    classification algorithm such as a logistic regression model from the PySpark
    library. The logistic regression model is configured with a regularization parameter,
    `regParam`, of 0.025\. We use the parameter to slightly improve the model by minimizing
    overfitting at the expense of a little bias.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The logistic regression model is trained and fitted on `trainingDF`, and then
    a new dataframe, `predictionDF`, is created with the newly transformed field, `prediction`,
    as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/180abef6-2eba-44ba-9da6-f2688e4996f6.png)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While we did use the user-defined function, `udf`, to manually create a numerical
    label column, we also could have used a built-in feature from PySpark called `StringIndexer`
    to assign numerical values to categorical labels. To see `StringIndexer` in action,
    visit [Chapter 5](9cc3bf45-b46d-4c37-920c-87d6fdab58c2.xhtml), *Predicting Fire
    Department Calls with Spark ML*.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about the TF-IDF model within PySpark, visit the following website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://spark.apache.org/docs/latest/mllib-feature-extraction.html#tf-idf](https://spark.apache.org/docs/latest/mllib-feature-extraction.html#tf-idf)'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating TF-IDF model performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, we are ready to evaluate our model's performance
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section will require importing the following libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '`metrics` from `sklearn`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BinaryClassificationEvaluator` from `pyspark.ml.evaluation`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section walks through the steps to evaluate the TF-IDF NLP model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a confusion matrix using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Evaluate the model using `metrics` from sklearn with the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Calculate the ROC score using the following script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section explains how we use the evaluation calculations to determine the
    accuracy of our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'A confusion matrix is helpful to quickly summarize the accuracy numbers between
    actual results and predicted results. Since we had a 75:25 split, we should see
    25 predictions from our training dataset. We can build a build a confusion matric
    using the following script: `predictionDF.crosstab(''label'', ''prediction'').show()`.
    The output of the script can be seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/7ec9d6e3-2748-475a-a432-bc4427fcc349.png)'
  prefs: []
  type: TYPE_IMG
- en: We are now at the stage of evaluating the accuracy of the model by comparing
    the `prediction` values against the actual `label` values. `sklearn.metrics` intakes
    two parameters, the `actual` values tied to the `label` column, as well as the
    `predicted` values derived from the logistic regression model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Please note that once again we are converting the column values from Spark dataframes
    to pandas dataframes using the `toPandas()` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two variables are created, `actual` and `predicted`, and an accuracy score
    of 91.7% is calculated using the `metrics.accuracy_score()` function, as seen
    in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/39d21f82-8cb9-4106-ba4a-aee671c529b8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The ROC (Receiver Operating Characteristic) is often associated with a curve
    measuring the true positive rate against the false positive rate. The greater
    the area under the curve, the better. The ROC score associated with the curve
    is another indicator that can be used to measure the performance of the model.
    We can calculate the `ROC` using the `BinaryClassificationEvaluator` as seen in
    the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/18d393de-bf20-448d-a5c3-2a7d35018680.png)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about the `BinaryClassificationEvaluator` from PySpark, visit
    the following website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://spark.apache.org/docs/2.2.0/api/java/index.html?org/apache/spark/ml/evaluation/BinaryClassificationEvaluator.html](https://spark.apache.org/docs/2.2.0/api/java/index.html?org/apache/spark/ml/evaluation/BinaryClassificationEvaluator.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Comparing model performance to a baseline score
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While it is great that we have a high accuracy score from our model of 91.7
    percent, it is also important to compare this to a baseline score. We dig deeper
    into this concept in this section.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section walks through the steps to calculate the baseline accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute the following script to retrieve the mean value from the `describe()`
    method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Subtract `1- mean value score` to calculate baseline accuracy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section explains the concept behind the baseline accuracy and how we can
    use it to understand the effectiveness of our model.
  prefs: []
  type: TYPE_NORMAL
- en: What if every `chat` conversation was flagged for `do_not_escalate` or vice
    versa. Would we have a baseline accuracy higher than 91.7 percent? The easiest
    way to figure this out is to run the `describe()` method on the `label` column
    from `predictionDF` using the following script: `predictionDF.describe('label').show()`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The output of the script can be seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/ae9f9a3b-b15f-4c4d-8e6c-8b49a096f6a9.png)'
  prefs: []
  type: TYPE_IMG
- en: The mean of `label` is at 0.2083 or ~21%, which means that a `label` of 1 occurs
    only 21% of the time. Therefore, if we labeled each conversation as `do_not_escalate`,
    we would be correct ~79% of the time, which is less than our model accuracy of
    91.7%.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Therefore, we can say that our model performs better than a blind baseline performance
    model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about the `describe()` method in a PySpark dataframe, visit the
    following website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.describe](http://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.describe)'
  prefs: []
  type: TYPE_NORMAL
