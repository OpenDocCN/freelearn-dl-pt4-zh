["```py\n# define input string\ndata = 'the quick brown fox jumped over the lazy dog'\nconsecutive_words = data.split()\nprint(data)\n\n# construct the dictionary\nall_words = list(set(consecutive_words))\n\n# define a mapping of word to integers\nword_to_int = dict((w, i) for i, w in enumerate(all_words))\nint_to_word = dict((i, w) for i, w in enumerate(all_words))\n# integer encode input data\ninteger_encoded = [word_to_int[w] for w in consecutive_words]\n\n# one hot encode\nonehot_encoded = list()\nfor value in integer_encoded:\n    letter = [0 for _ in range(len(all_words))]\n    letter[value] = 1\n    onehot_encoded.append(letter)\n_ = [print(x) for x in onehot_encoded]\n\ndef argmax(vector):\n    # since vector is actually a list and its one hot encoding hence the\n    # maximum value is always 1\n    return vector.index(1)\n\n# invert encoding\ninverted = int_to_word[argmax(onehot_encoded[0])]\nprint(inverted)\n```", "```py\nDocument1: \"John likes to watch movies. Mary likes too.\"\nDocument2: \"John also likes to watch football games.\"\n```", "```py\n['also', 'football', 'games', 'john', 'likes', 'mary', 'movies', 'to', 'too', 'watch']\n```", "```py\nDocument1: {'likes': 2, 'John': 1, 'to': 1, 'watch': 1, 'movies': 1, 'Mary': 1, 'too': 1}\nDocument2: {'John': 1, 'also': 1, 'likes': 1, 'to': 1, 'watch': 1, 'football': 1, 'games': 1}\n```", "```py\nimport collections, re\ntexts = ['John likes to watch movies. Mary likes too.', 'John also likes to watch football games.']\nbagsofwords = [collections.Counter(re.findall(r'\\w+', txt)) for txt in texts]\n```", "```py\n\nprint(bagsofwords[0])\nprint(bagsofwords[1])\nsumbags = sum(bagsofwords, collections.Counter())\nprint(sumbags)\n```", "```py\nP(\"Even now They talked in Their tombs.\") = P(\"Even\") * P(\"now\") * P(\"They\") * P(\"talked\") * P(\"in\") * P(\"Their\") * P(\"tombs\")\n```", "```py\nP(\"Even now They talked in Their tombs.\") = P(\"Even\" | start of sentence) * (\"now\" | \"Even\") * (\"They\" | \"now\") * (\"talked\" | \"They\") * (\"in\" | \"talked\") * (\"Their\" | \"in\") * (\"tombs\" | \"Their\") * (end of sentence | \"tombs\")\n```", "```py\n$ cat data/persuasion.txt | tr '\\n' ' ' | tr -s ' ' | tr -sc 'A-Za-z' '\\012' |   sed -e '1!{$!p' -e '}' | paste -d' ' - - | sort | uniq -c | sort -nr > data/persuasion_bigrams.txt\n```", "```py\ndef get_next_word(ngram_file, word1=None, sentence_length=0):\n    with open(ngram_file) as f:\n        for line in f:\n            _, w1, w2 = line.split()\n            if word1 is None or word1 == w1:\n                sentence_length -= 1\n                word1 = w2\n                return w1, word1, sentence_length\n\ndef build_sentence(ngram_file, sentence_length):\n    first_word = None\n    sentence = ''\n    while sentence_length > 0:\n        w1, first_word, sentence_length = get_next_word(ngram_file, first_word, sentence_length)\n        sentence = sentence + ' ' + w1\n    final_sentence = sentence + ' ' + first_word + '.'\n    return final_sentence\n\nprint(build_sentence('data/persuasion_bigrams.txt', 10))\n```", "```py\n$ python build_sentence_ngrams.py\n of the same time to be a very well as she.\n```", "```py\n\"shadenfreude\" = {\"sha\", \"had\", \"ade\", ..., \"shad\", \"frue\", ..., \"freude\", ..., \"shadenfreude\"}\n```", "```py\ncouples = []\nlabels = []\nfor i, wi in enumerate(sequence):\n    if not wi:\n        continue\n\n    window_start = max(0, i - window_size)\n    window_end = min(len(sequence), i + window_size + 1)\n    for j in range(window_start, window_end):\n        if j != i:\n            wj = sequence[j]\n            if not wj:\n                continue\n            couples.append([wi, wj])\n            if categorical:\n                labels.append([0, 1])\n            else:\n                labels.append(1)\n```", "```py\nnum_negative_samples = int(len(labels) * negative_samples)\nwords = [c[0] for c in couples]\nrandom.shuffle(words)\n\ncouples += [[words[i % len(words)],\n            random.randint(1, vocabulary_size - 1)] # basically get some out of context word indices\n            for i in range(num_negative_samples)]\n```", "```py\n\nif categorical:\n    labels += [[1, 0]] * num_negative_samples # opposite of what you would define for positive samples\nelse:\n    labels += [0] * num_negative_samples\n```", "```py\nfor _ in range(epochs):\n    loss = 0.\n    for i, doc in enumerate(tokenizer.texts_to_sequences(corpus)):\n        data, labels = skipgrams(sequence=doc, vocabulary_size=V, window_size=5, negative_samples=5.)\n        x = [np.array(x) for x in zip(*data)]\n        y = np.array(labels, dtype=np.int32)\n        if x:\n            loss += model.train_on_batch(x, y)\n\n    print(loss)\n```", "```py\nembedding_dim = 100\n\n# inputs\nw_inputs = Input(shape=(1, ), dtype='int32')\nw = Embedding(V, embedding_dim)(w_inputs)\n\n# context\nc_inputs = Input(shape=(1, ), dtype='int32')\nc = Embedding(V, embedding_dim)(c_inputs)\no = Dot(axes=2)([w, c])\no = Reshape((1,), input_shape=(1, 1))(o)\no = Activation('sigmoid')(o)\n\nft_model = Model(inputs=[w_inputs, c_inputs], outputs=o)\n# ft_model.summary()\nft_model.compile(loss='binary_crossentropy', optimizer='adam')\n\nImage(model_to_dot(ft_model, show_shapes=True).create(prog='dot', format='png'))\n```", "```py\nwith open('vectors.txt' ,'w') as f:\n    f.write('{} {}\\n'.format(V-1, embedding_dim))\n    vectors = ft_model.get_weights()[0]\n    for word, i in tokenizer.word_index.items():\n        f.write('{} {}\\n'.format(word, ' '.join(map(str, list(vectors[i, :])))))\n```", "```py\ndef generate_data_for_cbow(corpus, window_size, V):\n    maxlen = window_size*2\n    corpus = tokenizer.texts_to_sequences(corpus)\n    for words in corpus:\n        L = len(words)\n        for index, word in enumerate(words):\n            contexts = []\n            labels = [] \n            s = index - window_size\n            e = index + window_size + 1\n\n            contexts.append([words[i] for i in range(s, e) if 0 <= i < L and i != index])\n            labels.append(word)\n            x = sequence.pad_sequences(contexts, maxlen=maxlen)\n            y = np_utils.to_categorical(labels, V)\n            yield (x, y)\n```", "```py\nfor ite in range(5):\n    loss = 0.\n    for x, y in generate_data_for_cbow(corpus, window_size, V):\n        loss += cbow.train_on_batch(x, y)\n\n    print(ite, loss)\n```", "```py\ncbow = Sequential()\ncbow.add(Embedding(input_dim=V, output_dim=embedding_dim, input_length=window_size*2))\ncbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embedding_dim,)))\ncbow.add(Dense(V, activation='softmax'))\n```", "```py\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef softmax(arr):\n    return np.exp(arr)/float(sum(np.exp(arr)))\ndef line_graph(x, y, x_title, y_title):\n    plt.plot(x, y)\n    plt.xlabel(x_title)\n    plt.ylabel(y_title)\n    plt.show()\ngraph_x = range(10)\ngraph_y = softmax(graph_x)\nprint('Graph X readings: {}'.format(graph_x))\nprint('Graph Y readings: {}'.format(graph_y))\nline_graph(graph_x, graph_y, 'Inputs', 'softmax scores')\n```"]