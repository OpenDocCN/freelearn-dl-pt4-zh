- en: Pose Estimation on 3D models Using ConvNets
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Welcome to our chapter on human pose estimation. In this chapter, we will be
    building a neural network that will predict 3D human poses using 2D images. We
    will do this with the help of transfer learning by using the VGG16 model architecture
    and modifying it accordingly for our current problem. By the end of this chapter,
    you will have a **deep learning** (**DL**) model that does a really good job of
    predicting human poses.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: '**Visual effects** (**VFX**) in movies are expensive. They involve using a
    lot of expensive sensors that will be placed on the body of the actor when shooting.
    The information from these sensors will then be used to build visual effects,
    all of which ends up being super expensive. We have been asked (in this hypothetical
    use case) by a major movie studio whether we can help their graphics department
    build cheaper and better visual effects by building a human pose estimator, which
    they will use to better estimate poses on the screen while editing.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: For this task, we will be using images from **Frames Labeled In Cinema** (**FLIC**).
    These images are not ready to be used for modeling just yet. So, get ready to
    spend a bit more time on preparing the image data in this chapter. Also, we will
    only be estimating the pose of arms, shoulders, and the head.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll learn about the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Processing/preparing images for pose estimation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The VGG16 model
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transfer learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building and understanding the training loop
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing the model
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It would be better if you implement the code snippets as you go along in this
    chapter, either in a Jupyter notebook or any source code editor. This will make
    it easier for you to follow along, as well as understand what each part of the
    code does.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: All of the Python files and the Jupyter Notebooks for this chapter can be found
    at [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter12](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter12).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Code implementation
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this exercise, we will be using the Keras deep learning library, which is
    a high-level neural network API capable of running on top of TensorFlow, Theano,
    and CNTK.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: If you ever have a question related to Keras, refer to this easy-to-understand
    Keras documentation at [https://keras.io](https://keras.io).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Please download the `Chapter12` folder from GitHub before moving forward with
    this chapter.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'This project involves downloading files from various sources that will be called
    inside the scripts. To make sure that the Python scripts or the Jupyter Notebook
    have no issues locating the downloaded files, follow these steps:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Open a Terminal and change your directory by using the `cd` command in the `Chapter12`
    folder.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Download the `FLIC-full` data file with the following command:'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Unzip the ZIP file with the following command:'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Remove the ZIP file with the following command:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Change directories in the `FLIC-full` folder by using the following command:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Download the file containing the training indices:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Change the directory back to the `Chapter12` folder.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Launch your Jupyter Notebook or run the Python scripts from the `Chapter12`
    directory.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further information on the `FLIC-full` data folder can be found at [https://bensapp.github.io/flic-dataset.html](https://bensapp.github.io/flic-dataset.html).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Importing the dependencies
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will be using `numpy`, `matplotlib`, `keras`, `tensorflow`, and the `tqdm`
    package in this exercise. Here, TensorFlow is used as the backend for Keras. You
    can install these packages with `pip`. For the MNIST data, we will be using the dataset
    that''s available in the `keras` module with a simple `import`:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'It is important that you set `seed` for reproducibility:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Exploring and pre-processing the data
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the `FLIC-full` data folder downloaded and unpacked, inside the `FLIC-full` folder
    you should find the `tr_plus_indices.mat` and `examples.mat` MATLAB files, and
    also the folder named `images`, inside which are the images that will be used
    in this project.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: You will find that the images have been captured from movies such as *2 Fast
    2 Furious*, *Along Came Polly*, *American Wedding*, and a few others. Each of
    these images is 480*720 px in size. These images are nothing but screenshots of
    scenes involving actors from the selected movies, which we will use for pose estimation.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s load the MATLAB file `examples.mat`. We will do this with the help of
    the `loadmat` module, which we have imported already, along with other imports.
    Also, let''s print out some of the information from this file:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Following is the output:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/218fec22-6838-416c-af68-de63f548ad27.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.1: Example file information from printout 1'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: 'From the printout, we can see that the MATLAB file has been loaded as a dictionary
    with four keys, one of which is the one we need: the `examples` key. Let''s see
    what this key holds:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Following is the output:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d9b8d6cd-c3f0-4827-884a-2e649bbb0a4b.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
- en: Figure 12.2: Example file information from printout 1
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: The notable thing here is that the value of the `examples` key is a numpy array
    of shape (1, 20928). You will also see that the array has been reshaped to shape
    `(20928,)`. The `examples` key contains the IDs of the images (in the `images`
    folder) and the corresponding pose coordinates that can be used for modeling.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s print out an image ID and its corresponding coordinates array with its
    shape. The image ID we need is stored at index `3`, and the corresponding coordinates
    are at index `2`. Let''s print these out:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Following is the output:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/212857ff-d2e7-4450-bcdc-f2048900451f.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
- en: Figure 12.3: Example file information from printout 2
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: 'From the preceding screenshot, we can see that the coordinates array is of
    shape (2,29):'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Following is the output:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c0036acc-54ae-44b1-8f58-654bf1668521.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.4: List of joint labels'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 'But, if you look back at the coordinates array that we printed in the preceding
    screenshot, out of the 29 coordinates, we only have information on 11 of the body
    joints/locations. These are as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Following is the output:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/24a820f3-2079-4162-9183-9a3a15431900.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.5: List of joint labels with coordinates'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 'For the purpose of this project, we only need information on the following
    body joints/locations:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Following is the output:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/33982f76-450e-4511-9f30-0b326d58db3f.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.6: Required joints and their indices in the array'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '`lsho`: Left shoulder'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '`lelb`: Left elbow'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '`lwri`: Left wrist'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '`rsho`: Right shoulder'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '`relb`: Right elbow'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '`rwri`: Right wrist'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '`leye`: Left eye'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '`reye`: Right eye'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '`nose`: Nose'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s define a function that takes in a dictionary of nine joint labels
    and coordinates and returns a list with seven coordinates (*7* (*x*,*y*) pairs).
    The reason for seven coordinates is that the `leye`, `reye`, and the `nose` coordinates
    are converted into one head coordinate when we take the mean across them:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now let''s load the `tr_plus_indices.mat` MATLAB file, just like we did previously:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: The reason why we need to use the `tr_plus_indices.mat` file is because it contains
    indices of images that should only be used for training, as well as some unlisted
    ones for testing. The reason for such a split is to make sure that the train set
    and the test set have frames from completely different movies so as to avoid overfitting.
    More on this can be found at [https://bensapp.github.io/flic-dataset.html](https://bensapp.github.io/flic-dataset.html).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Following is the output:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/facd390a-cf75-42f5-aa65-a71b2491ef90.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.7: train_indices file information printout 1'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: 'From the preceding screenshot, you can see that the MATLAB file has been loaded
    as a dictionary with four keys, one of which is `tr_plus_indices`, which is the
    one we need. Let''s look at the content of this key:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Following is the output:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d444061b-4688-40f8-910a-e6371a2d6dee.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.8: train_indices file information printout 2'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the `tr_plus_indices` key corresponds to a (17380*1) shaped
    array. We will reshape this to `(17380, )` for convenience.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '`tr_plus_indices` contains the indices of the data in the `examples` key of
    the `examples.mat` file, which should only be used for training. Using this information,
    we will subset the data into a train set and a test set:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'For the remaining part of this code snippet, please refer to the `deeppose.ipynb`
    file here : [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the train data has 17,380 data points, with each data point
    having an image ID and *7*(*x*,*y*) joint coordinates. Similarly, the test data
    has 3,548 data points.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding snippet, we first initialize four empty lists, two for saving
    train and test image IDs, and two for saving train and test joints. Then, for
    each data point in the `examples` key, we do the following:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们首先初始化了四个空列表，两个用于保存训练和测试图像 ID，两个用于保存训练和测试关节。然后，对于 `examples` 键中的每个数据点，我们执行以下操作：
- en: Extract the file name.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取文件名。
- en: Extract the joint coordinates.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取关节坐标。
- en: ZIP the target joints (target joint labels) and the corresponding joint coordinates
    and convert them into a dictionary.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将目标关节（目标关节标签）与相应的关节坐标配对，并将其转换为字典。
- en: Feed the dictionary to the `joint_coordinates` function to obtain the joints
    needed for this task.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将字典传递给 `joint_coordinates` 函数，以获取此任务所需的关节。
- en: Append the image IDs and the resulting joints from the previous step to a train
    or test list by using the `train_indices` list.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `train_indices` 列表，将之前步骤中得到的图像 ID 和相应的关节添加到训练或测试列表中。
- en: Finally, convert the lists into train and test data frames and save them as
    a CSV file. Make sure that you don't set the index and header parameters to `False`
    when saving the data frame as a CSV file.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将列表转换为训练和测试数据框，并将其保存为 CSV 文件。保存数据框时，请确保不将索引和标题参数设置为 `False`。
- en: 'Let''s load the `train_joints.csv` and `test_joints.csv` files we saved in
    the previous step and print out some details:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们加载在前一步保存的 `train_joints.csv` 和 `test_joints.csv` 文件，并打印出一些细节：
- en: '[PRE17]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Following is the output:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '![](img/2bc02243-2847-4e3f-a809-0e189df92240.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2bc02243-2847-4e3f-a809-0e189df92240.png)'
- en: 'Figure 12.9: Printout of image IDs and the joint''s array shape'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.9：图像 ID 和关节数组形状的打印输出
- en: 'Now, let''s load some images from the `images` folder and plot them to see
    what they look like:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们从 `images` 文件夹加载一些图像，并绘制它们，看看它们的样子：
- en: '[PRE18]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Following is the output:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '![](img/7856b703-fe37-4781-8fdb-66b7bdacda1e.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7856b703-fe37-4781-8fdb-66b7bdacda1e.png)'
- en: 'Figure 12.10: Plot of eight images from the images folder in the FLIC_full
    folder'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.10：来自 FLIC_full 文件夹中的图像文件夹的八张图像的绘图
- en: 'We can see that each image is of shape (480*720*3). Our next task will be to
    crop the original image and focus on the person of interest by using the joint
    coordinates that are available to us. We do this by resizing the images into a
    shape of 224*24*3 so that we can feed them into the VGG16 model. Finally, we will
    also build a `plotting` function to plot the joints on the image:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到每张图像的形状为 (480*720*3)。接下来的任务是通过使用我们所拥有的关节坐标来裁剪原始图像并聚焦于感兴趣的对象。我们将图像调整为 224*224*3
    的大小，以便将其输入到 VGG16 模型中。最后，我们还将构建一个 `plotting` 函数，用于在图像上绘制关节：
- en: '![](img/9fe4c5d7-476a-4402-a583-bfdd7c2a9121.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9fe4c5d7-476a-4402-a583-bfdd7c2a9121.png)'
- en: 'Figure 12.11: Plot showing the transformation each image has to go through'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.11：显示每张图像必须经过的变换的绘图
- en: Preparing the data
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备
- en: Now let's implement the functions that will perform the tasks that we discussed
    when we ended the previous section.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们实现之前章节结尾时讨论的任务所需的函数。
- en: Cropping
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 裁剪
- en: 'We will first start off with the `image_cropping()` function. This function
    accepts an image ID and its corresponding joint coordinates. It loads the image
    into memory and then crops the image so that it only includes the section of the
    image that''s bound within the coordinates. The cropped image is then padded so
    that the joints and limbs are completely visible. For the added padding, the joint
    coordinates are also adjusted accordingly. When it has done this, the image is
    returned. This is the most important part of the transformation. Take your time
    and dissect the function to see exactly what is happening (the `crop_pad_inf` and `crop_pad_sup`
    parameters control the amount of padding):'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先从 `image_cropping()` 函数开始。此函数接受图像 ID 及其相应的关节坐标。它将图像加载到内存中，然后裁剪图像，只保留图像中由坐标框定的部分。裁剪后的图像将被填充，以便关节和肢体完全可见。对于添加的填充，关节坐标也会相应调整。完成此操作后，图像将被返回。这是转换中最重要的部分。花点时间分析这个函数，看看到底发生了什么（`crop_pad_inf`
    和 `crop_pad_sup` 参数控制填充量）：
- en: '[PRE19]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'For the remaining part of this code snippet please refer to the file `deeppose.ipynb`
    here : [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb)
    [](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此代码片段的剩余部分，请参阅此处的文件`deeppose.ipynb`：[https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb)
- en: 'Let''s pass an image ID and its joints to the `image_cropping()` function and
    plot the output image:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将图像 ID 及其关节传递给`image_cropping()`函数，并绘制输出图像：
- en: '[PRE20]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Following is the output:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '![](img/cf6ad6e1-8611-4155-8cab-5e90b983a008.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cf6ad6e1-8611-4155-8cab-5e90b983a008.png)'
- en: 'Figure 12.12: Plot of the resulting cropped image compared to the original
    image'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.12：裁剪后的图像与原始图像的绘图对比
- en: Resizing
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整大小
- en: 'In the *Cropping* section, we saw that the original image of shape (480*720*3)
    is cropped to shape (393*254*3). However, the VGG16 architecture accepts images
    of shape (224*224*3). Hence, we will define a function called `image_resize()`
    that does the resizing for us. It accepts the cropped image and the joint resulting
    from the `image_cropping()` function as input and returns the resized image and
    its joint coordinates:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在*Cropping*部分中，我们看到形状为(480*720*3)的原始图像被裁剪为形状为(393*254*3)。然而，VGG16架构接受形状为(224*224*3)的图像。因此，我们将定义一个名为`image_resize()`的函数来完成调整大小。它接受裁剪后的图像及其由`image_cropping()`函数产生的关节作为输入，并返回调整大小后的图像及其关节坐标：
- en: '[PRE21]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Following is the output:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '![](img/d66c1090-a53d-4026-985b-34ba59dc99ee.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d66c1090-a53d-4026-985b-34ba59dc99ee.png)'
- en: 'Figure 12.13: Plot of the resized image'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.13：调整大小后图像的绘图
- en: After passing the cropped image to the `image_resize()` function, we can see
    that the resulting image is of shape (224*224*3). Now this image and its joints
    can be passed into the model for training.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 将裁剪后的图像传递给`image_resize()`函数后，我们可以看到生成的图像形状为(224*224*3)。现在可以将此图像及其关节传递到模型进行训练。
- en: Plotting the joints and limbs
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 绘制关节和四肢
- en: 'Let''s also define the plotting functions that will plot the limbs on the resized
    image. The following defined `plot_joints()` function accepts the resized image
    and its joints and returns an image of the same shape with the limbs plotted on
    top:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也定义绘制功能，它将在调整大小后的图像上绘制四肢。以下定义的`plot_joints()`函数接受调整大小后的图像及其关节，并返回相同形状的带有绘制四肢的图像：
- en: '[PRE22]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'For the remaining part of this code snippet please refer to the `deeppose.ipynb`
    file here: [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此代码片段的剩余部分，请参阅此处的`deeppose.ipynb`文件：[https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb)
- en: 'Following is the output:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '![](img/04effbe1-f2bc-41a1-8d0c-3d789946e1d5.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/04effbe1-f2bc-41a1-8d0c-3d789946e1d5.png)'
- en: 'Figure 12.14: Plot showing the true joint coordinates on top of the image'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.14：显示真实关节坐标在图像顶部的绘图
- en: Transforming the images
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转换图像
- en: 'Now let''s transform the images and their corresponding joints to the desired
    form by using the functions we have defined previously. We will do this with the
    help of the `model_data()` function, which is defined as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用我们之前定义的函数将图像及其对应的关节转换为所需形式。我们将借助定义如下的`model_data()`函数来实现这一点：
- en: '[PRE23]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'For the remaining part of this code snippet, please refer to the `deeppose.ipynb`
    file here: [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此代码片段的剩余部分，请参阅此处的`deeppose.ipynb`文件：[https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb)
- en: The preceding defined `model_data()` function accepts three parameters: `image_ids`
    (array of image IDs), `joints` (array of joints), and a Boolean parameter called `train`.
    Set the `train` parameter to `True` when transforming the training images and
    joints and to `False` when transforming the test images and joints.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 前述定义的 `model_data()` 函数接受三个参数：`image_ids`（图像 ID 数组）、`joints`（关节点数组），以及一个布尔类型的参数
    `train`。在转换训练图像和关节点时，将 `train` 参数设置为 `True`，在转换测试图像和关节点时，将其设置为 `False`。
- en: 'When the `train` parameter is set to `True`, perform the following steps:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 当 `train` 参数设置为 `True` 时，执行以下步骤：
- en: Initialize an empty list to store the ID of the transformed image and its joint
    coordinates.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个空列表，用于存储转换后的图像 ID 及其关节点坐标。
- en: A new directory called `train` will be created inside the `images` folder if
    the folder does not exist.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 `images` 文件夹中不存在 `train` 文件夹，则将在该文件夹内创建一个新的 `train` 文件夹。
- en: An image and its joint coordinates are first passed to the `image_cropping`
    function we defined previously, which will return the cropped image and joint
    coordinates.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先将图像及其关节点坐标传递给我们之前定义的 `image_cropping` 函数，该函数将返回裁剪后的图像和关节点坐标。
- en: The result of *Step 3* is passed to the `image_resize` function, which will
    then resize the image to the desired shape. In our case, this is 224*224*3.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*步骤 3* 的结果被传递给 `image_resize` 函数，然后该函数将图像调整为所需的形状。在我们的例子中，这是 224*224*3。'
- en: The resized image is then written into the `train` folder via the OpenCV `imwrite()`
    function with a new image ID (for example, `train0.jpg`).
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，调整大小后的图像通过 OpenCV 的 `imwrite()` 函数写入 `train` 文件夹，并附上新的图像 ID（例如 `train0.jpg`）。
- en: The new image ID and its joints are appended to the list initialized in *Step
    1.*
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 新的图像 ID 和关节点将被附加到 *步骤 1* 中初始化的列表中。
- en: '*Step 3* through *Step 6* are repeated until all of the training images are
    transformed.'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*步骤 3* 到 *步骤 6* 会重复执行，直到所有训练图像都被转换。'
- en: The list defined in *Step 1* now contains the new image IDs and the joint coordinates,
    which are then converted to a data frame and saved as a CSV file in the `train`
    folder.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 *步骤 1* 中定义的列表现在包含了新的图像 ID 和关节点坐标，这些数据将被转换为数据框并保存为 CSV 文件，存放在 `train` 文件夹中。
- en: For transforming the test data, the preceding procedure is repeated by setting
    the `train` parameter to `False` and feeding the test image IDs and the joints.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 为了转换测试数据，前述过程会重复执行，将 `train` 参数设置为 `False`，并输入测试图像 ID 和关节点。
- en: The train and test data frames that get generated inside the `model_data()`
    function are stored as a CSV file with no header and no index column. Take this
    into consideration when loading these files.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `model_data()` 函数中生成的训练和测试数据框将作为没有头部和索引列的 CSV 文件存储。加载这些文件时要注意这一点。
- en: Defining hyperparameters for training
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义训练的超参数
- en: 'The following are some of the hyperparameters that have been defined that we
    will be using throughout our code. These are totally configurable:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们在代码中将使用的一些已定义的超参数，这些都可以进行配置：
- en: '[PRE24]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Experiment with different learning rates, optimizers, batch size, as well as
    smoothing value to see how these factors affect the quality of your model. If
    you get better results, show these to the deep learning community.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试不同的学习率、优化器、批量大小，以及平滑值，看看这些因素如何影响模型的质量。如果获得了更好的结果，可以向深度学习社区展示。
- en: Building the VGG16 model
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建 VGG16 模型
- en: The VGG16 model is a deep convolution neural network image classifier. The model
    uses a combination of Conv2D, MaxPooling2D, and Dense layers to form the final
    architecture, and the activation function that's used is ReLU. It accepts color
    images of shape 224*224*3, and is capable of predicting 1,000 classes. This means that
    the final Dense layer has 1,000 neurons, and it uses softmax activation to get
    scores for each class.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: VGG16 模型是一种深度卷积神经网络图像分类器。该模型使用 Conv2D、MaxPooling2D 和 Dense 层的组合形成最终的架构，且使用 ReLU
    激活函数。它接受形状为 224*224*3 的彩色图像，并能够预测 1,000 个类别。这意味着最终的 Dense 层有 1,000 个神经元，并使用 softmax
    激活函数来获得每个类别的得分。
- en: Defining the VGG16 model
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义 VGG16 模型
- en: In this project, we want to feed in images of shape 224*224*3 and be able to
    predict the joint coordinates for the body in the image. That is, we want to be
    able to predict 14 numerical values (*7* (*x*,*y*) pairs). Therefore, we modify
    the final Dense layer to have 14 neurons and use ReLU activation instead of sigmoid.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我们希望输入形状为 224*224*3 的图像，并能够预测图像中身体的关节点坐标。也就是说，我们希望能够预测 14 个数值（*7*（*x*，*y*）对）。因此，我们将最后的
    Dense 层修改为 14 个神经元，并使用 ReLU 激活函数代替 sigmoid。
- en: Training a deep learning model such as VGG16 can take up to a week on a local
    machine. This is a lot of time. An alternative to this in our case is to use the
    weights of a trained VGG16 model through transfer learning.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: We will do this with the help of the applications module in Keras that we imported
    in the beginning, along with the other imports.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we will load part of the VGG16 model up to, but not
    including, the Flatten layer and the corresponding weights. Setting the `include_top`
    parameter to `False` does this for us:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: The first line of the following snippet will also download the VGG16 weights
    from the Keras server, so you don't have to worry about downloading the weights
    file from anywhere else.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Following is the output:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8ceeddcd-bb4b-4e32-a851-699bd00f3362.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.15: Summary of the VGG16 model (up to Flatten)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: From the summary, we can see that all of the layers of the VGG16 model up to, but
    not including, the Flatten layer have been loaded with their weights.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: To learn more about the additional functionality of the applications module
    of Keras, take a look at the official documentation at [https://keras.io/applications/](https://keras.io/applications/).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'We don''t want weights of any of these layers to be trained. So, in the following
    code, we need to set the `trainable` parameter of each layer to `False`:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'As a next step, flatten the output of the preceding section of the model and
    then add three Dense layers, of which two layers have 1,024 neurons each with
    a dropout between them, and a final Dense layer with 14 neurons to obtain the
    14 joint coordinates. We will only be training the weights of the layers defined
    in the following code snippet:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Once all of the layers have been defined and configured, we will combine them
    by using the `Model` function in Keras, as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Following is the output:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/656b47f0-c68e-4ac3-9fa9-77310a06ebc9.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.16: Summary of the customized VGG16 model'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: From the summary, we can see that `26,755,086` parameters are trainable and
    that `14,714,688` parameters are untrainable, since we have set them as untrainable.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: 'The model is then compiled with `mean_squared_error` as `loss`. The `optimizer`
    used here is Adam, which has a learning rate of 0.0001, as defined by the optimizer
    variable in the hyperparameter section:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Training loop
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that the VGG16 model is all set to be used for training, let's load the
    `train_joints.csv` file from the `train` folder containing the IDs of the cropped
    and resized images with their joint coordinates.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, split the data into an 80:20 train and validation set by using the `train_test_split`
    module from `sklearn`. We imported it with the other imports at the beginning
    of this chapter. Since the validation data is small, load all of the corresponding
    images into memory:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Be mindful of how many validation images you load into memory, as this may become
    an issue with systems that have less RAM.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Explore the data with the pandas `head`, `tail`, and `info` functions. Please
    note that when loading the `.csv` file using pandas, set the `header` parameter
    to `False` so that pandas knows that the file has no header.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now define the `training()` function, which will train the VGG16 model
    on the train images. This function accepts the VGG16 model, train image IDs, train
    joints, validation images, and validation joints as parameters. The following
    steps define what is happening in the `training()` function:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: The function defines empty lists by using `loss_lst` to store the train loss
    and `val_loss_lst` to store the validation loss. It also defines a counter count
    to keep track of the total number of batches.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It then creates a batch of train image IDs and their corresponding joints.
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the batch image IDs, it loads the corresponding images into memory by
    using the OpenCV `imread()` function.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It then converts the loaded train images into a `float`, which it feeds along
    with the joint IDs to the `train_on_batch()` function of the model for the fit.
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At every 40^(th) batch, it evaluates the model on the validation data and stores
    the train and validation loss in the defined lists.
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It then repeats *Steps 2* through *5* for the desired number of epochs.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Following is the code:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'For the remaining part of this code snippet, please refer to the `deeppose.ipynb`
    file here: [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter12/deeppose.ipynb)'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 'The output is as follows:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2296d449-0116-40b7-a1ca-55132b90f859.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
- en: 'The following is the output at the end of the code''s execution:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/048171cb-94b9-4623-a205-f0b0a8cc14c1.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.17: Loss output when training the model'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: If you are using a small GPU for training, reduce the batch size to avoid GPU
    memory issues. Also, remember that a smaller batch size may or may not result
    in the same fit that this chapter indicates.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Plot training and validation loss
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With `loss_lst` and `val_loss_lst` containing the train and validation MSE
    loss at intervals of 40 batches, let''s plot this and see how the learning has
    progressed:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Following is the output:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/445bc33e-63bd-4581-bbce-2f481f74228f.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.18: Plot of train and validation loss'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: A smoother train validation loss plot can be obtained by reducing the store
    hyperparameter. A small store value will result in a longer training time.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Predictions
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is what we have been waiting for...
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Making test predictions!
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: 'We will define a function that takes the model as input and tests the model
    on the test data we have preprocessed and saved in the `test` folder. Along with
    predictions, it will also save test images with the true and predicted joints
    plotted on it by using the `plot_limb()` and the `plot_joints()` functions we
    defined in the preceding section:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Following is the output:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/42596d64-7380-405a-8807-c875b26a8887.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.19: Test loss'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: On a test set with 200 images, the test MSE loss is 454.80, which is very close
    to the validation MSE loss of 503.85, indicating that the model is not overfitting
    on the train data.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: Train the model for a few more epochs if possible, and check if a better fit
    is possible. Be mindful of how many test images you want to load into memory for
    evaluation since it might become a problem on machines with RAM limitations.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s plot the images we saved during testing to get a measure of how
    the true joints compare to the predicted joints:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Following is the output:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dc8f0519-8697-48cb-98e9-306819b4afd3.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.20: Test images with true and predicted joints plotted on top'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding picture, we can see that the model is doing a really good
    job of predicting the seven joints on unseen images.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Scripts in modular form
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The entire script can be split into four modules named `train.py`, `test.py`,
    `plotting.py`, and `crop_resize_transform.py`. You should be able to find these
    scripts in the `Chapter12` folder. Follow the instructions under the *Code implementation* section
    of this chapter to run the scripts. Set `Chapter12` as the project folder in your
    favorite source code editor and just run the `train.py` file.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: The `train.py` Python file will import functions from all of the other modules
    in places where they are needed for execution.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Now let's walk through the contents of each file.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Module 1 – crop_resize_transform.py
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This Python file contains the `image_cropping()`, `image_resize()`, and `model_data()`
    functions, as shown:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Module 2 – plotting.py
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This Python file contains two functions, namely `plot_limb()` and `plot_joints()`,
    as shown:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Module 3 – test.py
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This module contains the `test()` function that will be called in the `train_dqn.py`
    script so that it can test the performance of the trained model, as shown:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Module 4 – train.py
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this module, we have the `joint_coordinates()` and `training()` functions,
    as well as the calls to train and test the VGG16 model:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Conclusion
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This project was all about building a **convolutional neural network **(**CNN**)
    classifier to solve the problem of estimating 3D human poses using frames captured
    from movies. Our hypothetical use case was to enable visual effects specialists
    to easily estimate the pose of actors (from their shoulders, necks, and heads
    from the frames in a video. Our task was to build the intelligence for this application.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: The modified VGG16 architecture we built using transfer learning has a test
    mean squared error loss of 454.81 squared units over 200 test images for each
    of the 14 coordinates (that is, the *7*(*x*, *y*) pairs). We can also say that
    the test root mean squared error over 200 test images for each of the 14 coordinates is
    21.326 units. What does this mean?
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: The **root mean squared error** (**RMSE**), in this case, is a measure of how
    far off the predicted joint coordinates/joint pixel location are from the actual
    joint coordinate/joint pixel location.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: An RMSE loss of 21.32 units is equivalent to having each predicted coordinate
    off by 21.32 pixels within an image of shape 224*224*3\. The test results plotted
    in *Figure 13.20* represent this measure.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Each coordinate being off by 21.32 pixels is good at a general level, but we
    want to build a product that will be used in movies for which the margin for error
    is much less, and being off by 21 pixels is not acceptable.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: 'To improve the model, you can do the following:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: Try using a lower learning rate for a larger number of epochs
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try using a different loss function (for example, **mean absolute error** (**MAE**))
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try using an even deeper model, such as RESNET50 or VGG19
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try centering and scaling the data
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get more data
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are some of the additional steps you should take if you are interested
    in becoming an expert in this specific area once you are done with this chapter.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we successfully built a deep convolution neural network/VGG16
    model in Keras on FLIC images. We got hands-on experience in preparing these images
    for modeling. We successfully implemented transfer learning, and understood that
    doing so will save us a lot of time. We defined some key hyperparameters as well
    in some places, and reasoned about why we used what we used. Finally, we tested
    the modified VGG16 model performance on unseen data and determined that we succeeded
    in achieving our goals.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
