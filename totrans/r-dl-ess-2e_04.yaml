- en: Training Deep Prediction Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous chapters covered a bit of the theory behind neural networks and
    used some neural network packages in R. Now it is time to dive in and look at
    training deep learning models. In this chapter, we will explore how to train and
    build feedforward neural networks, which are the most common type of deep learning
    model. We will use MXNet to build deep learning models to perform classification
    and regression using a retail dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with deep feedforward neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common activation functions – rectifiers, hyperbolic tangent, and maxout
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to the MXNet deep learning library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use case – Using MXNet for classification and regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting started with deep feedforward neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A deep feedforward neural network is designed to approximate a function, *f()*,
    that maps some set of input variables, *x*, to an output variable, *y*. They are
    called feedforward neural networks because information flows from the input through
    each successive layer as far as the output, and there are no feedback or recursive
    loops (models including both forward and backward connections are referred to
    as recurrent neural networks).
  prefs: []
  type: TYPE_NORMAL
- en: Deep feedforward neural networks are applicable to a wide range of problems,
    and are particularly useful for applications such as image classification. More
    generally, feedforward neural networks are useful for prediction and classification
    where there is a clearly defined outcome (what digit an image contains, whether
    someone is walking upstairs or walking on a flat surface, the presence/absence
    of disease, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep feedforward neural networks can be constructed by chaining layers or functions
    together. For example, a network with four hidden layers is shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e278f4de-76ae-4249-bfba-ac2915bfdd11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.1: A deep feedforward neural network'
  prefs: []
  type: TYPE_NORMAL
- en: This diagram of the model is a directed acyclic graph. Represented as a function,
    the overall mapping from the input, *X*, to the output, *Y*, is a multilayered
    function. The first hidden layer is *H**[1]=f^((1))(X, w[1] a[1]**)*, the second
    hidden layer is *H[2]=f^((2))(H[1], w[2] a[2])*, and so on. These multiple layers
    can allow complex functions and transformations to be built up from relatively
    simple ones.
  prefs: []
  type: TYPE_NORMAL
- en: If sufficient hidden neurons are included in a layer, it can approximate to
    the desired degree of precision with many different types of functions. Feedforward
    neural networks can approximate non-linear functions by applying non-linear transformations
    between layers. These non-linear functions are known as activation functions,
    which we will cover in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: The weights for each layer will be learned as the model is trained through forward-
    and backward-propagation. Another key piece of the model that must be determined
    is the cost, or loss, function. The two most commonly used cost functions are
    cross-entropy, which is used for classification tasks, and **mean squared error**
    (**MSE**), which is used for regression tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The activation function determines the mapping between input and a hidden layer.
    It defines the functional form for how a neuron gets activated. For example, a
    linear activation function could be defined as: *f(x) = x*, in which case the
    value for the neuron would be the raw input, *x*. A linear activation function
    is shown in the top panel of *Figure 4.2*. Linear activation functions are rarely
    used because in practice deep learning models would find it difficult to learn
    non-linear functional forms using linear activation functions. In previous chapters,
    we used the hyperbolic tangent as an activation function, namely *f(x) = tanh(x)*.
    Hyperbolic tangent can work well in some cases, but a potential limitation is
    that at either low or high values, it saturates, as shown in the middle panel
    of the figure  4.2.'
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps the most popular activation function currently, and a good first choice
    (Nair, V., and Hinton, G. E. (2010)), is known as a *rectifier*. There are different
    kinds of rectifiers, but the most common is defined by the *f(x) = max(0, x)* function,
    which is known as **relu**. The relu activation is flat below zero and linear
    above zero; an example is shown in Figure 4.2.
  prefs: []
  type: TYPE_NORMAL
- en: The final type of activation function we will discuss is maxout (Goodfellow,
    Warde­-Farley, Mirza, Courville, and Bengio (2013)). A maxout unit takes the maximum
    value of its input, although as usual, this is after weighting so it is not the
    case that the input variable with the highest value will always win. Maxout activation
    functions seem to work particularly well with dropout.
  prefs: []
  type: TYPE_NORMAL
- en: 'The relu activation is the most commonly-used activation function and it is
    the default option for the deep learning models in the rest of this book. The
    following graphs for some of the activation functions we have discussed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d1f135e3-40af-4b26-a5f3-0848bfd14096.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.2: Common activation functions'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to the MXNet deep learning library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The deep learning libraries we will use in this book are MXNet, Keras, and TensorFlow.
    Keras is a frontend API, which means it is not a standalone library as it requires
    a lower-level library in the backend, usually TensorFlow. The advantage of using
    Keras rather than TensorFlow is that it has a simpler interface. We will use Keras
    in later chapters in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Both MXNet and TensorFlow are multipurpose numerical computation libraries that
    can use GPUs for mass parallel matrix operations. As such, multi-dimensional matrices
    are central to both libraries. In R, we are familiar with the vector, which is
    a one-dimensional array of values of the same type. The R data frame is a two-dimensional
    array of values, where each column can have different types. The R matrix is a
    two-dimensional array of values with the same type. Some machine learning algorithms
    in R require a matrix as input. We saw an example of this in [Chapter 2](cb00118a-2bba-4e43-ba55-c4552c508b7e.xhtml), *Training
    a Prediction Model*, with the RSNSS package.
  prefs: []
  type: TYPE_NORMAL
- en: In R, it is unusual to use data structures with more than two dimensions, but
    deep learning uses them extensively. For example, if you have a 32 x 32 color
    image, you could store the pixel values in a 32 x 32 x 3 matrix, where the first
    two dimensions are the width and height, and the last dimension is for the red,
    green, and blue colors. This can be extended further by adding another dimension
    for a collection of images. This is called a batch and allows the processor (CPU/GPU)
    to process multiple images concurrently. The batch size is a hyper-parameter and
    the value selected depends on the size of the input data and memory capacity.
    If our batch size were 64, our matrix would be a 4-dimensional matrix of size
    32 x 32 x 3 x 64 where the first 2 dimensions are the width and height, the third
    dimension is the colors, and the last dimension is the batch size, 64\. The important
    thing to realize is that this is just another way of representing data. In R,
    we would store the same data as a 2-dimensional matrix (or dataframe) with 64
    rows and 32 x 32 x 3 = 3,072 columns. All we are doing is reshaping the data,
    we are not changing it.
  prefs: []
  type: TYPE_NORMAL
- en: These n-dimensional matrices, which contain elements of the same type, are the
    cornerstone of using MXNet and TensorFlow. In MXNet, they are referred to as NDArrays.
    In TensorFlow, they are known as **tensors**. These n-dimensional matrices are
    important because they mean that we can feed the data into GPUs more efficiently;
    GPUs can process data in batches more efficiently than processing single rows
    of data. In the preceding example, we use 64 images in a batch, so the deep learning
    library will process input data in chunks of 32 x 32 x 3 x 64.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will use the MXNet deep learning library. MXNet originated at
    Carnegie Mellon University and is heavily supported by Amazon, they choose it
    as their default Deep Learning library in 2016\. In 2017, MXNet was accepted as
    an Apache Incubator project, ensuring that it would remain as open source software.
    Here is a very simple example of an NDArray (matrix) operation in MXNet in R.
    If you have not already installed the MXNet package for R, go back to [Chapter
    1](00c01383-1886-46d0-9435-29dfb3e08055.xhtml), *Getting Started with Deep Learning*,
    for instructions, or use this link: [https://mxnet.apache.org/install/index.html](https://mxnet.apache.org/install/index.html):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We can break down this code line by line:'
  prefs: []
  type: TYPE_NORMAL
- en: Line 1 loads the MXNet package.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Line 2 sets the CPU context. This tells MXNet where to process your computations,
    either on the CPU or on a GPU, if one is available.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Line 3 creates a 2-dimensional NDArray of size 2 x 3 where each value is 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Line 4 creates another 2-dimensional NDArray of size 2 x 3\. Each value will
    be 3 because we perform element-wise multiplication and add 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Line 5 shows that b is an external pointer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Line 6 shows that the class of b is MXNDArray.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Line 7 displays the results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can perform mathematical operations, such as multiplication and addition,
    on the `b`variable. However, it is important to realize that, while this behaves
    similarly to an R matrix, it is not a native R object. We can see this when we
    output the type and class of this variable.
  prefs: []
  type: TYPE_NORMAL
- en: When developing deep learning models, there are usually two distinct steps.
    First you create the model architecture and then you train the model. The main
    reason for this is because most deep learning libraries employ symbolic programming
    rather than the imperative programming you are used to. Most of the code you have
    previously written in R is an imperative program, which executes code sequentially.
    For mathematical optimization tasks, such as deep learning, this may not be the
    most efficient method of execution. Most deep learning libraries, including MXNet
    and TensorFlow, use symbolic programming. For symbolic programming, a computation
    graph for the program execution is designed first. This graph is then compiled
    and executed. When the computation graph is generated, the input, output, and
    graph operations are already defined, meaning that the code can be optimized.
    This means that for deep learning, symbolic programs are usually more efficient
    than imperative programs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a simple example of the type of optimization using symbolic programs:'
  prefs: []
  type: TYPE_NORMAL
- en: '*M = (M1 * M2) + (M3* M4)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'An imperative program would calculate this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Mtemp1 = (M1 * M2)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Mtemp2 = (M3* M4)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*M = Mtemp1 + Mtemp2*'
  prefs: []
  type: TYPE_NORMAL
- en: 'A symbolic program would first create a computation graph, which might look
    like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2905dc18-fd1f-43ca-8d79-a409fa5c3261.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.3: Example of a computation graph'
  prefs: []
  type: TYPE_NORMAL
- en: '*M1*, *M2*, *M3*, and *M4* are symbols that need to be operated on. The graph
    shows the dependencies for operations; the *+* operation requires the two preceding
    multiplication operations to be done before it can execute. But there is no dependency
    between the two multiplication steps, so these can be executed in parallel. This
    type of optimization means the code can execute much faster.'
  prefs: []
  type: TYPE_NORMAL
- en: 'From a coding point of view, this means is that you have two steps in creating
    a deep learning model – first you define the architecture of the model and then
    you train the model. You create *layers *for your deep learning model and each
    layer has symbols that are placeholders. So for example, the first layer is usually:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '`data` is a placeholder for the input, which we will insert later. The output
    of each layer feeds into the next layer as input. This might be a convolutional
    layer, a dense layer, an activation layer a dropout layer, and so on. The following
    code example shows how the layers continue to feed into each other; this is taken
    from a full example later in this chapter. Notice how the symbol for each layer
    is used as input in the next layer, this is how the model is built layer after
    layer. The `data1` symbol is passed into the first call to `mx.symbol.FullyConnected`,
    the `fc1` symbol is passed into the first call to `mx.symbol.Activation`, and
    so on.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'When you execute this code, it runs instantly as nothing is executed at this
    stage. Eventually, you pass the last layer into a function to train the model.
    In MXNet, this is the `mx.model.FeedForward.create` function. At this stage, the
    computation graph is computed and the model begins to be trained:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This is when the deep learning model is created and trained. More information
    on the MXNet architecture is available online; the following links will get you
    started:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://mxnet.apache.org/tutorials/basic/symbol.html](https://mxnet.apache.org/tutorials/basic/symbol.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://mxnet.incubator.apache.org/architecture/program_model.html](https://mxnet.incubator.apache.org/architecture/program_model.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the earlier code snippets, we saw some layers for a deep learning model,
    including `mx.symbol.FullyConnected`, `mx.symbol.Activation`, and `mx.symbol.Dropout`.
    Layers are how models are constructed; they are computational transformations
    of data. For example, `mx.symbol.FullyConnected` is the first type of layer operation
    we matrix operation we introduced in [Chapter 1](00c01383-1886-46d0-9435-29dfb3e08055.xhtml),
    *Getting Started with Deep Learning*. It is *fully connected* because all input
    values are connected to all nodes in the layer. In other deep learning libraries,
    such as Keras, it is called a **dense** layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `mx.symbol.Activation` layer performs an activation function on the output
    of the previous layer. The `mx.symbol.Dropout` layer performs dropout on the output
    from the previous layer. Other common layer types in MXNet are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`mxnet.symbol.Convolution`: Performs a convolutional operation that matches
    patterns across the data. It is mostly used in computer vision tasks, which we
    will see in [Chapte](1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml)[r](1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml)
    [5](1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml), *Image Classification Using Convolutional
    Neural Networks*. They can also be used for Natural Language Processing, which
    we will see in [Chapter 6](03f666ab-60ce-485a-8090-c158b29ef306.xhtml), *Natural
    Language Processing Using Deep Learning*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mx.symbol.Pooling`: Performs pooling on the output from the previous layer.
    Pooling reduces the number of elements by taking the average, or max value, from
    sections of the input. These are commonly used with convolutional layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mx.symbol.BatchNorm`: Used to normalize the weights from the previous layer.
    This is done for the same reason you normalize input data before model-building:
    it helps the model to train better. It also prevents vanishing and exploding gradients
    where gradients get very, very small or very, very large during training. This
    can cause the model to fail to converge, that is, training will fail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mx.symbol.SoftmaxOutput`: Calculates a softmax result from the output from
    the previous layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are recognized patterns for using these layers, for example, an activation
    layer normally follows a fully-connected layer. A dropout layer is usually applied
    after the activation function, but can be between the fully connected layer and
    the activation function. Convolutional layers and pooling layers are often used
    together in image tasks in that order. At this stage, there is no need to try
    to memorize when to use these layers; you will encounter plenty of examples in
    the rest of this book!
  prefs: []
  type: TYPE_NORMAL
- en: If all this seems confusing, take some comfort in knowing that a lot of the
    difficult work in applying these layers is abstracted away from you. In the previous
    chapter, when we built a neural network, we had to manage all the input output
    from the layers. This meant ensuring that the matrix dimensions were correct so
    that the operations worked. Deep Learning libraries, such as MXNet and TensorFlow,
    take care of this for you.
  prefs: []
  type: TYPE_NORMAL
- en: Building a deep learning model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have covered the basics, let''s look at building our first true
    deep learning model! We will use the `UHI HAR` dataset that we used in [Chapter
    2](cb00118a-2bba-4e43-ba55-c4552c508b7e.xhtml), *Training a Prediction Model*.
    The following code does some data preparation: it loads the data and selects only
    the columns that store mean values (those that have the word `mean` in the column
    name). The `y` variables are from 1 to 6; we will subtract one so that the range
    is 0 to 5\. The code for this section is in `Chapter4/uci_har.R`. It requires
    the `UHI HAR` dataset to be in the data folder; download it from [https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones](https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones) and
    unzip it into the data folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will transpose the data and convert it into a matrix. MXNet expects
    the data to be width `x` height rather than height `x` width:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to define the computation graph. We create a placeholder for
    the data and create two fully connected (or dense) layers followed by relu activations.
    The first layer has 64 nodes and the second layer has 32 nodes. We create a final fully-connected
    layer with six nodes – the number of distinct classes in our y variable. We use
    a softmax activation to convert the numbers from the last six nodes into probabilities
    for each class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'When you run the previous code, nothing actually executes. To train the model,
    we create a `devices` object to indicate where the code should be run, CPU or
    GPU. Then you pass the symbol for last layer (softmax) into the `mx.model.FeedForward.create`
    function. This function has other parameters, which are more properly known as
    hyper-parameters. These include the epochs (`num.round`), which control how many
    times we pass through the data, the learning rate (`learning.rate`), which controls
    how much the gradients are updated during each pass, momentum (`momentum`), which
    is a hyper-parameter that can help the model to train faster, and the weights
    initializer (`initializer`), which controls how the weights and biases for nodes
    are initially set. We also pass in the evaluation metric (`eval.metric`),which
    is how the model is to be evaluated, and a callback function (`epoch.end.callback`),
    which is used to output progress information. When we run the function, it trains
    the model and outputs the progress as per the value we used for the `epoch.end.callback`
    parameter, namely every epoch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have trained our model, let''s see how it does on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Not bad! We have achieved an accuracy of `87.11%` on our test set.
  prefs: []
  type: TYPE_NORMAL
- en: Wait, where are the backward propagation, derivatives, and so on, that we covered
    in previous chapters? The answer to that is deep learning libraries largely manage
    this automatically for you. In MXNet, automatic differentiation is included in
    a package called the autograd package, which differentiates a graph of operations
    with the chain rule. It is one less thing to worry about when building deep learning
    models. For more information, go to [https://mxnet.incubator.apache.org/tutorials/gluon/autograd.html](https://mxnet.incubator.apache.org/tutorials/gluon/autograd.html).
  prefs: []
  type: TYPE_NORMAL
- en: Use case – using MXNet for classification and regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will use a new dataset to create a binary classification
    task. The dataset we will use here is a transactional dataset that is available
    at [https://www.dunnhumby.com/sourcefiles](https://www.dunnhumby.com/sourcefiles).
    This dataset has been made available from dunnhumby, which is perhaps best known
    for its link to the Tesco (a British grocery store) club-card, which is one of
    the largest retail loyalty systems in the world. I recommend the following book,
    which describes how dunnhumby helped Tesco to become the number one retailer by
    applying analytics to their retail loyalty program: *Humby, Clive, Terry Hunt,
    and Tim Phillips. Scoring points. Kogan Page Publishers, 2008*. Even though this
    book is relatively old, it remains one of the best use cases to describe how to
    roll out a business-transformation program based on data analytics.'
  prefs: []
  type: TYPE_NORMAL
- en: Data download and exploration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you go to the preceding link, there are a few different data options; the
    one we will use is called **Let’s Get Sort-of-Real**. This dataset is data for
    over two years for a fictional retail loyalty scheme. The data consists of purchases
    that are linked by basket ID and customer code, that is, we can track transactions
    by customers over time. There are a number of options here, including the full
    dataset, which is 4.3 GB zipped and over 40 GB unzipped. For our first models,
    we will use the smallest dataset, and will download the data titled **All transactions
    for a randomly selected sample of 5,000 customers**; this is 1/100^(th) the size
    of the full database.
  prefs: []
  type: TYPE_NORMAL
- en: I wish to thank dunnhumby for releasing this dataset and for allowing us permission
    to use it. One of the problems in deep learning and machine learning in general
    is the lack of large scale real-life datasets that people can practice their skills
    on. When a company makes the effort to release such a dataset, we should appreciate
    the effort and not use the dataset outside the terms and conditions specified.
    Please take the time to read the terms and conditions and use the dataset for
    personal learning purposes only. Remember that any misuse of this dataset (or
    datasets released by other companies) means that companies will be more reluctant
    to make other datasets available in the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have read the terms and conditions and downloaded the dataset to your
    computer, unzip it into a directory called `dunnhumby/in` under the `code` folder.
    Ensure the files are unzipped directly under this folder, and not a sub-directory,
    or you may have to copy them after unzipping the data. The data files are in **comma-delimited**
    (**CSV**) format, with a separate file for each week of data. The files can be
    opened and viewed using a text editor. We will use some of the fields in *Table
    4.1* for our analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Field name** | **Description** | **Format** |'
  prefs: []
  type: TYPE_TB
- en: '| `BASKET_ID` | Basket ID, or transaction ID. All items in a basket share the
    same `basket_id` value. | Numeric |'
  prefs: []
  type: TYPE_TB
- en: '| `CUST_CODE` | Customer Code. This links the transactions/visits to a customer.
    | Char |'
  prefs: []
  type: TYPE_TB
- en: '| `SHOP_DATE` | Date when shopping occurred. Date is specified in the yyyymmdd
    format. | Char |'
  prefs: []
  type: TYPE_TB
- en: '| `STORE_CODE` | Store code. | Char |'
  prefs: []
  type: TYPE_TB
- en: '| `QUANTITY` | Number of items of the same product bought in this basket. |
    Numeric |'
  prefs: []
  type: TYPE_TB
- en: '| `SPEND` | Spend associated to the items bought. | Numeric |'
  prefs: []
  type: TYPE_TB
- en: '| `PROD_CODE` | Product Code. | Char |'
  prefs: []
  type: TYPE_TB
- en: '| `PROD_CODE_10` | Product Hierarchy Level 10 Code. | Char |'
  prefs: []
  type: TYPE_TB
- en: '| `PROD_CODE_20` | Product Hierarchy Level 20 Code. | Char |'
  prefs: []
  type: TYPE_TB
- en: '| `PROD_CODE_30` | Product Hierarchy Level 30 Code. | Char |'
  prefs: []
  type: TYPE_TB
- en: '| `PROD_CODE_40` | Product Hierarchy Level 40 Code. | Char |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4.1: Partial data dictionary for transactional dataset'
  prefs: []
  type: TYPE_NORMAL
- en: The data stores details of customer transactions. Every unique item that a person
    purchases in a shopping transaction is represented by one line, and items in a
    transaction will have the same `BASKET_ID` field. A transaction can also be linked
    to a customer using the `CUST_CODE` field. A PDF is included in the ZIP files
    if you want more information on the field types.
  prefs: []
  type: TYPE_NORMAL
- en: We are going to use this dataset for a churn prediction task. A churn prediction
    task is where we predict which customers will return in the next `x` days. Churn
    prediction is used to find customers who are in danger of leaving your program.
    It is used by companies in shopping loyalty schemes, mobile phone subscriptions,
    TV subscriptions, and so on to ensure they maintain enough customers. For most
    companies that rely on revenue from recurring subscriptions, it is more effective
    to spend resources on maintaining their existing customer base than trying to
    acquire new customers. This is because of the high cost of acquiring new customers.
    Also, as time progresses after a customer has left, it is harder to win them back,
    so there is a small window of time in which to send them special offers that may
    entice them to stay.
  prefs: []
  type: TYPE_NORMAL
- en: As well as binary classification, we will build a regression model. This will
    predict the amount that a person will spend in the next 14 days. Fortunately,
    we can build a dataset that is suitable for both prediction tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data was supplied as 117 CSV files (ignore `time.csv`, which is a lookup
    file). The first step is to perform some basic data exploration to verify that
    the data was downloaded successfully and then perform some basic data quality
    checks. This is an important first step in any analysis: especially when you are
    using an external dataset, you should run some validation checks on the data before
    creating any machine learning models. The `Chapter4/0_Explore.Rmd` script creates
    a summary file and does some exploratory analysis of the data. This is an RMD
    file, so it needs to be run from RStudio. For brevity, and because this book is
    about deep learning and not data processing, I will include just some of the output
    and plots from this script rather than reproducing all the code. You should also
    run the code in this file to ensure the data was imported correctly, although
    it may take a few minutes the first time it runs. Here are some summaries on the
    data from that script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'If we compare this to the website and the PDF, it looks in order. We have over
    2.5 million records, and data for 5,000 customers across 761 stores. The data-exploration
    script also creates some plots to give us a feel for the data. *Figure 4.3* shows
    the sales over the 117 weeks; we see the variety in the data (it is not a flat
    line indicating that each day is different) and there are no gaps indicating missing
    data. There are seasonal patterns, with large peaks toward the end of the calendar
    year, namely the holiday season:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8165fb22-2fed-4e42-bfbc-46083cd50ba8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.3: Sales plotted over time.'
  prefs: []
  type: TYPE_NORMAL
- en: The plot in figure 4.3 shows that the data has been imported successfully. The
    data looks consistent and is what we expect for a retail transaction file, we
    do not see any gaps and there is seasonality.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each item a person purchases, there is a product code (`PROD_CODE`) and
    four department codes (`PROD_CODE_10`, `PROD_CODE_20`, `PROD_CODE_30`, `PROD_CODE_40`). We
    will use these department codes in our analysis; the code in `Chapter4/0_Explore.Rmd`
    creates a summary for them. We want to see how many unique values there are for
    each department code, whether the codes represent a hierarchy (each code has at
    most one parent), and whether there are repeated codes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We have 4,997 unique product codes with 4 department codes. Our department
    codes go from `PROD_CODE_10`, which has 250 unique codes, to `PROD_CODE_40`, which
    has 9 unique codes. This is a product department code hierarchy, where `PROD_CODE_40` is
    the primary category and `PROD_CODE_10` is the lowest department code in the hierarchy.
    Each code in `PROD_CODE_10`,`PROD_CODE_20`, and`PROD_CODE_30` has only one parent;
    for example, there are no repeating codes, that is, a department code belongs
    in only one super-category. We are not given a lookup file to say what these codes
    represent, but an example of a product code hierarchy for a product might be something
    similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'To get a sense of these department codes, we can also plot the sales data over
    time by the number of unique product department codes in *Figure 4.4*. This plot
    is also created in `Chapter4/0_Explore.Rmd`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6eabed35-9c9b-4357-951d-7fe5c1e250a5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.4: Unique product codes purchased by date'
  prefs: []
  type: TYPE_NORMAL
- en: Note that for this graph, the *y* axis is unique product codes, not sales. This
    data also looks consistent; there are some peaks and dips in the data, but they
    are not as pronounced as in *Figure 4.3*, which is as expected.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the data for our models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have downloaded and validated the data, we can use it to create
    a dataset for our binary classification and regression model tasks. We want to
    be able to predict which customers will visit the shop in the next two weeks for
    the binary classification task, and how much they will spend in the next two weeks
    for the regression task. The `Chapter4/prepare_data.R` script transforms the raw
    transactional data into a format suitable for machine learning. You need to run
    the code to create the dataset for the models, but you do not have to understand
    exactly how it works. Feel free to skip ahead if you want to focus on the deep
    learning model building.
  prefs: []
  type: TYPE_NORMAL
- en: We need to transform our data into a suitable format for prediction tasks. This
    should be a single row for each instance we want to predict. The columns will
    include some fields that are features (`X` variables) and another field that is
    our predictor value (`Y` variable). We want to predict whether a customer returns
    or not and their spend, so our dataset will have a single row per customer with
    features and predictor variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to find the cut-off date that separates the variables used
    to predict (`X`) and the variable we will predict for (`Y`). The code looks at
    the data, finds the last transaction date; and then subtracts 13 days from that
    date. This is a cut-off date, we want to predict which customers will spend in
    our shops *on or after* the cut-off date; based on what happens *before* the cut-off
    date. The data before the cut-off date will be used to make our X, or feature
    variables, and sales data on or after the cut-off date will be used to make our
    Y, or predictor variables. The following is that part of the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: If this code does not run, the most probable reason is that the source data
    was not saved in the correct location. The dataset must be unzipped into a directory
    called dunnhumby/in under the code folder, that is, at the same level as the chapter
    folders.
  prefs: []
  type: TYPE_NORMAL
- en: The last date in our data is `20080706`, which July 7^(th), 2008, and the cut-off
    date is June 23^(rd), 2008\. Although we have data going back to 2006, we will
    only use sales data from 2008\. Any data that is older than six months is unlikely
    to influence a future customer sale. The task is to predict whether a customer
    will return between June 23^(rd), 2008 - July 7^(th), 2008 based on their activity
    before June 23^(rd), 2008.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now need to create features from our data; so that we can use the spend
    broken down by department code, we will use the `PROD_CODE_40` field. We could
    just group the sales on this department code, but that would give equal weighting
    to spends in Jan 2008 as to spends in June 2008\. We would like to incorporate
    some time factor in our predictor columns. Instead, we will create features on
    a combination of the department code and the week. This will allow our models
    to place more importance on recent activities. First, we group by customer code,
    week, and department code, and create the `fieldName` column. We then pivot this
    data to create our features (`X`) dataset. The cell values in this dataset are
    the sales for that customer (row) and that week-department code (column). Here
    is an example of how the data is transformed for two customers. *Table 4.2* shows
    the sales spend by week and the `PROD_CODE_40` field. *Table 4.3* then uses a
    pivot to create a dataset that has a single row per customer and the aggregate
    fields are now columns with the spend as values:'
  prefs: []
  type: TYPE_NORMAL
- en: '| `CUST_CODE` | `PROD_CODE_40` | `SHOP_WEEK` | `fieldName` | `Sales` |'
  prefs: []
  type: TYPE_TB
- en: '| `cust_001` | D00001 | 200801 | `D00001_200801` | 10.00 |'
  prefs: []
  type: TYPE_TB
- en: '| `cust_002` | D00001 | 200801 | `D00001_200801` | 12.00 |'
  prefs: []
  type: TYPE_TB
- en: '| `cust_001` | D00015 | 200815 | `D00015_200815` | 15.00 |'
  prefs: []
  type: TYPE_TB
- en: '| `cust_001` | D00020 | 200815 | `D00020_200815` | 20.00 |'
  prefs: []
  type: TYPE_TB
- en: '| `cust_002` | D00030 | 200815 | `D00030_200815` | 25.00 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4.2: Summary of sales by customer code, department code, and week'
  prefs: []
  type: TYPE_NORMAL
- en: '| `CUST_CODE` | `D00001_200801` | `D00015_200815` | `D00020_200815` | `D00030_200815`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `cust_001` | 10.00 | 15.00 | 20.00 |  |'
  prefs: []
  type: TYPE_TB
- en: '| `cust_002` | 12.00 |  |  | 25.00 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4.3: Data from Table 4.2 after transformation'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code that does this transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The predictor (`Y`) variable is a flag as to whether the customer visited the
    site on the weeks from `200818` to `200819`. We perform a grouping on the data
    after the cut-off date and group the sales by customer, these form the basis of
    our `Y` values. We join the `X` and `Y` datasets, ensuring that we keep all rows
    on the `X` side by doing a left-join. Finally we create a 1/0 flag for our binary
    classification task. When we are done, we see that we have `3933` records in our
    dataset : `1560` customers who did not return and `2373` customers who did. We
    finish by saving our file for the model-building. The following code shows these
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We use the sales data to create our predictor fields, but there were some customer
    attributes that we ignored for this task. These fields included `Customers Price Sensitivity` and
    `Customers Lifestage`. The main reason we did not use these fields was to avoid
    data-leakage. Data-leakage can occur when building prediction models; it occurs
    when some of your fields have values that will not be available or different when
    creating datasets in production. These fields could cause data-leakage because
    we do not know when they were set; it could have been when a customer signs up,
    or it could be a process that runs nightly. If these were created after our cut-off
    date, this would mean that these fields could unfairly predict our `Y` variables.
  prefs: []
  type: TYPE_NORMAL
- en: For example, `Customers Price Sensitivity` has values for `Less Affluent`, `Mid
    Market`, and `Up Market`, which probably are derived from what the customer purchases.
    Therefore, using these fields in a churn-prediction task would result in data-leakage
    if these fields were updated after the cut-off date used to create our dataset
    for our prediction models. A value of `Up Market` for `Customers Price Sensitivity` could
    be strongly linked to return spend, but this value is actually a summary of the
    value it is predicting. Data-leakage is one of the main causes of data models
    performing worse in production as the model was trained with data that is linked
    to the Y variable and can never exist in reality. You should always check for
    data-leakage for time-series tasks and ask yourself whether any field (especially
    lookup attributes) could have been modified after the date used to create the
    model data.
  prefs: []
  type: TYPE_NORMAL
- en: The binary classification model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code from the previous section creates a new file called `predict.csv` in
    the `dunnhumby` folder. This dataset has a single row for each customer with a
    0/1 field indicating whether they visited in the last two weeks and predictor
    variables based on sales data before those two weeks. Now we can proceed to build
    some machine learning models. The `Chapter4/binary_predict.R` file contains the
    code for our first prediction task, binary classification. The first part of the
    code loads the data and creates an array of  predictor variables by including
    all columns except the customer ID, the binary classification predictor variable,
    and the regression predictor variable. The feature columns are all numeric fields
    that are heavily right-skewed distributed, so we apply a log transformation to
    those fields. We add `0.01` first to avoid getting a non-numeric result from attempting
    to get a log of a zero value *(log(0)= -Inf)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following plot shows the data before transformation, on the left, and the
    data after transformation, on the right:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/024086e4-ca40-4310-977a-96187c710ba4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.5: Distribution of a feature variable before and after transformation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The large bar on the left in the second plot is where the original field was
    zero *(log(0+0.01) = -4.6)*. The following code loads the data, performs the log
    transformation, and creates the previous plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we train a deep learning model, we train three machine learning models
    – a logistic regression model, a `Random Forest` model, and an `XGBoost` model
    – on the data as a benchmark. This code section contains the data load, transformation,
    and three models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We create logistic regression, `Random Forest`, and `XGBoost` models for a
    number of reasons. Firstly, most of the work is already done in preparing the
    data, so it is trivial to do so. Secondly, it gives us a benchmark to compare
    our deep learning model to. Thirdly, if there were a problem in the data-preparation
    tasks, these machine learning algorithms would highlight these problems more rapidly
    because they will be quicker than training a deep learning model. In this case,
    we only have a few thousand records, so these machine learning algorithms will
    easily run on this data. If the data were too large for these algorithms, I would
    consider taking a smaller sample and running our benchmark tasks on that smaller sample.
    There are many machine learning algorithms to choose from, but I used these algorithms
    as benchmarks for the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression is a basic model and is always a good benchmark to use
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Random Forest` is known to train well using the default parameters and is
    robust to overfitting and correlated variables (which we have here)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`XGBoost` is consistently rated as the one of the best-performing machine learning
    algorithms'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All three algorithms achieve a similar amount of accuracy, the highest accuracy
    was achieved by `Random Forest` with an 80.2% accuracy. We now know that this
    dataset is suitable for prediction tasks and we have a benchmark to compare against.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we will build a deep learning model using MXNet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The deep learning model achieved a `77.16%` accuracy on the test data, which
    is only beaten by the `Random Forest` model. This shows that a deep learning model
    can be competitive against the best machine learning algorithms. It also shows
    that deep learning models on classification tasks do not always beat other machine
    learning algorithms. We used these models to provide a benchmark, so that we would
    know that our deep learning model was getting decent results; it gives us confidence
    that our deep learning model is competitive.
  prefs: []
  type: TYPE_NORMAL
- en: Our deep learning model uses 20% dropout in each layer and weight decay for
    regularization. Without dropout, the model overtrained significantly. This was
    probably because the features are highly correlated, as our columns are the spend
    in various departments. It figures that if one column is for a type of bread,
    and another column is for a type of milk, then these change together, namely someone
    who has more transactions and spends more is likely to buy both.
  prefs: []
  type: TYPE_NORMAL
- en: The regression model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The previous section developed a deep learning model for a binary classification
    task, this section develops a deep learning model to predict a continuous numeric
    value, regression analysis. We use the same dataset that we used for the binary
    classification task, but we use a different target column to predict for. In that
    task, we wanted to predict whether a customer would return to our stores in the
    next 14 days. In this task, we want to predict how much a customer will spend
    in our stores in the next 14 days. We follow a similar process; we load and prepare
    our dataset by applying log transformations to the data. The code is in `Chapter4/regression.R`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We then perform regression analysis on the data using `lm` to create a benchmark
    before creating a deep learning model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We output two metrics, rmse and mae, for our regression task. We covered these
    earlier in the chapter. Mean absolute error measures the absolute differences
    between the predicted value and the actual value. **Root mean squared error**
    (**rmse**) penalizes the square of the differences between the predicted value
    and the actual value, so one big error costs more than the sum of the small errors.
    Now let''s look at the deep learning regression code. First we load the data and
    define the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we train the model; note that the first comment shows how to switch to
    using a GPU instead of a CPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: For regression metrics, lower is better, so our rmse metric on the deep learning
    model (28.92) is an improvement on the original regression model (29.30). Interestingly,
    the mae on the the deep learning model (14.33) is actually worse than the original
    regression model (13.89). Since rsme penalizes big differences between actual
    and predicted values more, this indicates that the errors in the deep learning
    model are less extreme than the regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Improving the binary classification model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section builds on the earlier binary classification task and looks to increase
    the accuracy for that task. The first thing we can do to improve the model is
    to use more data, 100 times more data in fact! We will download the entire dataset,
    which is over 4 GB data in zip files and 40 GB of data when the files are unzipped.
    Go back to the download link ([https://www.dunnhumby.com/sourcefiles](https://www.dunnhumby.com/sourcefiles))
    and select **Let’s Get Sort-of-Real** again and download all the files for the
    **Full dataset**. There are nine files to download and the CSV files should be
    unzipped into the `dunnhumby/in` folder. Remember to check that the CSV files
    are in this folder and not a subfolder. You need to run the code in `Chapter4/prepare_data.R`
    again. When this completes, the `predict.csv` file should have 390,000 records.
  prefs: []
  type: TYPE_NORMAL
- en: You can try to follow along here, but be aware that preparing the data and running
    the deep learning model are going to take a long time. You also may run into problems
    if you have a slow computer. I tested this code on an Intel i5 processor with
    32 GB RAM, and it took the model 30 minutes to run. It also requires over 50 GB
    hard disk space to store the unzipped files and temporary files. If you have problems
    running it on your local computer, another option is to run this example in the
    cloud, which we will cover in a later chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for this section is in the `Chapter4/binary_predict2.R` script. Since
    we have more data, we can build a more complicated model. We have 100 times more
    data, so our new model adds an extra layer, and more nodes to our hidden layers.
    We have decreased the amount of regularization and the learning rate. We have
    also added more epochs. Here is the the code in `Chapter4/binary_predict2.R`,
    which constructs and trains the deep learning model. We have not included the
    boilerplate code to load and prepare the data, as that has not changed from the
    original script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The accuracy has increased from `77.16%` in the earlier model to `77.88%` for
    this model. This may not seem significant, but if we consider that the large dataset
    has almost 390,000 rows, the increase in accuracy of 0.72% represents about 2,808
    customers that are now classified correctly. If each of these customers is worth
    $50, that is an additional $140,000 in revenue.
  prefs: []
  type: TYPE_NORMAL
- en: In general, as you add more data, your model should become more complicated
    to generalize across all the patterns in the data. We will cover more of this
    in [Chapter 6](13e9a742-84df-48e5-bbfd-ade33dcdd01a.xhtml), *Tuning and Optimizing
    Models*, but I would encourage you to experiment with the code in `Chapter4/binary_predict.R`.
    Try changing the hyper-parameters or adding more layers. Even a small improvement
    of 0.1 - 0.2% in accuracy is significant. If you manage to get over 78% accuracy
    on this dataset, consider it a good achievement.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to explore further, there are other methods to investigate. These
    involve making changes in how the data for the model is created. If you really
    want to stretch yourself, here are a few more ideas you can try:'
  prefs: []
  type: TYPE_NORMAL
- en: Our current features are a combination of department codes and weeks, we use
    the `PROD_CODE_40` field as the department code. This has only nine unique values,
    so for every week, only nine fields represent that data. If you use `PROD_CODE_30`, `PROD_CODE_20`, or
    `PROD_CODE_10`, you will create a lot more features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In a similar manner, rather than using a combination of department codes and
    weeks, you could try department codes and day. This might create too many features,
    but I would consider doing this for the last 14 days before the cut-off date.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experiment with different methods of preparing the data. We use log scale, which
    works well for our binary classification task, but is not the best method for
    a regression task, as it does not create data with a normal distribution. Try
    applying z-scaling and min-max standardization to the data. If you do this, you
    must ensure that it is applied correctly to the test data before evaluating the
    model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The training data uses the sales amount. You could change this to item quantities
    or the number of transactions an item is in.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You could create new features. One potentially powerful example would be to
    create fields based on a day of the week, or a day of the month. We could create
    features for the spend amounts and number of visits for each day of the week.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We could create features based on the average size of a shopping basket, how
    frequently a customer visits, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We could try a different model architecture that can take advantage of time-series
    data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are all things I would try if I was given this task as a work assignment.
    In traditional machine learning, adding more features often leads to problems
    as most traditional machine learning algorithms struggle with high-dimensionality
    data. Deep learning models can handle these cases, so there usually is no harm
    in adding more features.
  prefs: []
  type: TYPE_NORMAL
- en: The unreasonable effectiveness of data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our first deep learning models on the binary classification task had fewer
    than 4,000 records. We did this so you could run the example quickly. For deep
    learning, you really need a lot more data, so we created a more complicated model
    with a lot more data, which gave us an increase in accuracy. This process demonstrated
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Establishing a baseline with other machine learning algorithms provides a good
    benchmark before using a deep learning model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We had to create a more complex model and adjust the hyper-parameters for our
    bigger dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Unreasonable Effectiveness of Data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The last point here is borrowed from an article by Peter Norvig, available
    at [https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35179.pdf](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35179.pdf).
    There is also a YouTube video with the same name. One of the main points in Norvig''s
    article is this: invariably simple models and a lot of data trump more elaborate
    models based on less data.'
  prefs: []
  type: TYPE_NORMAL
- en: We have increased the accuracy on our deep learning model by 0.38%. Considering
    that our dataset has highly correlated variables and that our domain is modelling
    human activities, this is not bad. People are, well predictable; so when attempting
    to predict what they do next, a small dataset usually works. In other domains,
    adding more data has much more of an effect. Consider a complex image-recognition
    task with color images where the image quality and format are not consistent.
    In that case, increasing our training data by a factor of 10 would have much more
    of an effect than in the earlier example. For many deep learning projects, you
    should include tasks to acquire more data from the very beginning of the project.
    This can be done by manually labeling the data, by outsourcing tasks (Amazon Turk),
    or by building some form of feedback mechanism in your application.
  prefs: []
  type: TYPE_NORMAL
- en: 'While other machine learning algorithms may also see an improvement in performance with
    more data, eventually adding more data will stop making a difference and performance
    will stagnate. This is because these algorithms were never designed for large
    high-dimensional data and so cannot model the complex patterns in very large datasets.
    However, you can build increasingly complex deep learning architectures that can
    model these complex patterns. This following plot illustrates how deep learning
    algorithms can continue to take advantage of more data and performance can still
    improve after performance on other machine algorithms stagnates:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ae562de4-ee33-4e60-b895-149310aa23f1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.6: How model accuracy increases by dataset size for deep learning
    models versus other machine learning models'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We covered a lot of ground in this chapter. We looked at activation functions
    and built our first true deep learning models using MXNet. Then we took a real-life
    dataset and created two use cases for applying a machine learning model. The first
    use case was to predict which customers will return in the future based on their
    past activity. This was a binary classification task. The second use case was
    to predict how much a customer will spend in the future based on their past activity. This
    was a regression task. We ran both models first on a small dataset and used different
    machine learning libraries to compare them against our deep learning model. Our
    deep learning model out-performed all of the algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: We then took this further by using a dataset that was 100 times bigger. We built
    a larger deep learning model and adjusted our parameters to get an increase in
    our binary classification task accuracy. We finished the chapter with a brief
    discussion on how deep learning models out-perform traditional machine learning
    algorithms on large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at computer vision tasks, which deep learning
    has revolutionized.
  prefs: []
  type: TYPE_NORMAL
