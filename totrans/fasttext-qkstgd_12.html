<html><head></head><body>
        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">References</h1>
                
            
            <article>
                


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Chapter 3</h1>
                
            
            <article>
                
<ul class="calibre10">
<li class="calibre11">Word representations: <a href="https://dl.acm.org/citation.cfm?id=1858721" class="calibre9">https://dl.acm.org/citation.cfm?id=1858721</a></li>
<li class="calibre11">One-hot encoding: <a href="https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/" class="calibre9">https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/</a></li>
<li class="calibre11">Representational learning: <a href="https://github.com/anujgupta82/Representation-Learning-for-NLP" class="calibre9">https://github.com/anujgupta82/Representation-Learning-for-NLP</a></li>
<li class="calibre11">N-grams: <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.53.9367" class="calibre9">http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.53.9367</a></li>
<li class="calibre11">TF-IDF: <a href="https://nlp.stanford.edu/IR-book/html/htmledition/inverse-document-frequency-1.html" class="calibre9">https://nlp.stanford.edu/IR-book/html/htmledition/inverse-document-frequency-1.html</a></li>
<li class="calibre11">Mikolov<span> </span><em class="calibre21">et al.</em><span> </span>2013: <a href="https://arxiv.org/abs/1310.4546" class="calibre9">https://arxiv.org/abs/1310.4546</a></li>
<li class="calibre11">Maas and cgpotts paper: <a href="https://web.stanford.edu/~cgpotts/papers/wvSent_acl2011.pdf" class="calibre9">https://web.stanford.edu/~cgpotts/papers/wvSent_acl2011.pdf</a></li>
<li class="calibre11">Bag of words in scikit-learn: <a href="http://scikit-learn.org/stable/modules/feature_extraction.html#the-bag-of-words-representation" class="calibre9">http://scikit-learn.org/stable/modules/feature_extraction.html#the-bag-of-words-representation</a></li>
<li class="calibre11">Kaggle word2vec <a href="https://www.kaggle.com/c/word2vec-nlp-tutorial" class="calibre9">https://www.kaggle.com/c/word2vec-nlp-tutorial</a></li>
<li class="calibre11">Heap's law: <a href="https://en.wikipedia.org/wiki/Heaps%27_law" class="calibre9">https://en.wikipedia.org/wiki/Heaps%27_law</a></li>
<li class="calibre11">Distributed representations of sentences and documents, Mikolov<span> </span><em class="calibre21">et al</em>: <a href="https://cs.stanford.edu/~quocle/paragraph_vector.pdf" class="calibre9">https://cs.stanford.edu/~quocle/paragraph_vector.pdf</a></li>
<li class="calibre11">CBOW: <a href="https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa" class="calibre9">https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa</a></li>
<li class="calibre11">Skip-gram: McCormick, C. (2016, April 19),<span> </span><em class="calibre21">Word2Vec Tutorial - The Skip-Gram Model</em> <a href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/" class="calibre9">http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/</a></li>
<li class="calibre11">TensorFlow implementation of word2vec: <a href="https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/examples/tutorials/word2vec/word2vec_basic.py" class="calibre9">https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/examples/tutorials/word2vec/word2vec_basic.py</a></li>
<li class="calibre11">Word2vec explained: <a href="https://arxiv.org/abs/1411.2738" class="calibre9">https://arxiv.org/abs/1411.2738</a></li>
<li class="calibre11">Deriving negative sampling: <a href="https://arxiv.org/abs/1402.3722" class="calibre9">https://arxiv.org/abs/1402.3722</a></li>
<li class="calibre11">Compositional distributional semantics: <a href="https://youtu.be/hTmKoHJw3Mg" class="calibre9">https://youtu.be/hTmKoHJw3Mg</a> </li>
<li class="calibre11">The fastText and skipgram: <a href="http://debajyotidatta.github.io/nlp/deep/learning/word-embeddings/2016/09/28/fast-text-and-skip-gram/" class="calibre9">http://debajyotidatta.github.io/nlp/deep/learning/word-embeddings/2016/09/28/fast-text-and-skip-gram/</a></li>
<li class="calibre11">The skip-gram and CBOW: <a href="https://iksinc.online/tag/continuous-bag-of-words-cbow/" class="calibre9">https://iksinc.online/tag/continuous-bag-of-words-cbow/</a></li>
<li class="calibre11">Stanford lectures on CBOW and skip-gram: <a href="https://cs224d.stanford.edu/lecture_notes/notes1.pdf" class="calibre9">https://cs224d.stanford.edu/lecture_notes/notes1.pdf</a></li>
<li class="calibre11"><a href="http://mccormickml.com/assets/word2vec/Alex_Minnaar_Word2Vec_Tutorial_Part_II_The_Continuous_Bag-of-Words_Model.pdf" class="calibre9">http://mccormickml.com/assets/word2vec/Alex_Minnaar_Word2Vec_Tutorial_Part_II_The_Continuous_Bag-of-Words_Model.pdf</a></li>
<li class="calibre11">The fasttext PyTorch: <a href="https://github.com/PetrochukM/PyTorch-NLP" class="calibre9">https://github.com/PetrochukM/PyTorch-NLP</a></li>
<li class="calibre11">Levy, Omer and Goldberg Yoav (2014), <em class="calibre21">Dependency-Based Word Embeddings</em>, 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014-Proceedings of the Conference, <a href="http://www.aclweb.org/anthology/P14-2050" class="calibre9">2. 302-308. 10.3115/v1/P14-2050</a></li>
<li class="calibre11"><em class="calibre21">Notes on Noise Contrastive Estimation and Negative Sampling</em>: <a href="https://arxiv.org/abs/1410.8251" class="calibre9">https://arxiv.org/abs/1410.8251</a></li>
<li class="calibre11"><em class="calibre21">Sebastian Ruder, on word embeddings - Part 2: Approximating the Softmax</em>,<span> </span><a href="http://ruder.io/word-embeddings-softmax" class="calibre9">http://ruder.io/word-embeddings-softmax</a>, 2016.</li>
<li class="calibre11">Scalable hierarchical distributed language model. <a href="http://papers.nips.cc/paper/3583-a-scalable-hierarchical-distributed-language-model.pdf" class="calibre9">http://papers.nips.cc/paper/3583-a-scalable-hierarchical-distributed-language-model.pdf</a></li>
<li class="calibre11">Softmax function and its derivative. <a href="https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/" class="calibre9">https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/</a></li>
<li class="calibre11"><em class="calibre21">What is Softmax Regression and How is it Related to Logistic Regression?</em>, Sebastian Raschka. <a href="https://www.kdnuggets.com/2016/07/softmax-regression-related-logistic-regression.html" class="calibre9">https://www.kdnuggets.com/2016/07/softmax-regression-related-logistic-regression.html</a></li>
<li class="calibre11"><a href="https://web.stanford.edu/class/cs224n/reports/2758157.pdf" class="calibre9">https://web.stanford.edu/class/cs224n/reports/2758157.pdf</a></li>
<li class="calibre11">Softmax regression, <a href="http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/" class="calibre9">http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/</a></li>
<li class="calibre11">Google Allo: <a href="https://research.googleblog.com/2016/05/chat-smarter-with-allo.html" class="calibre9">https://research.googleblog.com/2016/05/chat-smarter-with-allo.html</a></li>
<li class="calibre11"><em class="calibre21">Hierarchical Probabilistic Neural Network Language Model</em>, Morin and Bengio, 2005, <a href="https://www.iro.umontreal.ca/~lisa/pointeurs/hierarchical-nnlm-aistats05.pdf" class="calibre9">https://www.iro.umontreal.ca/~lisa/pointeurs/hierarchical-nnlm-aistats05.pdf</a></li>
<li class="calibre11"><em class="calibre21">A Scalable Hierarchical Distributed Language Model. Mnih, Andriy and Hinton</em>, Geoffrey E. 2009, <a href="https://papers.nips.cc/paper/3583-a-scalable-hierarchical-distributed-language-model" class="calibre9">https://papers.nips.cc/paper/3583-a-scalable-hierarchical-distributed-language-model</a></li>
<li class="calibre11"><em class="calibre21">Self Organised Hierarchical Softmax</em>, <a href="https://arxiv.org/pdf/1707.08588.pdf" class="calibre9"><span>arXiv:1707.08588v1 [cs.CL] 26 Jul 2017</span></a></li>
<li class="calibre11"><em class="calibre21">Effective Text Clustering Method Based on Huffman Encoding Algorithm</em>, Nikhil Pawar, 2012, <a href="https://www.ijsr.net/archive/v3i12/U1VCMTQ1NjE=.pdf" class="calibre9">https://www.ijsr.net/archive/v3i12/U1VCMTQ1NjE=.pdf</a> </li>
<li class="calibre11">Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean, <em class="calibre21">Distributed representations of words and phrases and their compositionality</em>, <em class="calibre21">In Advances in Neural Information Processing Systems 26</em>: <em class="calibre21">27th Annual Conference on Neural Information Processing Systems 2013</em>, <em class="calibre21">Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States, pages 3111–3119, 2013</em>.</li>
<li class="calibre11"><a href="http://debajyotidatta.github.io/nlp/deep/learning/word-embeddings/2016/09/28/fast-text-and-skip-gram/" class="calibre9">http://debajyotidatta.github.io/nlp/deep/learning/word-embeddings/2016/09/28/fast-text-and-skip-gram/</a></li>
<li class="calibre11"><a href="https://github.com/nzw0301/keras-examples/blob/master/Skip-gram-with-NS.ipynb" class="calibre9">https://github.com/nzw0301/keras-examples/blob/master/Skip-gram-with-NS.ipynb</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Chapter 4</h1>
                
            
            <article>
                
<ul class="calibre10">
<li class="calibre11"><span>Vladimir Zolotov and David Kung 2017, <em class="calibre21">Analysis and Optimization of fastText Linear Text Classifier</em>, <a href="http://arxiv.org/abs/1702.05531" class="calibre9">http://arxiv.org/abs/1702.05531</a></span></li>
<li class="calibre11"><em class="calibre21">Text classification of linear models</em>, <a href="http://www.cs.umd.edu/class/fall2017/cmsc723/slides/slides_03.pdf" class="calibre9">http://www.cs.umd.edu/class/fall2017/cmsc723/slides/slides_03.pdf</a></li>
<li class="calibre11"><em class="calibre21">What is text classification</em>, Stanford, <a href="https://nlp.stanford.edu/IR-book/html/htmledition/the-text-classification-problem-1.html#sec:classificationproblem" class="calibre9">https://nlp.stanford.edu/IR-book/html/htmledition/the-text-classification-problem-1.html#sec:classificationproblem</a></li>
<li class="calibre11"><a href="https://nlp.stanford.edu/IR-book/html/htmledition/text-classification-and-naive-bayes-1.html" class="calibre9">https://nlp.stanford.edu/IR-book/html/htmledition/text-classification-and-naive-bayes-1.html</a></li>
<li class="calibre11"><a href="https://research.fb.com/fasttext/" class="calibre9">https://research.fb.com/fasttext/</a></li>
<li class="calibre11"><em class="calibre21">Bag of tricks for efficient classification,</em> <a href="https://arxiv.org/pdf/1607.01759.pdf" class="calibre9">arXiv:1607.01759v3 [cs.CL] 9 Aug 2016</a></li>
<li class="calibre11"><a href="https://github.com/poliglot/fasttext" class="calibre9">https://github.com/poliglot/fasttext</a></li>
<li class="calibre11">Joseph Turian, Lev Ratinov, and Yoshua Bengio, 2010, <em class="calibre21">Word representations: A simple and general method for semi-supervised learning,</em> <em class="calibre21">In</em><span> </span><em class="calibre21">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</em><span> </span><em class="calibre21">(ACL '10)</em>, <em class="calibre21">Association for Computational Linguistics, Stroudsburg, PA, USA, 384-394</em>.</li>
<li class="calibre11"><a href="https://arxiv.org/abs/1607.00570" class="calibre9">arXiv:1607.00570v1 [cs.IR] 2 Jul 2016</a></li>
<li class="calibre11"><em class="calibre21">[Weinberger et al.2009] Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola, and Josh Attenberg, 2009</em>, <em class="calibre21">Feature hashing for large scale multitask learning. In ICML</em></li>
<li class="calibre11"><a href="https://developers.googleblog.com/2018/04/text-embedding-models-contain-bias.html" class="calibre9">https://developers.googleblog.com/2018/04/text-embedding-models-contain-bias.html</a></li>
<li class="calibre11">Softmax classifier in PyTorch: <a href="https://www.youtube.com/watch?v=lvNdl7yg4Pg" class="calibre9">https://www.youtube.com/watch?v=lvNdl7yg4Pg</a></li>
<li class="calibre11"><em class="calibre21">Hierarchical loss for classification</em>: <a href="https://arxiv.org/abs/1709.01062" class="calibre9">arXiv:1709.01062v1 [cs.LG]</a>, 1 September 2017</li>
<li class="calibre11">Svenstrup, Dan &amp; Meinertz Hansen, Jonas &amp; Winther, Ole. (2017), Hash Embeddings for Efficient Word Representations</li>
<li class="calibre11">Explanation kernel trick, <a href="https://www.quora.com/How-does-Kernel-compute-inner-product-in-higher-dimensional-space-without-visiting-that-space/answer/Jeremy-McMinis" class="calibre9">https://www.quora.com/How-does-Kernel-compute-inner-product-in-higher-dimensional-space-without-visiting-that-space/answer/Jeremy-McMinis</a></li>
<li class="calibre11"><a href="https://medium.com/value-stream-design/introducing-one-of-the-best-hacks-in-machine-learning-the-hashing-trick-bf6a9c8af18f" class="calibre9">https://medium.com/value-stream-design/introducing-one-of-the-best-hacks-in-machine-learning-the-hashing-trick-bf6a9c8af18f</a></li>
<li class="calibre11"><em class="calibre21">Extremely Fast Text Feature Extraction for Classification and Indexing</em> by George Forman and Evan Kirshenbaum</li>
<li class="calibre11">Armand Joulin, <a href="https://arxiv.org/abs/1612.03651" class="calibre9">et. al. FastText.zip</a>: <em class="calibre21">Compressing text classification model</em>, 2016, <a href="https://arxiv.org/abs/1612.03651" class="calibre9">https://arxiv.org/abs/1612.03651</a></li>
<li class="calibre11">Vector quantization, <a href="https://www.slideshare.net/rajanisharmaa/vector-quantization" class="calibre9">https://www.slideshare.net/rajanisharmaa/vector-quantization</a></li>
<li class="calibre11"><a href="http://shodhganga.inflibnet.ac.in/bitstream/10603/132782/14/12_chapter%204.pdf" class="calibre9">http://shodhganga.inflibnet.ac.in/bitstream/10603/132782/14/12_chapter%204.pdf</a></li>
<li class="calibre11"><span><em class="calibre21">Voronoi Projection-Based Fast Nearest-Neighbor Search Algorithms: Box-Search and Mapping Table-Based Search Techniques, V. Ramasubramanian, K.K. Paliwa</em>l. 1997, <a href="https://www.sciencedirect.com/science/article/pii/S1051200497903006" class="calibre9">https://www.sciencedirect.com/science/article/pii/S1051200497903006</a></span></li>
<li class="calibre11"><em class="calibre21">How To Implement Learning Vector Quantization From Scratch With Python. Jason Brownie</em>, 2016, <a href="https://machinelearningmastery.com/implement-learning-vector-quantization-scratch-python/" class="calibre9">https://machinelearningmastery.com/implement-learning-vector-quantization-scratch-python/</a></li>
<li class="calibre11">Herve Jegou, Matthijs Douze, and Cordelia Schmid, <em class="calibre21">Product quantization for nearest neighbor search</em>, IEEE Trans. PAMI, January 2011.</li>
<li class="calibre11">Song Han, Huizi Mao, and William J Dally. Deep compression: <em class="calibre21">Compressing deep neural networks with pruning, trained quantization and huffman coding</em>, <em class="calibre21">In ICLR</em>, 2016</li>
<li class="calibre11">Tiezheng Ge, Kaiming He, Qifa Ke, and Jian Sun, <em class="calibre21">Optimized product quantization for approximate nearest neighbor search</em>. <em class="calibre21">In CVPR</em>, June 2013</li>
<li class="calibre11"><em class="calibre21">Expectation Maximisation</em>, Joydeep Bhattacharjee. <a href="https://medium.com/technology-nineleaps/expectation-maximization-4bb203841757" class="calibre9">https://medium.com/technology-nineleaps/expectation-maximization-4bb203841757</a></li>
<li class="calibre11">Chen, Wenlin, Wilson, James T., Tyree, Stephen, Weinberger, Kilian Q, and Chen, Yixin, <em class="calibre21">Compressing neural networks with the hashing trick</em>, <a href="https://arxiv.org/abs/1504.04788" class="calibre9">arXiv:1504.04788, 2015</a></li>
<li class="calibre11">Kai Zeng, Kun She, and Xinzheng Niu, <em class="calibre21">Feature Selection with Neighborhood Entropy-Based Cooperative Game Theory</em>, <em class="calibre21">Computational Intelligence and Neuroscience</em>, vol. 2014, Article ID 479289, 10 pages, 2014, <a href="https://doi.org/10.1155/2014/479289" class="calibre9">https://doi.org/10.1155/2014/479289</a>.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Chapter 5</h1>
                
            
            <article>
                
<ul class="calibre10">
<li class="calibre11"><em class="calibre21">Software Framework for Topic Modelling with Large Corpora</em>, Radim, 2010</li>
<li class="calibre11">Gensim fastText Tutorial: <a href="https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/FastText_Tutorial.ipynb" class="calibre9">https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/FastText_Tutorial.ipynb</a></li>
<li class="calibre11">P. Bojanowski, E. Grave, A. Joulin, T. Mikolov, <em class="calibre21">Enriching Word Vectors with Subword Information</em>, <a href="https://arxiv.org/abs/1607.04606" class="calibre9">https://arxiv.org/abs/1607.04606</a></li>
<li class="calibre11"><a href="http://proceedings.mlr.press/v37/kusnerb15.pdf" class="calibre9">http://proceedings.mlr.press/v37/kusnerb15.pdf</a></li>
<li class="calibre11">Tomas Mikolov, Quoc V Le, Ilya Sutskever, 2013, (<em class="calibre21">Exploiting Similarities among Languages for Machine Translation</em>) (<a href="https://arxiv.org/pdf/1309.4168.pdf" class="calibre9">https://arxiv.org/pdf/1309.4168.pdf</a>)</li>
<li class="calibre11">Georgiana Dinu, Angelikie Lazaridou, and Marco Baroni. 2014, <em class="calibre21">Improving zero-shot learning by mitigating the hubness problem</em> (<a href="https://arxiv.org/pdf/1412.6568.pdf" class="calibre9">https://arxiv.org/pdf/1412.6568.pdf</a>)</li>
<li class="calibre11">The fastText normalization, <a href="https://www.kaggle.com/mschumacher/using-fasttext-models-for-robust-embeddings/notebook" class="calibre9">https://www.kaggle.com/mschumacher/using-fasttext-models-for-robust-embeddings/notebook</a></li>
<li class="calibre11">Luong, Minh-Thang and Socher, Richard and Manning, Christopher D. 2013, (<em class="calibre21">Better Word Representations with Recursive Neural Networks for Morphology</em>) (<a href="https://nlp.stanford.edu/~lmthang/morphoNLM/" class="calibre9">https://nlp.stanford.edu/~lmthang/morphoNLM/</a>)</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Chapter 6</h1>
                
            
            <article>
                
<ul class="calibre10">
<li class="calibre11">Yoav Goldberg (2015), <em class="calibre21">A Primer on Neural Network Models for Natural</em><br class="title-page-name"/>
<em class="calibre21">Language Processing</em>, (<a href="https://arxiv.org/abs/1510.00726" class="calibre9">https://arxiv.org/abs/1510.00726</a>)</li>
<li class="calibre11"><a href="http://www.wildml.com/2015/11/understanding-convolutional-neural-networksfor-nlp/" class="calibre9">http://www.wildml.com/2015/11/understanding-convolutional-neural-networksfor-nlp/</a></li>
<li class="calibre11"><a href="http://mathworld.wolfram.com/Convolution.html" class="calibre9">http://mathworld.wolfram.com/Convolution.html</a></li>
<li class="calibre11"><a href="http://www.joshuakim.io/understanding-how-convolutional-neural-network-cnn-perform-text-classification-with-word-embeddings/" class="calibre9">http://www.joshuakim.io/understanding-how-convolutional-neural-network-cnn-perform-text-classification-with-word-embeddings/</a></li>
<li class="calibre11"><a href="https://machinelearningmastery.com/best-practices-document-classification-deep-learning/" class="calibre9">https://machinelearningmastery.com/best-practices-document-classification-deep-learning/</a></li>
<li class="calibre11"><a href="https://keras.io/layers/embeddings/" class="calibre9">https://keras.io/layers/embeddings/</a></li>
<li class="calibre11"><a href="https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html" class="calibre9">https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html</a></li>
<li class="calibre11"><a href="https://pytorch.org/docs/stable/nn.html" class="calibre9">https://pytorch.org/docs/master/nn.html</a></li>
<li class="calibre11"><a href="https://stackoverflow.com/a/35688187" class="calibre9">https://stackoverflow.com/a/35688187</a></li>
<li class="calibre11"><a href="http://www.brightideasinanalytics.com/rnn-pretrained-word-vectors/" class="calibre9">http://www.brightideasinanalytics.com/rnn-pretrained-word-vectors/</a> </li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Chapter 7</h1>
                
            
            <article>
                
<ul class="calibre10">
<li class="calibre11"><a href="https://developer.android.com/training/basics/firstapp/" class="calibre9">https://developer.android.com/training/basics/firstapp/</a></li>
<li class="calibre11"><a href="https://github.com/sszuev/fastText_java" class="calibre9">https://github.com/sszuev/fastText_java</a></li>
<li class="calibre11"><a href="https://stackoverflow.com/a/35369267" class="calibre9">https://stackoverflow.com/a/35369267</a></li>
<li class="calibre11"><a href="https://developer.android.com/studio/publish/app-signing" class="calibre9">https://developer.android.com/studio/publish/app-signing</a></li>
<li class="calibre11"><a href="https://github.com/vinhkhuc/JFastText/blob/master/examples/api/src/main/java/ApiExample.java" class="calibre9">https://github.com/vinhkhuc/JFastText/blob/master/examples/api/src/main/java/ApiExample.java</a></li>
<li class="calibre11">The fastText issues: <a href="https://github.com/vinhkhuc/JFastText/issues/28" class="calibre9">https://github.com/vinhkhuc/JFastText/issues/28</a></li>
<li class="calibre11"><a href="https://github.com/linkfluence/fastText4j/tree/master/src/main/java/fasttext" class="calibre9">https://github.com/linkfluence/fastText4j/tree/master/src/main/java/fasttext</a></li>
</ul>


            </article>

            
        </section>
    </body></html>