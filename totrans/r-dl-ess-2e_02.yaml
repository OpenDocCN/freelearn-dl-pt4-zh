- en: Training a Prediction Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter shows you how to build and train basic neural networks in R through
    hands-on examples and shows how to evaluate different hyper-parameters for models
    to find the best set. Another important issue in deep learning is dealing with overfitting,
    which is when a model performs well on the data it was trained on but poorly on
    unseen data. We will briefly look at this topic in this chapter, and cover it
    in more depth in [Chapter 3](6e6dd858-9f00-454a-8434-a95c59e85b25.xhtml), *Deep
    Learning Fundamentals*. The chapter closes with an example use case classifying
    activity data from a smartphone as walking, going up or down stairs, sitting,
    standing, or lying down.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks in R
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Binary classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing a neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-classification using the nnet and RSNNS packages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The problem of overfitting data—the consequences explained
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use case—building and applying a neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural networks in R
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will build several neural networks in this section. First, we will use the
    neuralnet package to create a neural network model that we can visualize. We will
    also use the `nnet` and `RSNNS` (Bergmeir, C., and Benítez, J. M. (2012)) packages.
    These are standard R packages and can be installed by the `install.packages` command
    or from the packages pane in RStudio. Although it is possible to use the `nnet`
    package directly, we are going to use it through the `caret` package, which is
    short for **Classification and Regression Training**. The `caret` package provides
    a standardized interface to work with many **machine learning** (**ML**) models
    in R, and also has some useful features for validation and performance assessment
    that we will use in this chapter and the next.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our first examples of building neural networks, we will use the `MNIST`
    dataset, which is a classic classification problem: recognizing handwritten digits
    based on pictures. The data can be downloaded from the Apache MXNet site ([https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/R/data/mnist_csv.zip](https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/R/data/mnist_csv.zip)).
    It is in the CSV format, where each column of the dataset, or feature, represents a
    pixel from the image. Each image has 784 pixels (28 x 28) and the pixels are in
    grayscale and range from 0 to 255\. The first column contains the digit label,
    and the rest are pixel values, to be used for classification.'
  prefs: []
  type: TYPE_NORMAL
- en: Building neural network models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code is in the `Chapter2` folder of the code for this book. If you have
    not already downloaded and unzipped the code, go back to [Chapter 1](00c01383-1886-46d0-9435-29dfb3e08055.xhtml),
    *Getting Started with Deep Learning*, for the link to download the code. Unzip
    the code into a folder in your machine, and you will see folders for different
    chapters. The code we will be following is `Chapter2\chapter2.R`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the `MNIST` dataset to build some neural network models. The first
    few lines in the script look to see whether the data file (`train.csv`) is in
    the data directory. If the file already exists in the data directory then it proceeds;
    if it isn''t, it downloads a ZIP file from [https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/R/data/mnist_csv.zip](https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/R/data/mnist_csv.zip),
    and unzips it into the data folder. This check means that you don''t have to download
    the data manually and the program only downloads the file once. Here is the code
    to download the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'As an alternative, the `MNIST` data is also available in Keras, so we can download
    it from that library and save it as a CSV file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'When you load any new dataset for the first time, the first thing you should
    do is a quick check on the data to ensure that the number of rows and columns
    are as expected, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The data looks OK, we have `42000` rows and `785` columns. The header was imported
    correctly and the values are numeric. Now that we have loaded the data and performed
    some validation checks on it, we can move on to modeling. Our first model will
    use the `neuralnet` library as this allows us to visualize the neural net. We
    will select only the rows where the label is either 5 or 6, and create a binary
    classification task to differentiate between them. Of course, you can pick any
    digits you choose, but using 5 and 6 is a good choice because they are similar
    graphically, and therefore our model will have to work harder than if we picked
    two digits that were not so similar, for example, 1 and 8\. We rename the labels
    as 0 and 1 for modeling and then separate that data into a train and a test split.
  prefs: []
  type: TYPE_NORMAL
- en: We then perform dimensionality-reduction using **principal components analysis** (**PCA**)
    on the training data—we use PCA because we want to reduce the number of predictor
    variables in our data to a reasonable number for plotting. PCA requires that we
    remove columns that have zero variance; these are the columns that have the same
    value for each instance. In our image data, there is a border around all images,
    that is, the values are all zero. Note how we find the columns that have zero
    variance using only the data used to train the model; it would be incorrect to
    apply this check first and then split the data for modelling.
  prefs: []
  type: TYPE_NORMAL
- en: '**Dimensionality-reduction**: Our image data is grayscale data with values
    from 0 (black) to 255 (white). These values are highly correlated, that is, if
    a pixel is black (that is, 0), it is likely that the pixels around it are either
    black or dark gray. Similarly if a pixel is white (255), it is likely that the
    pixels around it are either white or light gray. Dimensionality-reduction is an
    unsupervised machine learning technique that takes an input dataset and produces
    an output dataset with the same'
  prefs: []
  type: TYPE_NORMAL
- en: number of rows but fewer columns. Crucially though, these fewer columns can
    explain most of the signal in the input dataset. PCA is one dimensionality-reduction
    algorithm. We use it here because we want to create a dataset with a small number
    of columns to plot the network, but we still want our algorithm to produce good
    results.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code selects the rows where the label is either 5 or 6 and creates
    a train/test split. It also removes columns where the variance is zero; these
    are columns that have the same value for every row:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We have reduced the number of column data from `784` to `624`, that is, `160`
    columns had the same value for all rows. Now, we perform PCA on the data and plot
    the cumulative sum of the variances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The cumulative sum of PCA explained variance shows how many principal components
    are needed to explain the proportion of variance in the input data. In layman''s
    terms, this plot shows that we can use the first 100 variables (the *principal
    components*) and this will account for over 80% of the variance in the original
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/de8c020f-25b5-4642-a63a-171364740a40.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.1: Cumulative sum of the Principal Components explained variance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The next code block selects out the principal components that account for 50%
    of our variance and use those variables to create a neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that 50% of the variance in the original data can be accounted by
    only 23 principal components. Next, we plot the neural network by calling the
    `plot` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces a plot similar to the following screenshot. We can see the input
    variables (**PC1** to **PC23**), the hidden layers and biases, and even the network
    weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bc6d5871-c471-4093-9ff1-75faca8dcfed.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.2: An example of a neural network with weights and biases'
  prefs: []
  type: TYPE_NORMAL
- en: We selected 23 principal components to use as predictors for our neural network
    library. We chose to use two hidden layers, the first with four nodes and the
    second with two nodes. The plot outputs the coefficients, which are not all decipherable
    from the plot, but there are functions to access them if required.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will create predictions on a holdout or test dataset that was not
    used to build either the dimensionality-reduction or the neural network model.
    We have to first pass the test data into the `predict` function, passing in the
    `df.pca` object created earlier, to get the principal components for the test
    data. We can then pass this data into the neural network prediction (filtering
    the columns to the first 23 principal components) and then show the confusion
    matrix and overall accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We achieved `97.86%` accuracy, which is not bad considering we only used 23
    principal components in our neural network. It is important to note that these
    23 variables are not directly comparable to any columns in the input dataset or
    each other. In fact, the whole point of PCA, or any dimensionality-reduction algorithm,
    is to produce columns that are not correlated with each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will move on to create models that perform multi-classification, that
    is, they can classify digits 0-9\. We will convert the labels (the digits 0 to
    9) to a factor so R knows that this is a classification not a regression problem.
    For real-world problems, you should use all the data available, but if we used
    all 42,000 rows, it would take a very long time to train using the neural network
    packages in R. We will select 5,000 rows for training and 1,000 rows for test
    purposes. We should select the rows at random and ensure that there is no overlap
    between the rows in our training and test datasets. We also separate the data
    into the features or predictors (`digits.x`) and the outcome (`digits.Y`). We
    are using all the columns except the labels as the predictors here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, before we get started building our neural network, let''s quickly
    check the distribution of the digits. This can be important as, for example, if
    one digit occurs very rarely, we may need to adjust our modeling approach to ensure
    that, it''s given enough weight in the performance evaluation if we care about
    accurately predicting that specific digit. The following code snippet creates
    a bar plot showing the frequency of each digit label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see from the plot that the categories are fairly evenly distributed
    so there is no need to increase the weight or importance given to any particular
    one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8a41bd71-dbfe-46fd-ac22-d442ffbfc1e9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.3: Distribution of *y* values for train dataset'
  prefs: []
  type: TYPE_NORMAL
- en: Now let's build and train our first neural network using the `nnet` package
    through the `caret` package wrapper. First, we use the `set.seed()` function and
    specify a specific seed so that the results are reproducible. The exact seed is
    not important, what matters is that the same seed is used each time you run the
    script. The `train()` function first takes the feature or predictor data (`x`),
    and then the outcome variable (`y`), as arguments. The `train()` function can
    work with a variety of models, determined via the method argument. Although many
    aspects of machine learning models are learned automatically, some parameters
    have to be set. These vary by the method used; for example, in neural networks,
    one parameter is the number of hidden units. The `train()` function provides an
    easy way to try a variety of these tuning parameters as a named data frame to
    the `tuneGrid` argument. It returns the performance measures for each set of tuning
    parameters and returns the best trained model. We will start with just five hidden
    neurons in our model, and a modest decay rate. The learning rate controls how
    much each iteration or step can influence the current weights. The decay rate
    is the regularization hyper-parameter, which is used to prevent the model from
    overfitting. Another argument, `trcontrol`, controls additional aspects of `train()`,
    and is used, when a variety of tuning parameters are being evaluated, to tell
    the caret package how to validate and pick the best tuning parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this example, we will set the method for training control to *none* as
    we only have one set of tuning parameters being used here. Finally, at the end,
    we can specify additional, named arguments that are passed on to the actual `nnet()`
    function (or whatever algorithm is specified). Because of the number of predictors
    (`784`), we increase the maximum number of weights to 10,000 and specify a maximum
    of 100 iterations. Due to the relatively small amount of data, and the paucity
    of hidden neurons, this first model does not take too long to run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The `predict()` function generates a set of predictions for the data. We will
    use the test dataset to evaluate the model; this contains records that were not
    used to train the model. We examine the distribution of the predicted digits in
    the following diagram.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'It is clear that this is not a good model because the distribution of the predicted
    values is very different from the distribution of the actual values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fad60498-d289-4519-af5e-4e4398bccfbc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.4: Distribution of *y* values from prediction model'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `barplot` is a simple check of the predictions, and shows us that our model
    is not very accurate. We can also calculate the accuracy by finding the percentage
    of rows from the predictions that match the actual value. The accuracy for this
    model is `54.8%`, which is not good. A more formal evaluation of the model''s
    performance is possible using the `confusionMatrix()` function in the `caret`
    package. Because there is a function by the same name in the `RSNNS` package,
    they are masked so we call the function using `caret::confusionMatrix` to ensure
    the function from the `caret` package is used. The following code shows the confusion
    matrix and performance metrics on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Because we had multiple digits, there are three main sections to the performance
    output. First, the actual frequency cross tab is shown. Correct predictions are
    on the diagonal, with various frequencies of misclassification on the off diagonals.
    Next are the overall statistics, which refer to the model's performance across
    all classes. Accuracy is simply the proportion of cases correctly classified,
    along with a 95% confidence interval, which can be useful, especially for smaller
    datasets where there may be considerable uncertainty in the estimate.
  prefs: []
  type: TYPE_NORMAL
- en: '`No Information Rate` refers to what accuracy could be expected without any
    information by merely guessing the most frequent class, in this case, 1, which
    occurred 11.16% of the time. The p-value tests whether the observed accuracy (44.3%)
    is significantly different from `No Information Rate` (11.2% ). Although statistically
    significant, this is not very meaningful for digit-classification, where we would
    expect to do far better than simply guessing the most frequent digit! Finally,
    individual performance metrics for each digit are shown. These are based on calculating
    that digit versus every other digit, so that each is a binary comparison.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have some basic understanding of how to set up, train, and evaluate
    model performance, we will try increasing the number of hidden neurons, which
    is one key way to improve model performance, at the cost of greatly increasing
    the model complexity. Recall from [Chapter 1](00c01383-1886-46d0-9435-29dfb3e08055.xhtml), *Getting
    Started with Deep Learning*, that every predictor or feature connects to each
    hidden neuron, and each hidden neuron connects to each outcome or output. With
    `784` features, each additional hidden neuron adds a substantial number of parameters,
    which also results in longer run times. Depending on your computer, be prepared
    to wait a number of minutes for these next model to finish:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This model is better than the previous model but the distribution of the predicted
    values is still uneven:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5a9739c7-1c4e-4251-bea0-3786fd878cf6.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.5: Distribution of *y* values from prediction model
  prefs: []
  type: TYPE_NORMAL
- en: 'Increasing the number of hidden neurons from 5 to 10 improved our in-sample
    performance from an overall accuracy of `54.8%` to `66.3%`, but this is still
    quite some way from ideal (imagine character-recognition software that mixed up
    over 30% of all the characters!). We increase again, this time to 40 hidden neurons,
    and wait even longer for the model to finish training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The distribution of the predicted values is even in this model, which is what
    we are looking for. However the accuracy is still only at 82.2%, which is quite
    low:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b4d0b983-1da7-448d-831c-9e7ec71fe882.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.6: Distribution of *y* values from prediction model
  prefs: []
  type: TYPE_NORMAL
- en: Using 40 hidden neurons has improved accuracy to `82.2%` overall and it took
    over 40 minutes to run on an i5 computer. Model performance for some digits is
    still not great. If this were a real research or business problem, we might continue
    trying additional neurons, tuning the decay rate, or modifying features in order
    to try to boost model performance further, but for now we will move on.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will take a look at how to train neural networks using the RSNNS package.
    This package provides an interface to a variety of possible models using the **Stuttgart
    Neural Network Simulator** (**SNNS**) code; however, for a basic, single­ hidden-layer,
    feed-forward neural network, we can use the `mlp()` convenience wrapper function,
    which stands for multi-layer perceptron. The RSNNS package is a bit trickier to
    use than the convenience of nnet via the `caret` package, but one benefit is that
    it can be far more flexible and allows for many other types of neural network
    architecture to be trained, including recurrent neural networks, and also has
    a greater variety of training strategies.
  prefs: []
  type: TYPE_NORMAL
- en: 'One difference between the nnet and RSNNS packages is that, for multi-class
    outcomes (such as digits), RSNNS requires a dummy encoding (that is, one-hot encoding),
    where each possible class is represented as a column coded as 0/1\. This is facilitated
    using the `decodeClassLabels()` function, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we had reasonably good success with 40 hidden neurons, we will use the
    same size here. Rather than standard propagation as the learning function, we
    will use resilient propagation, based on the work of Riedmiller, M., and Braun,
    H. (1993). Resilient back-propagation is an **optimization** to standard back-propagation
    that applies faster weight-update mechanisms. One of the problems that occurs
    as the neural network increases in complexity is that they take a long time to
    train. We will discuss this in depth in subsequent chapters, but for now, you
    just need to know that this neural network is faster because it keeps track of
    past derivatives and takes bigger steps if they were in the same direction during
    back-propagation. Note also that, because a matrix of outcomes is passed, although
    the predicted probability will not exceed 1 for any single digit, the sum of predicted
    probabilities across all digits may exceed 1 and also may be less than 1 (that
    is, for some cases, the model may not predict they are very likely to represent
    any of the digits). The `predict` function returns a matrix where each column
    represents a single digit, so we use the `encodeClassLabels()` function to convert
    back into a single vector of digit labels to plot and evaluate the model''s performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The following bar plot shows that the predicted values are relatively evenly
    distributed among the categories. This matches the distribution of the actual
    category values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9ce6c7c5-8599-4a8c-a1ba-1890a556db51.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.7: Distribution of *y* values from prediction model
  prefs: []
  type: TYPE_NORMAL
- en: The accuracy is 81.70% and it ran in 3 minutes on my computer. This is only
    slightly lower than when we used nnet with 40 hidden nodes, which took 40 minutes
    on the same machine! This demonstrates the importance of using an optimizer, which
    we will see in subsequent chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Generating predictions from a neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For any given observation, there can be a probability of membership in any of
    a number of classes (for example, an observation may have a 40% chance of being
    a *5*, a 20% chance of being a *6*, and so on). To evaluate the performance of
    the model, some choices have to be made about how to go from the probability of
    class membership to a discrete classification. In this section, we will explore
    a few of these options in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'As long as there are no perfect ties, the simplest method is to classify observations
    based on the highest predicted probability. Another approach, which the RSNNS
    package calls the **winner takes all** (**WTA**) method, chooses the class with
    the highest probability, provided the following conditions are met:'
  prefs: []
  type: TYPE_NORMAL
- en: There are no ties for highest probabilities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The highest probability is above a user-defined threshold (the threshold could
    be zero)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The remaining classes all have a predicted probability under the maximum minus
    another user-defined threshold
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Otherwise, observations are classified as unknown. If both thresholds are zero
    (the default), this equates to saying that there must be one unique maximum. The
    advantage of such an approach is that it provides some quality control. In the
    digit-classification example we have been exploring, there are 10 possible classes.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose 9 of the digits had a predicted probability of 0.099, and the remaining
    class had a predicted probability of 0.109\. Although one class is technically
    more likely than the others, the difference is fairly trivial and we may conclude
    that the model cannot with any certainty classify that observation. A final method,
    called 402040, classifies if only one value is above a user-defined threshold,
    and all other values are below another user-defined threshold; if multiple values
    are above the first threshold, or any value is not below the second threshold,
    it treats the observation as unknown. Again, the goal here is to provide some
    quality control.
  prefs: []
  type: TYPE_NORMAL
- en: It may seem like this is unnecessary because uncertainty in predictions should
    come out in the model performance. However, it can be helpful to know if your
    model was highly certain in its prediction and right or wrong, or uncertain and
    right or wrong.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, in some cases, not all classes are equally important. For example,
    in a medical context where a variety of biomarkers and genes are collected on
    patients and used to classify whether they are, at risk of cancer, or at risk
    of heart disease, even a 40% chance of having cancer may be enough to warrant
    further investigation, even if they have a 60% chance of being healthy. This has
    to do with the performance measures we saw earlier where, beyond overall accuracy,
    we can assess aspects such as sensitivity, specificity, and positive and negative
    predictive values. There are cases where overall accuracy is less important than
    making sure no one is missed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows the raw probabilities for the in-sample data, and
    the impact these different choices have on the predicted values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We now proceed to examine problems related to overfitting the data and the impact
    on the evaluation of the model's performance.
  prefs: []
  type: TYPE_NORMAL
- en: The problem of overfitting data – the consequences explained
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A common issue in machine learning is overfitting data. Generally, overfitting
    is used to refer to the phenomenon where the model performs better on the data
    used to train the model than it does on data not used to train the model (holdout
    data, future real use, and so on). Overfitting occurs when a model memorizes part
    of the training data and fits what is essentially noise in the training data.
    The accuracy in the training data is high, but because the noise changes from
    one dataset to the next, this accuracy does not apply to unseen data, that is,
    we can say that the model does not generalize very well.
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting can occur at any time, but tends to become more severe as the ratio
    of parameters to information increases. Usually, this can be thought of as the
    ratio of parameters to observations, but not always. For example, suppose we have
    a very imbalanced dataset where the outcome we want to predict is a rare event
    that occurs in 1 in 5 million cases. In that case, a sample size of 15 million
    may only have 3 positive cases. Even though the sample size is large, the information
    is low. To consider a simple-but-extreme case, imagine fitting a straight line
    to two data points. The fit will be perfect, and in those two training data, your
    linear-regression model will appear to have fully accounted for all variations
    in the data. However, if we then applied that line to another 1,000 cases, it
    might not fit very well at all.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous sections, we generated out-of-sample predictions for the our
    models, that is, we evaluated accuracy on test (or holdout) data. But we never
    checked whether our models were overfitting, that is, the accuracy levels on the
    test data. We can examine how well the model generalizes by checking the accuracy
    on the in-sample predictions. We can see that the accuracy on the in-sample data
    is 84.7%, compared to 81.7% on the holdout data. There is a 3.0% loss; or, put
    differently, using training data to evaluate model performance resulted in an
    overly optimistic estimate of the accuracy, and that overestimate was 3.0%:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Since we fitted several models earlier of varying complexity, we could examine
    the degree of overfitting or overly optimistic accuracy from in-sample versus
    out­ of-sample performance measures across them. The code here should be easy
    enough to follow. We call the predict function for our models and do not pass
    in any new data; this returns the predictions for the data the model was trained
    with. The rest of the code is boilerplate code to create the graphic plot.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The code produces the following plot, which shows the accuracy metrics and
    the confidence intervals for those metrics. One thing we notice from this plot
    is that, as the models get more complex, the gap between performance on the in-sample performance
    measures and the out-sample performance measures increases. This highlights that
    more complex models tend to overfit, that is, they perform better on the in-sample data
    than the unseen out-sample data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/88f817b7-5f97-42d6-8606-09e4ab39f888.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.8: In-sample and out-sample performance measures for on neural network
    models'
  prefs: []
  type: TYPE_NORMAL
- en: Use case – building and applying a neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To close the chapter, we will discuss a more realistic use case for neural networks.
    We will use a public dataset by Anguita, D., Ghio, A., Oneto, L., Parra, X., and
    Reyes-Ortiz, J. L. (2013) that uses smartphones to track physical activity. The
    data can be downloaded at [https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones](https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones).
    The smartphones had an accelerometer and gyroscope from which 561 features from
    both time and frequency were used.
  prefs: []
  type: TYPE_NORMAL
- en: The smartphones were worn during walking, walking upstairs, walking downstairs,
    standing, sitting, and lying down. Although this data came from phones, similar
    measures could be derived from other devices designed to track activity, such
    as various fitness-tracking watches or bands. So this data can be useful if we
    want to sell devices and have them automatically track how many of these different
    activities the wearer engages in.
  prefs: []
  type: TYPE_NORMAL
- en: 'This data has already been normalized to range from -1 to + 1; usually we might
    want to perform some normalization if it has not already been applied. Download
    the data from the link and unzip it into the data folder that is on the same level
    as the chapter folder; we will use it in later chapters as well. We can import the
    training and testing data, as well as the labels. We will then take a quick look
    at the distribution of the outcome variable in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following bar plot, which shows that the categories are relatively
    evenly balanced:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b105e064-6747-4bd6-97b7-2aaf6d21844d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.9: Distribution of *y* values for UCI HAR dataset'
  prefs: []
  type: TYPE_NORMAL
- en: We are going to evaluate a variety of tuning parameters to show how we might
    experiment with different approaches to try to get the best possible model. We
    will use different hyper-parameters and evaluate which model performs the best.
  prefs: []
  type: TYPE_NORMAL
- en: Because the models can take some time to train and R normally only uses a single
    core, we will use some special packages to enable us to run multiple models in parallel.
    These packages are `parallel`, `foreach`, and `doSNOW`, which should have been
    loaded if you ran the script from the first line.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can pick our tuning parameters and set up a local cluster as the backend
    for the `foreach` R package for parallel for loops. Note that if you do this on
    a machine with fewer than five cores, you should change `makeCluster(5)` to a
    lower number:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we are ready to train all the models. The following code shows a parallel
    for loop, using code that is similar to what we have already seen, but this time
    setting some of the arguments based on the tuning parameters we previously stored
    in the list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Because generating out-of-sample predictions can also take some time, we will
    do that in parallel as well. However, first we need to export the model results
    to each of the workers on our cluster, and then we can calculate the predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can merge the actual and fitted or predicted values together into
    a dataset, calculate performance measures on each one, and store the overall results
    together for examination and comparison. We can use almost identical code to the
    code that follows to generate out-of-sample performance measures. That code is
    not shown in the book, but is available in the code bundle provided with the book.
    Some additional data-management is required here as sometimes a model may not
    predict each possible response level, but this can make for non-symmetrical frequency
    cross tabs, unless we convert the variable to a factor and specify the levels.
    We also drop `o` values, which indicate the model was uncertain about how to classify
    an observation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'If we print the in-sample and out-of-sample performance, we can see how each
    of our models did and the effect of varying some of the tuning parameters. The
    output is shown in the following code. The fourth column (null accuracy) is dropped
    as it is not as important for this comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: As a reminder, the in-sample results evaluate the predictions on the training
    data and the out-sample results evaluate the predictions on the holdout (or test)
    data. The best set of hyper-parameters is the last set, where we get an accuracy
    of 93.8% on unseen data. This shows that we are able to classify the types of
    activity people are engaged in quite accurately based on the data from their smartphones.
    We can also see that the more complex models perform better on the in-sample data,
    which is not always the case with out-of-sample performance measures.
  prefs: []
  type: TYPE_NORMAL
- en: For each model, we have large differences between the accuracy for the in-sample
    data against the out-of-sample data; the models clearly overfit. We will get into
    ways to combat this overfitting in [Chapter 3](6e6dd858-9f00-454a-8434-a95c59e85b25.xhtml),* Deep
    Learning Fundamentals*, as we train deep neural networks with multiple hidden
    layers.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the slightly worse out-of-sample performance, the models still do well – far
    better than chance alone – and, for our example use case, we could pick the best
    model and be quite confident that using this will provide a good classification
    of a user's activities.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter showed how to get started building and training neural networks
    to classify data, including image recognition and physical activity data. We looked
    at packages that can visualize a neural network and we created a number of models
    to perform classification on data with 10 different categories. Although we only
    used some neural network packages rather than deep learning packages, our models
    took a long time to train and we had issues with overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the basic neural network models in this chapter took a long time to
    train, even though we did not use all the data available. For the MNIST data,
    we used approx. 8,000 rows for our binary classification task and only 6,000 rows
    for our multi-classification task. Even so, one model took almost an hour to train.
    Our deep learning models will be much more complicated and should be able to process
    millions of records. You can now see why specialist hardware is required for training
    deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, we see that a potential pitfall in machine learning is that more complex
    models will be more likely to overfit the training data, so that evaluating performance
    in the same data used to train the model results in biased, overly optimistic
    estimates of the model performance. Indeed, this can even make a difference as
    to which model is chosen as the best. Overfitting is also an issue for deep neural
    networks. In the next chapter, we will discuss various techniques used to prevent
    overfitting and obtain more accurate estimates of model performance.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter we will look at building a neural network from scratch and
    see how it applies to deep learning. We will also discuss some methods to deal
    with overfitting.
  prefs: []
  type: TYPE_NORMAL
