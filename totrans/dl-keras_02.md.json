["```py\nfrom keras.applications.vgg16 import VGG16\nfrom keras.models import Model\nfrom keras.preprocessing import image\nfrom keras.applications.vgg16 import preprocess_input\nimport numpy as np\n\n# pre-built and pre-trained deep learning VGG16 model\nbase_model = VGG16(weights='imagenet', include_top=True)\nfor i, layer in enumerate(base_model.layers):\n  print (i, layer.name, layer.output_shape)\n\n```", "```py\n# The script MUST contain a function named azureml_main\n# which is the entry point for this module.\n\n# imports up here can be used to\nimport pandas as pd\nimport theano\nimport theano.tensor as T\nfrom theano import function\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nimport numpy as np\n# The entry point function can contain up to two input arguments:\n#   Param<dataframe1>: a pandas.DataFrame\n#   Param<dataframe2>: a pandas.DataFrame\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    # Execution logic goes here\n    # print('Input pandas.DataFrame #1:rnrn{0}'.format(dataframe1))\n\n    # If a zip file is connected to the third input port is connected,\n    # it is unzipped under \".Script Bundle\". This directory is added\n    # to sys.path. Therefore, if your zip file contains a Python file\n    # mymodule.py you can import it using:\n    # import mymodule\n    model = Sequential()\n    model.add(Dense(1, input_dim=784, activation=\"relu\"))\n    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n    data = np.random.random((1000,784))\n    labels = np.random.randint(2, size=(1000,1))\n    model.fit(data, labels, nb_epoch=10, batch_size=32)\n    model.evaluate(data, labels)\n\n    return dataframe1,\n\n```", "```py\nmodel = Sequential()\nmodel.add(Dense(N_HIDDEN, input_shape=(784,)))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(DROPOUT))\nmodel.add(Dense(N_HIDDEN))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(DROPOUT))\nmodel.add(Dense(nb_classes))\nmodel.add(Activation('softmax'))\nmodel.summary()\n\n```", "```py\nkeras.layers.core.Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n\n```", "```py\nkeras.layers.recurrent.Recurrent(return_sequences=False, go_backwards=False, stateful=False, unroll=False, implementation=0)\n\nkeras.layers.recurrent.SimpleRNN(units, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0)\n\nkeras.layers.recurrent.GRU(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0)\n\nkeras.layers.recurrent.LSTM(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0)\n\n```", "```py\nkeras.layers.convolutional.Conv1D(filters, kernel_size, strides=1, padding='valid', dilation_rate=1, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n\nkeras.layers.convolutional.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n\nkeras.layers.pooling.MaxPooling1D(pool_size=2, strides=None, padding='valid')\n\nkeras.layers.pooling.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)\n\n```", "```py\nkeras.layers.core.Dropout(rate, noise_shape=None, seed=None)\n\n```", "```py\nkeras.layers.normalization.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)\n\n```", "```py\n# save as JSON json_string = model.to_json()\n# save as YAML yaml_string = model.to_yaml() \n# model reconstruction from JSON: from keras.models import model_from_json model = model_from_json(json_string) # model reconstruction from YAML model = model_from_yaml(yaml_string)\n\n```", "```py\nfrom keras.models import load_model model.save('my_model.h5')\n# creates a HDF5 file 'my_model.h5' del model\n# deletes the existing model\n# returns a compiled model\n# identical to the previous one model = load_model('my_model.h5')\n\n```", "```py\nkeras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0,  \npatience=0, verbose=0, mode='auto')\n\n```", "```py\nclass LossHistory(keras.callbacks.Callback):     def on_train_begin(self, logs={}):         self.losses = []     def on_batch_end(self, batch, logs={}):         self.losses.append(logs.get('loss')) model = Sequential() model.add(Dense(10, input_dim=784, init='uniform')) model.add(Activation('softmax')) model.compile(loss='categorical_crossentropy', optimizer='rmsprop') history = LossHistory() model.fit(X_train,Y_train, batch_size=128, nb_epoch=20,  \nverbose=0, callbacks=[history]) print history.losses\n\n```", "```py\nfrom __future__ import division, print_function \nfrom keras.callbacks import ModelCheckpoint \nfrom keras.datasets import mnist \nfrom keras.models import Sequential \nfrom keras.layers.core import Dense, Dropout \nfrom keras.utils import np_utils \nimport numpy as np \nimport os \n\nBATCH_SIZE = 128 \nNUM_EPOCHS = 20 \nMODEL_DIR = \"/tmp\" \n\n(Xtrain, ytrain), (Xtest, ytest) = mnist.load_data() \nXtrain = Xtrain.reshape(60000, 784).astype(\"float32\") / 255 \nXtest = Xtest.reshape(10000, 784).astype(\"float32\") / 255 \nYtrain = np_utils.to_categorical(ytrain, 10) \nYtest = np_utils.to_categorical(ytest, 10) \nprint(Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape) \n\nmodel = Sequential() \nmodel.add(Dense(512, input_shape=(784,), activation=\"relu\")) \nmodel.add(Dropout(0.2)) \nmodel.add(Dense(512, activation=\"relu\")) \nmodel.add(Dropout(0.2)) \nmodel.add(Dense(10, activation=\"softmax\")) \n\nmodel.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", \n              metrics=[\"accuracy\"]) \n\n# save best model \ncheckpoint = ModelCheckpoint( \n    filepath=os.path.join(MODEL_DIR, \"model-{epoch:02d}.h5\")) \nmodel.fit(Xtrain, Ytrain, batch_size=BATCH_SIZE, nb_epoch=NUM_EPOCHS, \n          validation_split=0.1, callbacks=[checkpoint])\n\n```", "```py\nkeras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0,  \nwrite_graph=True, write_images=False)\n\n```", "```py\ntensorboard --logdir=/full_path_to_your_logs\n\n```", "```py\npip install quiver_engine \n\nfrom quiver_engine import server     server.launch(model)\n\n```"]