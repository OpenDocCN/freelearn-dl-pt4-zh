["```py\n>>> sum([0.9**(2*i) + 2*(0.9**(2*i+1)) for i in range(50)]) \n14.736450674121663 \n>>> sum([2*(0.9**(2*i)) + 0.9**(2*i+1) for i in range(50)]) \n15.262752483911719\n```", "```py\nimport typing as tt \nimport gymnasium as gym \nfrom collections import defaultdict, Counter \nfrom torch.utils.tensorboard.writer import SummaryWriter \n\nENV_NAME = \"FrozenLake-v1\" \nGAMMA = 0.9 \nTEST_EPISODES = 20\n```", "```py\nState = int \nAction = int \nRewardKey = tt.Tuple[State, Action, State] \nTransitKey = tt.Tuple[State, Action]\n```", "```py\nclass Agent: \n    def __init__(self): \n        self.env = gym.make(ENV_NAME) \n        self.state, _ = self.env.reset() \n        self.rewards: tt.Dict[RewardKey, float] = defaultdict(float) \n        self.transits: tt.Dict[TransitKey, Counter] = defaultdict(Counter) \n        self.values: tt.Dict[State, float] = defaultdict(float)\n```", "```py\n def play_n_random_steps(self, n: int): \n        for _ in range(n): \n            action = self.env.action_space.sample() \n            new_state, reward, is_done, is_trunc, _ = self.env.step(action) \n            rw_key = (self.state, action, new_state) \n            self.rewards[rw_key] = float(reward) \n            tr_key = (self.state, action) \n            self.transits[tr_key][new_state] += 1 \n            if is_done or is_trunc: \n                self.state, _ = self.env.reset() \n            else: \n                self.state = new_state\n```", "```py\n def calc_action_value(self, state: State, action: Action) -> float: \n        target_counts = self.transits[(state, action)] \n        total = sum(target_counts.values()) \n        action_value = 0.0 \n        for tgt_state, count in target_counts.items(): \n            rw_key = (state, action, tgt_state) \n            reward = self.rewards[rw_key] \n            val = reward + GAMMA * self.values[tgt_state] \n            action_value += (count / total) * val \n        return action_value\n```", "```py\n def select_action(self, state: State) -> Action: \n        best_action, best_value = None, None \n        for action in range(self.env.action_space.n): \n            action_value = self.calc_action_value(state, action) \n            if best_value is None or best_value < action_value: \n                best_value = action_value \n                best_action = action \n        return best_action\n```", "```py\n def play_episode(self, env: gym.Env) -> float: \n        total_reward = 0.0 \n        state, _ = env.reset() \n        while True: \n            action = self.select_action(state) \n            new_state, reward, is_done, is_trunc, _ = env.step(action) \n            rw_key = (state, action, new_state) \n            self.rewards[rw_key] = float(reward) \n            tr_key = (state, action) \n            self.transits[tr_key][new_state] += 1 \n            total_reward += reward \n            if is_done or is_trunc: \n                break \n            state = new_state \n        return total_reward\n```", "```py\n def value_iteration(self): \n        for state in range(self.env.observation_space.n): \n            state_values = [ \n                self.calc_action_value(state, action) \n                for action in range(self.env.action_space.n) \n            ] \n            self.values[state] = max(state_values)\n```", "```py\nif __name__ == \"__main__\": \n    test_env = gym.make(ENV_NAME) \n    agent = Agent() \n    writer = SummaryWriter(comment=\"-v-iteration\")\n```", "```py\n iter_no = 0 \n    best_reward = 0.0 \n    while True: \n        iter_no += 1 \n        agent.play_n_random_steps(100) \n        agent.value_iteration()\n```", "```py\n reward = 0.0 \n        for _ in range(TEST_EPISODES): \n            reward += agent.play_episode(test_env) \n        reward /= TEST_EPISODES \n        writer.add_scalar(\"reward\", reward, iter_no) \n        if reward > best_reward: \n            print(f\"{iter_no}: Best reward updated {best_reward:.3} -> {reward:.3}\") \n            best_reward = reward \n        if reward > 0.80: \n            print(\"Solved in %d iterations!\" % iter_no) \n            break \n    writer.close()\n```", "```py\nChapter05$ ./01_frozenlake_v_iteration.py \n3: Best reward updated 0.0 -> 0.1 \n4: Best reward updated 0.1 -> 0.15 \n7: Best reward updated 0.15 -> 0.45 \n9: Best reward updated 0.45 -> 0.7 \n11: Best reward updated 0.7 -> 0.9 \nSolved in 11 iterations!\n```", "```py\n def value_iteration(self): \n        for state in range(self.env.observation_space.n): \n            for action in range(self.env.action_space.n): \n                action_value = 0.0 \n                target_counts = self.transits[(state, action)] \n                total = sum(target_counts.values()) \n                for tgt_state, count in target_counts.items(): \n                    rw_key = (state, action, tgt_state) \n                    reward = self.rewards[rw_key] \n                    best_action = self.select_action(tgt_state) \n                    val = reward + GAMMA * self.values[(tgt_state, best_action)] \n                    action_value += (count / total) * val \n                self.values[(state, action)] = action_value\n```", "```py\n def select_action(self, state: State) -> Action: \n        best_action, best_value = None, None \n        for action in range(self.env.action_space.n): \n            action_value = self.values[(state, action)] \n            if best_value is None or best_value < action_value: \n                best_value = action_value \n                best_action = action \n        return best_action\n```", "```py\nChapter05$ ./02_frozenlake_q_iteration.py \n8: Best reward updated 0.0 -> 0.35 \n11: Best reward updated 0.35 -> 0.45 \n14: Best reward updated 0.45 -> 0.55 \n15: Best reward updated 0.55 -> 0.65 \n17: Best reward updated 0.65 -> 0.75 \n18: Best reward updated 0.75 -> 0.9 \nSolved in 18 iterations!\n```"]