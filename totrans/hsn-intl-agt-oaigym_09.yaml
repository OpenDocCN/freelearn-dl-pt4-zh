- en: Exploring the Learning Environment Landscape - Roboschool, Gym-Retro, StarCraft-II,
    DeepMindLab
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You have come a long way in your quest to get hands-on experience in building
    intelligent agents to solve a variety of challenging problems. In the previous
    chapters, we looked into several environments that are available in OpenAI Gym.
    In this chapter, we will look beyond the Gym and look at some of the other well
    developed environments that you can use to train your intelligent agents or run
    experiments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we look at other open source libraries that provide good learning environments
    for developing intelligent agents, let''s have a look at a recent class of environments
    added to the OpenAI Gym library. If, like me, you are interested in robotics,
    you will like this one a lot. Yes! It is the robotics class of environments, which
    provides very useful environments for robotic manipulation tasks such as fetching,
    sliding, pushing, and so on with a robotic arm. These robotics environments are
    based on the MuJoCo engine and you may recall from [Chapter 3](part0056.html#1LCVG0-22c7fc7f93b64d07be225c00ead6ce12)[,](part0056.html#1LCVG0-22c7fc7f93b64d07be225c00ead6ce12) *Getting
    Started with OpenAI Gym and Deep Reinforcement Learning*, that the MuJoCo engine
    requires a paid license, unless you are a student and using MuJoCo for personal
    or class use. A summary of these robotics environments is shown in the following
    screenshot, with the environment names and a brief description for each, so that
    you can check them out if you are interested in exploring such problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00290.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Gym interface-compatible environments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will have a deeper look into environments that are compatible
    with the Gym interface out of the box. You should be able to use any of the agents
    we developed in the previous chapters in these environments. Let's get started
    and look at a few very useful and promising learning environments.
  prefs: []
  type: TYPE_NORMAL
- en: Roboschool
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Roboschool[ (https://github.com/openai/roboschool)](https://github.com/openai/roboschool)
    provides several environments for controlling robots in simulation. It was released
    by OpenAI and the environments have the same interface as the OpenAI Gym environments
    that we have been using in this book. The Gym's MuJoCo-based environments offer
    a rich variety of robotic tasks, but MuJoCo requires a license for use after the
    free trial. Roboschool provides eight environments that quite closely match the
    MuJoCo ones, which is a good news as it offers a free alternative. Apart from
    these eight environments, Roboschool also offers several new and challenging environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table shows a quick comparison between the MuJoCo Gym environments
    and the Roboschool environments:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Brief description** | **MuJoCo environment** | **Roboschool environment**
    |'
  prefs: []
  type: TYPE_TB
- en: '| Make a one-legged 2D robot hop forward as fast as possible | Hopper-v2![](img/00291.jpeg)
    | RoboschoolHopper-v1![](img/00292.jpeg) |'
  prefs: []
  type: TYPE_TB
- en: '| Make a 2D robot walk | Walker2d-v2![](img/00293.jpeg) | RoboschoolWalker2d-v1![](img/00294.jpeg)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Make a four-legged 3D robot walk  | Ant-v2![](img/00295.jpeg) | RoboschoolAnt-v1![](img/00296.jpeg)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Make a bipedal 3D robot walk forward as fast as possible without falling
    | Humanoid-v2![](img/00297.jpeg) | RoboschoolHumanoid-v1![](img/00298.jpeg) |'
  prefs: []
  type: TYPE_TB
- en: 'A full list of environments that are available as part of the Roboschool library,
    with their state and action spaces, is provided in the following table for your
    quick reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Env ID | Roboschool env | obs space | action space |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| RoboschoolInvertedPendulum-v1 | ![](img/00299.jpeg) | Box(5,) | Box(1,) |'
  prefs: []
  type: TYPE_TB
- en: '| RoboschoolInvertedPendulumSwingup-v1 | ![](img/00300.jpeg) | Box(5,) | Box(1,)
    |'
  prefs: []
  type: TYPE_TB
- en: '| RoboschoolInvertedDoublePendulum-v1 | ![](img/00301.jpeg) | Box(9,) | Box(1,)
    |'
  prefs: []
  type: TYPE_TB
- en: '| RoboschoolReacher-v1 | ![](img/00302.jpeg) | Box(9,) | Box(2,) |'
  prefs: []
  type: TYPE_TB
- en: '| RoboschoolHopper-v1 | ![](img/00303.jpeg) | Box(15,) | Box(3,) |'
  prefs: []
  type: TYPE_TB
- en: '| RoboschoolWalker2d-v1 | ![](img/00304.jpeg) | Box(22,) | Box(6,) |'
  prefs: []
  type: TYPE_TB
- en: '| RoboschoolHalfCheetah-v1 | ![](img/00305.jpeg) | Box(26,) | Box(6,) |'
  prefs: []
  type: TYPE_TB
- en: '| RoboschoolAnt-v1 | ![](img/00306.jpeg) | Box(28,) | Box(8,) |'
  prefs: []
  type: TYPE_TB
- en: '| RoboschoolHumanoid-v1 | ![](img/00307.jpeg) | Box(44,) | Box(17,) |'
  prefs: []
  type: TYPE_TB
- en: '| RoboschoolHumanoidFlagrun-v1 | ![](img/00308.jpeg) | Box(44,) | Box(17,)
    |'
  prefs: []
  type: TYPE_TB
- en: '| RoboschoolHumanoidFlagrunHarder-v1 | ![](img/00309.jpeg) | Box(44,) | Box(17,)
    |'
  prefs: []
  type: TYPE_TB
- en: '| RoboschoolPong-v1 | ![](img/00310.jpeg) | Box(13,) | Box(2,) |'
  prefs: []
  type: TYPE_TB
- en: Quickstart guide to setting up and running Roboschool environments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Roboschool environments make use of the open source Bulletphysics engine instead
    of the proprietary MuJoCo engine. Let''s quickly have a look at a Roboschool environment
    so that you know how to use any environment from the Roboschool library if you
    happen to find it useful for your work. To get started, we will have to first
    install the Roboschool Python library in our `rl_gym_book` conda environment.
    Because the library depends on several components, including the Bulletphysics
    engine, there are several installation steps involved, which are listed in the
    official Roboschool GitHub repository here: [https://github.com/openai/roboschool](https://github.com/openai/roboschool).
    To make things simpler, you can use the script in the book''s code repository
    at `ch9/setup_roboschool.sh `to automatically compile and install the `Roboschool`
    library for you. Follow these steps to run the script:'
  prefs: []
  type: TYPE_NORMAL
- en: Activate the `rl_gym_book` conda environment using `source activate rl_gym_book`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate to the `ch9` folder with `cd ch9`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make sure that the script's execution bit is set as `chmod a+x setup_roboschool.sh`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the script with `sudo`:`./setup_roboschool.sh`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This should install the required system dependencies, fetch and compile a compatible
    source code of the bullet3 physics engine; pull the Roboschool source code to
    the `software` folder under your home directory; and finally compile, build, and
    install the Roboschool library in the `rl_gym_book` conda environment. If the
    setup completes successfully, you will see the following message printed on the
    console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You can run a quickstart demo script using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This will launch a funny-to-watch robo-race in which you will see a hopper,
    half-cheetah, and humanoid running a race! The interesting aspect is that each
    of the robots is being controlled by a reinforcement learning-based trained policy.
    The race will look similar to this snapshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00311.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Once it has been installed, you can create a Roboschool environment and use
    one of the agents we developed in an earlier chapter to train and run on these
    environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use the `run_roboschool_env.py` script in this chapter''s code repository
    at  [https://github.com/PacktPublishing/Hands-On-Intelligent-Agents-with-OpenAI-Gym/tree/master/ch9](https://github.com/PacktPublishing/Hands-On-Intelligent-Agents-with-OpenAI-Gym/tree/master/ch9) to
    check out any of the Roboschool environments. For example, to check out the `RoboschoolInvertedDoublePendulum-v1`
    environment, you can run the following script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You can use any other Roboschool environment name from the previous table, as
    well as new Roboschool environments when they are made available.
  prefs: []
  type: TYPE_NORMAL
- en: Gym retro
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Gym Retro ([https://github.com/openai/retro](https://github.com/openai/retro))
    is a relatively new (released on May 25, 2018) Python library released by OpenAI ([https://blog.openai.com/gym-retro/](https://blog.openai.com/gym-retro/))
    as a research platform for developing reinforcement learning algorithms for game
    playing. Although the Atari suite of 60+ games was available in OpenAI Gym, the
    total number of games available was limited. Gym Retro supports the use of games
    developed for several console/retro gaming platforms, such as Nintendo''s NES,
    SNES, Game Boy consoles, Sega Genesis, and Sega Master System to name a few. This
    is made possible with the use of emulators using the Libretro API:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00312.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Gym Retro provides convenient wrappers to turn more than 1,000 such video games
    into Gym interface-compatible learning environments! Isn't that great! Several
    new learning environments but with the same interface, so that we can easily train
    and test the agents we have developed so far without any necessary changes to
    the code...
  prefs: []
  type: TYPE_NORMAL
- en: 'To get a feel for how easy it is to use the environments in Gym Retro, let''s
    put the installation steps aside for a moment and quickly look at the code to
    create a new Gym Retro environment once it is installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This code snipped will create an `env` object that has the same interfaces and
    methods, such as `step(...)`, `reset()` and `render()`, as all the Gym environments
    we have seen before.
  prefs: []
  type: TYPE_NORMAL
- en: Quickstart guide to setup and run Gym Retro
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s try out the Gym Retro library quickly by installing the pre-built binaries
    using pip with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the installation is successful, we can have a sneak peek into one of the
    available Gym Retro environments using the following script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this script will bring up a window with the Airstriker game and show
    the spaceship taking random actions. The game window will look something like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00313.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Before we move on, one thing to note is that the **ROM** (**Read-Only Memory**)
    file that contains the whole game data is not made freely available for all games.
    The ROMs for some non-commercial console games such as Airstriker (used in the
    previous script), Fire, Dekadrive, Automaton, Fire, Lost Marbles, and so on are
    included with the Gym Retro library and are free to use. Other games, such as
    the Sonic series (Sonic The Hedgehog, Sonic The Hedgehog 2, Sonic 3 & Knuckles)
    require ROMs to be purchased for legal use from places such as [Steam](https://store.steampowered.com/app/71113/Sonic_The_Hedgehog/).
    This is a barrier for hobbyists, students, and other enthusiasts who wish to develop
    algorithms using such environments. But at least this barrier is relatively small,
    as it costs about USD 1.69 on Steam for the Sonic The Hedgehog ROM. Once you have
    the ROM files for the games, the Gym Retro library provides a script to import
    them into the library like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Note that when creating a new Gym Retro environment, we need the name of the
    game as well as the state of the game `retro.make(game='NAME_OF_GAME', state='NAME_OF_STATE')`
  prefs: []
  type: TYPE_NORMAL
- en: 'To get a list of available Gym Retro environments, you can run the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'And to get a list of available game states, you can run the following Python
    script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: So far, we have gotten ourselves familiar with the Gym Retro library. Let's
    analyze what advantages or new features this library provides us in addition to
    what we have already seen and used. First, the Gym Retro library utilizes newer
    games consoles (such as the SEGA Genesis) than the Atari console. For comparison,
    the SEGA Genesis games console has 500 times as much RAM as the Atari console,
    enabling better visuals and a greater range of controls. This provides us with
    learning environments that are relatively sophisticated, and some more complex
    tasks and challenges for our intelligent agents to learn and solve. Second, several
    of these console games are progressive in nature, where the complexity of the
    game typically increases with every level and the levels have several similarities
    in some aspects (such as the goal, object appearances, physics, and so on) while
    also providing diversity in other aspects (such as the layout, new objects, and
    so on). Such a training environment with levels of progressively increasing difficulty
    help in developing intelligent agents that can learn to solve tasks in general
    without being very task/environment-specific (such as overfitting in supervised
    learning). Agents can learn to transfer their skills and learning from one level
    to another, and then to another game. This area is under active research and is
    usually known as curriculum learning, staged learning, or incremental evolution.
    After all, ultimately we are interested in developing intelligent agents that
    can learn to solve tasks in general and not just the specific tasks the agent
    was trained on. The Gym Retro library provides some useful, though game-only,
    environments to facilitate such experiments and research.
  prefs: []
  type: TYPE_NORMAL
- en: Other open source Python-based learning environments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss recent Python-based learning environments that
    provide a good platform for intelligent agent development but don't necessarily
    have a Gym-compatible environment interface. Although they do not provide Gym-compatible
    interfaces, the environments we will be discussing in this section were carefully
    selected to make sure that either a Gym wrapper (to make it compatible with the
    Gym interface) is available, or they are easy to implement in order to use and
    experiment with the agents we have developed through this book. As you can guess,
    this list of good Python-based learning environments for developing intelligent
    agents will grow in the future, as this area is being very actively researched
    at the moment. The book's code repository will have information and quickstart
    guides for new environments as they become available in the future. Sign up for
    update notifications at the book's GitHub repository to get those updates. In
    the following sub-sections, we will discuss some of the most promising learning
    environments that are readily available for use.
  prefs: []
  type: TYPE_NORMAL
- en: StarCraft II - PySC2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: StarCraft II is very popular, in fact one of the most successful real-time strategy
    games ever, and is played by millions of people worldwide. It even has a world
    championship league ([https://wcs.starcraft2.com/en-us/](https://wcs.starcraft2.com/en-us/))!
    The environment is quite complex and the main goal is to build an army base, manage
    an economy, defend the base, and destroy enemies. The player controls the base
    camp and the army from a third-person view of the scene. If you are not familiar
    with StarCarft, you should watch a few games online to get a feel for how complex
    and fast-paced the game is.
  prefs: []
  type: TYPE_NORMAL
- en: For humans to play this real-time strategy game well, it takes a lot of practice (several
    months even; in fact, professional players train for several years), planning,
    and quick responses. Although software agents can press several software buttons
    per frame to make pretty fast moves, speed of action is not the only factor that
    contributes to victory. The agent has to multi-task and micro-manage the army
    units, and maximize their score, which is several orders of magnitude more complex
    than the Atari games.
  prefs: []
  type: TYPE_NORMAL
- en: Blizzard, the company that made StarCraft II, released the StarCraft II API,
    which provides the necessary hooks to interface with the StarCraft II game and
    control it without limitations. That enables several new possibilities, such as
    the development of the intelligent agents that we are after. They even have a
    separate **End User License Agreement** (**EULA**) for open use of the environment
    under an AI and machine learning license! This was a very welcome move by a company
    such as Blizzard, which is in the business of making and selling games. The open
    source the **StarCraft2** (**SC2**) client protocol implementation and provides
    the Linux installation package, along with several accessories such as the map
    packs, free to download from their GitHub page at [https://github.com/Blizzard/s2client-proto](https://github.com/Blizzard/s2client-proto).
    On top of that, Google DeepMind has open sourced their PySC2 library, which exposes
    the SC2 client interfaces through Python and provides a wrapper that makes it
    an RL environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the PySC2 UI, with the feature layers available
    as observations to the agents shown on the right and a simplified overview of
    the game scene on the left:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00314.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: If you are interested in these types of environment, and especially if you are
    a game developer, you might be interested in the Dota 2 environment as well. Dota
    2 is a real-time strategy game, like StarCraft II, and is played between two teams
    of five players with each player controlling a hero character. You can learn more
    about how OpenAI developed a team of five neural network-based agents that have
    learned to work in a team, played 180 years' worth of games in a day, learned
    to overcome several challenges (including high-dimensional and continuous state
    and action spaces, and long-term horizons), all while playing against themselves
    using self-play! You can read more about the five-agent team at https://blog.openai.com/openai-five/.
  prefs: []
  type: TYPE_NORMAL
- en: Quick start guide to setup and run StarCraft II PySC2 environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will look at how you can quickly set up and get started with the StarCraft
    II environment. As always, use the README files in the code repository for the
    latest up-to-date instructions, as things as the links and the versions may change.
    If you have not done so already, star and watch the book's code repository to
    get notified about changes and updates.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading the StarCraft II Linux packages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Download the latest Linux packages for the StarCraft game from https://github.com/Blizzard/s2client-proto#downloads
    and extract it onto your hard disk at `~/StarCraftII`. For example, to download
    version 4.1.2 to your `~/StarCraftII/` folder, you can use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s unzip and extract the files to the `~/StarCraftII/` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Note that, as mentioned on the download page, the files are password protected
    with the password `'iagreetotheeula`.
  prefs: []
  type: TYPE_NORMAL
- en: By typing that, Blizzard ensures we agree to be bound by the terms of their
    AI and machine learning license, found on the download page.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading the SC2 maps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will need the StarCraft II map packs and the mini games pack to get started.
  prefs: []
  type: TYPE_NORMAL
- en: Download the map packs from [https://github.com/Blizzard/s2client-proto#map-packs](https://github.com/Blizzard/s2client-proto#map-packs)
  prefs: []
  type: TYPE_NORMAL
- en: Extract them to your `~/StarCraftII/Maps` directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, let''s download the Ladder maps released for 2018 season 2 using
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s unzip the maps to the `~/StarCraftII/Maps` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will download and unzip the mini game map files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Installing PySC2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s install the PySC2 library for the RL environment interface, along with
    the required dependencies. This step is going to be straightforward, as there
    is a PyPi Python package for the PySC2 library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Playing StarCraftII yourself or running sample agents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To test whether the installation went fine and to see what the StarCarftII
    learning environment looks like, you can quickly start up a randomly-acting agent
    on the Simple64 map or the CollectMineralShards map using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also load another available map for the environment. For example, the
    following command loads the CollectMineralShards map:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This should bring up a UI showing you the actions taken by the random agent,
    which gives you an idea of what the valid actions are and helps you to visualize
    what is going on in the environment as the agent is acting.
  prefs: []
  type: TYPE_NORMAL
- en: 'To play the game yourself, PySC2 offers a human agent interface, which is quite
    useful for debugging (and, if you are interested, playing!) purposes. The following
    is the command to run and play the game yourself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also run a sample agent that is scripted to collect mineral shards,
    which is one of the tasks in the game, using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Watch the book's code repository for new agent source code and instructions
    to train and test new agents with advanced skills. You can also customize the
    agents we developed in the previous chapter to learn to play StarCraftII. If you
    do, send a pull request to the book's code repository, shoot the author an email,
    or shout it out so that everyone knows what cools things you have done!
  prefs: []
  type: TYPE_NORMAL
- en: DeepMind lab
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DeepMind Lab ([https://github.com/deepmind/lab](https://github.com/deepmind/lab))
    is a 3D learning environment that provides a suite of environments with challenging
    tasks, such as 3D navigation through mazes and puzzle solving. It is built based
    on a handful of pieces of open source software, including the famous Quake III
    Arena.
  prefs: []
  type: TYPE_NORMAL
- en: 'The environment interface is very similar to the Gym interface that we have
    used extensively in this book so far. To get a feel for what the environment interface
    actually looks like, have a look at the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This code, though not one-to-one compatible with the OpenAI Gym interface, provides
    a very similar interface.
  prefs: []
  type: TYPE_NORMAL
- en: DeepMind Lab learning environment interface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will briefly discuss the environment interface for **DeepMind Lab (DM Lab)**,
    so that you are familiar with it, can see the similarities with the OpenAI Gym
    interface, and can start experimenting with agents in DM Lab environments!
  prefs: []
  type: TYPE_NORMAL
- en: reset(episode=-1, seed=None)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is similar to the `reset()` method we saw in the Gym interface, but unlike
    Gym environments, DM Lab's `reset` method call does not return the observation.
    We will see later how to get observations, so for now, we will discuss DM Lab's `reset(episode=-1,
    seed=None)` method. It resets the environment to an initial state and needs to
    be called at the end of every episode, in order for a new episode to be created.
    The optional `episode` argument takes an integer value to specify the level in
    a specific episode. If the `episode` value is not set, or is set to `-1`, the
    levels are loaded in numerical order. The `seed` argument is also optional and
    is used to seed the environment's random number generator for reproducibility
    purposes.
  prefs: []
  type: TYPE_NORMAL
- en: step(action, num_steps=1)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is similar to the Gym interface's `step(action)` method, but like with
    the `reset(...)` method, the call to this method does not return the next observation
    (or reward, done, and info). Calling this method advances the environment by `num_steps`
    number of frames, executing the action defined by `action` in every frame. This
    action-repeat behavior is useful in cases where we would like the same action
    to be applied for four or so consecutive frames, which was actually found by several
    researchers to help with learning. There are Gym environment wrappers that accomplish
    this action-repeat behavior.
  prefs: []
  type: TYPE_NORMAL
- en: observations()
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is the method we would use after the call to `reset(...)` or `step(action)`
    to receive the observations from DM Lab environments. This method returns a Python
    dictionary object with every type of observation that we specified from the list
    of available types for the environment. For example, if we wanted **RGBD** (**Red-Green-Blue-Depth**)
    information about the environment as the observation type, we can specify that
    when we initialize the environment using the `''RGBD''` key, we can then retrieve
    this information from the returned observation dictionary using the same `''RGBD''`
    key. A simple example to illustrate this is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: There are also other observation types that are supported by DM Lab environments.
    We can use `observation_spec()` to get a list of supported observation types,
    which we will discuss very shortly.
  prefs: []
  type: TYPE_NORMAL
- en: is_running()
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This method is analogous (in the opposite sense) to the `done` Boolean returned
    by the Gym interface's `step(action)` method.
  prefs: []
  type: TYPE_NORMAL
- en: This method will return `False` when the environment is done with an episode
    or stops running. It will return `True` as long as the environment is running.
  prefs: []
  type: TYPE_NORMAL
- en: observation_spec()
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This method is similar to `env.observation_space()`, which we used with the
    Gym environments. This method returns a list specifying all the available observations
    supported by the DM Lab environment. It also includes specifications about level-dependent
    custom observations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The specifications contain the name, type, and shape of the tensor or string
    that will be returned if that specification name is specified in the observation
    list (such as the `''RGBD''` example previously). For example, the following code
    snippet lists two items in the list that will be returned to give you an idea
    about what the specs contain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'To quickly understand how this method can be used, let''s look at the following
    lines of code and the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: action_spec()
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar to the `observation_spec()` the `action_spec()` method returns a list
    containing the min, max, and a name for each of the elements in the space.  The
    `min` and `max` values respectively represent the minimum and maximum value that
    the corresponding element in the action space can be set to. The length of this
    list will equal the dimension/shape of the action space. This is analogous to
    `env.action_space`, which we have been using with the Gym environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet gives us a quick look into what the return values
    from a call to this method will look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: num_steps()
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This utility method is like a counter that counts the number of frames executed
    by the environment since the last `reset()` call.
  prefs: []
  type: TYPE_NORMAL
- en: fps()
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This utility method returns the number of frames (or environment steps) executed
    per actual (wall-clock) second. This is useful to keep track of the environment
    execution speeds and how fast the agent can sample from the environment.
  prefs: []
  type: TYPE_NORMAL
- en: events()
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This utility method can be useful for debugging as it returns a list of events
    that have occurred since the last call to `reset()` or `step(...)`. The returned
    tuple contains a name and a list of observations.
  prefs: []
  type: TYPE_NORMAL
- en: close()
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like the `close()` method available with Gym environments, this method also
    closes the environment instance and releases the underlying resources, such as
    the Quake III Arena instance.
  prefs: []
  type: TYPE_NORMAL
- en: Quick start guide to setup and run DeepMind Lab
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the familiarity we got after the brief discussion about the DeepMind Lab
    environment interface in the previous section, we are ready to get some hands
    on experience with this learning environment. In the following sub-sections, we
    will go through the steps to setup DeepMind Lab and  run a sample agent.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up and installing DeepMind Lab and its dependencies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The DeepMind Lab library uses Bazel as the build tool, which in turn requires
    Java. The book''s code repository has a script that you can run to easily set
    up DeepMind Lab. You can find the script under the chapter9 folder at [https://github.com/PacktPublishing/Hands-On-Intelligent-Agents-with-OpenAI-Gym/tree/master/ch9](https://github.com/PacktPublishing/Hands-On-Intelligent-Agents-with-OpenAI-Gym/tree/master/ch9).
    You can run the script using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This script will take some time to finish, but will automatically install all
    the necessary packages and libraries, including Bazel and its dependencies, and
    set everything up for you.
  prefs: []
  type: TYPE_NORMAL
- en: Playing the game, testing a randomly acting agent, or training your own!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once the installation is complete, you can test the game using your keyboard
    inputs by running the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also test it with the help of a randomly acting agent using the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'To get started with your own agent development, you can use the example agent
    script that was already configured to interact with the DeepMind Lab environment.
    You can find the script at `~/HOIAWOG/ch9/deepmindlab/python/random_agent.py`.
    To start training that agent, you can use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at several interesting and valuable learning environments,
    saw how their interfaces are set up, and even got hands-on with those environments
    using the quickstart guides for each environment and the setup scripts available
    in the book's code repository. We first looked at environments that have interfaces
    compatible with the OpenAI Gym interface that we are now very familiar with. Specifically
    in this category, we explored the Roboschool and Gym Retro environments.
  prefs: []
  type: TYPE_NORMAL
- en: We also looked at other useful learning environments that did not necessarily
    have a Gym-compatible environment interface, but had a very similar API and so
    it was easy to adapt our agent code or implement a wrapper around the learning
    environment to make it compatible with the Gym API. Specifically, we explored
    the famous real-time strategy game-based StarCraft II environment and the DeepMind
    Lab environment. We also very briefly touched upon the DOTA2 environment, which
    has been used to train both single agents and a team of agents, trained by OpenAI,
    which successfully defeated amateur human players, and even some professional
    gaming teams, in the DOTA 2 contest.
  prefs: []
  type: TYPE_NORMAL
- en: We saw the different sets of tasks and environments available in each of these
    learning environment libraries and tried out examples to get a feel for the environment,
    and also to get an idea about how we can use the agents we developed in the previous
    chapters to train and solve the challenging tasks in these relatively new learning
    environments.
  prefs: []
  type: TYPE_NORMAL
