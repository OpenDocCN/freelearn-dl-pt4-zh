["```py\nimport tensorflow as tf\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport data_utils\nimport matplotlib.pyplot as plt\n```", "```py\ndef data_padding(x, y, length = 15):\n    for i in range(len(X)):\n        x[i] = x[i] + (length - len(x[i])) * [spanish_word2idx['<pad>']]\n        y[i] = [english_word2idx['<go>']] + y[i] + (length - len(y[i])) * [english_word2idx['<pad>']]\n```", "```py\nX_train,  X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1)\n```", "```py\ninput_sequence_length = 15\noutput_sequence_length = 16\n```", "```py\nspanish_vocab_size = len(spanish_vocab) + 2 # + <pad>, <unk>\nenglish_vocab_size = len(english_vocab) + 4 # + <pad>, <eos>, <go>\n```", "```py\nencoder_inputs = [tf.placeholder(dtype=tf.int32, shape=[None], name=\"encoder{}\".format(i)) for i in range(input_sequence_length)]\n\ndecoder_inputs = [tf.placeholder(dtype=tf.int32, shape=[None], name=\"decoder{}\".format(i)) for i in range(output_sequence_length)]\n\ntargets = [decoder_inputs[i] for i in range(output_sequence_length - 1)]\ntargets.append(tf.placeholder(dtype = tf.int32, shape=[None], name=\"last_output\"))\n\ntarget_weights = [tf.placeholder(dtype = tf.float32, shape = [None], name=\"target_w{}\".format(i)) for i in range(output_sequence_length)]\n```", "```py\nsize = 512 # num_hidden_units\nembedding_size = 100\n\nwith tf.variable_scope(\"model_params\"):\n    w_t = tf.get_variable('proj_w', [english_vocab_size, size], tf.float32)\n    b = tf.get_variable('proj_b', [english_vocab_size], tf.float32)\n    w = tf.transpose(w_t)\n    output_projection = (w, b)\n\n    outputs, states = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n                      encoder_inputs,\n                      decoder_inputs,\n                      tf.contrib.rnn.BasicLSTMCell(size),\n                      num_encoder_symbols=spanish_vocab_size,\n                      num_decoder_symbols=english_vocab_size,\n                      embedding_size=embedding_size,\n                      feed_previous=False,\n                      output_projection=output_projection,\n                      dtype=tf.float32)\n```", "```py\nloss = tf.contrib.legacy_seq2seq.sequence_loss(outputs, targets, target_weights, softmax_loss_function = sampled_loss)\n\nlearning_rate = 5e-3 (5*10^(-3) = 0.005)\noptimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n```", "```py\ndef sampled_loss(labels, logits):\n    return tf.nn.sampled_softmax_loss(\n        weights=w_t,\n        biases=b,\n        labels=tf.reshape(labels, [-1, 1]),\n        inputs=logits,\n        num_sampled=size,\n        num_classes=english_vocab_size\n    )\n```", "```py\ndef train():\n    init = tf.global_variables_initializer()\n    saver = tf.train.Saver()\n\n    with tf.Session() as sess:\n        sess.run(init)\n        for step in range(steps):\n            feed = feed_dictionary_values(X_train, Y_train)\n            sess.run(optimizer, feed_dict=feed)\n            if step % 5 == 4 or step == 0:\n                loss_value = sess.run(loss, feed_dict = feed)\n                losses.append(loss_value)\n                print(\"Step {0}/{1} Loss {2}\".format(step, steps, \n                loss_value))\n            if step % 20 == 19:\n                saver.save(sess, 'ckpt/', global_step = step)\n```", "```py\ndef feed_dictionary_values(x, y, batch_size):\n    feed = {}\n    indices_x = np.random.choice(len(x), size=batch_size, replace=False)\n    indices_y = np.random.choice(len(y), size=batch_size, replace=False)\n\n    for i in range(input_sequence_length):\n        feed[encoder_inputs[i].name] = np.array([x[j][i] for j in indices_x], dtype=np.int32)\n\n    for i in range(output_sequence_length):\n        feed[decoder_inputs[i].name] = np.array([y[j][i] for j in indices_y], dtype=np.int32)\n\n    feed[targets[len(targets)-1].name] = np.full(shape = [batch_size], fill_value=english_word2idx['<pad>'], dtype=np.int32)\n\n    for i in range(output_sequence_length - 1):\n        batch_weights = np.ones(batch_size, dtype=np.float32)\n        target = feed[decoder_inputs[i+1].name]\n        for j in range(batch_size):\n            if target[j] == english_word2idx['<pad>']:\n                batch_weigths[j] = 0.0\n        feed[target_weights[i].name] = batch_weigths\n\n    feed[target_weights[output_sequence_length - 1].name] = np.zeros(batch_size, dtype=np.float32)\n\n    return feed\n```", "```py\n\"encoder0\", \"encoder1\", ..., \"encoder14\" (input_sequence_length=15), \"decoder0\", \"decoder1\" through to \"decoder15\" (output_sequence_length=16), \"last_output\", \"target_w0\", \"target_w1\", and so on, through to \"target_w15\" \n```", "```py\n          import tensorflow as tf\n          import numpy as np\n          import neural_machine_translation as nmt\n```", "```py\n          # Placeholders\n          encoder_inputs = [tf.placeholder(dtype = tf.int32, shape = \n          [None],   \n          name = 'encoder{}'.format(i)) for i in   \n          range(nmt.input_sequence_length)]\n          decoder_inputs = [tf.placeholder(dtype = tf.int32, shape = \n          [None],   \n          name = 'decoder{}'.format(i)) for i in   \n          range(nmt.output_sequence_length)]\n          with tf.variable_scope(\"model_params\", reuse=True):\n              w_t = tf.get_variable('proj_w', [nmt.english_vocab_size, \n              nmt.size], tf.float32)\n              b = tf.get_variable('proj_b', [nmt.english_vocab_size], \n               tf.float32)\n              w = tf.transpose(w_t)\n               output_projection = (w, b)\n\n              outputs, states = \n               tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n                        encoder_inputs,\n                        decoder_inputs,\n                        tf.contrib.rnn.BasicLSTMCell(nmt.size),\n                        num_encoder_symbols = nmt.spanish_vocab_size,\n                        num_decoder_symbols = nmt.english_vocab_size,\n                        embedding_size = nmt.embedding_size,\n                        feed_previous = True,\n                        output_projection = output_projection,\n                        dtype = tf.float32)\n```", "```py\n        outputs_proj = [tf.matmul(outputs[i], output_projection[0]) +   \n        output_projection for i in range(nmt.output_sequence_length)]\n```", "```py\n       spanish_sentences = [\"Como te llamas\", \"Mi nombre es\", \"Estoy \n         leyendo un libro\",\"Que tal\", \"Estoy bien\", \"Hablas espanol\", \n         \"Que hora es\", \"Hola\", \"Adios\", \"Si\", \"No\"]\n\n         spanish_sentences_encoded = [[nmt.spanish_word2idx.get(word, \n          0) for word in sentence.split()] for sentence in \n          spanish_sentences]\n\n       for i in range(len(spanish_sentences_encoded)):\n           spanish_sentences_encoded[i] += (nmt.input_sequence_length -\n           len(spanish_sentences_encoded[i])) * \n           [nmt.spanish_word2idx['<pad>']]\n```", "```py\n          saver = tf.train.Saver()\n          path = tf.train.latest_checkpoint('ckpt')\n          with tf.Session() as sess:\n             saver.restore(sess, path)\n\n           feed = {}\n                for i in range(nmt.input_sequence_length):\n                  feed[encoder_inputs[i].name] =   \n            np.array([spanish_sentences_encoded[j][i] for j in    \n            range(len(spanish_sentences_encoded))], dtype = np.int32)\n\n             feed[decoder_inputs[0].name] =  \n           np.array([nmt.english_word2idx['<go>']] *     \n          len(spanish_sentences_encoded), dtype = np.int32)\n\n            output_sequences = sess.run(outputs_proj, feed_dict = feed)\n\n              for i in range(len(english_sentences_encoded)):\n                   ouput_seq = [output_sequences[j][i] for j in \n                    range(nmt.output_sequence_length)]\n                    words = decode_output(ouput_seq)\n\n                for j in range(len(words)):\n                   if words[j] not in ['<eos>', '<pad>', '<go>']:\n                       print(words[j], end=\" \")\n\n                 print('\\n--------------------------------')\n```", "```py\n         def decode_output(output_sequence):\n             words = []\n            for i in range(nmt.output_sequence_length):\n                smax = nmt.softmax(output_sequence[i])\n                maxId = np.argmax(smax)\n               words.append(nmt.english_idx2word[maxId])\n              return words\n```", "```py\n1--------------------------------\nComo te llamas\nWhat's your name\n\n2--------------------------------\nMi nombre es\nMy name is\n3--------------------------------\nEstoy leyendo un libro\nI am reading a book\n4--------------------------------\nQue tal\nHow are you\n5--------------------------------\nEstoy bien\nI am good\n6--------------------------------\nHablas espanol\nDo you speak Spanish\n7--------------------------------\nQue hora es\nWhat time is it\n8--------------------------------\nHola\nHi\n9--------------------------------\nAdios\nGoodbye\n10--------------------------------\nSi \nYes\n11--------------------------------\nNo\nNo\n```"]