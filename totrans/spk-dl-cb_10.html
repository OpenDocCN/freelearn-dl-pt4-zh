<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Face Recognition Using Deep Convolutional Networks</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>In this chapter, we will cover the following recipes:</span></p>
<ul>
<li>Downloading and loading the MIT-CBCL dataset into the memory</li>
<li>Plotting and visualizing images from the directory</li>
<li>Preprocessing images</li>
<li>Model building, training, and analysis</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p>In today's world, the need to maintain the security<span> of information is becoming increasingly important, as well as increasingly difficult. There are various methods by which this security can be enforced (passwords, fingerprint IDs, PIN numbers, and so on). However, when it comes to ease of use, accuracy, and low intrusiveness, face recognition algorithms have been doing very well. With the availability of high-speed computing and the evolution of deep convolutional networks, it has been made possible to further increase the robustness of these algorithms. They have gotten so advanced that they are now being used as the primary security feature in many electronic devices (for example, iPhoneX) and even banking applications. The goal of this chapter is to develop a robust, pose-invariant face recognition algorithm for use in security systems. For the purposes of this chapter, we will be using the openly available </span><kbd>MIT-CBCL</kbd><span> dataset of face images of 10 different subjects.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Downloading and loading the MIT-CBCL dataset into the memory</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will understand how to download the MIT-CBCL dataset and load it into the memory.</p>
<p><span>With a predicted worth of $15 billion by 2025, the biometrics industry is poised to grow like never before. Some of the examples of physiological characteristics used for biometric authentication include fingerprints, DNA, face, retina or ear features, and voice. While technologies such as DNA authentication and fingerprints are quite advanced, face recognition brings its own advantages to the table.</span></p>
<p>Ease of use and robustness due to recent developments in deep learning models are some of the driving factors behind face recognition algorithms gaining so much popularity.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>The following key points need to be considered for this recipe:</p>
<ul>
<li><span>The <kbd>MIT-CBCL</kbd> dataset is composed of 3,240 images (324 images per subject). In our model, we will make arrangements to augment the data in order to increase model robustness. We will employ techniques such as shifting the subject, rotation, zooming, and shearing of the subject to obtain this augmented data.</span></li>
<li><span>We will use 20% of the dataset to test our model (648 images) by randomly selecting these images from the dataset. Similarly, we randomly select 80% of the images in the dataset and use this as our training dataset (2,592 images).</span></li>
<li><span>The biggest challenge is cropping the images to the exact same size so that they can be fed into the neural network.</span></li>
<li><span>It is a known fact that it is much easier to design a network when all the input images are of the same size. However, since some of the subjects in these images have a side profile or rotated/tilted profiles, we have to adapt our network to take input images of different sizes.</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>The steps are as follows.</p>
<ol>
<li>Download the <kbd>MIT-CBCL</kbd> dataset by visiting the <span class="packt_screen">FACE RECOGNITION HOMEPAGE</span>, which contains a number of databases for face recognition experiments. The link, as well as a screenshot of the homepage, is provided as follows:<br/>
<a href="http://www.face-rec.org/databases/">http://www.face-rec.org/databases/</a>:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1548 image-border" src="assets/dcf4205e-a1b1-4f44-8376-2232d9d0f29a.png" style="width:45.33em;height:40.42em;"/></div>
<ol start="2">
<li>Navigate down to the link that is named <span class="packt_screen">MIT-CBCL Face Recognition Database</span> and click on it, as shown in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1395 image-border" src="assets/1519d67a-ca1b-4fde-9f75-b4ee218f11e5.png" style="width:46.75em;height:44.08em;"/></div>
<ol start="3">
<li>Once you have clicked on it, it will take you to a license page on which you are required to accept the license agreement and proceed to the download page. Once on the download page, click on <kbd>download now</kbd>. This downloads a zip file of about 116 MB. Go ahead and extract the contents into the working directory.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>The functionality is as follows:</p>
<ol>
<li>The license agreement requires the appropriate citation for the use of the database in any projects. This database was developed by the research team from the Massachusetts Institute of Technology.</li>
<li><span>Credit is hereby given to the Massachusetts Institute of Technology and to the center for biological and computational learning for providing the database of facial images. The license also requires the mentioning of the paper titled <em>Component-based Face Recognition with 3D Morphable Models, First IEEE Workshop on Face Processing in Video,</em> Washington, D.C., 2004, B. Weyrauch, J. Huang, B. Heisele, and V. Blanz.<br/></span></li>
<li>The following screenshot describes the license agreement as well as the link to download the dataset:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/9858e3c4-eb48-4590-92c2-bf365cc2a843.png" style="width:67.08em;height:34.83em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Face Recognition Database Homepage</div>
<ol start="4">
<li>Once the dataset is downloaded and extracted, you will see a folder titled <span class="packt_screen">MIT-CBCL-facerec-database</span>.</li>
</ol>
<p> </p>
<ol start="5">
<li>For the purposes of this chapter, we will only be using the images in the <strong><kbd>training-synthetic</kbd></strong> folder, which contains all 3,240 images, as shown in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1396 image-border" src="assets/138da0da-a780-4240-a60e-2ff5f944cbd9.png" style="width:46.83em;height:24.42em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>For this chapter, you will require the following libraries to be imported by Python:</p>
<ul>
<li><kbd>os</kbd></li>
<li><kbd>matplotlib</kbd></li>
<li><kbd>numpy</kbd></li>
<li><kbd>keras</kbd></li>
<li><kbd>TensorFlow<br/></kbd></li>
</ul>
<p>The following section of the chapter will deal with importing the necessary libraries and preprocessing the images before building the neural network model and loading them into it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>For complete information on the packages used in this chapter, visit the following links:</p>
<ul>
<li><a href="https://matplotlib.org/">https://matplotlib.org/</a></li>
<li><a href="https://docs.python.org/2/library/os.html">https://docs.python.org/2/library/os.html</a></li>
<li><a href="https://www.tensorflow.org/get_started/">https://www.tensorflow.org/get_started/</a></li>
<li><a href="https://keras.io/layers/about-keras-layers/">https://keras.io/layers/about-keras-layers/</a></li>
<li><a href="https://docs.scipy.org/doc/numpy-1.9.1/reference/">https://docs.scipy.org/doc/numpy-1.9.1/reference/</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Plotting and visualizing images from the directory</h1>
                </header>
            
            <article>
                
<p>This section will describe how to read and visualize the downloaded images before they are preprocessed and fed into the neural network for training. This is an important step in this chapter because the images need to be visualized to get a better understanding of the image sizes so they can be accurately cropped to omit the background and preserve only the necessary facial features.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Before beginning, complete the initial setup of importing the necessary libraries and functions as well as setting the path of the working directory.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>The steps are as follows:</p>
<ol>
<li class="mce-root"><span>Download the necessary libraries using the following lines of code. The output must result in a line that says</span> <kbd>Using TensorFlow backend</kbd><span>, as shown in the screenshot that follows:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">%matplotlib inline<br/>from os import listdir<br/>from os.path import isfile, join<br/>import matplotlib.pyplot as plt<br/>import matplotlib.image as mpimg<br/>import numpy as np<br/>from keras.models import Sequential<br/>from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D<br/>from keras.optimizers import Adam<br/>from keras.layers.normalization import BatchNormalization<br/>from keras.utils import np_utils<br/>from keras.layers import MaxPooling2D<br/>from keras.preprocessing.image import ImageDataGenerator</pre>
<p class="mce-root" style="padding-left: 60px">The importing of the libraries is as shown:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1397 image-border" src="assets/22238e64-6ac9-41c1-b103-7c46992e285e.png" style="width:37.75em;height:16.33em;"/></div>
<ol start="2">
<li>Print and set the current working directory as shown in the following screenshot. In our case, the desktop was set as the working directory:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1398 image-border" src="assets/06db13f8-4651-4e03-9bba-45a894469c1a.png" style="width:23.67em;height:12.50em;"/></div>
<ol start="3">
<li>Read all the images directly from the folder by using the commands illustrated in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1399 image-border" src="assets/e1bdd972-61a3-4a30-8463-afff8c8f6d86.png" style="width:43.00em;height:7.67em;"/></div>
<ol start="4">
<li class="mce-root">Print a few random images from the dataset using the <kbd>plt.imshow (images[])</kbd> command, as shown in the following screenshots, to get a better idea of the face profiles in the images. This will also give an idea of the size of the image, which will be required at a later stage:</li>
</ol>
<div class="CDPAlignCenter CDPAlign" style="color: black;font-size: 1em"><img class="alignnone size-full wp-image-1400 image-border" src="assets/22170372-8478-4b66-a6ba-646c59aed38b.png" style="width:37.67em;height:26.67em;"/></div>
<div class="CDPAlignLeft CDPAlign" style="color: black;font-size: 1em"/>
<ol start="5">
<li>Shown here are the images of different test subjects from the first image.</li>
</ol>
<div class="CDPAlignCenter CDPAlign" style="color: black;font-size: 1em"><img class="alignnone size-full wp-image-1401 image-border" src="assets/b7fd9236-5600-4f17-80c0-f1882d6d528f.png" style="width:26.75em;height:22.33em;"/></div>
<div class="CDPAlignCenter CDPAlign" style="color: black;font-size: 1em"><img class="alignnone size-full wp-image-1402 image-border" src="assets/4165425e-c59f-4eb3-bb12-457e8f5026ce.png" style="width:26.08em;height:20.75em;"/><br/></div>
<div class="CDPAlignCenter CDPAlign" style="color: black;font-size: 1em"><img class="alignnone size-full wp-image-1403 image-border" src="assets/895e8032-8b1c-4c90-944e-f3b210adc0af.png" style="width:30.42em;height:21.75em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>The functionality is as follows:</p>
<ol>
<li>The <kbd>mypath</kbd> variable sets the path to read all the files from. The <kbd>training-synthetic</kbd> folder is specified in this step, as only the files in this folder are going to be used for this chapter.</li>
<li>The <kbd>onlyfiles</kbd> variable is used in order to count all the files under the folder whose path is provided in the previous step by looping through all the files contained in the folder. This will be required in the next step for reading and storing the images.</li>
<li>The <kbd>images</kbd> variable is used to create an empty array of size 3,240 in order to store the images, which are all 200 x 200-pixels.</li>
<li>Next, by looping through all the files using the <kbd>onlyfiles</kbd> variable as an argument in the for loop, each image contained in the folder is read and stored into the previously defined <kbd>images</kbd> array using the <kbd>matplotlib.image</kbd> function.</li>
<li>Finally, on printing randomly chosen images by specifying different indices of the images you will notice that each image is a 200 x 200-pixel array and each subject may either be facing forward or rotated between zero and fifteen degrees on either side.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>The following points are of note:</p>
<ul>
<li>An interesting feature of this database is that the fourth digit of each filename describes which subject is in the respective image.</li>
<li>The names of the images are unique in the sense that the four<span>th</span> <span>digit represents the individual in the respective image. Two examples of image names are </span><span><kbd>0001_-4_0_0_60_45_1.pgm</kbd> and <kbd>0006_-24_0_0_0_75_15_1.pgm</kbd>.</span> <span>One can easily understand that the four</span><span>th</span> <span>digits represent the secon</span><span>d</span> <span>and seventh</span><span> </span><span>individual respectively.</span></li>
<li>We will need to store this information for later use while making predictions. This will help the neural network during training by knowing what subject's facial features it is learning.</li>
<li>The filenames of each image can be read into an array, and each of the ten subjects can be segregated by using the following lines of code:</li>
</ul>
<pre style="padding-left: 60px">y =np.empty([3240,1],dtype=int)<br/>for x in range(0, len(onlyfiles)):<br/>    if onlyfiles[x][3]=='0': y[x]=0<br/>    elif onlyfiles[x][3]=='1': y[x]=1<br/>    elif onlyfiles[x][3]=='2': y[x]=2<br/>    elif onlyfiles[x][3]=='3': y[x]=3<br/>    elif onlyfiles[x][3]=='4': y[x]=4<br/>    elif onlyfiles[x][3]=='5': y[x]=5<br/>    elif onlyfiles[x][3]=='6': y[x]=6<br/>    elif onlyfiles[x][3]=='7': y[x]=7<br/>    elif onlyfiles[x][3]=='8': y[x]=8<br/>    elif onlyfiles[x][3]=='9': y[x]=9</pre>
<ul>
<li>The preceding code will initialize an empty one-dimensional <kbd>numpy</kbd> array of size 3,240 (the number of images in the <kbd>training-synthetic</kbd> folder) and store the relevant subjects in different arrays by looping through the whole set of files.</li>
<li>The <kbd>if</kbd> statements are basically checking what the fourth digit is under each filename and storing that digit in the initialized <kbd>numpy</kbd> array.</li>
<li>The output in the iPython notebook for the same is shown in the following screenshot:</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1404 image-border" src="assets/4a2396a6-3cea-4144-a3f4-c3089ae3beb7.png" style="width:30.42em;height:16.00em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>The following blog describes a method of cropping images in Python and can be used for image preprocessing which will be required in the following section:<br/>
<a href="https://www.blog.pythonlibrary.org/2017/10/03/how-to-crop-a-photo-with-python/"/></p>
<ul>
<li><a href="https://www.blog.pythonlibrary.org/2017/10/03/how-to-crop-a-photo-with-python/">https://www.blog.pythonlibrary.org/2017/10/03/how-to-crop-a-photo-with-python/</a></li>
</ul>
<p>More information about the Adam Optimizer and its use cases can be found by visiting the following links:</p>
<ul>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer">https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer</a></li>
<li><a href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</a></li>
<li><a href="https://www.coursera.org/lecture/deep-neural-network/adam-optimization-algorithm-w9VCZ">https://www.coursera.org/lecture/deep-neural-network/adam-optimization-algorithm-w9VCZ</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preprocessing images</h1>
                </header>
            
            <article>
                
<p>In the previous section, you may have noticed how all the images are not a front view of the face profiles, and that there are also slightly rotated side profiles. You may also have noticed some unnecessary background areas in each image that needs to be omitted. This section will describe how to preprocess and handle the images so that they are ready to be fed into the network for training.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Consider the following:</p>
<ul>
<li><span>A lot of algorithms are devised to crop the significant part of an image; for example, SIFT, LBP, Haar-cascade filter, and so on.</span></li>
<li><span>We will, however, tackle this problem with a very simplistic naïve code to crop the facial portion from the image. This is one of the novelties of this algorithm.</span></li>
<li><span>We have found that the pixel intensity of the unnecessary background part is 28.</span></li>
<li><span>Remember that each image is a three-channel matrix of 200 x 200-pixels. This means that every image contains three matrices or Tensors of red, green, and blue pixels with an intensity ranging from 0 to 255.</span></li>
<li><span>Therefore, we will discard any row or column of the images that contain only 28s as the pixel intensities.</span></li>
<li><span>We will also make sure that all the images have the same pixel size after the cropping action to achieve the highest parallelizability of the convolutional neural network.</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>The steps are as follows:</p>
<ol>
<li>Define the <kbd>crop()</kbd> function to crop images to obtain only the significant part, as shown in the following lines of code:</li>
</ol>
<pre style="padding-left: 60px"> #function for cropping images to obtain only the significant part<br/> def crop(img):<br/>      a=28*np.ones(len(img)) <br/>      b=np.where((img== a).all(axis=1)) <br/>      img=np.delete(img,(b),0) <br/>      plt.imshow(img)<br/>      img=img.transpose()<br/>      d=28*np.ones(len(img[0]))<br/>      e=np.where((img== d).all(axis=1))<br/>      img=np.delete(img,e,0) <br/>      img=img.transpose()<br/>      print(img.shape) <br/>      super_threshold_indices = img &lt; 29 <br/>      img[super_threshold_indices] = 0<br/>      plt.imshow (img)<br/>      return img[0:150, 0:128]</pre>
<ol start="2">
<li>Use the following lines of code to loop through every image in the folder and crop it using the preceding defined function:</li>
</ol>
<pre style="padding-left: 60px">#cropping all the images<br/> image = np.empty([3240,150,128],dtype=int)<br/> for n in range(0, len(images)):<br/>     image[n]=crop(images[n])</pre>
<ol start="3">
<li>Next, randomly choose an image and print it to check that it has been cropped from a 200 x 200 sized image to a different size. We have chosen image 23 in our case. This can be done using the following lines of code:</li>
</ol>
<pre style="padding-left: 60px"> print (image[22])<br/> print (image[22].shape)</pre>
<ol start="4">
<li class="mce-root"><span>Next, split the data into a test and train set using</span> <kbd>80%</kbd> <span>of the images in the folder as the training set and the remaining</span> <kbd>20% </kbd><span>as the test set. This can be done with the following commands:</span></li>
</ol>
<pre style="padding-left: 60px"># Split data into 80/20 split for testing and training<br/>test_ind=np.random.choice(range(3240), 648, replace=False) train_ind=np.delete(range(0,len(onlyfiles)),test_ind)</pre>
<ol start="5">
<li>Once the data has finished splitting, segregate the training and test images using the following commands:</li>
</ol>
<pre style="padding-left: 60px"> # slicing the training and test images <br/> y1_train=y[train_ind]<br/> x_test=image[test_ind]<br/> y1_test=y[test_ind]</pre>
<ol start="6">
<li>Next, reshape all the cropped images into sizes of 128 x 150, since this is the size that is to be fed into the neural network. This can be done using the following commands:</li>
</ol>
<pre style="padding-left: 60px">#reshaping the input images<br/> x_train = x_train.reshape(x_train.shape[0], 128, 150, 1)<br/> x_test = x_test.reshape(x_test.shape[0], 128, 150, 1)</pre>
<ol start="7">
<li>Once the data is done reshaping, convert it into <kbd>float32</kbd> type, which will make it easier to handle in the next step when it is normalized. Converting from int to float32 can be done using the following commands:</li>
</ol>
<pre style="padding-left: 60px"> #converting data to float32<br/> x_train = x_train.astype('float32')<br/> x_test = x_test.astype('float32')</pre>
<ol start="8">
<li>After reshaping and converting the data into the float32 type, it has to be normalized in order to adjust all the values to a similar scale. This is an important step in preventing data redundancy. Perform normalization using the following commands:</li>
</ol>
<pre style="padding-left: 60px"> #normalizing data<br/> x_train/=255<br/> x_test/=255<br/> #10 digits represent the 10 classes<br/> number_of_persons = 10</pre>
<ol start="9">
<li>The final step is to convert the reshaped, normalized images into vectors, as this is the only form of input the neural network understands. Convert the images into vectors using the following commands:</li>
</ol>
<pre style="padding-left: 60px"> #convert data to vectors<br/> y_train = np_utils.to_categorical(y1_train, number_of_persons)<br/> y_test = np_utils.to_categorical(y1_test, number_of_persons)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>The functionality is as follows:</p>
<ol>
<li>The <kbd>crop()</kbd> function executes the following tasks:
<ol>
<li>Multiplies all pixels with an intensity of 28 with a numpy array of 1s and stores in variable <kbd>a</kbd>.</li>
<li>Checks for all instances where an entire column consists of only pixel intensities of 28 and stores in variable <kbd>b</kbd>.</li>
<li>Deletes all columns (or <em>Y</em> axes) where pixel intensities are 28 for the entire column.</li>
<li>Plots the resulting image. </li>
</ol>
</li>
</ol>
<ol>
<li style="list-style-type: none">
<ol start="5">
<li>Transposes the image in order to perform the preceding set of operations on all the rows (or <em>X</em> axes) in a similar manner.</li>
</ol>
</li>
<li style="list-style-type: none">
<ol start="6">
<li>Multiplies all pixels with an intensity of 28 with a <kbd>numpy</kbd> array of 1s and stores in variable <kbd>d</kbd>.</li>
<li>Checks for all instances where an entire column consists of only pixel intensities of 28 and stores in variable <kbd>e</kbd>.</li>
<li> Deletes all columns (from the transposed image) where pixel intensities are 28 for the entire column.</li>
<li>Transposes the image to get back the original image.</li>
<li>Prints the shape of the image.</li>
<li>Wherever a pixel intensity of less than 29 is found, replaces those pixel intensities with zeros, which will result in the cropping of all those pixels by making them white.</li>
<li>Plots the resulting image.</li>
<li>Reshapes the resulting image to a size of 150 x 128 pixels.</li>
</ol>
</li>
</ol>
<p style="padding-left: 60px">The output for the <kbd>crop()</kbd> function, as seen on the Jupyter notebook during execution, is shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1405 image-border" src="assets/b7bfb81b-bb1a-4f8d-ace1-6a3350b8b618.png" style="width:42.50em;height:13.75em;"/></div>
<ol start="2">
<li>Next, the defined <kbd>crop()</kbd> function is applied to all the files contained in the <kbd>training-synthetic</kbd> folder by looping through every file. This will result in an output as shown in the following screenshots:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1406 image-border" src="assets/659f69f2-71dc-47c8-a41d-8abaff1fbbce.png" style="width:29.92em;height:25.50em;"/></div>
<p style="padding-left: 60px">The output continues as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1408 image-border" src="assets/33234eb4-b5c5-4825-a908-9b1a5f58455a.png" style="width:12.67em;height:18.92em;"/></div>
<div class="packt_infobox">Notice that only the relevant facial features are preserved and the resulting shapes of all the cropped images are less than 200 x 200, which was the initial size. </div>
<ol start="3">
<li>On printing the image and shape of any random image, you will notice that every image is now resized to a 150 x 128-pixel array, and you will see the following output:
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1409 image-border" src="assets/82830a1d-393e-4620-9e03-fefd30f652fd.png" style="width:26.17em;height:17.08em;"/></div>
</li>
<li>Splitting the images into test and train sets as well as segregating them into variables named <kbd>x_train</kbd>, <kbd>y1_train</kbd>, <kbd>x_test</kbd>, and <kbd>y1_test</kbd> will result in the output shown in the following screenshot:
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1410 image-border" src="assets/b6480e9c-aed7-44ed-aebf-d18dbfcfb3f2.png" style="width:75.25em;height:7.92em;"/></div>
</li>
<li>Segregating the data is done as follows:</li>
</ol>
<div class="CDPAlignCenter CDPAlign" style="color: black;font-size: 1em"><img class="alignnone size-full wp-image-1411 image-border" src="assets/a8dbcce6-9e1e-495d-b135-e87c64fe5c58.png" style="width:42.08em;height:8.92em;"/></div>
<ol start="6">
<li>Reshaping the training and test images and converting the data type to float32 will result in the output seen in the following screenshot:
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1412 image-border" src="assets/3ee9a218-e25c-4f53-bcfc-631ea9dc78ea.png" style="width:41.75em;height:9.92em;"/></div>
</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>Consider the following:</p>
<ul>
<li>Once the images are done preprocessing they still need to be normalized and converted into vectors (in this case tensors) before being fed into the network.</li>
<li>Normalization,<span> in the simplest case, means adjusting values measured on different scales to a notionally common scale, often prior to averaging. It is always a good idea to normalize data in order to prevent gradients from exploding or vanishing as seen in the vanishing and exploding gradient problems during gradient descent. Normalization also ensures there is no data redundancy.</span></li>
<li>Normalization of the data is done by dividing each pixel in each image by <kbd>255</kbd> since the pixel values range between 0 and <kbd>255</kbd>. This will result in the output shown in the following screenshot:
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1413 image-border" src="assets/cfeaed2d-a914-4816-aecf-05d58a71e66b.png" style="width:31.25em;height:7.58em;"/><br/></div>
</li>
<li>
<p>Next, the images are converted to input vectors with ten different classes using the <kbd>to_categorical()</kbd> function from <kbd>numpy_utils</kbd>, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1414 image-border" src="assets/4dc71bb9-c7cb-4cba-bae7-f2c5d78246e9.png" style="width:44.42em;height:4.92em;"/></div>
</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>Additional resources are as follows:</p>
<ul>
<li>For more information on data normalization, check the following link:<br/>
<a href="https://www.quora.com/What-is-normalization-in-machine-learning">https://www.quora.com/What-is-normalization-in-machine-learning</a></li>
<li>For information on overfitting and why data is split into test and training sets, visit the following link:<br/>
<a href="https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6">https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6</a></li>
<li>For more information on encoding variables and their importance, visit the following link:<br/>
<a href="http://pbpython.com/categorical-encoding.html">http://pbpython.com/categorical-encoding.html</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model building, training, and analysis</h1>
                </header>
            
            <article>
                
<div class="page">
<div class="layoutArea">
<div class="column">
<p><span>We will use a standard sequential model from the <kbd>keras</kbd> library to build the CNN. The network will consist of three convolutional layers, two maxpooling layers, and four fully connected layers. The input layer and the subsequent hidden layers have 16 neurons, while the maxpooling layers contain a pool size of (2,2). The four fully connected layers consist of two dense layers and one flattened layer and one dropout layer. Dropout 0.25 was used to reduce the overfitting problem. Another novelty</span> <span>of this algorithm is the use of data</span> <span>augmentation to fight the overfitting phenomenon. Data augmentation is carried by rotating, shifting, shearing, and zooming the images to different extents to fit the model.</span></p>
<p><span>The <kbd>relu</kbd> function is used as the activation function in both the input and hidden layers, while the <kbd>softmax</kbd> classifier is used in the output layer to classify the test images based on the predicted output.</span></p>
</div>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="mce-root CDPAlignLeft CDPAlign">The network which will be constructed can be visualized as shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1422 image-border" src="assets/93d4ebf3-1c06-4776-b868-dc7838a9294f.png" style="width:42.25em;height:26.42em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>The steps are as follows:</p>
<ol>
<li>Define the model using the <kbd>Sequential()</kbd> function in the Keras framework using the following commands:</li>
</ol>
<pre style="padding-left: 60px">model = Sequential()<br/>model.add(Conv2D(16, (3, 3), input_shape=(128,150,1)))  <br/>model.add(Activation('relu')) <br/>model.add(Conv2D(16, (3, 3))) <br/>model.add(Activation('relu'))<br/>model.add(MaxPooling2D(pool_size=(2,2))) <br/>model.add(Conv2D(16,(3, 3))) <br/>model.add(Activation('relu'))<br/>model.add(MaxPooling2D(pool_size=(2,2))) <br/>model.add(Flatten()) <br/><br/>model.add(Dense(512))<br/>model.add(Activation('relu'))<br/>model.add(Dropout(0.25)) <br/>model.add(Dense(10))<br/><br/>model.add(Activation('softmax')) </pre>
<ol start="2">
<li>Print the summary of the model to get a better understanding of how the model is built and to ensure that it is built as per the preceding specifications. This can be done by using the <kbd>model.summary()</kbd> command.</li>
<li>Next, compile the model using the following command:</li>
</ol>
<pre style="padding-left: 60px">model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=        ['accuracy'])</pre>
<ol start="4">
<li>In order to prevent overfitting and improve model accuracy further, implement some form of data augmentation. In this step, the images will be sheared, shifted on a horizontal as well as the vertical axis, zoomed in, and rotated. The ability of the model to learn and identify these anomalies will dictate how robust the model is. Augment the data using the following commands:</li>
</ol>
<pre style="padding-left: 60px"># data augmentation to minimize overfitting<br/>gen = ImageDataGenerator(rotation_range=8, <br/>        width_shift_range=0.08, shear_range=0.3,<br/>        height_shift_range=0.08,zoom_range=0.08)<br/>test_gen = ImageDataGenerator()<br/>train_generator = gen.flow(x_train, y_train, batch_size=16) <br/>test_generator = test_gen.flow(x_test, y_test, batch_size=16)</pre>
<ol start="5">
<li>Finally, fit and evaluate the model after data augmentation using the following commands:</li>
</ol>
<pre style="padding-left: 60px">model.fit_generator(train_generator, epochs=5, validation_data=test_generator)<br/> <br/>scores = model.evaluate(x_test, y_test, verbose=0)<br/>print("Recognition Error: %.2f%%" % (100-scores[1]*100))</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>The functionality is as follows:</p>
<ol>
<li class="mce-root">By using the sequential function, a nine-layer convolutional neural network is defined with each layer performing the following functions:
<ol>
<li>The first layer is a convolutional layer with 16 neurons and performs convolution on the input tensor/matrix. The size of the feature map is defined to be a 3 x 3 matrix. The input shape needs to be specified for the first layer since the neural network needs to know what type of input to expect. Since all the images have been cropped to a size of 128 x 150 pixels, this will be the input shape defined for the first layer of the network as well. The activation function used in this layer is a <strong>rectified linear unit</strong> (<strong>relu</strong>).</li>
<li>The second layer of the network (first hidden layer) is another convolution layer with 16 neurons as well. Again, a <kbd>relu</kbd> will be used as the activation function for this layer.</li>
<li>The third layer of the network (second hidden layer) is a max pooling layer with a pool size of 2 x 2. The function of this layer is to extract all the valid features learned by performing convolution in the first two layers and reducing the size of the matrix with all the learned features. Convolution is nothing but a matrix multiplication between the feature map and the input matrix (in our case, an image). The resulting values, which form the convolution process, are stored by the network in a matrix. The maximum values from these stored values will define a certain feature in the input image. These maximum values are what will be preserved by the max pooling layer, which will omit the non-relevant features.</li>
<li>The fourth layer of the network (third hidden layer) is another convolutional layer with a feature map of 3 x 3 again. The activation function used in this layer will again be a <kbd>relu</kbd> function.</li>
<li>The fifth layer of the network (fourth hidden layer) is a max pooling layer with a pool size of 2 x 2.</li>
<li>The sixth layer of the network (fifth hidden layer) is a flatten layer that will convert the matrix containing all the learned features (stored in the form of numbers) into a single row instead of a multi-dimensional matrix.</li>
</ol>
</li>
</ol>
<ol>
<li style="list-style-type: none">
<ol start="7">
<li>The seventh layer in the network (sixth hidden layer) is a dense layer with 512 neurons and a <kbd>relu</kbd> activation. Each neuron will basically process a certain weight and bias, which is nothing but a representation of all the learned features from a particular image. This is done in order to easily classify the image by using a <kbd>softmax</kbd> classifier on the dense layer.</li>
<li>The eighth layer in the network (seventh hidden layer) is a dropout layer with a dropout probability of 0.25 or 25%. This layer will randomly <kbd>dropout</kbd> 25% of the neurons during the training process and help prevent overfitting by encouraging the network to learn a given feature using many alternative paths.</li>
<li>The final layer in the network is a dense layer with just 10 neurons and the <kbd>softmax</kbd> classifier. This is the eighth hidden layer and will also serve as the output layer of the network.</li>
</ol>
</li>
<li>The output after defining the model must look like the one in the following screenshot:</li>
</ol>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1415 image-border" src="assets/84106f1b-84f4-4055-8308-7482d1c56abc.png" style="text-align: center;color: black;font-size: 1em;width:43.33em;height:21.25em;"/></div>
<ol start="3">
<li>On printing the <kbd>model.summary()</kbd> function, you must see an output like the one in the following screenshot:
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1416 image-border" src="assets/b68373e3-00ab-4a55-83cd-b797ac325262.png" style="width:29.58em;height:30.83em;"/></div>
</li>
<li>The model is compiled using categorical crossentropy, which is a function to measure and compute the loss from the network while transferring information from one layer to the subsequent layers. The model will make use of the <kbd>Adam()</kbd> optimizer function from the Keras framework, which will basically dictate how the network optimizes the weights and biases while learning the features. The output of the <kbd>model.compile()</kbd> function must look like the following screenshot:</li>
</ol>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1417 image-border" src="assets/f159955e-5e31-47ef-89bc-ee6d45165b31.png" style="text-align: center;color: black;font-size: 1em;width:150.00em;height:9.75em;"/></div>
<ol start="5">
<li>Since the neural network is quite dense and the number of total images is only 3,240, we devise a method to prevent overfitting. This is done by generating more images from the training set by performing data augmentation. In this step, the images are generated through the <kbd>ImageDataGenerator()</kbd> function. This function takes the training and test sets and augments images by:
<ul>
<li>Rotating them</li>
<li>Shearing them</li>
<li>Shifting the width, which is basically widening the images</li>
<li>Shifting the images on a horizontal axis</li>
<li>Shifting the images on a vertical axis</li>
</ul>
</li>
</ol>
<p style="padding-left: 90px">The output of the preceding function must look like the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign" style="padding-left: 30px"><img class="alignnone size-full wp-image-1418 image-border" src="assets/dbfa63f1-cb81-4f83-a17b-4e7356ad096e.png" style="width:45.50em;height:7.00em;"/></div>
<ol start="6">
<li>Finally, the model is fitted to the data and evaluated after training over 5 epochs. The output we obtained is shown in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign" style="padding-left: 30px"><img class="alignnone size-full wp-image-1419 image-border" src="assets/92f8e4d9-e9f4-4c41-bf72-f6ad2cb50118.png" style="width:51.75em;height:20.25em;"/></div>
<ol start="7">
<li>As you can see, we obtained an accuracy of 98.46%, which resulted in an error rate of 1.54%. This is pretty good, but convolutional networks have advanced so much that we can improve this error rate by tuning a few hyperparameters or using a deeper network.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root CDPAlignLeft CDPAlign">Using a deeper CNN with 12 layers (one extra convolution and one extra max pooling layer) resulted in an improvement of accuracy to 99.07%, as shown in the following screenshot:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1420 image-border" src="assets/44198743-7e72-4c9d-a09f-48339f26cebf.png" style="width:52.50em;height:19.00em;"/></div>
<p>Using data normalization after every two layers during model building, we were further able to improve the accuracy to 99.85%, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1421 image-border" src="assets/72303ea6-b9b5-4037-978c-64b9e1812be1.png" style="width:92.17em;height:33.83em;"/></div>
<p>You may obtain different results, but feel free to run the training step a few times. The following are some of the steps you can take to experiment with the network in the future to understand it better:</p>
<ul>
<li>Try to tune hyperparameters better and implement a higher dropout percentage and see how the network responds.</li>
<li>The accuracy greatly reduced when we tried using different activation functions or a smaller (less dense) network.</li>
<li>Also, change the size of the feature maps and max pooling layer and see how this influences training time and model accuracy.</li>
<li>Try including more neurons in a less dense CNN and tune it to improve accuracy. This may also result in a faster network that trains in less time.</li>
<li>Use more training data. Explore other online repositories and find larger databases to train the network. Convolutional neural networks usually perform better when the size of the training data is increased.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>The following published papers are good resources to obtain a better understanding of convolutional neural networks. They may be used as further reading in order to gain more understanding of various applications of convolutional neural networks:</p>
<ul>
<li><a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks">http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks</a></li>
<li><a href="https://arxiv.org/abs/1408.5882">https://arxiv.org/abs/1408.5882</a></li>
<li><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Karpathy_Large-scale_Video_Classification_2014_CVPR_paper.pdf">https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Karpathy_Large-scale_Video_Classification_2014_CVPR_paper.pdf</a></li>
<li><a href="http://www.cs.cmu.edu/~bhiksha/courses/deeplearning/Fall.2016/pdfs/Simard.pdf">http://www.cs.cmu.edu/~bhiksha/courses/deeplearning/Fall.2016/pdfs/Simard.pdf</a></li>
<li><a href="https://dl.acm.org/citation.cfm?id=2807412">https://dl.acm.org/citation.cfm?id=2807412</a></li>
<li><a href="https://ieeexplore.ieee.org/abstract/document/6165309/">https://ieeexplore.ieee.org/abstract/document/6165309/</a></li>
<li><a href="http://openaccess.thecvf.com/content_cvpr_2014/papers/Oquab_Learning_and_Transferring_2014_CVPR_paper.pdf">http://openaccess.thecvf.com/content_cvpr_2014/papers/Oquab_Learning_and_Transferring_2014_CVPR_paper.pdf</a></li>
<li><a href="http://www.aaai.org/ocs/index.php/IJCAI/IJCAI11/paper/download/3098/3425">http://www.aaai.org/ocs/index.php/IJCAI/IJCAI11/paper/download/3098/3425</a></li>
<li><a href="https://ieeexplore.ieee.org/abstract/document/6288864/">https://ieeexplore.ieee.org/abstract/document/6288864/</a></li>
</ul>


            </article>

            
        </section>
    </body></html>