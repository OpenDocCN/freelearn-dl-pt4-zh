["```py\nclass Q_Learner(object):\n    def __init__(self, env):\n        self.obs_shape = env.observation_space.shape\n        self.obs_high = env.observation_space.high\n        self.obs_low = env.observation_space.low\n        self.obs_bins = NUM_DISCRETE_BINS  # Number of bins to Discretize each observation dim\n        self.bin_width = (self.obs_high - self.obs_low) / self.obs_bins\n        self.action_shape = env.action_space.n\n        # Create a multi-dimensional array (aka. Table) to represent the\n        # Q-values\n        self.Q = np.zeros((self.obs_bins + 1, self.obs_bins + 1,\n self.action_shape))  # (51 x 51 x 3)\n        self.alpha = ALPHA  # Learning rate\n        self.gamma = GAMMA  # Discount factor\n        self.epsilon = 1.0\n```", "```py\nimport torch\n\nclass SLP(torch.nn.Module):\n    \"\"\"\n    A Single Layer Perceptron (SLP) class to approximate functions\n    \"\"\"\n    def __init__(self, input_shape, output_shape, device=torch.device(\"cpu\")):\n        \"\"\"\n        :param input_shape: Shape/dimension of the input\n        :param output_shape: Shape/dimension of the output\n        :param device: The device (cpu or cuda) that the SLP should use to store the inputs for the forward pass\n        \"\"\"\n        super(SLP, self).__init__()\n        self.device = device\n        self.input_shape = input_shape[0]\n        self.hidden_shape = 40\n        self.linear1 = torch.nn.Linear(self.input_shape, self.hidden_shape)\n        self.out = torch.nn.Linear(self.hidden_shape, output_shape)\n\n    def forward(self, x):\n        x = torch.from_numpy(x).float().to(self.device)\n        x = torch.nn.functional.relu(self.linear1(x))\n        x = self.out(x)\n        return x\n```", "```py\nimport torch\nfrom function_approximator.perceptron import SLP EPSILON_MIN = 0.005\nmax_num_steps = MAX_NUM_EPISODES * STEPS_PER_EPISODE\nEPSILON_DECAY = 500 * EPSILON_MIN / max_num_steps\nALPHA = 0.05  # Learning rate\nGAMMA = 0.98  # Discount factor\nNUM_DISCRETE_BINS = 30  # Number of bins to Discretize each observation dim\n\nclass Shallow_Q_Learner(object):\n    def __init__(self, env):\n        self.obs_shape = env.observation_space.shape\n        self.obs_high = env.observation_space.high\n        self.obs_low = env.observation_space.low\n        self.obs_bins = NUM_DISCRETE_BINS  # Number of bins to Discretize each observation dim\n        self.bin_width = (self.obs_high - self.obs_low) / self.obs_bins\n        self.action_shape = env.action_space.n\n        # Create a multi-dimensional array (aka. Table) to represent the\n        # Q-values\n        self.Q = SLP(self.obs_shape, self.action_shape)\n        self.Q_optimizer = torch.optim.Adam(self.Q.parameters(), lr=1e-5)\n        self.alpha = ALPHA  # Learning rate\n        self.gamma = GAMMA  # Discount factor\n        self.epsilon = 1.0\n\n    def discretize(self, obs):\n        return tuple(((obs - self.obs_low) / self.bin_width).astype(int))\n\n    def get_action(self, obs):\n        discretized_obs = self.discretize(obs)\n        # Epsilon-Greedy action selection\n        if self.epsilon > EPSILON_MIN:\n            self.epsilon -= EPSILON_DECAY\n        if np.random.random() > self.epsilon:\n            return np.argmax(self.Q(discretized_obs).data.to(torch.device('cpu')).numpy())\n        else:  # Choose a random action\n            return np.random.choice([a for a in range(self.action_shape)])\n\n    def learn(self, obs, action, reward, next_obs):\n        #discretized_obs = self.discretize(obs)\n        #discretized_next_obs = self.discretize(next_obs)\n        td_target = reward + self.gamma * torch.max(self.Q(next_obs))\n        td_error = torch.nn.functional.mse_loss(self.Q(obs)[action], td_target)\n        #self.Q[discretized_obs][action] += self.alpha * td_error\n        self.Q_optimizer.zero_grad()\n        td_error.backward()\n        self.Q_optimizer.step() \n```", "```py\n#!/usr/bin/env python import gym import random import torch from torch.autograd import Variable import numpy as np from utils.decay_schedule import LinearDecaySchedule from function_approximator.perceptron import SLP\n\nenv = gym.make(\"CartPole-v0\")\nMAX_NUM_EPISODES = 100000\nMAX_STEPS_PER_EPISODE = 300\n\nclass Shallow_Q_Learner(object):\n    def __init__(self, state_shape, action_shape, learning_rate=0.005,\n                 gamma=0.98):\n        self.state_shape = state_shape\n        self.action_shape = action_shape\n        self.gamma = gamma # Agent's discount factor\n        self.learning_rate = learning_rate # Agent's Q-learning rate\n        # self.Q is the Action-Value function. This agent represents Q using a\n        # Neural Network.\n        self.Q = SLP(state_shape, action_shape)\n        self.Q_optimizer = torch.optim.Adam(self.Q.parameters(), lr=1e-3)\n        # self.policy is the policy followed by the agent. This agents follows\n        # an epsilon-greedy policy w.r.t it's Q estimate.\n        self.policy = self.epsilon_greedy_Q\n        self.epsilon_max = 1.0\n        self.epsilon_min = 0.05\n        self.epsilon_decay = LinearDecaySchedule(initial_value=self.epsilon_max,\n                                    final_value=self.epsilon_min,\n                                    max_steps= 0.5 * MAX_NUM_EPISODES * MAX_STEPS_PER_EPISODE)\n        self.step_num = 0\n\n    def get_action(self, observation):\n        return self.policy(observation)\n\n    def epsilon_greedy_Q(self, observation):\n        # Decay Epsilion/exploratin as per schedule\n        if random.random() < self.epsilon_decay(self.step_num):\n            action = random.choice([i for i in range(self.action_shape)])\n        else:\n            action = np.argmax(self.Q(observation).data.numpy())\n\n        return action\n\n    def learn(self, s, a, r, s_next):\n        td_target = r + self.gamma * torch.max(self.Q(s_next))\n        td_error = torch.nn.functional.mse_loss(self.Q(s)[a], td_target)\n        # Update Q estimate\n        #self.Q(s)[a] = self.Q(s)[a] + self.learning_rate * td_error\n        self.Q_optimizer.zero_grad()\n        td_error.backward()\n        self.Q_optimizer.step()\n\nif __name__ == \"__main__\":\n    observation_shape = env.observation_space.shape\n    action_shape = env.action_space.n\n    agent = Shallow_Q_Learner(observation_shape, action_shape)\n    first_episode = True\n    episode_rewards = list()\n    for episode in range(MAX_NUM_EPISODES):\n        obs = env.reset()\n        cum_reward = 0.0 # Cumulative reward\n        for step in range(MAX_STEPS_PER_EPISODE):\n            # env.render()\n            action = agent.get_action(obs)\n            next_obs, reward, done, info = env.step(action)\n            agent.learn(obs, action, reward, next_obs)\n\n            obs = next_obs\n            cum_reward += reward\n\n            if done is True:\n                if first_episode: # Initialize max_reward at the end of first episode\n                    max_reward = cum_reward\n                    first_episode = False\n                episode_rewards.append(cum_reward)\n                if cum_reward > max_reward:\n                    max_reward = cum_reward\n                print(\"\\nEpisode#{} ended in {} steps. reward ={} ; mean_reward={} best_reward={}\".\n                      format(episode, step+1, cum_reward, np.mean(episode_rewards), max_reward))\n                break\n    env.close()\n```", "```py\n(rl_gym_book) praveen@ubuntu:~/rl_gym_book/ch6$ python shallow_Q_Learner.py\n```", "```py\nfrom collections import namedtuple\nExperience = namedtuple(\"Experience\", ['obs', 'action', 'reward', 'next_obs',\n                                       'done'])\n```", "```py\nclass ExperienceMemory(object):\n    \"\"\"\n    A cyclic/ring buffer based Experience Memory implementation\n    \"\"\"\n    def __init__(self, capacity=int(1e6)):\n        \"\"\"\n        :param capacity: Total capacity (Max number of Experiences)\n        :return:\n        \"\"\"\n        self.capacity = capacity\n        self.mem_idx = 0 # Index of the current experience\n        self.memory = []\n```", "```py\ndef store(self, experience):\n        \"\"\"\n        :param experience: The Experience object to be stored into the memory\n        :return:\n        \"\"\"\n        self.memory.insert(self.mem_idx % self.capacity, experience)\n        self.mem_idx += 1\n```", "```py\nimport random\n    def sample(self, batch_size):\n        \"\"\"\n\n        :param batch_size: Sample batch_size\n        :return: A list of batch_size number of Experiences sampled at random from mem\n        \"\"\"\n        assert batch_size <= len(self.memory), \"Sample batch_size is more than available exp in mem\"\n        return random.sample(self.memory, batch_size)\n\n```", "```py\ndef get_size(self):\n        \"\"\"\n\n        :return: Number of Experiences stored in the memory\n        \"\"\"\n        return len(self.memory)\n```", "```py\nreplay_experience method that shows how we sample from the experience memory and call a soon-to-be-implemented method that lets the agent learn from the sampled batch of experiences:\n```", "```py\ndef replay_experience(self, batch_size=REPLAY_BATCH_SIZE):\n        \"\"\"\n        Replays a mini-batch of experience sampled from the Experience Memory\n        :param batch_size: mini-batch size to sample from the Experience Memory\n        :return: None\n        \"\"\"\n        experience_batch = self.memory.sample(batch_size)\n        self.learn_from_batch_experience(experience_batch)\n```", "```py\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndef learn_from_batch_experience(self, experiences):\n        \"\"\"\n        Updated the DQN based on the learning from a mini-batch of experience.\n        :param experiences: A mini-batch of experience\n        :return: None\n        \"\"\"\n        batch_xp = Experience(*zip(*experiences))\n        obs_batch = np.array(batch_xp.obs)\n        action_batch = np.array(batch_xp.action)\n        reward_batch = np.array(batch_xp.reward)\n        next_obs_batch = np.array(batch_xp.next_obs)\n        done_batch = np.array(batch_xp.done)\n\n        td_target = reward_batch + ~done_batch * \\\n                np.tile(self.gamma, len(next_obs_batch)) * \\\n                self.Q(next_obs_batch).detach().max(1)[0].data\n\n        td_target = td_target.to(device)\n        action_idx = torch.from_numpy(action_batch).to(device)\n        td_error = torch.nn.functional.mse_loss(\n            self.Q(obs_batch).gather(1, action_idx.view(-1, 1)),\n            td_target.float().unsqueeze(1))\n\n        self.Q_optimizer.zero_grad()\n        td_error.mean().backward()\n        self.Q_optimizer.step()\n```", "```py\n#!/usr/bin/env python\n\nclass LinearDecaySchedule(object):\n    def __init__(self, initial_value, final_value, max_steps):\n        assert initial_value > final_value, \"initial_value should be < final_value\"\n        self.initial_value = initial_value\n        self.final_value = final_value\n        self.decay_factor = (initial_value - final_value) / max_steps\n\n    def __call__(self, step_num):\n        current_value = self.initial_value - self.decay_factor * step_num\n        if current_value < self.final_value:\n            current_value = self.final_value\n        return current_value\n\nif __name__ == \"__main__\":\n    import matplotlib.pyplot as plt\n    epsilon_initial = 1.0\n    epsilon_final = 0.05\n    MAX_NUM_EPISODES = 10000\n    MAX_STEPS_PER_EPISODE = 300\n    linear_sched = LinearDecaySchedule(initial_value = epsilon_initial,\n                                    final_value = epsilon_final,\n                                    max_steps = MAX_NUM_EPISODES * MAX_STEPS_PER_EPISODE)\n    epsilon = [linear_sched(step) for step in range(MAX_NUM_EPISODES * MAX_STEPS_PER_EPISODE)]\n    plt.plot(epsilon)\n    plt.show()\n```", "```py\nimport torch\n\nclass CNN(torch.nn.Module):\n    \"\"\"\n    A Convolution Neural Network (CNN) class to approximate functions with visual/image inputs\n    \"\"\"\n    def __init__(self, input_shape, output_shape, device=\"cpu\"):\n        \"\"\"\n        :param input_shape: Shape/dimension of the input image. Assumed to be resized to C x 84 x 84\n        :param output_shape: Shape/dimension of the output.\n        :param device: The device (cpu or cuda) that the CNN should use to store the inputs for the forward pass\n        \"\"\"\n        # input_shape: C x 84 x 84\n        super(CNN, self).__init__()\n        self.device = device\n        self.layer1 = torch.nn.Sequential(\n            torch.nn.Conv2d(input_shape[0], 64, kernel_size=4, stride=2, padding=1),\n            torch.nn.ReLU()\n        )\n        self.layer2 = torch.nn.Sequential(\n            torch.nn.Conv2d(64, 32, kernel_size=4, stride=2, padding=0),\n            torch.nn.ReLU()\n        )\n        self.layer3 = torch.nn.Sequential(\n            torch.nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=0),\n            torch.nn.ReLU()\n        )\n        self.out = torch.nn.Linear(18 * 18 * 32, output_shape)\n\n    def forward(self, x):\n        x = torch.from_numpy(x).float().to(self.device)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = x.view(x.shape[0], -1)\n        x = self.out(x)\n        return x\n```", "```py\nself.DQN in the deep_Q_learner.py script:\n```", "```py\nself.Q = self.DQN(state_shape, action_shape, device).to(device)\nself.Q_optimizer = torch.optim.Adam(self.Q.parameters(), lr=self.learning_rate)\nif self.params['use_target_network']:\n self.Q_target = self.DQN(state_shape, action_shape, device).to(device)\n```", "```py\ndef learn_from_batch_experience(self, experiences):\n        batch_xp = Experience(*zip(*experiences))\n        obs_batch = np.array(batch_xp.obs)\n        action_batch = np.array(batch_xp.action)\n        reward_batch = np.array(batch_xp.reward)\n        next_obs_batch = np.array(batch_xp.next_obs)\n        done_batch = np.array(batch_xp.done)\n\n        if self.params['use_target_network']:\n if self.step_num % self.params['target_network_update_freq'] == 0:\n # The *update_freq is the Num steps after which target net is updated.\n # A schedule can be used instead to vary the update freq.\n self.Q_target.load_state_dict(self.Q.state_dict())\n td_target = reward_batch + ~done_batch * \\\n np.tile(self.gamma, len(next_obs_batch)) * \\\n self.Q_target(next_obs_batch).max(1)[0].data\n        else:\n            td_target = reward_batch + ~done_batch * \\\n                np.tile(self.gamma, len(next_obs_batch)) * \\\n                self.Q(next_obs_batch).detach().max(1)[0].data\n\n        td_target = td_target.to(device)\n        action_idx = torch.from_numpy(action_batch).to(device)\n        td_error = torch.nn.functional.mse_loss( self.Q(obs_batch).gather(1, action_idx.view(-1, 1)),\n                                                       td_target.float().unsqueeze(1))\n\n        self.Q_optimizer.zero_grad()\n        td_error.mean().backward()\n        writer.add_scalar(\"DQL/td_error\", td_error.mean(), self.step_num)\n        self.Q_optimizer.step()\n```", "```py\nparameters.json file that we will use to configure the parameters of the agent and the environment. The JavaScript Object Notation (JSON) file is a convenient and human-readable format for such data representation. We will discuss what each of these parameters mean in the later sections of this chapter. For now, we will concentrate on how we can use such a file to specify or change the parameters used by the agent and the environment:\n```", "```py\n{\n  \"agent\": {\n    \"max_num_episodes\": 70000,\n    \"max_steps_per_episode\": 300,\n    \"replay_batch_size\": 2000,\n    \"use_target_network\": true,\n    \"target_network_update_freq\": 2000,\n    \"lr\": 5e-3,\n    \"gamma\": 0.98,\n    \"epsilon_max\": 1.0,\n    \"epsilon_min\": 0.05,\n    \"seed\": 555,\n    \"use_cuda\": true,\n    \"summary_filename_prefix\": \"logs/DQL_\"\n  },\n  \"env\": {\n    \"type\": \"Atari\",\n    \"episodic_life\": \"True\",\n    \"clip_reward\": \"True\",\n    \"useful_region\": {\n        \"Default\":{\n                \"crop1\": 34,\n                \"crop2\": 34,\n                \"dimension2\": 80\n        }\n    }\n  }\n}\n```", "```py\n#!/usr/bin/env python\nimport JSON\n\nclass ParamsManager(object):\n    def __init__(self, params_file):\n        \"\"\"\n        A class to manage the Parameters. Parameters include configuration parameters and Hyper-parameters\n        :param params_file: Path to the parameters JSON file\n        \"\"\"\n        self.params = JSON.load(open(params_file, 'r'))\n```", "```py\n    def get_params(self):\n        \"\"\"\n        Returns all the parameters\n        :return: The whole parameter dictionary\n        \"\"\"\n        return self.params\n```", "```py\n def get_env_params(self):\n        \"\"\"\n        Returns the environment configuration parameters\n        :return: A dictionary of configuration parameters used for the environment\n        \"\"\"\n        return self.params['env']\n    def get_agent_params(self):\n        \"\"\"\n        Returns the hyper-parameters and configuration parameters used by the agent\n        :return: A dictionary of parameters used by the agent\n        \"\"\"\n        return self.params['agent']\n```", "```py\n\n    def update_agent_params(self, **kwargs):\n        \"\"\"\n        Update the hyper-parameters (and configuration parameters) used by the agent\n        :param kwargs: Comma-separated, hyper-parameter-key=value pairs. Eg.: lr=0.005, gamma=0.98\n        :return: None\n        \"\"\"\n        for key, value in kwargs.items():\n            if key in self.params['agent'].keys():\n                self.params['agent'][key] = value\n```", "```py\n#!/usr/bin/env python\n\nimport gym\nimport torch\nimport random\nimport numpy as np\n\nimport environment.atari as Atari\nfrom utils.params_manager import ParamsManager\nfrom utils.decay_schedule import LinearDecaySchedule\nfrom utils.experience_memory import Experience, ExperienceMemory\nfrom function_approximator.perceptron import SLP\nfrom function_approximator.cnn import CNN\nfrom tensorboardX import SummaryWriter\nfrom datetime import datetime\nfrom argparse import ArgumentParser\n\nargs = ArgumentParser(\"deep_Q_learner\")\nargs.add_argument(\"--params-file\",\n                  help=\"Path to the parameters JSON file. Default is parameters.JSON\",\n                  default=\"parameters.JSON\",\n                  type=str,\n                  metavar=\"PFILE\")\nargs.add_argument(\"--env-name\",\n                  help=\"ID of the Atari environment available in OpenAI Gym. Default is Pong-v0\",\n                  default=\"Pong-v0\",\n                  type=str,\n                  metavar=\"ENV\")\nargs = args.parse_args()\n\nparams_manager= ParamsManager(args.params_file)\nseed = params_manager.get_agent_params()['seed']\nsummary_file_path_prefix = params_manager.get_agent_params()['summary_file_path_prefix']\nsummary_file_name = summary_file_path_prefix + args.env_name + \"_\" + datetime.now().strftime(\"%y-%m-%d-%H-%M\")\nwriter = SummaryWriter(summary_file_name)\nglobal_step_num = 0\nuse_cuda = params_manager.get_agent_params()['use_cuda']\n# new in PyTorch 0.4\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() and use_cuda else \"cpu\")\ntorch.manual_seed(seed)\nnp.random.seed(seed)\nif torch.cuda.is_available() and use_cuda:\n    torch.cuda.manual_seed_all(seed)\n\nclass Deep_Q_Learner(object):\n    def __init__(self, state_shape, action_shape, params):\n        \"\"\"\n        self.Q is the Action-Value function. This agent represents Q using a Neural Network\n        If the input is a single dimensional vector, uses a Single-Layer-Perceptron else if the input is 3 dimensional\n        image, use a Convolutional-Neural-Network\n\n        :param state_shape: Shape (tuple) of the observation/state\n        :param action_shape: Shape (number) of the discrete action space\n        :param params: A dictionary containing various Agent configuration parameters and hyper-parameters\n        \"\"\"\n        self.state_shape = state_shape\n        self.action_shape = action_shape\n        self.params = params\n        self.gamma = self.params['gamma'] # Agent's discount factor\n        self.learning_rate = self.params['lr'] # Agent's Q-learning rate\n\n        if len(self.state_shape) == 1: # Single dimensional observation/state space\n            self.DQN = SLP\n        elif len(self.state_shape) == 3: # 3D/image observation/state\n            self.DQN = CNN\n\n        self.Q = self.DQN(state_shape, action_shape, device).to(device)\n        self.Q_optimizer = torch.optim.Adam(self.Q.parameters(), lr=self.learning_rate)\n        if self.params['use_target_network']:\n            self.Q_target = self.DQN(state_shape, action_shape, device).to(device)\n        # self.policy is the policy followed by the agent. This agents follows\n        # an epsilon-greedy policy w.r.t it's Q estimate.\n        self.policy = self.epsilon_greedy_Q\n        self.epsilon_max = 1.0\n        self.epsilon_min = 0.05\n        self.epsilon_decay = LinearDecaySchedule(initial_value=self.epsilon_max,\n                                    final_value=self.epsilon_min,\n                                    max_steps= self.params['epsilon_decay_final_step'])\n        self.step_num = 0\n\n        self.memory = ExperienceMemory(capacity=int(self.params['experience_memory_capacity'])) # Initialize an Experience memory with 1M capacity\n\n    def get_action(self, observation):\n        if len(observation.shape) == 3: # Single image (not a batch)\n            if observation.shape[2] < observation.shape[0]: # Probably observation is in W x H x C format\n                # Reshape to C x H x W format as per PyTorch's convention\n                observation = observation.reshape(observation.shape[2], observation.shape[1], observation.shape[0])\n            observation = np.expand_dims(observation, 0) # Create a batch dimension\n        return self.policy(observation)\n\n    def epsilon_greedy_Q(self, observation):\n        # Decay Epsilon/exploration as per schedule\n        writer.add_scalar(\"DQL/epsilon\", self.epsilon_decay(self.step_num), self.step_num)\n        self.step_num +=1\n        if random.random() < self.epsilon_decay(self.step_num):\n            action = random.choice([i for i in range(self.action_shape)])\n        else:\n            action = np.argmax(self.Q(observation).data.to(torch.device('cpu')).numpy())\n\n        return action\n\n    def learn(self, s, a, r, s_next, done):\n        # TD(0) Q-learning\n        if done: # End of episode\n            td_target = reward + 0.0 # Set the value of terminal state to zero\n        else:\n            td_target = r + self.gamma * torch.max(self.Q(s_next))\n        td_error = td_target - self.Q(s)[a]\n        # Update Q estimate\n        #self.Q(s)[a] = self.Q(s)[a] + self.learning_rate * td_error\n        self.Q_optimizer.zero_grad()\n        td_error.backward()\n        self.Q_optimizer.step()\n\n    def learn_from_batch_experience(self, experiences):\n        batch_xp = Experience(*zip(*experiences))\n        obs_batch = np.array(batch_xp.obs)\n        action_batch = np.array(batch_xp.action)\n        reward_batch = np.array(batch_xp.reward)\n        next_obs_batch = np.array(batch_xp.next_obs)\n        done_batch = np.array(batch_xp.done)\n\n        if self.params['use_target_network']:\n            if self.step_num % self.params['target_network_update_freq'] == 0:\n                # The *update_freq is the Num steps after which target net is updated.\n                # A schedule can be used instead to vary the update freq.\n                self.Q_target.load_state_dict(self.Q.state_dict())\n            td_target = reward_batch + ~done_batch * \\\n                np.tile(self.gamma, len(next_obs_batch)) * \\\n                self.Q_target(next_obs_batch).max(1)[0].data\n        else:\n            td_target = reward_batch + ~done_batch * \\\n                np.tile(self.gamma, len(next_obs_batch)) * \\\n                self.Q(next_obs_batch).detach().max(1)[0].data\n\n        td_target = td_target.to(device)\n        action_idx = torch.from_numpy(action_batch).to(device)\n        td_error = torch.nn.functional.mse_loss( self.Q(obs_batch).gather(1, action_idx.view(-1, 1)),\n                                                       td_target.float().unsqueeze(1))\n\n        self.Q_optimizer.zero_grad()\n        td_error.mean().backward()\n        writer.add_scalar(\"DQL/td_error\", td_error.mean(), self.step_num)\n        self.Q_optimizer.step()\n\n    def replay_experience(self, batch_size = None):\n        batch_size = batch_size if batch_size is not None else self.params['replay_batch_size']\n        experience_batch = self.memory.sample(batch_size)\n        self.learn_from_batch_experience(experience_batch)\n\n    def save(self, env_name):\n        file_name = self.params['save_dir'] + \"DQL_\" + env_name + \".ptm\"\n        torch.save(self.Q.state_dict(), file_name)\n        print(\"Agent's Q model state saved to \", file_name)\n\n    def load(self, env_name):\n        file_name = self.params['load_dir'] + \"DQL_\" + env_name + \".ptm\"\n        self.Q.load_state_dict(torch.load(file_name))\n        print(\"Loaded Q model state from\", file_name)\n\nif __name__ == \"__main__\":\n    env_conf = params_manager.get_env_params()\n    env_conf[\"env_name\"] = args.env_name\n    # If a custom useful_region configuration for this environment ID is available, use it if not use the Default\n    custom_region_available = False\n    for key, value in env_conf['useful_region'].items():\n        if key in args.env_name:\n            env_conf['useful_region'] = value\n            custom_region_available = True\n            break\n    if custom_region_available is not True:\n        env_conf['useful_region'] = env_conf['useful_region']['Default']\n    print(\"Using env_conf:\", env_conf)\n    env = Atari.make_env(args.env_name, env_conf)\n    observation_shape = env.observation_space.shape\n    action_shape = env.action_space.n\n    agent_params = params_manager.get_agent_params()\n    agent = Deep_Q_Learner(observation_shape, action_shape, agent_params)\n    if agent_params['load_trained_model']:\n        try:\n            agent.load(env_conf[\"env_name\"])\n        except FileNotFoundError:\n            print(\"WARNING: No trained model found for this environment. Training from scratch.\")\n    first_episode = True\n    episode_rewards = list()\n    for episode in range(agent_params['max_num_episodes']):\n        obs = env.reset()\n        cum_reward = 0.0 # Cumulative reward\n        done = False\n        step = 0\n        #for step in range(agent_params['max_steps_per_episode']):\n        while not done:\n            if env_conf['render']:\n                env.render()\n            action = agent.get_action(obs)\n            next_obs, reward, done, info = env.step(action)\n            #agent.learn(obs, action, reward, next_obs, done)\n            agent.memory.store(Experience(obs, action, reward, next_obs, done))\n\n            obs = next_obs\n            cum_reward += reward\n            step += 1\n            global_step_num +=1\n\n            if done is True:\n                if first_episode: # Initialize max_reward at the end of first episode\n                    max_reward = cum_reward\n                    first_episode = False\n                episode_rewards.append(cum_reward)\n                if cum_reward > max_reward:\n                    max_reward = cum_reward\n                    agent.save(env_conf['env_name'])\n                print(\"\\nEpisode#{} ended in {} steps. reward ={} ; mean_reward={:.3f} best_reward={}\".\n                      format(episode, step+1, cum_reward, np.mean(episode_rewards), max_reward))\n                writer.add_scalar(\"main/ep_reward\", cum_reward, global_step_num)\n                writer.add_scalar(\"main/mean_ep_reward\", np.mean(episode_rewards), global_step_num)\n                writer.add_scalar(\"main/max_ep_rew\", max_reward, global_step_num)\n                if agent.memory.get_size() >= 2 * agent_params['replay_batch_size']:\n                    agent.replay_experience()\n\n                break\n    env.close()\n    writer.close()\n```", "```py\nclass ClipRewardEnv(gym.RewardWrapper):\n    def __init__(self, env):\n        gym.RewardWrapper.__init__(self, env)\n\n    def reward(self, reward):\n        \"\"\" Clip rewards to be either -1, 0 or +1 based on the sign\"\"\"\n        return np.sign(reward)\n```", "```py\ndef process_frame_84(frame, conf):\n    frame = frame[conf[\"crop1\"]:conf[\"crop2\"] + 160, :160]\n    frame = frame.mean(2)\n    frame = frame.astype(np.float32)\n    frame *= (1.0 / 255.0)\n    frame = cv2.resize(frame, (84, conf[\"dimension2\"]))\n    frame = cv2.resize(frame, (84, 84))\n    frame = np.reshape(frame, [1, 84, 84])\n    return frame\n\nclass AtariRescale(gym.ObservationWrapper):\n    def __init__(self, env, env_conf):\n        gym.ObservationWrapper.__init__(self, env)\n        self.observation_space = Box(0.0, 1.0, [1, 84, 84])\n        self.conf = env_conf\n\n    def observation(self, observation):\n        return process_frame_84(observation, self.conf)\n```", "```py\n(x - numpy.mean(x)) / numpy.std(x)\n```", "```py\n(x - numpy.min(x)) / (numpy.max(x) - numpy.min(x))\n```", "```py\n2 * (x - numpy.min(x)) / (numpy.max(x) - numpy.min(x)) - 1\n```", "```py\nclass NormalizedEnv(gym.ObservationWrapper):\n    def __init__(self, env=None):\n        gym.ObservationWrapper.__init__(self, env)\n        self.state_mean = 0\n        self.state_std = 0\n        self.alpha = 0.9999\n        self.num_steps = 0\n\n    def observation(self, observation):\n        self.num_steps += 1\n        self.state_mean = self.state_mean * self.alpha + \\\n            observation.mean() * (1 - self.alpha)\n        self.state_std = self.state_std * self.alpha + \\\n            observation.std() * (1 - self.alpha)\n\n        unbiased_mean = self.state_mean / (1 - pow(self.alpha, self.num_steps))\n        unbiased_std = self.state_std / (1 - pow(self.alpha, self.num_steps))\n\n        return (observation - unbiased_mean) / (unbiased_std + 1e-8)\n```", "```py\nclass NoopResetEnv(gym.Wrapper):\n    def __init__(self, env, noop_max=30):\n        \"\"\"Sample initial states by taking random number of no-ops on reset.\n        No-op is assumed to be action 0.\n        \"\"\"\n        gym.Wrapper.__init__(self, env)\n        self.noop_max = noop_max\n        self.noop_action = 0\n        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n\n    def reset(self):\n        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n        self.env.reset()\n        noops = random.randrange(1, self.noop_max + 1) # pylint: disable=E1101\n        assert noops > 0\n        obs = None\n        for _ in range(noops):\n            obs, _, done, _ = self.env.step(self.noop_action)\n        return obs\n\n    def step(self, ac):\n        return self.env.step(ac)\n```", "```py\nclass FireResetEnv(gym.Wrapper):\n    def __init__(self, env):\n        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n        gym.Wrapper.__init__(self, env)\n        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n        assert len(env.unwrapped.get_action_meanings()) >= 3\n\n    def reset(self):\n        self.env.reset()\n        obs, _, done, _ = self.env.step(1)\n        if done:\n            self.env.reset()\n        obs, _, done, _ = self.env.step(2)\n        if done:\n            self.env.reset()\n        return obs\n\n    def step(self, ac):\n        return self.env.step(ac)\n```", "```py\nclass EpisodicLifeEnv(gym.Wrapper):\n    def __init__(self, env):\n        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n        Done by DeepMind for the DQN and co. since it helps value estimation.\n        \"\"\"\n        gym.Wrapper.__init__(self, env)\n        self.lives = 0\n        self.was_real_done = True\n\n    def step(self, action):\n        obs, reward, done, info = self.env.step(action)\n        self.was_real_done = True\n        # check current lives, make loss of life terminal,\n        # then update lives to handle bonus lives\n        lives = info['ale.lives']\n        if lives < self.lives and lives > 0:\n            # for Qbert sometimes we stay in lives == 0 condition for a few frames\n            # so its important to keep lives > 0, so that we only reset once\n            # the environment advertises done.\n            done = True\n            self.was_real_done = False\n        self.lives = lives\n        return obs, reward, done, info\n\n    def reset(self):\n        \"\"\"Reset only when lives are exhausted.\n        This way all states are still reachable even though lives are episodic,\n        and the learner need not know about any of this behind-the-scenes.\n        \"\"\"\n        if self.was_real_done:\n            obs = self.env.reset()\n            self.lives = 0\n        else:\n            # no-op step to advance from terminal/lost life state\n            obs, _, _, info = self.env.step(0)\n            self.lives = info['ale.lives']\n        return obs\n```", "```py\nclass MaxAndSkipEnv(gym.Wrapper):\n    def __init__(self, env=None, skip=4):\n        \"\"\"Return only every `skip`-th frame\"\"\"\n        gym.Wrapper.__init__(self, env)\n        # most recent raw observations (for max pooling across time steps)\n        self._obs_buffer = deque(maxlen=2)\n        self._skip = skip\n\n    def step(self, action):\n        total_reward = 0.0\n        done = None\n        for _ in range(self._skip):\n            obs, reward, done, info = self.env.step(action)\n            self._obs_buffer.append(obs)\n            total_reward += reward\n            if done:\n                break\n\n        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n        return max_frame, total_reward, done, info\n\n    def reset(self):\n        \"\"\"Clear past frame buffer and init. to first obs. from inner env.\"\"\"\n        self._obs_buffer.clear()\n        obs = self.env.reset()\n        self._obs_buffer.append(obs)\n        return obs\n```", "```py\ndef make_env(env_id, env_conf):\n    env = gym.make(env_id)\n    if 'NoFrameskip' in env_id:\n        assert 'NoFrameskip' in env.spec.id\n        env = NoopResetEnv(env, noop_max=30)\n        env = MaxAndSkipEnv(env, skip=env_conf['skip_rate'])\n\n    if env_conf['episodic_life']:\n        env = EpisodicLifeEnv(env)\n\n    if 'FIRE' in env.unwrapped.get_action_meanings():\n        env = FireResetEnv(env)\n\n    env = AtariRescale(env, env_conf['useful_region'])\n    env = NormalizedEnv(env)\n\n    if env_conf['clip_reward']:\n        env = ClipRewardEnv(env)\n    return env\n\n```", "```py\n#!/usr/bin/env python\n#!/usr/bin/env python\n\nimport gym\nimport torch\nimport random\nimport numpy as np\n\nimport environment.atari as Atari\nimport environment.utils as env_utils\nfrom utils.params_manager import ParamsManager\nfrom utils.decay_schedule import LinearDecaySchedule\nfrom utils.experience_memory import Experience, ExperienceMemory\nfrom function_approximator.perceptron import SLP\nfrom function_approximator.cnn import CNN\nfrom tensorboardX import SummaryWriter\nfrom datetime import datetime\nfrom argparse import ArgumentParser\n\nargs = ArgumentParser(\"deep_Q_learner\")\nargs.add_argument(\"--params-file\", help=\"Path to the parameters json file. Default is parameters.json\",\n                 default=\"parameters.json\", metavar=\"PFILE\")\nargs.add_argument(\"--env-name\", help=\"ID of the Atari environment available in OpenAI Gym. Default is Seaquest-v0\",\n                  default=\"Seaquest-v0\", metavar=\"ENV\")\nargs.add_argument(\"--gpu-id\", help=\"GPU device ID to use. Default=0\", default=0, type=int, metavar=\"GPU_ID\")\nargs.add_argument(\"--render\", help=\"Render environment to Screen. Off by default\", action=\"store_true\", default=False)\nargs.add_argument(\"--test\", help=\"Test mode. Used for playing without learning. Off by default\", action=\"store_true\",\n                  default=False)\nargs = args.parse_args()\n\nparams_manager= ParamsManager(args.params_file)\nseed = params_manager.get_agent_params()['seed']\nsummary_file_path_prefix = params_manager.get_agent_params()['summary_file_path_prefix']\nsummary_file_path= summary_file_path_prefix + args.env_name + \"_\" + datetime.now().strftime(\"%y-%m-%d-%H-%M\")\nwriter = SummaryWriter(summary_file_path)\n# Export the parameters as json files to the log directory to keep track of the parameters used in each experiment\nparams_manager.export_env_params(summary_file_path + \"/\" + \"env_params.json\")\nparams_manager.export_agent_params(summary_file_path + \"/\" + \"agent_params.json\")\nglobal_step_num = 0\nuse_cuda = params_manager.get_agent_params()['use_cuda']\n# new in PyTorch 0.4\ndevice = torch.device(\"cuda:\" + str(args.gpu_id) if torch.cuda.is_available() and use_cuda else \"cpu\")\ntorch.manual_seed(seed)\nnp.random.seed(seed)\nif torch.cuda.is_available() and use_cuda:\n    torch.cuda.manual_seed_all(seed)\n\nclass Deep_Q_Learner(object):\n    def __init__(self, state_shape, action_shape, params):\n        ...\n\n    def get_action(self, observation):\n        ...\n\n    def epsilon_greedy_Q(self, observation):\n        ...\n\n    def learn(self, s, a, r, s_next, done):\n        ...\n\n    def learn_from_batch_experience(self, experiences):\n        ...\n\n    def replay_experience(self, batch_size = None):\n        ...\n\n    def load(self, env_name):\n        ...\n\nif __name__ == \"__main__\":\n    env_conf = params_manager.get_env_params()\n    env_conf[\"env_name\"] = args.env_name\n    # If a custom useful_region configuration for this environment ID is available, use it if not use the Default\n    ...\n    # If a saved (pre-trained) agent's brain model is available load it as per the configuration\n    if agent_params['load_trained_model']:\n    ...\n\n    # Start the training process\n    episode = 0\n    while global_step_num <= agent_params['max_training_steps']:\n        obs = env.reset()\n        cum_reward = 0.0 # Cumulative reward\n        done = False\n        step = 0\n        #for step in range(agent_params['max_steps_per_episode']):\n        while not done:\n            if env_conf['render'] or args.render:\n                env.render()\n            action = agent.get_action(obs)\n            next_obs, reward, done, info = env.step(action)\n            #agent.learn(obs, action, reward, next_obs, done)\n            agent.memory.store(Experience(obs, action, reward, next_obs, done))\n\n            obs = next_obs\n            cum_reward += reward\n            step += 1\n            global_step_num +=1\n\n            if done is True:\n                episode += 1\n                episode_rewards.append(cum_reward)\n                if cum_reward > agent.best_reward:\n                    agent.best_reward = cum_reward\n                if np.mean(episode_rewards) > prev_checkpoint_mean_ep_rew:\n                    num_improved_episodes_before_checkpoint += 1\n                if num_improved_episodes_before_checkpoint >= agent_params[\"save_freq_when_perf_improves\"]:\n                    prev_checkpoint_mean_ep_rew = np.mean(episode_rewards)\n                    agent.best_mean_reward = np.mean(episode_rewards)\n                    agent.save(env_conf['env_name'])\n                    num_improved_episodes_before_checkpoint = 0\n                print(\"\\nEpisode#{} ended in {} steps. reward ={} ; mean_reward={:.3f} best_reward={}\".\n                      format(episode, step+1, cum_reward, np.mean(episode_rewards), agent.best_reward))\n                writer.add_scalar(\"main/ep_reward\", cum_reward, global_step_num)\n                writer.add_scalar(\"main/mean_ep_reward\", np.mean(episode_rewards), global_step_num)\n                writer.add_scalar(\"main/max_ep_rew\", agent.best_reward, global_step_num)\n                # Learn from batches of experience once a certain amount of xp is available unless in test only mode\n                if agent.memory.get_size() >= 2 * agent_params['replay_start_size'] and not args.test:\n                    agent.replay_experience()\n\n                break\n    env.close()\n    writer.close()\n```", "```py\n(rl_gym_book) praveen@ubuntu:~/HOIAWOG/ch6$ python deep_Q_learner.py --env \"ENV_ID\"\n```", "```py\n(rl_gym_book) praveen@ubuntu:~/HOIAWOG/ch6$ python deep_Q_learner.py --env \"PongNoFrameskip-v4\"\n```", "```py\n(rl_gym_book) praveen@ubuntu:~/HOIAWOG/ch6$ tensorboard --logdir=logs/\n```", "```py\n(rl_gym_book) praveen@ubuntu:~/HOIAWOG/ch6$ python deep_Q_learner.py --env \"Seaquest-v0\" --test --render\n```"]