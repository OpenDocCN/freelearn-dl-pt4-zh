["```py\n    df_1 = pd.DataFrame({'product': ['red shirt', 'red shirt', 'red shirt',\n                                 'white dress'],\n                     'price': [49.33, 49.33, 32.49, 199.99]}),\n    df_2 = pd.DataFrame({'product': ['red shirt', 'blue pants',\n                                 'white tuxedo', 'white dress'],\n                     'in_stock': [True, True, False, False]})    \n```", "```py\n    from sklearn.preprocessing import LabelEncoder\n    rating_encoder = LabelEncoder()\n    _df = df.copy()\n    _df.rating = rating_encoder.fit_transform(df.rating)\n    _df \n```", "```py\n    ordinal_map = {rating: index for index, rating in enumerate\n    (['low','medium', 'high'])}\n    print(ordinal_map)\n    df.rating = df.rating.map(ordinal_map) \n```", "```py\n    features = ['price', 'rating', 'product_blue pants',\n    'product_red shirt', 'product_white dress',\n    'product_white tuxedo']\n    X = df[features].values\n    target = 'in_stock'\n    y = df[target].values\n    from sklearn.model_selection import train_test_split\n    X_train, X_test, y_train, y_test = \\\n    train_test_split(X, y, test_size=0.3) \n```", "```py\n      %%bash\n      head ../data/hr-analytics/hr_data.csv \n```", "```py\n    with open('../data/hr-analytics/hr_data.csv') as f:\n    print(len(f.read().splitlines())) \n```", "```py\n    df.left.value_counts().plot('barh')\n    print(df.left.isnull().sum()) \n```", "```py\n    for f in df.columns:\n    try:\n    fig = plt.figure()\n    …\n    print('-'*30) \n```", "```py\n    fill_value = df.time_spend_company.median()\n    df.time_spend_company = df.time_spend_company.fillna(fill_value) \n```", "```py\n  sns.boxplot(x='number_project', y='average_montly_hours', data=df) \n```", "```py\n    mean_per_project = df.groupby('number_project')\\.\n    average_montly_hours.mean()\n    mean_per_project = dict(mean_per_project)\n    print(mean_per_project) \n```", "```py\n      fill_values = df.number_project.map(mean_per_project)\n      df.average_montly_hours = df.average_montly_hours.fillna(fill_values) \n```", "```py\n  df.left = df.left.map({'no': 0, 'yes': 1})\n  df = pd.get_dummies(df) \n```", "```py\n     df.to_csv('../data/hr-analytics/hr_data_processed.csv', index=False) \n```", "```py\n  sns.jointplot('satisfaction_level', \n 'last_evaluation', data=df, kind='hex') \n```", "```py\n      plot_args = dict(shade=True, shade_lowest=False)\n      for i, c in zip((0, 1), ('Reds', 'Blues')):\n      sns.kdeplot(df.loc[df.left==i, 'satisfaction_level'],\n          df.loc[df.left==i, 'last_evaluation'],\n          cmap=c, **plot_args) \n```", "```py\n     from sklearn.model_selection import train_test_split\n     features = ['satisfaction_level', 'last_evaluation']\n        X_train, X_test, y_train, y_test = \n        train_test_split(df[features].values, df['left'].values,\n        test_size=0.3, random_state=1) \n```", "```py\n    from sklearn.preprocessing import StandardScaler\n    scaler = StandardScaler()\n    X_train_std = scaler.fit_transform(X_train)\n    X_test_std = scaler.transform(X_test) \n```", "```py\n    from sklearn.svm import\n    SVC svm = SVC(kernel='linear', C=1, random_state=1)\n    svm.fit(X_train_std, y_train)\n```", "```py\n    from sklearn.metrics import accuracy_score\n    y_pred = svm.predict(X_test_std)\n    acc = accuracy_score(y_test, y_pred)\n    print('accuracy = {:.1f}%'.format(acc*100))\n    >> accuracy = 75.9% \n```", "```py\n      from sklearn.metrics import confusion_matrix\n      cmat = confusion_matrix(y_test, y_pred)\n      scores = cmat.diagonal() / cmat.sum(axis=1) * 100\n      print('left = 0 : {:.2f}%'.format(scores[0]))\n      print('left = 1 : {:.2f}%'.format(scores[1]))\n      >> left = 0 : 100.00%\n      >> left = 1 : 0.00% \n```", "```py\n      from mlxtend.plotting import plot_decision_regions\n      N_samples = 200\n      X, y = X_train_std[:N_samples], y_train[:N_samples]\n      plot_decision_regions(X, y, clf=svm) \n```", "```py\n     svm = SVC(kernel='rbf', C=1, random_state=1)\n     svm.fit(X_train_std, y_train)\n```", "```py\n  check_model_fit(svm, X_test_std, y_test) \n```", "```py\n      from sklearn.neighbors import KNeighborsClassifier\n      KNeighborsClassifier?\n```", "```py\n    knn = KNeighborsClassifier(n_neighbors=3)\n    knn.fit(X_train_std, y_train) \n\n    check_model_fit(knn, X_test_std, y_test) \n\n```", "```py\n    knn = KNeighborsClassifier(n_neighbors=25)\n    knn.fit(X_train_std, y_train)\n    check_model_fit(knn, X_test_std, y_test) \n```", "```py\n       from sklearn.ensemble import RandomForestClassifier\n       forest = RandomForestClassifier(n_estimators=50,\n       max_depth=5,\n       random_state=1)\n       forest.fit(X_train, y_train)\n       check_model_fit(forest, X_test, y_test) \n```", "```py\n    from sklearn.tree import export_graphviz\n    import graphviz\n    dot_data = export_graphviz(\n        forest.estimators_[0],\n        out_file=None,\n        feature_names=features,\n        class_names=['no', 'yes'],\n        filled=True, rounded=True,\n        special_characters=True)\n    graph = graphviz.Source(dot_data)\n    graph \n```", "```py\n      df = pd.read_csv('../data/hr-analytics/hr_data_processed.csv')\n      features = ['satisfaction_level', 'last_evaluation']\n      X = df[features].values\n      y = df.left.values \n```", "```py\n   clf = RandomForestClassifier(n_estimators=100, max_depth=5) \n```", "```py\n    from sklearn.model_selection import cross_val_score\n    np.random.seed(1)\n    scores = cross_val_score(\n        estimator=clf,\n        X=X,\n        y=y,\n        cv=10)\n    print('accuracy = {:.3f} +/- {:.3f}'.format(scores.mean(), scores.\n    std()))\n    >> accuracy = 0.923 +/- 0.005 \n```", "```py\n    >> array([ 0.93404397, 0.91533333, 0.92266667, 0.91866667,\n        0.92133333,\n        0.92866667, 0.91933333, 0.92 , 0.92795197, 0.92128085]) \n```", "```py\n      from sklearn.model_selection import StratifiedKFold\n      …\n          print('fold: {:d} accuracy: {:s}'.format(k+1, str(class_acc)))\n      return class_accuracy \n```", "```py\n    from sklearn.model_selection import cross_val_score\n    np.random.seed(1)\n    …\n    >> fold: 10 accuracy: [ 0.98861646 0.70588235]\n    >> accuracy = [ 0.98722476 0.71715647] +/- [ 0.00330026 0.02326823] \n```", "```py\n    from sklearn.model_selection import validation_curve\n\n    clf = RandomForestClassifier(n_estimators=10)\n    max_depths = np.arange(3, 16, 3)\n    train_scores, test_scores = validation_curve(\n        estimator=clf,\n        X=X,\n        y=y,\n        param_name='max_depth',\n        param_range=max_depths,\n        cv=10);\n```", "```py\n    plot_validation_curve(train_scores, test_scores, max_depths,\n    xlabel='max_depth')\n```", "```py\n    features = ['satisfaction_level', 'last_evaluation', 'number_project',\n    'average_montly_hours', 'time_spend_company', 'work_accident',\n        …\n        X = df[features].values\n        y = df.left.values \n```", "```py\n    %%time\n    np.random.seed(1)\n    clf = RandomForestClassifier(n_estimators=20)\n    max_depths = [3, 4, 5, 6, 7,\n    9, 12, 15, 18, 21]\n    train_scores, test_scores = validation_curve(\n    estimator=clf,\n        X=X,\n        y=y,\n    param_name='max_depth',\n    param_range=max_depths,\n    cv=5); \n```", "```py\n    plot_validation_curve(train_scores, test_scores,\n    max_depths, xlabel='max_depth'); \n```", "```py\n     np.random.seed(1) \n     clf = RandomForestClassifier(n_estimators=200, max_depth=6) \n     scores = cross_val_class_score(clf, X, y)\n     print('accuracy = {} +/- {}'\\ .format(scores.mean(axis=0),\n     scores.std(axis=0)))\n     >> accuracy = [ 0.99553722 0.85577359] +/- [ 0.00172575 0.02614334] \n```", "```py\n    fig = plt.figure(figsize=(5, 7))\n    sns.boxplot(data=pd.DataFrame(scores, columns=[0, 1]),\n    palette=sns.color_palette('Set1'))\n    plt.xlabel('Left')\n    plt.ylabel('Accuracy') \n```", "```py\n    pd.Series(clf.feature_importances_, name='Feature importance',\n        index=df[features].columns)\\\n        .sort_values()\\\n        .plot.barh()\n    plt.xlabel('Feature importance')\n```", "```py\n    from sklearn.decomposition import PCA\n    pca_features = \\\n    …\n    pca = PCA(n_components=3)\n    X_pca = pca.fit_transform(X_reduce) \n```", "```py\n    >> array([[-0.67733089, 0.75837169, -0.10493685],\n    >> [ 0.73616575, 0.77155888, -0.11046422],\n    >> [ 0.73616575, 0.77155888, -0.11046422],\n        >> ...,\n    >> [-0.67157059, -0.3337546 , 0.70975452],\n    >> [-0.67157059, -0.3337546 , 0.70975452],\n    >> [-0.67157059, -0.3337546 , 0.70975452]]) \n```", "```py\n    df['first_principle_component'] = X_pca.T[0]\n    df['second_principle_component'] = X_pca.T[1]\n    df['third_principle_component'] = X_pca.T[2] \n```", "```py\n    features = ['satisfaction_level', 'number_project', 'time_spend_\n        company',\n    'average_montly_hours', 'last_evaluation',\n    'first_principle_component',\n    'second_principle_component',\n    'third_principle_component']\n    X = df[features].values\n    y = df.left.values \n```", "```py\n    np.random.seed(1)\n    clf = RandomForestClassifier(n_estimators=200, max_depth=6)\n    scores = cross_val_class_score(clf, X, y)\n    print('accuracy = {} +/- {}'\\.format(scores.mean(axis=0), \n    scores.std(axis=0)))\n    >> accuracy = [ 0.99562463 0.90618594] +/- [ 0.00166047 0.01363927] \n```", "```py\n    fig = plt.figure(figsize=(5, 7))\n    sns.boxplot(data=pd.DataFrame(scores, columns=[0, 1]),\n    palette=sns.color_palette('Set1'))\n    plt.xlabel('Left')\n    plt.ylabel('Accuracy') \n```", "```py\n    np.random.seed(1)\n    clf = RandomForestClassifier(n_estimators=200, max_depth=6)\n    clf.fit(X, y)\n```", "```py\n    from sklearn.externals import joblib\n    joblib.dump(clf, 'random-forest-trained.pkl') \n```", "```py\n     clf = joblib.load('random-forest-trained.pkl') \n```", "```py\n    sandra = df.iloc[573]\n    X = sandra[features]\n        X\n    >> satisfaction_level 0.360000\n    >> number_project 2.000000\n    >> time_spend_company 3.000000\n    >> average_montly_hours 148.000000\n    >> last_evaluation 0.470000\n    >> first_principle_component 0.742801\n    >> second_principle_component -0.514568\n    >> third_principle_component -0.677421\n```", "```py\n    clf.predict([X])\n    >> array([1]) \n```", "```py\n    clf.predict_proba([X])\n    >> array([[ 0.06576239, 0.93423761]])\n```", "```py\n   X.average_montly_hours = 100\n   X.time_spend_company = 1\n     clf.predict_proba([X])\n     >> array([[ 0.61070329, 0.38929671]])\n```"]