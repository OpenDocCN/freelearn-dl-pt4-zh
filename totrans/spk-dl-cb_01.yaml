- en: Setting Up Spark for Deep Learning Development
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, the following recipes will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: Downloading an Ubuntu Desktop image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing and configuring Ubuntu with VMWare Fusion on macOS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing and configuring Ubuntu with Oracle VirtualBox on Windows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing and configuring Ubuntu Desktop for Google Cloud Platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing and configuring Spark and prerequisites on Ubuntu Desktop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating Jupyter notebooks with Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Starting and configuring a Spark cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stopping a Spark cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning is the focused study of machine learning algorithms that deploy
    neural networks as their main method of learning. Deep learning has exploded onto
    the scene just within the last couple of years. Microsoft, Google, Facebook, Amazon,
    Apple, Tesla and many other companies are all utilizing deep learning models in
    their apps, websites, and products. At the same exact time, Spark, an in-memory
    compute engine running on top of big data sources, has made it easy to process
    volumes of information at record speeds and ease. In fact, Spark has now become
    the leading big data development tool for data engineers, machine learning engineers,
    and data scientists.
  prefs: []
  type: TYPE_NORMAL
- en: Since deep learning models perform better with more data, the synergy between
    Spark and deep learning allowed for a perfect marriage. Almost as important as
    the code used to execute deep learning algorithms is the work environment that
    enables optimal development. Many talented minds are eager to develop neural networks
    to help answer important questions in their research. Unfortunately, one of the
    greatest barriers to the development of deep learning models is access to the
    necessary technical resources required to learn on big data. The purpose of this
    chapter is to create an ideal virtual development environment for deep learning
    on Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading an Ubuntu Desktop image
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark can be set up for all types of operating systems, whether they reside
    on-premise or in the cloud. For our purposes, Spark will be installed on a Linux-based
    virtual machine with Ubuntu as the operating system. There are several advantages
    to using Ubuntu as the go-to virtual machine, not least of which is cost. Since
    they are based on open source software, Ubuntu operating systems are free to use
    and do not require licensing. Cost is always a consideration and one of the main
    goals of this publication is to minimize the financial footprint required to get
    started with deep learning on top of a Spark framework.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are some minimum recommendations required for downloading the image file:'
  prefs: []
  type: TYPE_NORMAL
- en: Minimum of 2 GHz dual-core processor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Minimum of 2 GB system memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Minimum of 25 GB of free hard drive space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Follow the steps in the recipe to download an Ubuntu Desktop image:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to create a virtual machine of Ubuntu Desktop, it is necessary to
    first download the file from the official website: [https://www.ubuntu.com/download/desktop.](https://www.ubuntu.com/download/desktop)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As of this writing, Ubuntu Desktop 16.04.3 is the most recent available version
    for download.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Access the following file in a `.iso` format once the download is complete:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`ubuntu-16.04.3-desktop-amd64.iso`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Virtual environments provide an optimal development workspace by isolating the
    relationship to the physical or host machine. Developers may be using all types
    of machines for their host environments such as a MacBook running macOS, a Microsoft
    Surface running Windows or even a virtual machine on the cloud with Microsoft
    Azure or AWS; however, to ensure consistency within the output of the code executed,
    a virtual environment within Ubuntu Desktop will be deployed that can be used
    and shared among a wide variety of host platforms.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are several options for desktop virtualization software, depending on
    whether the host environment is on a Windows or a macOS. There are two common
    software applications for virtualization when using macOS:'
  prefs: []
  type: TYPE_NORMAL
- en: VMWare Fusion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To learn more about Ubuntu Desktop, you can visit [https://www.ubuntu.com/desktop](https://www.ubuntu.com/desktop).
  prefs: []
  type: TYPE_NORMAL
- en: Installing and configuring Ubuntu with VMWare Fusion on macOS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section will focus on building a virtual machine using an Ubuntu operating
    system with **VMWare Fusion**.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A previous installation of VMWare Fusion is required on your system. If you
    do not currently have this, you can download a trial version from the following
    website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.vmware.com/products/fusion/fusion-evaluation.html](https://www.vmware.com/products/fusion/fusion-evaluation.html)'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Follow the steps in the recipe to configure Ubuntu with VMWare Fusion on macOS:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once VMWare Fusion is up and running, click on the *+* button on the upper-left-hand
    side to begin the configuration process and select New..., as seen in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/de3baeba-f285-420c-bb37-c88f4fe56c6b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once the selection has been made, select the option to Install from Disk or
    Image, as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/01d7eef1-bcf5-4e26-bbf6-112b89a9ed07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Select the operating system''s `iso` file that was downloaded from the Ubuntu
    Desktop website, as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/c8da4a79-082f-40e7-91f7-4dc24cfc3723.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The next step will ask whether you want to choose Linux Easy Install. It is
    recommended to do so, as well as to incorporate a Display Name/Password combination
    for the Ubuntu environment, as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/ad3cc8d3-194b-4985-b259-de865c1cd1b2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The configuration process is almost complete. A Virtual Machine Summary is
    displayed with the option to Customize Settings to increase the Memory and Hard
    Disk, as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/81476499-034d-4a51-9dc7-0694f7c89a21.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Anywhere from 20 to 40 GB hard disk space is sufficient for the virtual machine;
    however, bumping up the memory to either 2 GB or even 4 GB will assist with the
    performance of the virtual machine when executing Spark code in later chapters.
    Update the memory by selecting Processors and Memory under the Settings of the
    virtual machine and increasing the Memory to the desired amount, as seen in the
    following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/ac19e635-36e1-493e-99ec-7e8c0f63c90d.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The setup allows for manual configuration of the settings necessary to get Ubuntu
    Desktop up and running successfully on VMWare Fusion. The memory and hard drive
    storage can be increased or decreased based on the needs and availability of the
    host machine.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'All that is remaining is to fire up the virtual machine for the first time,
    which initiates the installation process of the system onto the virtual machine.
    Once all the setup is complete and the user has logged in, the Ubuntu virtual
    machine should be available for development, as seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/58e6f288-6ea4-45fa-995f-0000b1aaf28e.png)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Aside from VMWare Fusion, there is also another product that offers similar
    functionality on a Mac. It is called Parallels Desktop for Mac. To learn more
    about VMWare and Parallels, and decide which program is a better fit for your
    development, visit the following websites:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.vmware.com/products/fusion.html](https://www.vmware.com/products/fusion.html)
    to download and install VMWare Fusion for Mac'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://parallels.com](https://parallels.com) to download and install the
    Parallels Desktop for Mac'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing and configuring Ubuntu with Oracle VirtualBox on Windows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unlike with macOS, there are several options to virtualize systems within Windows.
    This mainly has to do with the fact that virtualization on Windows is very common
    as most developers are using Windows as their host environment and need virtual
    environments for testing purposes without affecting any of the dependencies that
    rely on Windows.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: VirtualBox from Oracle is a common virtualization product and is free to use. Oracle
    VirtualBox provides a straightforward process to get an Ubuntu Desktop virtual
    machine up and running on top of a Windows environment.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Follow the steps in this recipe to configure Ubuntu with **VirtualBox** on
    Windows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Initiate an Oracle VM VirtualBox Manager. Next, create a new virtual machine
    by selecting the New icon and specify the Name, Type, and Version of the machine,
    as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/bf279c58-86ff-4159-97af-87ede0451a29.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Select Expert Mode as several of the configuration steps will get consolidated,
    as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/e7276b57-d427-46ff-8ee4-ab5fccf0dc49.png)'
  prefs: []
  type: TYPE_IMG
- en: Ideal memory size should be set to at least `2048` MB, or preferably `4096`
    MB, depending on the resources available on the host machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, set an optimal hard disk size for an Ubuntu virtual machine performing
    deep learning algorithms to at least 20 GB, if not more, as seen in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/771301d8-e257-4669-9ea2-da6a1fe610b6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Point the virtual machine manager to the start-up disk location where the Ubuntu
    `iso` file was downloaded to and then Start the creation process, as seen in the
    following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/b8ba0256-a82f-4945-be82-ebd713d814d9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After allotting some time for the installation, select the Start icon to complete
    the virtual machine and get it ready for development as seen in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/accc7615-2469-412b-815a-dc5e57ca3773.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The setup allows for manual configuration of the settings necessary to get Ubuntu
    Desktop up and running successfully on Oracle VirtualBox. As was the case with
    VMWare Fusion, the memory and hard drive storage can be increased or decreased
    based on the needs and availability of the host machine.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Please note that some machines that run Microsoft Windows are not set up by
    default for virtualization and users may receive an initial error indicating the
    VT-x is not enabled. This can be reversed and virtualization may be enabled in
    the BIOS during a reboot.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To learn more about Oracle VirtualBox and decide whether or not it is a good
    fit, visit the following website and select Windows hosts to begin the download
    process: [https://www.virtualbox.org/wiki/Downloads](https://www.virtualbox.org/wiki/Downloads).
  prefs: []
  type: TYPE_NORMAL
- en: Installing and configuring Ubuntu Desktop for Google Cloud Platform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Previously, we saw how Ubuntu Desktop could be set up locally using VMWare Fusion.
    In this section, we will learn how to do the same on **Google Cloud Platform**.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The only requirement is a Google account username. Begin by logging in to your
    Google Cloud Platform using your Google account. Google provides a free 12-month
    subscription with $300 credited to your account. The setup will ask for your bank
    details; however, Google will not charge you for anything without explicitly letting
    you know first. Go ahead and verify your bank account and you are good to go.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Follow the steps in the recipe to configure Ubuntu Desktop for Google Cloud
    Platform:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once logged in to your Google Cloud Platform, access a dashboard that looks
    like the one in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/20aa4814-55f9-4f4d-9983-1f90bcacf666.png)'
  prefs: []
  type: TYPE_IMG
- en: Google Cloud Platform Dashboard
  prefs: []
  type: TYPE_NORMAL
- en: 'First, click on the product services button in the top-left-hand corner of
    your screen. In the drop-down menu, under Compute, click on VM instances, as shown
    in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/76ecccfc-6e47-4c0b-9f1f-7184d9a438da.png)'
  prefs: []
  type: TYPE_IMG
- en: Create a new instance and name it. We are naming it `ubuntuvm1` in our case.
    Google Cloud automatically creates a project while launching an instance and the
    instance will be launched under a project ID. The project may be renamed if required.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After clicking on **Create Instance**, select the zone/area you are located
    in.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Ubuntu 16.04LTS** under the boot disk as this is the operating system
    that will be installed in the cloud. Please note that LTS stands for version,
    and will have long-term support from Ubuntu’s developers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, under the boot disk options, select SSD persistent disk and increase
    the size to 50 GB for some added storage space for the instance, as shown in the
    following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/83579b58-9f04-47bf-a163-122c32d5c7ab.png)'
  prefs: []
  type: TYPE_IMG
- en: Next, set Access scopes to **Allow full access to all Cloud APIs**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Under firewall, please check to **allow HTTP traffic** as well as **allow HTTPS
    traffic**, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/78a55da5-aaab-47a9-9b2a-ecce5f968e63.png)'
  prefs: []
  type: TYPE_IMG
- en: Selecting options  Allow HTTP traffic and HTTPS Traffic
  prefs: []
  type: TYPE_NORMAL
- en: Once the instance is configured as shown in this section, go ahead and create
    the instance by clicking on the Create button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After clicking on the Create button, you will notice that the instance gets
    created with a unique internal as well as external IP address. We will require
    this at a later stage. SSH refers to secure shell tunnel, which is basically an
    encrypted way of communicating in client-server architectures. Think of it as
    data going to and from your laptop, as well as going to and from Google's cloud
    servers, through an encrypted tunnel.
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the newly created instance. From the drop-down menu, click on **open
    in browser window**, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/ce7e2d0d-b7ab-43e3-bc6f-b704754b2e87.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You will see that Google opens up a shell/terminal in a new window, as shown
    in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/0bc13f24-6ab6-4142-af80-ec149762f03f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once the shell is open, you should have a window that looks like the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/0f75c9ed-6771-467c-90aa-a142fda0531d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Type the following commands in the Google cloud shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'When presented with a prompt to continue or not, type `y` and select ENTER, as
    shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/5f678a2c-6ce3-4ce1-a8f2-d4e8e1d25159.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once done with the preceding steps, type the following commands to set up the
    `vncserver` and allow connections to the local shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, launch the server by typing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This will prompt you to enter a password, which will later be used to log in
    to the Ubuntu Desktop virtual machine. This password is limited to eight characters
    and needs to be set and verified, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/3fd3e60e-3169-458d-b65c-9d7fd18d986f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A startup script is automatically generated by the shell, as shown in the following
    screenshot. This startup script can be accessed and edited by copying and pasting
    its `PATH` in the following manner:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/bf39c16f-7c13-4754-9580-00e19e8cd583.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In our case, the command to view and edit the script is:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This `PATH` may be different in each case. Ensure you set the right `PATH`.
    The `vim` command opens up the script in the text editor on a Mac.
  prefs: []
  type: TYPE_NORMAL
- en: The local shell generated a startup script as well as a log file. The startup
    script needs to be opened and edited in a text editor, which will be discussed
    next.
  prefs: []
  type: TYPE_NORMAL
- en: 'After typing the `vim` command, the screen with the startup script should look
    something like this screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/7977693a-2615-449d-8586-da3e72be7ef3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Type `i` to enter `INSERT` mode. Next, delete all the text in the startup script.
    It should then look like the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/1a731d52-da18-4838-b240-ed8541ce3ca5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Copy paste the following code into the startup script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The script should appear in the editor, as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/5d563f4a-e5cd-4c69-b378-d830fbdc2d16.png)'
  prefs: []
  type: TYPE_IMG
- en: Press Esc to exit out of `INSERT` mode and type `:wq` to write and quit the
    file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once the startup script has been configured, type the following command in
    the Google shell to kill the server and save the changes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This command should produce a process ID that looks like the one in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/3f1637f0-1ff1-44bc-9977-12b159852754.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Start the server again by typing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The next series of steps will focus on securing the shell tunnel into the Google
    Cloud instance from the local host. Before typing anything on the local shell/terminal,
    ensure that Google Cloud is installed. If not already installed, do so by following
    the instructions in this quick-start guide located at the following website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://cloud.google.com/sdk/docs/quickstart-mac-os-x](https://cloud.google.com/sdk/docs/quickstart-mac-os-x)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once Google Cloud is installed, open up the terminal on your machine and type
    the following commands to connect to the Google Cloud compute instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Ensure that the instance name, project ID, and zone are specified correctly
    in the preceding commands. On pressing ENTER, the output on the local shell changes
    to what is shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/224843c1-1ecf-4dd6-adbe-cd86313046a1.png)'
  prefs: []
  type: TYPE_IMG
- en: Once you see the name of your instance followed by `":~$"`, it means that a
    connection has successfully been established between the local host/laptop and
    the Google Cloud instance. After successfully SSHing into the instance, we require
    software called **VNC Viewer** to view and interact with the Ubuntu Desktop that
    has now been successfully set up on the Google Cloud Compute engine. The following
    few steps will discuss how this is achieved.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'VNC Viewer may be downloaded using the following link:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://www.realvnc.com/en/connect/download/viewer/](https://www.realvnc.com/en/connect/download/viewer/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once installed, click to open VNC Viewer and in the search bar, type in `localhost::5901`,
    as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/3b2c57ec-d2d4-4125-9d39-531aade0352f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, click on **continue** when prompted with the following screen:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/59f9d2d6-3490-451c-bd76-ec7a39ace528.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This will prompt you to enter your password for the virtual machine. Enter
    the password that you set earlier while launching the `tightvncserver` command
    for the first time, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/0180764b-79ed-4f45-8b9f-3e8f0fd0beb4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You will finally be taken into the desktop of your Ubuntu virtual machine on
    Google Cloud Compute. Your Ubuntu Desktop screen must now look something like
    the following screenshot when viewed on VNC Viewer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/b8b46dba-ef51-4461-8370-582f5e3ac544.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You have now successfully set up VNC Viewer for interactions with the Ubuntu
    virtual machine/desktop. Anytime the Google Cloud instance is not in use, it is
    recommended to suspend or shut down the instance so that additional costs are
    not being incurred. The cloud approach is optimal for developers who may not have
    access to physical resources with high memory and storage.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While we discussed Google Cloud as a cloud option for Spark,  it is possible
    to leverage Spark on the following cloud platforms as well:'
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft Azure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Web Services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to learn more about Google Cloud Platform and sign up for a free subscription,
    visit the following website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://cloud.google.com/](https://cloud.google.com/)'
  prefs: []
  type: TYPE_NORMAL
- en: Installing and configuring Spark and prerequisites on Ubuntu Desktop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before Spark can get up and running, there are some necessary prerequisites
    that need to be installed on a newly minted Ubuntu Desktop. This section will
    focus on installing and configuring the following on Ubuntu Desktop:'
  prefs: []
  type: TYPE_NORMAL
- en: Java 8 or higher
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anaconda
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The only requirement for this section is having administrative rights to install
    applications onto the Ubuntu Desktop.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section walks through the steps in the recipe to install Python 3, Anaconda,
    and Spark on Ubuntu Desktop:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install Java on Ubuntu through the terminal application, which can be found
    by searching for the app and then locking it to the launcher on the left-hand
    side, as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/24db8263-019c-4cf5-a627-c589a21012c3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Perform an initial test for Java on the virtual machine by executing the following
    command at the terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following four commands at the terminal to install Java:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'After accepting the necessary license agreements for Oracle, perform a secondary
    test of Java on the virtual machine by executing `java -version` once again in
    the terminal. A successful installation for Java will display the following outcome
    in the terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, install the most recent version of Anaconda. Current versions of Ubuntu
    Desktop come preinstalled with Python. While it is convenient that Python comes
    preinstalled with Ubuntu, the installed version is for Python 2.7, as seen in
    the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The current version of Anaconda is v4.4 and the current version of Python 3
    is v3.6\. Once downloaded, view the Anaconda installation file by accessing the
    `Downloads` folder using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Once in the `Downloads` folder, initiate the installation for Anaconda by executing
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Please note that the version of Anaconda, as well as any other software installed,
    may differ as newer updates are released to the public. The version of Anaconda
    that we are using in this chapter and in this book can be downloaded from [https://repo.continuum.io/archive/Anaconda3-4.4.0-Linux-x86.sh](https://repo.continuum.io/archive/Anaconda3-4.4.0-Linux-x86.sh)
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the Anaconda installation is complete, restart the Terminal application
    to confirm that Python 3 is now the default Python environment through Anaconda
    by executing `python --version` in the terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The Python 2 version is still available under Linux, but will require an explicit
    call when executing a script, as seen in the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Visit the following website to begin the Spark download and installation process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://spark.apache.org/downloads.html](https://spark.apache.org/downloads.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Select the download link. The following file will be downloaded to the `Downloads`
    folder in Ubuntu:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`spark-2.2.0-bin-hadoop2.7.tgz`'
  prefs: []
  type: TYPE_NORMAL
- en: 'View the file at the terminal level by executing the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the `tgz` file by executing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Another look at the Downloads directory using `ls` shows both the `tgz` file
    and the extracted folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Move the extracted folder from the `Downloads` folder to the `Home` folder
    by executing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the `spark-2.2.0-bin-hadoop2.7` folder has been moved to the **Home**
    folder, which can be viewed when selecting the **Files** icon on the left-hand
    side toolbar, as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/e0cfa68f-3713-472f-adfd-1d929b47b819.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Spark is now installed. Initiate Spark from the terminal by executing the following
    script at the terminal level:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform a final test to ensure Spark is up and running at the terminal by executing
    the following command to ensure that the `SparkContext` is driving the cluster
    in the local environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section explains the reasoning behind the installation process for Python,
    Anaconda, and Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Spark runs on the **Java virtual machine** (**JVM**), the Java **Software Development
    Kit** (**SDK**) is a prerequisite installation for Spark to run on an Ubuntu virtual
    machine.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In order for Spark to run on a local machine or in a cluster, a minimum version
    of Java 6 is required for installation.
  prefs: []
  type: TYPE_NORMAL
- en: Ubuntu recommends the `sudo apt install` method for Java as it ensures that
    packages downloaded are up to date.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Please note that if Java is not currently installed, the output in the terminal
    will show the following message:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: While Python 2 is fine, it is considered legacy Python. Python 2 is facing an
    end of life date in 2020; therefore, it is recommended that all new Python development
    be performed with Python 3, as will be the case in this publication. Up until
    recently, Spark was only available with Python 2\. That is no longer the case.
    Spark works with both Python 2 and 3. A convenient way to install Python 3, as
    well as many dependencies and libraries, is through Anaconda. Anaconda is a free
    and open source distribution of Python, as well as R. Anaconda manages the installation
    and maintenance of many of the most common packages used in Python for data science-related
    tasks.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'During the installation process for Anaconda, it is important to confirm the
    following conditions:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Anaconda is installed in the `/home/username/Anaconda3` location
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The Anaconda installer prepends the Anaconda3 install location to a `PATH` in `/home/username/.bashrc`
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: After Anaconda has been installed, download Spark. Unlike Python, Spark does
    not come preinstalled on Ubuntu and therefore, will need to be downloaded and
    installed.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For the purposes of development with deep learning, the following preferences
    will be selected for Spark:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Spark release**: **2.2.0** (Jul 11 2017)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Package type**: Prebuilt for Apache Hadoop 2.7 and later'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Download type**: Direct download'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Once Spark has been successfully installed, the output from executing Spark
    at the command line should look something similar to that shown in the following
    screenshot:![](img/3c3ad567-d964-46d1-a678-5269c0d0f49b.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Two important features to note when initializing Spark are that it is under
    the `Python 3.6.1` | `Anaconda 4.4.0 (64-bit)` | framework and that the Spark
    logo is version 2.2.0.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Congratulations! Spark is successfully installed on the local Ubuntu virtual
    machine. But, not everything is complete. Spark development is best when Spark
    code can be executed within a Jupyter notebook, especially for deep learning.
    Thankfully, Jupyter has been installed with the Anaconda distribution performed
    earlier in this section.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
- en: You may be asking why we did not just use `pip install pyspark` to use Spark
    in Python. Previous versions of Spark required going through the installation
    process that we did in this section. Future versions of Spark, starting with 2.2.0
    will begin to allow installation directly through the `pip` approach. We used
    the full installation method in this section to ensure that you will be able to
    get Spark installed and fully-integrated, in case you are using an earlier version
    of Spark.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about Jupyter notebooks and their integration with Python, visit
    the following website:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[http://jupyter.org](http://jupyter.org)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To learn more about Anaconda and download a version for Linux, visit the following
    website:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[https://www.anaconda.com/download/](https://www.anaconda.com/download/).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Integrating Jupyter notebooks with Spark
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
- en: When learning Python for the first time, it is useful to use Jupyter notebooks
    as an **interactive developing environment** (**IDE**). This is one of the main
    reasons why Anaconda is so powerful. It fully integrates all of the dependencies
    between Python and Jupyter notebooks. The same can be done with PySpark and Jupyter
    notebooks. While Spark is written in Scala, PySpark allows for the translation
    of code to occur within Python instead.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of the work in this section will just require accessing the `.bashrc` script
    from the terminal.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
- en: 'PySpark is not configured to work within Jupyter notebooks by default, but
    a slight tweak of the `.bashrc` script can remedy this issue. We will walk through
    these steps in this section:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Access the `.bashrc` script by executing the following command:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Scrolling all the way to the end of the script should reveal the last command
    modified, which should be the `PATH` set by Anaconda during the installation earlier
    in the previous section. The `PATH` should appear as seen in the following:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Underneath, the `PATH` added by the Anaconda installer can include a custom
    function that helps communicate the Spark installation with the Jupyter notebook
    installation from Anaconda3\. For the purposes of this chapter and remaining chapters,
    we will name that function `sparknotebook`. The configuration should appear as
    the following for `sparknotebook()`:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The updated `.bashrc` script should look like the following once saved:![](img/8e10315b-22c1-42d2-a4cf-634a382cae47.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Save and exit from the `.bashrc` file. It is recommended to communicate that
    the `.bashrc` file has been updated by executing the following command and restarting
    the terminal application:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
- en: Our goal in this section is to integrate Spark directly into a Jupyter notebook
    so that we are not doing our development at the terminal and instead utilizing
    the benefits of developing within a notebook. This section explains how the Spark
    integration within a Jupyter notebook takes place.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We will create a command function, `sparknotebook`, that we can call from the
    terminal to open up a Spark session through Jupyter notebooks from the Anaconda
    installation. This requires two settings to be set in the `.bashrc` file:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: PySpark Python be set to python 3
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: PySpark driver for python to be set to Jupyter
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `sparknotebook` function can now be accessed directly from the terminal
    by executing the following command:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The function should then initiate a brand new Jupyter notebook session through
    the default web browser. A new Python script within Jupyter notebooks with a `.ipynb` extension
    can be created by clicking on the New button on the right-hand side and by selecting Python
    3 under Notebook: as seen in the following screenshot:![](img/759bfb12-f3d8-470b-a1b5-b5d6cfdb3508.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Once again, just as was done at the terminal level for Spark, a simple script
    of `sc` will be executed within the notebook to confirm that Spark is up and running
    through Jupyter:![](img/1fca9542-b5e1-4e76-8c72-63d72fcaa4ec.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Ideally, the Version, Master, and AppName should be identical to the earlier
    output when `sc` was executed at the terminal. If this is the case, then PySpark
    has been successfully installed and configured to work with Jupyter notebooks.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
- en: It is important to note that if we were to call a Jupyter notebook through the
    terminal without specifying `sparknotebook`, our Spark session will never be initiated
    and we will receive an error when executing the `SparkContext` script.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We can access a traditional Jupyter notebook by executing the following at
    the terminal:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once we start the notebook, we can try and execute the same script for `sc.master`
    as we did previously, but this time we will receive the following error:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/fcbadbb7-4201-41b4-8440-6208053013ee.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are many managed offerings online of companies offering Spark through
    a notebook interface where the installation and configuration of Spark with a
    notebook have already been managed for you. These are the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Hortonworks ([https://hortonworks.com/](https://hortonworks.com/))
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloudera ([https://www.cloudera.com/](https://www.cloudera.com/))
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: MapR ([https://mapr.com/](https://mapr.com/))
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: DataBricks ([https://databricks.com/](https://mapr.com/))
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Starting and configuring a Spark cluster
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
- en: For most chapters, one of the first things that we will do is to initialize
    and configure our Spark cluster.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
- en: Import the following before initializing cluster.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`from pyspark.sql import SparkSession`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
- en: This section walks through the steps to initialize and configure a Spark cluster.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Import `SparkSession` using the following script:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Configure `SparkSession` with a variable named `spark` using the following
    script:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
- en: This section explains how the `SparkSession` works as an entry point to develop
    within Spark.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Staring with Spark 2.0, it is no longer necessary to create a `SparkConf` and
    `SparkContext` to begin development in Spark. Those steps are no longer needed
    as importing `SparkSession` will handle initializing a cluster.  Additionally,
    it is important to note that `SparkSession` is part of the `sql` module from `pyspark`.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can assign properties to our `SparkSession`:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`master`: assigns the Spark master URL to run on our `local` machine with the
    maximum available number of cores.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`appName`: assign a name for the application'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`config`: assign `6gb` to the `spark.executor.memory`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`getOrCreate`: ensures that a `SparkSession` is created if one is not available
    and retrieves an existing one if it is available'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
- en: For development purposes, while we are building an application on smaller datasets,
    we can just use `master("local")`.  If we were to deploy on a production environment,
    we would want to specify `master("local[*]")` to ensure we are using the maximum
    cores available and get optimal performance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about `SparkSession.builder`, visit the following website:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[https://spark.apache.org/docs/2.2.0/api/java/org/apache/spark/sql/SparkSession.Builder.html](https://spark.apache.org/docs/2.2.0/api/java/org/apache/spark/sql/SparkSession.Builder.html)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Stopping a Spark cluster
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
- en: Once we are done developing on our cluster, it is ideal to shut it down and
    preserve resources.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
- en: This section walks through the steps to stop the `SparkSession`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Execute the following script:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`spark.stop()`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Confirm that the session has closed by executing the following script:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`sc.master`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
- en: This section explains how to confirm that a Spark cluster has been shut down.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If the cluster has been shut down, you will receive the error message seen in
    the following screenshot when executing another Spark command in the notebook:![](img/803973af-d603-4362-a9bb-770ad84e903c.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
- en: Shutting down Spark clusters may not be as critical when working in a local
    environment; however, it will prove costly when Spark is deployed in a cloud environment
    where you are charged for compute power.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
