<html><head></head><body>
<div id="sbo-rt-content"><section class="level2 chapterHead" id="chapter-5-tabular-learning-and-the-bellman-equation">
<h1 class="chapterNumber">5</h1>
<h1 class="chapterTitle" id="sigil_toc_id_408">
<span id="x1-820005"/>Tabular Learning and the Bellman Equation
    </h1>
<p>In the previous chapter, you became acquainted with your first <span class="cmbx-10x-x-109">reinforcement</span> <span class="cmbx-10x-x-109">learning </span>(<span class="cmbx-10x-x-109">RL</span>) algorithm, the <span id="dx1-82001"/>cross-entropy method, along with its strengths and weaknesses. In this new part of the book, we will look at another group of methods that has much more flexibility and power: Q-learning. This chapter will establish the required background shared by those methods.</p>
<p>We will also revisit the FrozenLake environment and explore how new concepts fit with this environment and help us to address issues related to its uncertainty.</p>
<p>In this chapter, we will:</p>
<ul>
<li>
<p>Review the value of the state and the value of the action, and learn how to calculate them in simple cases</p>
</li>
<li>
<p>Talk about the Bellman equation and how it establishes the optimal policy if we know the values of states</p>
</li>
<li>
<p>Discuss the value iteration method and try it on the FrozenLake environment</p>
</li>
<li>
<p>Do the same for the Q-iteration method</p>
</li>
</ul>
<p>Despite the simplicity of the environments in this chapter, it establishes the required preparation for deep Q-learning, which is a very powerful and generic RL method.</p>
<section class="level3 sectionHead" id="value-state-and-optimality">
<h1 class="heading-1" id="sigil_toc_id_73"> <span id="x1-830005.1"/>Value, state, and optimality</h1>
<p>You may <span id="dx1-83001"/>remember <span id="dx1-83002"/>our definition<span id="dx1-83003"/> of the value of the state from <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="ch005.xhtml#x1-190001"><span class="cmti-10x-x-109">1</span></a>. This is a very important notion and the time has come to explore it further.</p>
<p>This whole part of the book is built around the value of the state and how to approximate it. We defined this value as an expected total reward (optionally discounted) that is obtainable from the state. In a formal way, the value of the state is given by</p>
<div class="math-display">
<img alt="œÄ (a |s) = P[At = a|St = s] " class="math-display" height="69" src="../Images/eq8.png" width="199"/>
</div>
<p>where <span class="cmmi-10x-x-109">r</span><sub><span class="cmmi-8">t</span></sub> is the local reward obtained at step <span class="cmmi-10x-x-109">t </span>of the episode.</p>
<p>The total reward could be discounted with 0 <span class="cmmi-10x-x-109">&lt; Œ≥ &lt; </span>1 or not discounted (when <span class="cmmi-10x-x-109">Œ≥ </span>= 1); it‚Äôs up to us how to define it. The value is always calculated in terms of some policy that our agent follows. To illustrate this, let‚Äôs consider a very simple environment with three states, as shown in <span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109">¬†</span><a href="#x1-83004r1"><span class="cmti-10x-x-109">5.1</span></a>:</p>
<div class="minipage">
<p><img alt="SsSeSerr=ta=n=n==1r2d3d12t " height="200" src="../Images/B22150_05_01.png" width="300"/> <span id="x1-83004r1"/></p>
<span class="id">Figure¬†5.1: An example of an environment‚Äôs state transition with rewards </span>
</div>
<ol>
<li>
<div id="x1-83006x1">
<p>The agent‚Äôs initial state.</p>
</div>
</li>
<li>
<div id="x1-83008x2">
<p>The final state that the agent is in after executing action ‚Äúright‚Äù from the initial state. The reward obtained from this is 1.</p>
</div>
</li>
<li>
<div id="x1-83010x3">
<p>The final state that the agent is in after action ‚Äúdown.‚Äù The reward obtained from this is 2.</p>
</div>
</li>
</ol>
<p>The <span id="dx1-83011"/>environment is <span id="dx1-83012"/>always deterministic ‚Äî every<span id="dx1-83013"/> action succeeds and we always start from state 1. Once we reach either state 2 or state 3, the episode ends. Now, the question is, what‚Äôs the value of state 1? This question is meaningless without information about our agent‚Äôs behavior or, in other words, its policy. Even in a simple environment, our agent can have an infinite amount of behaviors, each of which will have its own value for state 1. Consider these examples:</p>
<ul>
<li>
<p>Agent always goes right</p>
</li>
<li>
<p>Agent always goes down</p>
</li>
<li>
<p>Agent goes right with a probability of 50% and down with a probability of 50%</p>
</li>
<li>
<p>Agent goes right in 10% of cases and in 90% of cases executes the ‚Äúdown‚Äù action</p>
</li>
</ul>
<p>To demonstrate how the value is calculated, let‚Äôs do it for all the preceding policies:</p>
<ul>
<li>
<p>The value of state 1 in the case of the ‚Äúalways right‚Äù agent is <span class="cmbx-10x-x-109">1.0</span> (every time it goes left, it obtains 1 and the episode ends)</p>
</li>
<li>
<p>For the ‚Äúalways down‚Äù agent, the value of state 1 is <span class="cmbx-10x-x-109">2.0</span></p>
</li>
<li>
<p>For the 50% right/50% down agent, the value is 1<span class="cmmi-10x-x-109">.</span>0<span class="cmsy-10x-x-109">‚ãÖ</span>0<span class="cmmi-10x-x-109">.</span>5+2<span class="cmmi-10x-x-109">.</span>0<span class="cmsy-10x-x-109">‚ãÖ</span>0<span class="cmmi-10x-x-109">.</span>5 = <span class="cmbx-10x-x-109">1</span><span class="cmmib-10x-x-109">.</span><span class="cmbx-10x-x-109">5</span></p>
</li>
<li>
<p>For the 10% right/90% down agent, the value is 1<span class="cmmi-10x-x-109">.</span>0<span class="cmsy-10x-x-109">‚ãÖ</span>0<span class="cmmi-10x-x-109">.</span>1+2<span class="cmmi-10x-x-109">.</span>0<span class="cmsy-10x-x-109">‚ãÖ</span>0<span class="cmmi-10x-x-109">.</span>9 = <span class="cmbx-10x-x-109">1</span><span class="cmmib-10x-x-109">.</span><span class="cmbx-10x-x-109">9</span></p>
</li>
</ul>
<p>Now, another question: what‚Äôs the optimal policy for this agent? The goal of RL is to get as much total reward as possible. For this one-step environment, the total reward is equal to the value of state 1, which, obviously, is at the maximum at policy 2 (always down).</p>
<p>Unfortunately, such simple environments with an obvious optimal policy are not that interesting in practice. For interesting environments, the optimal policies are much harder to formulate and it‚Äôs even harder to prove their optimality. However, don‚Äôt worry; we are moving toward the point when we will be able to make computers learn the optimal behavior on their own.</p>
<p>From the preceding example, you may have a false impression that we should always take the action with the highest reward. In general, it‚Äôs not that simple. To demonstrate this, let‚Äôs extend our preceding environment <span id="dx1-83014"/>with yet another state that <span id="dx1-83015"/>is reachable<span id="dx1-83016"/> from state 3. State 3 is no longer a terminal state but a transition to state 4, with a bad reward of -20. Once we have chosen the ‚Äúdown‚Äù action in state 1, this bad reward is unavoidable, as from state 3, we have only one exit to state 4. So, it‚Äôs a trap for the agent, which has decided that ‚Äúbeing greedy‚Äù is a good strategy.</p>
<div class="minipage">
<p><img alt="SsSeSSerrr=ta=n==n===1r2d34d12‚àít20 " height="200" src="../Images/B22150_05_02.png" width="300"/> <span id="x1-83017r2"/></p>
<span class="id">Figure¬†5.2: The same environment, with an extra state added </span>
</div>
<p>With that addition, our values for state 1 will be calculated this way:</p>
<ul>
<li>
<p>The ‚Äúalways right‚Äù agent is the same: <span class="cmbx-10x-x-109">1.0</span></p>
</li>
<li>
<p>The ‚Äúalways down‚Äù agent gets 2<span class="cmmi-10x-x-109">.</span>0 + (<span class="cmsy-10x-x-109">‚àí</span>20) = <span class="cmbsy-10x-x-109">‚àí</span><span class="cmbx-10x-x-109">18</span></p>
</li>
<li>
<p>The 50%/50% agent gets 0<span class="cmmi-10x-x-109">.</span>5 <span class="cmsy-10x-x-109">‚ãÖ </span>1<span class="cmmi-10x-x-109">.</span>0 + 0<span class="cmmi-10x-x-109">.</span>5 <span class="cmsy-10x-x-109">‚ãÖ </span>(2<span class="cmmi-10x-x-109">.</span>0 + (<span class="cmsy-10x-x-109">‚àí</span>20)) = <span class="cmbsy-10x-x-109">‚àí</span><span class="cmbx-10x-x-109">8</span><span class="cmmib-10x-x-109">.</span><span class="cmbx-10x-x-109">5</span></p>
</li>
<li>
<p>The 10%/90% agent gets 0<span class="cmmi-10x-x-109">.</span>1 <span class="cmsy-10x-x-109">‚ãÖ </span>1<span class="cmmi-10x-x-109">.</span>0 + 0<span class="cmmi-10x-x-109">.</span>9 <span class="cmsy-10x-x-109">‚ãÖ </span>(2<span class="cmmi-10x-x-109">.</span>0 + (<span class="cmsy-10x-x-109">‚àí</span>20)) = <span class="cmbsy-10x-x-109">‚àí</span><span class="cmbx-10x-x-109">16</span><span class="cmmib-10x-x-109">.</span><span class="cmbx-10x-x-109">1</span></p>
</li>
</ul>
<p>So, the best policy for this new environment is now policy 1: always go right. We spent <span id="dx1-83018"/>some time discussing na√Øve and trivial environments so that you realize the complexity of this optimality problem and can appreciate the results of Richard Bellman better. Bellman was an American <span id="dx1-83019"/>mathematician<span id="dx1-83020"/> who formulated and proved his famous Bellman equation. We will talk about it in the next section.</p>
</section>
<section class="level3 sectionHead" id="the-bellman-equation-of-optimality">
<h1 class="heading-1" id="sigil_toc_id_74"> <span id="x1-840005.2"/>The Bellman equation of optimality</h1>
<p>To explain <span id="dx1-84001"/>the <span id="dx1-84002"/>Bellman equation, it‚Äôs better to go a bit abstract. Don‚Äôt be afraid; I‚Äôll provide concrete examples later to support your learning! Let‚Äôs start with a deterministic case, when all our actions have a 100% guaranteed outcome. Imagine that our agent observes state <span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">0</span></sub> and has <span class="cmmi-10x-x-109">N </span>available actions. Every action leads to another state, <span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">‚Ä¶</span><span class="cmmi-10x-x-109">s</span><sub><span class="cmmi-8">N</span></sub>, with a respective reward, <span class="cmmi-10x-x-109">r</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">‚Ä¶</span><span class="cmmi-10x-x-109">r</span><sub><span class="cmmi-8">N</span></sub>. Also, assume that we know the values, <span class="cmmi-10x-x-109">V</span> <sub><span class="cmmi-8">i</span></sub>, of all states connected to state <span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">0</span></sub>. What will be the best course of action that the agent can take in such a state?</p>
<div class="minipage">
<p><img alt="rrrr====rrrr123N sssssaaaa0123N====VVVV123N123N " height="200" src="../Images/B22150_05_03.png" width="300"/> <span id="x1-84003r3"/></p>
<span class="id">Figure¬†5.3: An abstract environment with <span class="cmmi-10x-x-109">N </span>states reachable from the initial state </span>
</div>
<p>If we choose the concrete action, <span class="cmmi-10x-x-109">a</span><sub><span class="cmmi-8">i</span></sub>, and calculate the value given to this action, then the value will be <span class="cmmi-10x-x-109">V</span> <sub><span class="cmr-8">0</span></sub>(<span class="cmmi-10x-x-109">a </span>= <span class="cmmi-10x-x-109">a</span><sub><span class="cmmi-8">i</span></sub>) = <span class="cmmi-10x-x-109">r</span><sub><span class="cmmi-8">i</span></sub> + <span class="cmmi-10x-x-109">V</span> <sub><span class="cmmi-8">i</span></sub>. So, to choose the best possible action, the agent needs to calculate the resulting values for every action and choose the maximum possible outcome. In other words, <span class="cmmi-10x-x-109">V</span> <sub><span class="cmr-8">0</span></sub> = max<sub><span class="cmmi-8">a</span><span class="cmsy-8">‚àà</span><span class="cmr-8">1</span><span class="cmmi-8">‚Ä¶</span><span class="cmmi-8">N</span></sub>(<span class="cmmi-10x-x-109">r</span><sub><span class="cmmi-8">a</span></sub> + <span class="cmmi-10x-x-109">V</span> <sub><span class="cmmi-8">a</span></sub>). If we are using the discount factor, <span class="cmmi-10x-x-109">Œ≥</span>, we need to multiply the value of the next state by gamma: <span class="cmmi-10x-x-109">V</span> <sub><span class="cmr-8">0</span></sub> = max<sub><span class="cmmi-8">a</span><span class="cmsy-8">‚àà</span><span class="cmr-8">1</span><span class="cmmi-8">‚Ä¶</span><span class="cmmi-8">N</span></sub>(<span class="cmmi-10x-x-109">r</span><sub><span class="cmmi-8">a</span></sub> + <span class="cmmi-10x-x-109">Œ≥V</span> <sub><span class="cmmi-8">a</span></sub>).</p>
<p>This may look very similar to our greedy example from the previous section, and, in fact, it is. However, there is one difference: when we act greedily, we do not only look at the immediate reward for the action, but at the immediate reward plus the long-term value of the state. This allows us to avoid a possible trap with a large immediate reward but a state that has a bad value.</p>
<p>Bellman proved that with that extension, our behavior will get the best possible outcome. In other words, it will be optimal. So, the preceding equation is called the Bellman equation of value (for a deterministic case).</p>
<p>It‚Äôs not very complicated to extend this idea for a stochastic case, when our actions have the chance of ending up in different states. What we need to do is calculate the expected value for every action, instead of just taking the value of the next state. To illustrate this, let‚Äôs consider one single action available from state <span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">0</span></sub>, with three possible outcomes:</p>
<div class="minipage">
<p><img alt="rrr===rrr 123 ssssappp0123=123VVV1123 " height="200" src="../Images/B22150_05_04.png" width="300"/> <span id="x1-84004r4"/></p>
<span class="id">Figure¬†5.4: An example of the transition from the state in a stochastic case </span>
</div>
<p>Here, we have<span id="dx1-84005"/> one <span id="dx1-84006"/>action, which can lead to three different states with different probabilities. With probability <span class="cmmi-10x-x-109">p</span><sub><span class="cmr-8">1</span></sub>, the action can end up in state <span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">1</span></sub>, with <span class="cmmi-10x-x-109">p</span><sub><span class="cmr-8">2</span></sub> in state <span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">2</span></sub>, and with <span class="cmmi-10x-x-109">p</span><sub><span class="cmr-8">3</span></sub> in state <span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">3</span></sub> (<span class="cmmi-10x-x-109">p</span><sub><span class="cmr-8">1</span></sub> + <span class="cmmi-10x-x-109">p</span><sub><span class="cmr-8">2</span></sub> + <span class="cmmi-10x-x-109">p</span><sub><span class="cmr-8">3</span></sub> = 1, of course). Every target state has its own reward (<span class="cmmi-10x-x-109">r</span><sub><span class="cmr-8">1</span></sub>, <span class="cmmi-10x-x-109">r</span><sub><span class="cmr-8">2</span></sub>, or <span class="cmmi-10x-x-109">r</span><sub><span class="cmr-8">3</span></sub>). To calculate the expected value after issuing action 1, we need to sum all values, multiplied by their probabilities:</p>
<div class="math-display">
<img alt="œÄ (a |s) = P[At = a|St = s] " class="math-display" height="23" src="../Images/eq9.png" width="552"/>
</div>
<p>or, more formally</p>
<div class="math-display">
<img alt="œÄ (a |s) = P[At = a|St = s] " class="math-display" height="63" src="../Images/eq10.png" width="601"/>
</div>
<p>Here, <span class="msbm-10x-x-109">ùîº</span> <sub><span class="cmmi-8">s</span><span class="cmsy-8">‚àº</span><span class="cmmi-8">S</span></sub> means taking the expected value over all states in our state space, <span class="cmmi-10x-x-109">S</span>.</p>
<p>By combining the Bellman equation, for a deterministic case, with a value for stochastic actions, we get the Bellman optimality equation for a general case:</p>
<div class="math-display">
<img alt="œÄ (a |s) = P[At = a|St = s] " class="math-display" height="50" src="../Images/eq11.png" width="543"/>
</div>
<p>Note that <span class="cmmi-10x-x-109">p</span><sub><span class="cmmi-8">a,i</span><span class="cmsy-8">‚Üí</span><span class="cmmi-8">j</span></sub> means the probability of action <span class="cmmi-10x-x-109">a</span>, issued in state <span class="cmmi-10x-x-109">i</span>, ending up in state <span class="cmmi-10x-x-109">j</span>. The interpretation is still the same: the optimal value of the state corresponds to the action, which gives us the maximum possible expected immediate reward, plus the discounted long-term reward for the next state. You may also notice that this definition is recursive: the value of the state is defined via the values of the immediately reachable states. This recursion may look like cheating: we define some value, pretending that we already know it. However, this is a very powerful and common technique in computer science and even in math in general (proof by induction is based on the same trick). This Bellman equation is a foundation not only in RL but also in much more general dynamic programming, which is a widely used method for solving practical optimization problems.</p>
<p>These values not only give us the best reward that we can obtain, but they basically give us the optimal policy to obtain that reward: if our agent knows the value for every state, then it automatically knows how to gather this reward. Thanks to Bellman‚Äôs optimality proof, at every state the agent ends up in, it needs to select the action with the maximum expected reward, which is a sum of the immediate reward and the one-step discounted long-term reward ‚Äì that‚Äôs it. So, those values are really useful to know. Before you get familiar with a practical way<span id="dx1-84007"/> to calculate<span id="dx1-84008"/> them, I need to introduce one more mathematical notation. It‚Äôs not as fundamental as the value of the state, but we need it for our convenience.</p>
</section>
<section class="level3 sectionHead" id="the-value-of-the-action">
<h1 class="heading-1" id="sigil_toc_id_75"> <span id="x1-850005.3"/>The value of the action</h1>
<p>To make our<span id="dx1-85001"/> life slightly easier, we can define different quantities, in addition to the value of the state, <span class="cmmi-10x-x-109">V </span>(<span class="cmmi-10x-x-109">s</span>), as the value of the action, <span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s,a</span>). Basically, this equals the total reward we can get by executing action <span class="cmmi-10x-x-109">a </span>in state <span class="cmmi-10x-x-109">s </span>and can be defined via <span class="cmmi-10x-x-109">V </span>(<span class="cmmi-10x-x-109">s</span>). Being a much less fundamental entity than <span class="cmmi-10x-x-109">V </span>(<span class="cmmi-10x-x-109">s</span>), this quantity gave a name to the whole family of methods called Q-learning, because it is more convenient. In these methods, our primary objective is to get values of <span class="cmmi-10x-x-109">Q </span>for every pair of state and action:</p>
<div class="math-display">
<img alt="œÄ (a |s) = P[At = a|St = s] " class="math-display" height="50" src="../Images/eq12.png" width="621"/>
</div>
<p><span class="cmmi-10x-x-109">Q</span>, for this state, <span class="cmmi-10x-x-109">s</span>, and action, <span class="cmmi-10x-x-109">a</span>, equals the expected immediate reward and the discounted long-term reward of the destination state. We also can define <span class="cmmi-10x-x-109">V </span>(<span class="cmmi-10x-x-109">s</span>) via <span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s,a</span>):</p>
<div class="math-display">
<img alt="œÄ (a |s) = P[At = a|St = s] " class="math-display" height="36" src="../Images/eq13.png" width="192"/>
</div>
<p>This just means that the value of some state equals to the value of the maximum action we can execute from this state.</p>
<p>Finally, we can express <span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s,a</span>) recursively (which will be used in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="#"><span class="cmti-10x-x-109">6</span></a>):</p>
<div class="math-display">
<img alt="œÄ (a |s) = P[At = a|St = s] " class="math-display" height="37" src="../Images/eq14.png" width="333"/>
</div>
<p>In the last formula, the index on the immediate reward, (<span class="cmmi-10x-x-109">s,a</span>), depends on the environment details:</p>
<ul>
<li>
<p>If the immediate reward is given to us after executing a particular action, <span class="cmmi-10x-x-109">a</span>, from state <span class="cmmi-10x-x-109">s</span>, index (<span class="cmmi-10x-x-109">s,a</span>) is used and the formula is exactly as shown above.</p>
</li>
<li>
<p>But if the reward is provided for reaching some state, <span class="cmmi-10x-x-109">s</span><span class="cmsy-10x-x-109">‚Ä≤</span>, via action <span class="cmmi-10x-x-109">a</span><span class="cmsy-10x-x-109">‚Ä≤</span>, the reward will have the index (<span class="cmmi-10x-x-109">s</span><span class="cmsy-10x-x-109">‚Ä≤</span><span class="cmmi-10x-x-109">,a</span><span class="cmsy-10x-x-109">‚Ä≤</span>) and will need to be moved into the max operator:</p>
<div class="math-display">
<img alt="œÄ (a |s) = P[At = a|St = s] " class="math-display" height="37" src="../Images/eq15.png" width="356"/>
</div>
</li>
</ul>
<p>That difference<span id="dx1-85002"/> is not very significant from a mathematical point of view, but it could be important during the implementation of the methods. The first situation is more common, so we will stick to the preceding formula.</p>
<p>To give you a concrete example, let‚Äôs consider an environment that is similar to FrozenLake, but has a much simpler structure: we have one initial state (<span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">0</span></sub>) surrounded by four target states, <span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">1</span></sub>, <span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">2</span></sub>, <span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">3</span></sub>, <span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">4</span></sub>, with different rewards:</p>
<div class="minipage">
<img alt="s0 ‚Äì initial state ssssss012431,s2,s3,s4 ‚Äì final states " height="300" src="../Images/B22150_05_05.png" width="600"/> <span id="x1-85003r5"/>
<span class="id">Figure¬†5.5: A simplified grid-like environment </span>
</div>
<p>Every action is probabilistic in the same way as in FrozenLake: with a 33% chance that our action will be executed without modifications, but with a 33% chance that we will slip to the left, relatively, of our target cell and a 33% chance that we will slip to the right. For simplicity, we use discount factor <span class="cmmi-10x-x-109">Œ≥ </span>= 1.</p>
<div class="minipage">
<p><img alt="sssssuledr000000000000rrrr01234pfoig.3.3.3.3.3.3.3.3.3.3.3.3====twh3333333333331234nt " height="200" src="../Images/B22150_05_06.png" width="300"/> <span id="x1-85004r6"/></p>
<span class="id">Figure¬†5.6: A transition diagram of the grid environment </span>
</div>
<p>Let‚Äôs calculate the values of the actions to begin with. Terminal states <span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">‚Ä¶</span><span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">4</span></sub> have no outbound connections, so <span class="cmmi-10x-x-109">Q </span>for those states is <span id="dx1-85005"/>zero for all actions. Due to this, the values of the terminal states are equal to their immediate reward (once we get there, our episode ends without any subsequent states): <span class="cmmi-10x-x-109">V</span> <sub><span class="cmr-8">1</span></sub> = 1, <span class="cmmi-10x-x-109">V</span> <sub><span class="cmr-8">2</span></sub> = 2, <span class="cmmi-10x-x-109">V</span> <sub><span class="cmr-8">3</span></sub> = 3, <span class="cmmi-10x-x-109">V</span> <sub><span class="cmr-8">4</span></sub> = 4.</p>
<p>The values of the actions for state 0 are a bit more complicated. Let‚Äôs start with the ‚Äúup‚Äù action. Its value, according to the definition, is equal to the expected sum of the immediate reward plus the long-term value for subsequent steps. We have no subsequent steps for any possible transition for the ‚Äúup‚Äù action:</p>
<div class="math-display">
<img alt="œÄ (a |s) = P[At = a|St = s] " class="math-display" height="23" src="../Images/eq16.png" width="784"/>
</div>
<p>Repeating this for the rest of the <span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">0</span></sub> actions results in the following:</p>
<table class="table-container">
<tbody>
<tr>
<td class="table-cell"><span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">0</span></sub><span class="cmmi-10x-x-109">,left</span>)</td>
<td class="table-cell">= 0<span class="cmmi-10x-x-109">.</span>33 <span class="cmsy-10x-x-109">‚ãÖ</span><span class="cmmi-10x-x-109">V</span> <sub><span class="cmr-8">1</span></sub> + 0<span class="cmmi-10x-x-109">.</span>33 <span class="cmsy-10x-x-109">‚ãÖ</span><span class="cmmi-10x-x-109">V</span> <sub><span class="cmr-8">2</span></sub> + 0<span class="cmmi-10x-x-109">.</span>33 <span class="cmsy-10x-x-109">‚ãÖ</span><span class="cmmi-10x-x-109">V</span> <sub><span class="cmr-8">3</span></sub> = 1<span class="cmmi-10x-x-109">.</span>98</td>
</tr>
<tr>
<td class="table-cell"><span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">0</span></sub><span class="cmmi-10x-x-109">,right</span>)</td>
<td class="table-cell">= 0<span class="cmmi-10x-x-109">.</span>33 <span class="cmsy-10x-x-109">‚ãÖ</span><span class="cmmi-10x-x-109">V</span> <sub><span class="cmr-8">4</span></sub> + 0<span class="cmmi-10x-x-109">.</span>33 <span class="cmsy-10x-x-109">‚ãÖ</span><span class="cmmi-10x-x-109">V</span> <sub><span class="cmr-8">1</span></sub> + 0<span class="cmmi-10x-x-109">.</span>33 <span class="cmsy-10x-x-109">‚ãÖ</span><span class="cmmi-10x-x-109">V</span> <sub><span class="cmr-8">3</span></sub> = 2<span class="cmmi-10x-x-109">.</span>64</td>
</tr>
<tr>
<td class="table-cell"><span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">0</span></sub><span class="cmmi-10x-x-109">,down</span>)</td>
<td class="table-cell">= 0<span class="cmmi-10x-x-109">.</span>33 <span class="cmsy-10x-x-109">‚ãÖ</span><span class="cmmi-10x-x-109">V</span> <sub><span class="cmr-8">3</span></sub> + 0<span class="cmmi-10x-x-109">.</span>33 <span class="cmsy-10x-x-109">‚ãÖ</span><span class="cmmi-10x-x-109">V</span> <sub><span class="cmr-8">2</span></sub> + 0<span class="cmmi-10x-x-109">.</span>33 <span class="cmsy-10x-x-109">‚ãÖ</span><span class="cmmi-10x-x-109">V</span> <sub><span class="cmr-8">4</span></sub> = 2<span class="cmmi-10x-x-109">.</span>97</td>
</tr>
</tbody>
</table>
<p>The final value for state <span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">0</span></sub> is the maximum of those actions‚Äô values, which is 2.97.</p>
<p>Q-values are much more convenient in practice, as for the agent, it‚Äôs much simpler to make decisions about actions based on <span class="cmmi-10x-x-109">Q </span>than on <span class="cmmi-10x-x-109">V </span>. In the case of <span class="cmmi-10x-x-109">Q</span>, to choose the action based on the state, the agent just needs to calculate <span class="cmmi-10x-x-109">Q </span>for all available actions using the current state and choose the action with the largest value of <span class="cmmi-10x-x-109">Q</span>. To do the same using values of the states, the agent needs to know not only the values, but also the probabilities for transitions. In practice, we <span id="dx1-85006"/>rarely know them in advance, so the agent needs to estimate transition probabilities for every action and state pair. Later in this chapter, you will see this in practice by solving the FrozenLake environment both ways. However, to be able to do this, we have one important thing still missing: a general way to calculate <span class="cmmi-10x-x-109">V</span> <sub><span class="cmmi-8">i</span></sub> and <span class="cmmi-10x-x-109">Q</span><sub><span class="cmmi-8">i</span></sub>.</p>
</section>
<section class="level3 sectionHead" id="the-value-iteration-method">
<h1 class="heading-1" id="sigil_toc_id_76"> <span id="x1-860005.4"/>The value iteration method</h1>
<p>In the simplistic<span id="dx1-86001"/> example you just saw, to calculate the values of the states and actions, we exploited the structure of the environment: we had no loops in transitions, so we could start from terminal states, calculate their values, and then proceed to the central state. However, just one loop in the environment builds an obstacle in our approach. Let‚Äôs consider such an environment with two states:</p>
<div class="minipage">
<p><img alt="ssrrŒ≥12==12= 0.9 " height="200" src="../Images/B22150_05_07.png" width="300"/> <span id="x1-86002r7"/></p>
<span class="id">Figure¬†5.7: A sample environment with a loop in the transition diagram </span>
</div>
<p>We start from state <span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">1</span></sub>, and the only action we can take leads us to state <span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">2</span></sub>. We get the reward, <span class="cmmi-10x-x-109">r </span>= 1, and the only transition from <span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">2</span></sub> is an action, which brings us back to <span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">1</span></sub>. So, the life of our agent is an infinite sequence of states [<span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,s</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,s</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,s</span><sub><span class="cmr-8">2</span></sub><span class="cmmi-10x-x-109">,</span><span class="cmmi-10x-x-109">‚Ä¶</span>]. To deal with this infinity loop, we can use a discount factor: <span class="cmmi-10x-x-109">Œ≥ </span>= 0<span class="cmmi-10x-x-109">.</span>9. Now, the question is, what are the values for both the states? The answer is not very complicated, in fact. Every transition from <span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">1</span></sub> to <span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">2</span></sub> gives us a reward of 1 and every back transition gives us 2. So, our sequence of rewards will be [1<span class="cmmi-10x-x-109">,</span>2<span class="cmmi-10x-x-109">,</span>1<span class="cmmi-10x-x-109">,</span>2<span class="cmmi-10x-x-109">,</span>1<span class="cmmi-10x-x-109">,</span>2<span class="cmmi-10x-x-109">,</span>1<span class="cmmi-10x-x-109">,</span>2<span class="cmmi-10x-x-109">,</span><span class="cmmi-10x-x-109">‚Ä¶</span>]. As there is only one action available in every state, our agent has no choice, so we can omit the <span class="cmtt-10x-x-109">max </span>operation in formulas (there is only one alternative).</p>
<p>The value for every state will be equal to the infinite sum:</p>
<table class="table-container">
<tbody>
<tr>
<td class="table-cell"><span class="cmmi-10x-x-109">V </span>(<span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">1</span></sub>)</td>
<td class="table-cell">= 1 + <span class="cmmi-10x-x-109">Œ≥</span>(2 + <span class="cmmi-10x-x-109">Œ≥</span>(1 + <span class="cmmi-10x-x-109">Œ≥</span>(2 + <span class="cmmi-10x-x-109">‚Ä¶</span>))) = <span class="cmex-10x-x-109">‚àë</span> <sub><span class="cmmi-8">i</span><span class="cmr-8">=0</span></sub><sup><span class="cmsy-8">‚àû</span></sup>1<span class="cmmi-10x-x-109">Œ≥</span><sup><span class="cmr-8">2</span><span class="cmmi-8">i</span></sup> + 2<span class="cmmi-10x-x-109">Œ≥</span><sup><span class="cmr-8">2</span><span class="cmmi-8">i</span><span class="cmr-8">+1</span></sup></td>
</tr>
<tr>
<td class="table-cell"><span class="cmmi-10x-x-109">V </span>(<span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">2</span></sub>)</td>
<td class="table-cell">= 2 + <span class="cmmi-10x-x-109">Œ≥</span>(1 + <span class="cmmi-10x-x-109">Œ≥</span>(2 + <span class="cmmi-10x-x-109">Œ≥</span>(1 + <span class="cmmi-10x-x-109">‚Ä¶</span>))) = <span class="cmex-10x-x-109">‚àë</span> <sub><span class="cmmi-8">i</span><span class="cmr-8">=0</span></sub><sup><span class="cmsy-8">‚àû</span></sup>2<span class="cmmi-10x-x-109">Œ≥</span><sup><span class="cmr-8">2</span><span class="cmmi-8">i</span></sup> + 1<span class="cmmi-10x-x-109">Œ≥</span><sup><span class="cmr-8">2</span><span class="cmmi-8">i</span><span class="cmr-8">+1</span></sup></td>
</tr>
</tbody>
</table>
<p>Strictly speaking, we can‚Äôt calculate the exact values for our states, but with <span class="cmmi-10x-x-109">Œ≥ </span>= 0<span class="cmmi-10x-x-109">.</span>9, the contribution of every transition quickly decreases over time. For example, after 10 steps, <span class="cmmi-10x-x-109">Œ≥</span><sup><span class="cmr-8">10</span></sup> = 0<span class="cmmi-10x-x-109">.</span>9<sup><span class="cmr-8">10</span></sup> <span class="cmsy-10x-x-109">‚âà </span>0<span class="cmmi-10x-x-109">.</span>349, but after 100 steps, it becomes just 0<span class="cmmi-10x-x-109">.</span>0000266. Due to this, we can stop after 50 iterations and still get quite a precise estimation:</p>
<pre class="lstlisting" id="listing-95"><code>&gt;&gt;&gt; sum([0.9**(2*i) + 2*(0.9**(2*i+1)) for i in range(50)]) 
14.736450674121663 
&gt;&gt;&gt; sum([2*(0.9**(2*i)) + 0.9**(2*i+1) for i in range(50)]) 
15.262752483911719</code></pre>
<p>The preceding<span id="dx1-86007"/> example can be used to get the gist of a more general procedure called the <span class="cmbx-10x-x-109">value iteration algorithm</span>. This allows us to numerically<span id="dx1-86008"/> calculate the values of the states and values of the actions of <span class="cmbx-10x-x-109">Markov</span> <span class="cmbx-10x-x-109">decision processes </span>(<span class="cmbx-10x-x-109">MDPs</span>) with known transition probabilities and rewards. The procedure (for values of the states) includes the following steps:</p>
<ol>
<li>
<div id="x1-86010x1">
<p>Initialize the values of all states, <span class="cmmi-10x-x-109">V</span> <sub><span class="cmmi-8">i</span></sub>, to some initial value (usually zero)</p>
</div>
</li>
<li>
<div id="x1-86012x2">
<p>For every state, <span class="cmmi-10x-x-109">s</span>, in the MDP, perform the Bellman update:</p>
<div class="math-display">
<img alt="œÄ (a |s) = P[At = a|St = s] " class="math-display" height="49" src="../Images/eq17.png" width="339"/>
</div>
</div>
</li>
<li>
<div id="x1-86014x3">
<p>Repeat step 2 for some <span id="dx1-86015"/>large number of steps or until changes become too small</p>
</div>
</li>
</ol>
<p>Okay, so that‚Äôs the theory. In practice, this method has certain obvious limitations. First of all, our state space should be discrete and small enough to perform multiple iterations over all states. This is not an issue for FrozenLake-4x4 and even for FrozenLake-8x8 (it exists in Gym as a more challenging version), but for CartPole, it‚Äôs not totally clear what to do. Our observation for CartPole is four float values, which represent some physical characteristics of the system. Potentially, even a small difference in those values could have an influence on the state‚Äôs value. One of the solutions for that could be discretization of our observation‚Äôs values; for example, we can split the observation space of CartPole into bins and treat every bin as an individual discrete state in space. However, this will create lots of practical problems, such as how large bin intervals should be and how much data from the environment we will need to estimate our values. I will address this issue in subsequent chapters, when we get to the usage of neural networks in Q-learning.</p>
<p>The second practical problem arises from the fact that we rarely know the transition probability for the actions and rewards matrix. Remember the interface provided by Gym to the agent‚Äôs writer: we observe the state, decide on an action, and only then do we get the next observation and reward for the transition. We don‚Äôt know (without peeking into Gym‚Äôs environment code) what the probability is of getting into state <span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">1</span></sub> from state <span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">0</span></sub> by issuing action <span class="cmmi-10x-x-109">a</span><sub><span class="cmr-8">0</span></sub>. What we do have is just the history from the agent‚Äôs interaction with the environment. However, in Bellman‚Äôs update, we need both a reward for every transition and the probability of this transition. So, the obvious answer to this issue is to use our agent‚Äôs experience as an estimation for both unknowns. Rewards could be used as they are. We just need to remember what reward we got on the transition from <span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">0</span></sub> to <span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">1</span></sub> using action <span class="cmmi-10x-x-109">a</span>, but to estimate probabilities, we <span id="dx1-86016"/>need to maintain counters for every tuple (<span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">0</span></sub><span class="cmmi-10x-x-109">,s</span><sub><span class="cmr-8">1</span></sub><span class="cmmi-10x-x-109">,a</span>) and normalize them.</p>
<p>Now that you‚Äôre familiar with the theoretical background, let‚Äôs look at this method in practice.</p>
</section>
<section class="level3 sectionHead" id="value-iteration-in-practice">
<h1 class="heading-1" id="sigil_toc_id_77"> <span id="x1-870005.5"/>Value iteration in practice</h1>
<p>In this section, we will look at how the value <span id="dx1-87001"/>iteration method will work for FrozenLake. The complete example is in <span class="cmtt-10x-x-109">Chapter05/01</span><span class="cmtt-10x-x-109">_frozenlake</span><span class="cmtt-10x-x-109">_v</span><span class="cmtt-10x-x-109">_iteration.py</span>. The central data structures in this example are as follows:</p>
<ul>
<li>
<p><span class="cmbx-10x-x-109">Reward table</span>: A dictionary with the composite key ‚Äúsource state‚Äù + ‚Äúaction‚Äù + ‚Äútarget state.‚Äù The value is obtained from the immediate reward.</p>
</li>
<li>
<p><span class="cmbx-10x-x-109">Transitions table</span>: A dictionary keeping counters of the experienced transitions. The key is the composite ‚Äústate‚Äù + ‚Äúaction,‚Äù and the value is another dictionary that maps the ‚Äútarget state‚Äù into a count of times that we have seen it.</p>
<p>For example, if in state 0 we execute action 1 ten times, after three times, it will lead us to state 4 and after seven times to state 5. Then entry with the key <span class="cmtt-10x-x-109">(0, 1) </span>in this table will be a <span class="cmtt-10x-x-109">dict </span>with the contents <span class="cmsy-10x-x-109">{</span><span class="cmtt-10x-x-109">4: 3, 5: 7</span><span class="cmsy-10x-x-109">}</span>. We can use this table to estimate the probabilities of our transitions.</p>
</li>
<li>
<p><span class="cmbx-10x-x-109">Value table</span>: A dictionary that maps a state into the calculated value of this state.</p>
</li>
</ul>
<p>The overall logic of our code is simple: in the loop, we play 100 random steps from the environment, populating the reward and transition tables. After those 100 steps, we perform a value iteration loop over all states, updating our value table. Then we play several full episodes to check our improvements using the updated value table. If the average reward for those test episodes is above the 0.8 boundary, then we stop training. During the test episodes, we also update our reward and transition tables to use all data from the environment.</p>
<p>Now let‚Äôs come to the code. We first import the used packages and define constants. Then we define several type aliases. They are not necessary, but make our code more readable:</p>
<div class="tcolorbox" id="tcolobox-75">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-96"><code>import typing as tt 
import gymnasium as gym 
from collections import defaultdict, Counter 
from torch.utils.tensorboard.writer import SummaryWriter 
 
ENV_NAME = "FrozenLake-v1" 
GAMMA = 0.9 
TEST_EPISODES = 20</code></pre>
</div>
</div>
<p>For the FrozenLake environment, both observation and action spaces are of the <span class="cmtt-10x-x-109">Box </span>class, so states and actions are represented by <span class="cmtt-10x-x-109">int </span>values. We also define types for our reward and transition tables‚Äô keys. For the reward table, it is a tuple with <span class="cmtt-10x-x-109">[State</span>, <span class="cmtt-10x-x-109">Action</span>, <span class="cmtt-10x-x-109">State] </span>and for the transition table it is <span class="cmtt-10x-x-109">[State</span>, <span class="cmtt-10x-x-109">Action]</span>:</p>
<div class="tcolorbox" id="tcolobox-76">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-97"><code>State = int 
Action = int 
RewardKey = tt.Tuple[State, Action, State] 
TransitKey = tt.Tuple[State, Action]</code></pre>
</div>
</div>
<p>Then we<span id="dx1-87014"/> define the <span class="cmtt-10x-x-109">Agent </span>class, which will keep our tables and contain functions that we will be using in the training loop. In the class constructor, we create the environment that we will be using for data samples, obtain our first observation, and define tables for rewards, transitions, and values:</p>
<div class="tcolorbox" id="tcolobox-77">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-98"><code>class Agent: 
    def __init__(self): 
        self.env = gym.make(ENV_NAME) 
        self.state, _ = self.env.reset() 
        self.rewards: tt.Dict[RewardKey, float] = defaultdict(float) 
        self.transits: tt.Dict[TransitKey, Counter] = defaultdict(Counter) 
        self.values: tt.Dict[State, float] = defaultdict(float)</code></pre>
</div>
</div>
<p>The function <span class="cmtt-10x-x-109">play</span><span class="cmtt-10x-x-109">_n</span><span class="cmtt-10x-x-109">_random</span><span class="cmtt-10x-x-109">_steps </span>is used to gather random experience from the environment and update the reward and transition tables. Note that we don‚Äôt need to wait for the end of the episode to start learning; we just perform <span class="cmmi-10x-x-109">N </span>steps and remember their outcomes. This is one of the differences between value iteration and the cross-entropy method, which can learn only on full episodes:</p>
<div class="tcolorbox" id="tcolobox-78">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-99"><code>    def play_n_random_steps(self, n: int): 
        for _ in range(n): 
            action = self.env.action_space.sample() 
            new_state, reward, is_done, is_trunc, _ = self.env.step(action) 
            rw_key = (self.state, action, new_state) 
            self.rewards[rw_key] = float(reward) 
            tr_key = (self.state, action) 
            self.transits[tr_key][new_state] += 1 
            if is_done or is_trunc: 
                self.state, _ = self.env.reset() 
            else: 
                self.state = new_state</code></pre>
</div>
</div>
<p>The next function (<span class="cmtt-10x-x-109">calc</span><span class="cmtt-10x-x-109">_action</span><span class="cmtt-10x-x-109">_value()</span>) calculates the value of the action from the state using our transition, reward, and values tables. We will use it for two purposes: to select the best action to perform from the state and to calculate the new value of the state on value iteration.</p>
<p>We do the following:</p>
<ol>
<li>
<div id="x1-87035x1">
<p>We extract transition counters for the given state and action from the transition table. Counters in this table have a form of <span class="cmtt-10x-x-109">dict</span>, with target states as the key and a count of experienced transitions as the value. We sum all counters to obtain the total count of times we have executed the action from the state. We will use this total value later to go from an individual counter to probability.</p>
</div>
</li>
<li>
<div id="x1-87037x2">
<p>Then we<span id="dx1-87038"/> iterate every target state that our action has landed on and calculate its contribution to the total action value using the Bellman equation. This contribution is equal to immediate reward plus discounted value for the target state. We multiply this sum to the probability of this transition and add the result to the final action value.</p>
</div>
</li>
</ol>
<p>This logic is illustrated in the following diagram:</p>
<div class="minipage">
<p><img alt="transit[(s,a)] = {s1:c1,s2:c2} total = c1 + c2 sssaccQ1212(s,a) = tco1tal(rs1 + Œ≥Vs1)+ tco2tal(rs2 + Œ≥Vs2) " height="300" src="../Images/B22150_05_08.png" width="600"/> <span id="x1-87039r8"/></p>
<span class="id">Figure¬†5.8: The calculation of the state‚Äôs value </span>
</div>
<p>In the preceding diagram, we do a calculation of the value for state <span class="cmmi-10x-x-109">s </span>and action <span class="cmmi-10x-x-109">a</span>. Imagine that, during our experience, we have executed this action several times (<span class="cmmi-10x-x-109">c</span><sub><span class="cmr-8">1</span></sub> + <span class="cmmi-10x-x-109">c</span><sub><span class="cmr-8">2</span></sub>) and it ends up in one of two states, <span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">1</span></sub> or <span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">2</span></sub>. How many times we have switched to each of these states is stored in our transition table as <span class="cmtt-10x-x-109">dict </span><span class="cmsy-10x-x-109">{</span><span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">1</span></sub>: <span class="cmmi-10x-x-109">c</span><sub><span class="cmr-8">1</span></sub>, <span class="cmmi-10x-x-109">s</span><sub><span class="cmr-8">2</span></sub>: <span class="cmmi-10x-x-109">c</span><sub><span class="cmr-8">2</span></sub><span class="cmsy-10x-x-109">}</span>.</p>
<p>Then, the approximate value for the state and action, <span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s,a</span>), will be equal to the probability of every state, multiplied by the value of the state. From the Bellman equation, this equals the sum of the immediate reward and the discounted long-term state value:</p>
<div class="tcolorbox" id="tcolobox-79">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-100"><code>    def calc_action_value(self, state: State, action: Action) -&gt; float: 
        target_counts = self.transits[(state, action)] 
        total = sum(target_counts.values()) 
        action_value = 0.0 
        for tgt_state, count in target_counts.items(): 
            rw_key = (state, action, tgt_state) 
            reward = self.rewards[rw_key] 
            val = reward + GAMMA * self.values[tgt_state] 
            action_value += (count / total) * val 
        return action_value</code></pre>
</div>
</div>
<p>The next function uses the function that I just described to make a decision about the best action to take from the given state. It iterates over all possible actions in the environment and calculates the value for every action. The action with the largest value wins and is returned as the action to take. This action selection process is deterministic, as the <span class="cmtt-10x-x-109">play</span><span class="cmtt-10x-x-109">_n</span><span class="cmtt-10x-x-109">_random</span><span class="cmtt-10x-x-109">_steps() </span>function introduces enough exploration. So, our agent will behave greedily in regard to our value approximation:</p>
<div class="tcolorbox" id="tcolobox-80">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-101"><code>    def select_action(self, state: State) -&gt; Action: 
        best_action, best_value = None, None 
        for action in range(self.env.action_space.n): 
            action_value = self.calc_action_value(state, action) 
            if best_value is None or best_value &lt; action_value: 
                best_value = action_value 
                best_action = action 
        return best_action</code></pre>
</div>
</div>
<p>The <span class="cmtt-10x-x-109">play</span><span class="cmtt-10x-x-109">_episode() </span>function uses <span class="cmtt-10x-x-109">select</span><span class="cmtt-10x-x-109">_action() </span>to find the<span id="dx1-87058"/> best action to take and plays one full episode using the provided environment. This function is used to play test episodes, during which we don‚Äôt want to mess with the current state of the main environment used to gather random data. So, we use the second environment passed as an argument. The logic is very simple and should already be familiar to you: we just loop over states accumulating the reward for one episode:</p>
<div class="tcolorbox" id="tcolobox-81">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-102"><code>    def play_episode(self, env: gym.Env) -&gt; float: 
        total_reward = 0.0 
        state, _ = env.reset() 
        while True: 
            action = self.select_action(state) 
            new_state, reward, is_done, is_trunc, _ = env.step(action) 
            rw_key = (state, action, new_state) 
            self.rewards[rw_key] = float(reward) 
            tr_key = (state, action) 
            self.transits[tr_key][new_state] += 1 
            total_reward += reward 
            if is_done or is_trunc: 
                break 
            state = new_state 
        return total_reward</code></pre>
</div>
</div>
<p>The final method of the <span class="cmtt-10x-x-109">Agent </span>class is our value iteration implementation and it is surprisingly simple, thanks to the functions we already defined. What we do is just loop over all states in the environment, then for every state, we calculate the values for the states reachable from it, obtaining candidates for the value of the state. Then we update the value of our current state with the maximum value of the action available from the state:</p>
<div class="tcolorbox" id="tcolobox-82">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-103"><code>    def value_iteration(self): 
        for state in range(self.env.observation_space.n): 
            state_values = [ 
                self.calc_action_value(state, action) 
                for action in range(self.env.action_space.n) 
            ] 
            self.values[state] = max(state_values)</code></pre>
</div>
</div>
<p>That‚Äôs all <span id="dx1-87081"/>of our agent‚Äôs methods, and the final piece is a training loop and the monitoring of the code:</p>
<div class="tcolorbox" id="tcolobox-83">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-104"><code>if __name__ == "__main__": 
    test_env = gym.make(ENV_NAME) 
    agent = Agent() 
    writer = SummaryWriter(comment="-v-iteration")</code></pre>
</div>
</div>
<p>We create the environment that we will be using for testing, the <span class="cmtt-10x-x-109">Agent </span>class instance, and the summary writer for TensorBoard:</p>
<div class="tcolorbox" id="tcolobox-84">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-105"><code>    iter_no = 0 
    best_reward = 0.0 
    while True: 
        iter_no += 1 
        agent.play_n_random_steps(100) 
        agent.value_iteration()</code></pre>
</div>
</div>
<p>The last two lines in the preceding code snippet are the key piece in the training loop. We first perform 100 random steps to fill our reward and transition tables with fresh data, and then we run value iteration over all states.</p>
<p>The rest of the code plays test episodes using the value table as our policy, then writes data into TensorBoard, tracks the best average reward, and checks for the training loop <span class="cmtt-10x-x-109">stop </span>condition:</p>
<div class="tcolorbox" id="tcolobox-85">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-106"><code>        reward = 0.0 
        for _ in range(TEST_EPISODES): 
            reward += agent.play_episode(test_env) 
        reward /= TEST_EPISODES 
        writer.add_scalar("reward", reward, iter_no) 
        if reward &gt; best_reward: 
            print(f"{iter_no}: Best reward updated {best_reward:.3} -&gt; {reward:.3}") 
            best_reward = reward 
        if reward &gt; 0.80: 
            print("Solved in %d iterations!" % iter_no) 
            break 
    writer.close()</code></pre>
</div>
</div>
<p>Okay, let‚Äôs run our program:</p>
<pre class="lstlisting" id="listing-107"><code>Chapter05$ ./01_frozenlake_v_iteration.py 
3: Best reward updated 0.0 -&gt; 0.1 
4: Best reward updated 0.1 -&gt; 0.15 
7: Best reward updated 0.15 -&gt; 0.45 
9: Best reward updated 0.45 -&gt; 0.7 
11: Best reward updated 0.7 -&gt; 0.9 
Solved in 11 iterations!</code></pre>
<p>Our solution is stochastic, and my experiments usually required 10 to 100 iterations to reach a solution, but in all cases, it took less than<span id="dx1-87111"/> a second to find a good policy that could solve the environment in 80% of runs. If you remember, about an hour was needed to achieve a 60% success ratio using the cross-entropy method, so this is a major improvement. There are two reasons for that.</p>
<p>First, the stochastic outcome of our actions, plus the length of the episodes (6 to 10 steps on average), makes it hard for the cross-entropy method to understand what was done right in the episode and which step was a mistake. Value iteration works with individual values of the state (or action) and incorporates the probabilistic outcome of actions naturally by estimating probability and calculating the expected value. So, it‚Äôs much simpler for value iteration and requires <span id="dx1-87112"/>much less data from the environment (which is called <span class="cmbx-10x-x-109">sample</span> <span class="cmbx-10x-x-109">efficiency </span>in RL).</p>
<p>The second reason is the fact that value iteration doesn‚Äôt need full episodes to start learning. In an extreme case, we can start updating our values just from a single example. However, for FrozenLake, due to the reward structure (we get 1 only after successfully reaching the target state), we still need to have at least one successful episode to start learning from a useful value table, which may be challenging to achieve <span id="dx1-87113"/>in more complex environments. For example, you can try switching the existing code to a larger version of FrozenLake, which has the name <span class="cmtt-10x-x-109">FrozenLake8x8-v1</span>. The larger version of FrozenLake can take from 150 to 1,000 iterations to solve, and, according to TensorBoard charts, most of the time it waits for the first successful episode, then it very quickly reaches convergence.</p>
<p>The following are two charts: the first one shows reward dynamics during training on FrozenLake-4x4 and the second is for the 8 <span class="cmsy-10x-x-109">√ó </span>8 version.</p>
<div class="minipage">
<p><img alt="PIC" height="200" src="../Images/B22150_05_09.png" width="300"/> <span id="x1-87114r9"/></p>
<span class="id">Figure¬†5.9: The reward dynamics for FrozenLake-4x4 </span>
</div>
<div class="minipage">
<p><img alt="PIC" height="200" src="../Images/B22150_05_10.png" width="300"/> <span id="x1-87115r10"/></p>
<span class="id">Figure¬†5.10: The reward dynamics on FrozenLake-8x8 </span>
</div>
<p>Now it‚Äôs<span id="dx1-87116"/> time to <span id="dx1-87117"/>compare the code that learns the values of the states, as we just discussed, with the code that learns the values of the actions.</p>
</section>
<section class="level3 sectionHead" id="q-iteration-for-frozenlake">
<h1 class="heading-1" id="sigil_toc_id_78"> <span id="x1-880005.6"/>Q-iteration for FrozenLake</h1>
<p>The whole <span id="dx1-88001"/>example is in the <span class="cmtt-10x-x-109">Chapter05/02</span><span class="cmtt-10x-x-109">_frozenlake</span><span class="cmtt-10x-x-109">_q</span><span class="cmtt-10x-x-109">_iteration.py </span>file, and the differences are really minor:</p>
<ul>
<li>
<p>The most obvious change is to our value table. In the previous example, we kept the value of the state, so the key in the dictionary was just a state. Now we need to store values of the Q-function, which has two parameters, <span class="cmtt-10x-x-109">state </span>and <span class="cmtt-10x-x-109">action</span>, so the key in the value table is now a composite of (<span class="cmtt-10x-x-109">State</span>, <span class="cmtt-10x-x-109">Action</span>) values.</p>
</li>
<li>
<p>The second difference is in our <span class="cmtt-10x-x-109">calc</span><span class="cmtt-10x-x-109">_action</span><span class="cmtt-10x-x-109">_value() </span>function. We just don‚Äôt need it anymore, as our action values are stored in the value table.</p>
</li>
<li>
<p>Finally, the most important change in the code is in the agent‚Äôs <span class="cmtt-10x-x-109">value</span><span class="cmtt-10x-x-109">_iteration() </span>method. Before, it was just a wrapper around the <span class="cmtt-10x-x-109">calc</span><span class="cmtt-10x-x-109">_action</span><span class="cmtt-10x-x-109">_value() </span>call, which did the job of Bellman approximation. Now, as this function has gone and been replaced by a value table, we need to do this approximation in the <span class="cmtt-10x-x-109">value</span><span class="cmtt-10x-x-109">_iteration() </span>method.</p>
</li>
</ul>
<p>Let‚Äôs look at the code. As it‚Äôs almost the same, I will jump directly to the most interesting <span class="cmtt-10x-x-109">value</span><span class="cmtt-10x-x-109">_iteration() </span>function:</p>
<div class="tcolorbox" id="tcolobox-86">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-108"><code>    def value_iteration(self): 
        for state in range(self.env.observation_space.n): 
            for action in range(self.env.action_space.n): 
                action_value = 0.0 
                target_counts = self.transits[(state, action)] 
                total = sum(target_counts.values()) 
                for tgt_state, count in target_counts.items(): 
                    rw_key = (state, action, tgt_state) 
                    reward = self.rewards[rw_key] 
                    best_action = self.select_action(tgt_state) 
                    val = reward + GAMMA * self.values[(tgt_state, best_action)] 
                    action_value += (count / total) * val 
                self.values[(state, action)] = action_value</code></pre>
</div>
</div>
<p>The code is very similar to <span class="cmtt-10x-x-109">calc</span><span class="cmtt-10x-x-109">_action</span><span class="cmtt-10x-x-109">_value() </span>in the previous<span id="dx1-88015"/> example <span id="dx1-88016"/>and, in fact, it does almost the same thing. For the given state and action, it needs to calculate the value of this action using statistics about target states that we have reached with the action. To calculate this value, we use the Bellman equation and our counters, which allow us to approximate the probability of the target state. However, in Bellman‚Äôs equation, we have the value of the state; now, we need to calculate it differently.</p>
<p>Before, we had it stored in the value table (as we approximated the value of the states), so we just took it from this table. We can‚Äôt do this anymore, so we have to call the <span class="cmtt-10x-x-109">select</span><span class="cmtt-10x-x-109">_action </span>method, which will choose for us the action with the largest Q-value, and then we take this Q-value as the value of the target state. Of course, we can implement another function that can calculate this value of the state, but <span class="cmtt-10x-x-109">select</span><span class="cmtt-10x-x-109">_action </span>does almost everything we need, so we will reuse it here.</p>
<p>There is another piece of this example that I‚Äôd like to emphasize here. Let‚Äôs look at our <span class="cmtt-10x-x-109">select</span><span class="cmtt-10x-x-109">_action </span>method:</p>
<div class="tcolorbox" id="tcolobox-87">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-109"><code>    def select_action(self, state: State) -&gt; Action: 
        best_action, best_value = None, None 
        for action in range(self.env.action_space.n): 
            action_value = self.values[(state, action)] 
            if best_value is None or best_value &lt; action_value: 
                best_value = action_value 
                best_action = action 
        return best_action</code></pre>
</div>
</div>
<p>As I said, we don‚Äôt have the <span class="cmtt-10x-x-109">calc</span><span class="cmtt-10x-x-109">_action</span><span class="cmtt-10x-x-109">_value </span>method anymore; so, to select an action, we just iterate over the actions and look up their values in our values table. It could look like a minor improvement, but if you think about the data that we used in <span class="cmtt-10x-x-109">calc</span><span class="cmtt-10x-x-109">_action</span><span class="cmtt-10x-x-109">_value</span>, it may become obvious why the learning of the Q-function is much more popular in RL than the learning of the V-function.</p>
<p>Our <span class="cmtt-10x-x-109">calc</span><span class="cmtt-10x-x-109">_action</span><span class="cmtt-10x-x-109">_value </span>function uses both information about the reward and probabilities. It‚Äôs not a huge problem for the value iteration method, which relies on this information during training. However, in the next chapter, you will learn about the value iteration method extension, which doesn‚Äôt require probability approximation, but just takes it from the environment samples. For such methods, this dependency on probability adds an extra burden for the agent. In the case of Q-learning, what the agent needs to make the decision is just Q-values.</p>
<p>I don‚Äôt want to say that V-functions are completely useless, because they are an essential part of the actor-critic method, which we will talk about in <span class="cmti-10x-x-109">Part 3 </span>of this book. However, in the area of value learning, Q-functions are the definite favorite. With regard to convergence speed, both our versions are almost identical (but the Q-learning version requires four times more memory for the value table).</p>
<p>The following<span id="dx1-88025"/> is the <span id="dx1-88026"/>output of the Q-learning version and it has no major differences from the value iteration version:</p>
<pre class="lstlisting" id="listing-110"><code>Chapter05$ ./02_frozenlake_q_iteration.py 
8: Best reward updated 0.0 -&gt; 0.35 
11: Best reward updated 0.35 -&gt; 0.45 
14: Best reward updated 0.45 -&gt; 0.55 
15: Best reward updated 0.55 -&gt; 0.65 
17: Best reward updated 0.65 -&gt; 0.75 
18: Best reward updated 0.75 -&gt; 0.9 
Solved in 18 iterations!</code></pre>
</section>
<section class="level3 sectionHead" id="summary-4">
<h1 class="heading-1" id="sigil_toc_id_79"> <span id="x1-890005.7"/>Summary</h1>
<p>My congratulations; you have made another step toward understanding modern, state-of-the-art RL methods! In this chapter, you learned about some very important concepts that are widely used in deep RL: the value of the state, the value of the action, and the Bellman equation in various forms.</p>
<p>We also covered the value iteration method, which is a very important building block in the area of Q-learning. Finally, you got to know how value iteration can improve our FrozenLake solution.</p>
<p>In the next chapter, you will learn about deep Q-networks, which started the deep RL revolution in 2013 by beating humans on lots of Atari 2600 games.</p>
</section>
</section>
</div></body></html>