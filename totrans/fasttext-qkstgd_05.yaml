- en: Word Representations in FastText
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you have taken a look at creating models in the command line, you might
    be wondering how fastText creates those word representations. In this chapter,
    you will get to know what happens behind the scenes and the algorithms that power
    fastText.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Word-to-vector representations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Types of word representations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting vector representations from text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model architecture in fastText
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The unsupervised model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: fastText skipgram implementation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CBOW** (**Continuous bag of words**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparison between skipgram and CBOW
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loss functions and optimizations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Softmax
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Context definitions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word-to-vector representations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Almost all machine learning and deep learning algorithms manipulate vectors
    and matrices. The reason they work is because of their base mathematics, which
    is heavily rooted in linear algebra. So, in short, for both supervised and unsupervised
    learning, you will need to create matrices of numbers. In other domains, this
    is not an issue as information is generally captured as numbers. For example,
    in retail, the sales information for how many units were sold or how much revenue
    the store is making in the current month is all numbers. Even in a more abstract
    field such as computer vision, the image is always stored as pixel intensity of
    the three basic colors: red, green, and blue. 0 for a particular color means no
    intensity and 255 means the highest possible intensity for the screen. Similarly,
    in the case of sound, it is stored as power spectral density coefficients. In
    the case of sound, the analog signal that is picked up by the microphone is then
    converted to a discrete time and discrete amplitude. The amplitude is essentially
    the number of bits that can be passed in a given amount of time and hence that
    is essentially a number.'
  prefs: []
  type: TYPE_NORMAL
- en: The challenge that comes in raw text in computer systems is that they are stored
    and analyzed as strings, which do not work well with these matrix systems. So
    you need a method to convert text into matrices.
  prefs: []
  type: TYPE_NORMAL
- en: Types of word representations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Depending on the target languages, there are various concepts that should be
    taken care of for an optimal word representation of the given corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Distributed word representation**: In a distributed representation, the sense
    of the word should not be concentrated on only one dimension but be distributed
    across all dimensions. If it is not distributed, then the resulting vectors may
    be too big, which can be a limiting factor when performing the necessary vector
    transformations both in terms of memory and the time needed to perform the transformations.
    A distributed representation is compact and can represent an exponential number
    of clusters in the number of dimensions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distributional word representation**: You can argue that there is some kind
    of similarity between "cat" and "dog" and another kind of similarity between "cat"
    and "tiger". Word representations that focus on capturing those kinds of implicit
    relationships are called distributional. To get such distributional properties,
    the following very common paradigm is used:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You shall know the meaning of the word by the company it keeps
  prefs: []
  type: TYPE_NORMAL
- en: '- John Rupert Firth (1962)'
  prefs: []
  type: TYPE_NORMAL
- en: So if we take an example with two sentences, "Mary has a cat" and "Mary has
    a dog", the context around "dog" and "cat" is the same and hence the word representation
    should be able to get the "pet" context by reading the two sentences.
  prefs: []
  type: TYPE_NORMAL
- en: '**Zipf''s law and Heap''s law**: We will have some more discussion on Zipf''s
    law when we go to the n-grams but we will state Heap''s law here:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of distinct words in a document (or set of documents) as a function
    of the document length (so called type-token relation).
  prefs: []
  type: TYPE_NORMAL
- en: Taken together, Heap's law and Zipf's law are essentially saying the same thing,
    which is that you will always have new words. Hence, you should not be throwing
    away rare words from the document and will need a word representation that is
    more welcoming of new words. You cannot model language, close the vocabulary,
    and say you are done.
  prefs: []
  type: TYPE_NORMAL
- en: Getting vector representations from text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will take a look at what it means to have a vector representation
    for the target group of text. We will start with one of the simplest forms of
    word vectors and how to implement it. Then we will explore the rationale behind
    some other types of word vectors and, finally, we will take an in-depth look at
    the algorithms that are used in fastText to create the word vectors.
  prefs: []
  type: TYPE_NORMAL
- en: One-hot encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the simplest approach, raw text can be taken as a collection of tokens,
    where are assumption is that each "word" contributes in some way to the meaning
    of the sentence as a whole. All words are meant to signify something specific
    and hence are categories by themselves. The presence of a word would mean the
    presence of the category for which the word stands for, and absence of the word
    would mean that the category is not there. Hence, the traditional method was to
    represent categorical variables as binary variables. First the dictionary of words
    is created and then each word is assigned a unique position. Then the vectors
    are created by putting 1 in the respective index and 0 for all other variables.
    This system of creating vectors is called one-hot encoding:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00018.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Implementation of the one-hot model is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Although one-hot encoding is simple to understand and implement, there are
    a number of disadvantages associated with it:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Not a distributed representation**: The number of dimensions for each vector
    grows with the size of the vocabulary. The matrix that is formed is highly sparse—which
    means that most of the individual values are 0s. Hence, the matrix manipulations
    become computationally too expensive even for a corpus of normal size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Out-of-vocabulary words**: It is not able to handle new words at test time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Not a distributional representation**: In one-hot encoding, all the vectors
    are equidistant from each other.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bag of words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The bag of words model is concerned about whether known words occur in the
    document and only the frequency of the tokens in the document will be taken into
    account. So to create the document matrix using the bag of words approach, the
    following algorithm is used:'
  prefs: []
  type: TYPE_NORMAL
- en: Find the number of separate words that are used in all the documents. Words
    are identified using spaces and punctuation as the separators.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the tokens, a feature space is created. For each document, each feature
    value is the count of the number of times the feature is present in the document.
    Hence, each row in the resultant matrix will correspond to each document. Count
    the number of tokens in each document. This is because each document will generate
    its own vector.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Normalize the vectors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For example, let''s say that there are two documents comprising the whole corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'So for all the sentences, our vocabulary is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'To get the bag of words, we count the number of times each word occurs in the
    sentence. So the following are the vectors formed for each document:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The main disadvantage of the bag of words approach is that the context of the
    word is lost. You can think of examples such as "Toy Dog" and "Dog Toy", which
    do not mean the same thing but will share the same vector. A simple implementation
    of bag of words is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: TF-IDF
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Just counting the number of tokens in a document may not give sufficient information
    about the whole corpus. The idea is that rarer words give more information about
    what the document is about. In TF-IDF, the term frequency is normalized by the
    document frequency. The intuition is that TF-IDF makes rare words more prominent
    and scales down the effect of common words.
  prefs: []
  type: TYPE_NORMAL
- en: N-grams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'N-gram-based approaches are based on Zipf''s law which states the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The nth most common word in a human language text occurs with a frequency inversely
    proportional to n.
  prefs: []
  type: TYPE_NORMAL
- en: In all languages, there are some words that are used more commonly than the
    others. The difference between more common words and less common words is not
    drastic but continuous. Another good corollary of this law is that if a class
    of documents corresponding to a specific frequency gets cut off, that will not
    massively affect the n-gram frequency profile. Hence, if we are comparing documents
    of the same category, they should have similar frequency profile.
  prefs: []
  type: TYPE_NORMAL
- en: 'N-grams frequency means the frequency of overlapping sequence of words. Here
    is a quote:'
  prefs: []
  type: TYPE_NORMAL
- en: '"Even now They talked in Their tombs."'
  prefs: []
  type: TYPE_NORMAL
- en: - H.P. Lovecraft
  prefs: []
  type: TYPE_NORMAL
- en: 'From this sentence you can obtain the following n-grams. "_" is to show the
    start and end of the sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1**-**grams (unigrams)**: Even, now, They, talked, in, Their, tombs (number
    of features: 7).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2**-**grams (bigrams)**: (_, Even), (Even, now), (now, They), (They, talked),
    (talked, in), (in, Their), (Their, tombs), (tombs, _) (number of features: 8).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**3**-**grams (trigrams)**: (_, _, Even), (_, Even, now), (Even, now, They),
    (now, They, talked), (They, talked, in), (talked, in, Their), (in, Their, tombs),
    (Their, tombs, _), (tombs, _, _) (features: 9).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**4**-**grams (trigrams)**: (_, _, _, Even), (_, _, Even, now), (_, Even, now,
    They), (Even, now, They, talked), (now, They, talked, in), (They, talked, in,
    Their), (talked, in, Their, tombs), (in, Their, tombs, _), (Their, tombs, _, _),
    (tombs, _, _, _) (features: 10).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'When dealing with only unigrams, the probability of the whole sentence can
    be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, in the case of bigrams, the probability of the whole sentence can
    be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: As per maximum likelihood estimation, the conditional probability of something
    like P("now" | "Even") can be given as the ratio of count of the observed occurrence
    of "Even now" together by the count of the observed occurrence of "Even". This
    probability model can now be used to predict new sentences.
  prefs: []
  type: TYPE_NORMAL
- en: Let's build a model in case of bigrams. This file has been taken from the servers
    of University of Maryland Institute for Advanced Computer Studies, [http://www.umiacs.umd.edu/](http://www.umiacs.umd.edu/) or
    you can use your own corpus. Keep it in the `data` folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the following command will remove the new lines, then squash all the consecutive
    spaces, then get all the bigrams and sort them as per frequency:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can build a sentence generator using this n-grams file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Using this n-gram sentence builder, we get the following sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: If this fascinates you, then try to build with trigrams or more.
  prefs: []
  type: TYPE_NORMAL
- en: The major drawback of n-grams is that they are extremely sparse and are not
    able to distinguish when encountering new words in the test data.
  prefs: []
  type: TYPE_NORMAL
- en: Model architecture in fastText
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: FastText models are a little bit different depending on whether they are unsupervised
    models or supervised models. In this chapter, we will mostly look at the unsupervised
    model.
  prefs: []
  type: TYPE_NORMAL
- en: The unsupervised model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In fastText, you can have the option to use two model architectures for computing
    a distributed representation of words. They are skipgram and CBOW. The model architectures
    used in fastText are both distributed architectures. So the aim is to learn a
    high-dimensional dense representation for each vocabulary term. The representation
    should be distributional as well as it tries to learn from context.
  prefs: []
  type: TYPE_NORMAL
- en: In both the architectures, you train a two-layer, shallow neural network to
    construct the context of words.
  prefs: []
  type: TYPE_NORMAL
- en: Skipgram
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In skipgram, a context window of *k* is considered. All the other positions
    are skipped and only the relationship between the panel and the word is explored.
    This is done by feeding a one-hot encoding of the word to a two-layer shallow
    neural network. Since the input is one-hot encoded, the hidden layer consists
    of only one row of input hidden weight matrix. The task for the neural network
    is to predict the *i*th context given the word:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00019.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The scores for each word are computed using the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00020.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *h* is a vector in the hidden layer and *W* is the hidden output weight
    matrix. After computing *u*, *c* multinomial weight distributions are computed,
    where *c* is the window size. The distributions are computed using the following
    equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00021.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *w[c,j]* is the *j*th word on the *c*th panel of the output layer; *w[O,c]*
    is the actual *c*th word in the output context words; *w[I]* is the only input
    word; and *u[c,j]* is the net input of the *j*th unit on the *c*th panel of the
    output layer. So you can see this is, in effect, trying to predict the context
    words given the input word. The probability is then converted into a softmax.
    If you try to visualize the above architecture, this should translate to something
    like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00022.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In addition, more distant words are given less weight-age by randomly sampling
    them. When you give the window size parameter, only the maximum window size is
    configured. In effect, the actual window size is randomly chosen between 1 and
    the maximum window size for each training sample. Thus the words that are the
    farthest are chosen with the probability of 1/*c*, whereas the nearest words are
    always chosen.
  prefs: []
  type: TYPE_NORMAL
- en: Subword information skipgram
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The skipgram model was taken in its original form from the word2vec implementation.
    The skipgram model is effective because it emphasizes the specific word and the
    word that comes along with it. But along with the specific word, the character
    of the n-grams may also have a lot of information. This is especially true of
    languages which are morphologically rich. In fastText, the authors took the skipgram
    implementation in word2vec, which is simply taking the vector representation of
    the whole word, and said that the vector representation of the word is actually
    the sum of the vector representations of the n-grams. Hence, in fastText, the
    scoring function (*u*) that you saw earlier is actually changed to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00023.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '*SCORE* = [*3-6 char level n-grams*] + [*word*]'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the library, n-grams for n greater or equal to 3 and less or equal to 6
    is taken along with the word. So for something like `Schadenfreude` the collection
    of n-grams taken are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The major advantage of this method is that in the case of out-of-vector words,
    the [word] vector is not present and hence the score function transforms to the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*SCORE* = [*3-6 char level n-grams*]'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing skipgram
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now let's try to understand the skipgram method through some Python code. The
    Keras library has a very good and easy to understand skipgram function that you
    can see and understand how the skipgram should be implemented. For code in this
    section, you can take a look at the `fasttext skipgram cbow.ipynb` notebook which
    is inspired by the Keras implementations.
  prefs: []
  type: TYPE_NORMAL
- en: As discussed in skipgram, the task for the model is to predict the *i*th context
    given the word. How this is achieved in practice is by taking pairs of words from
    the document and then saying output is `1` in case the second word is the context
    word.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, given a sequence or an individual document (which, in this case, is probably
    a particular sentence), first create two lists: `couples` and `labels`. Now, for
    each target word, get the context window and in the context window for each combination
    of target word and context word, capture the combination in the `couples` list,
    and capture the label in the `labels` list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we have captured only positive values right now, we will need to capture
    some negative cases as well to train the model effectively. In the negative sampling
    case, for the number of negative samples, randomly generate some out-of-context
    word indexes with the target words that we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'You can encapsulate the previous logic in a function, as has been done in the
    Keras function `skipgrams`  and then return the combinations (denoted by the `couples`
    list) and the labels. This will be then passed on to the neural network, which
    will train on these combinations and corresponding labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The skipgram model is essentially a hidden layer sandwiched between an input
    layer and the output layer. We can create a simple Keras model to capture that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This creates the following model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00024.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, once the model is trained, we get the vectors from the trained weights
    of the embedding dimension:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Now we can save the vectors in our file and load them up when necessary.
  prefs: []
  type: TYPE_NORMAL
- en: CBOW
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CBOW is the opposite of skipgram, where the specific word is taken as the target
    given the context. The number of words that are used as context words depends
    on the context word. So in this case, taking the previous example "Even now They
    talked in Their tombs", we can take the whole context `["Even" "now" "They" "in"
    "Their" "tombs."]` and generate the word "talked" from it.
  prefs: []
  type: TYPE_NORMAL
- en: 'So the algorithm is to take the one-hot vectors of all the words since now
    we are taking all the context words as the input. Considering that the window
    size is k, there will be 2 million one-hot vectors. Then take the embedding words
    vectors for all the words. Average out the word vectors to get the cumulative
    context. The output of the hidden layer is thus generated using the following
    equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00025.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: It is worth noting that the hidden layer is one of the main differences between
    skipgram and CBOW in terms of being mirror images of one another.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate the score with the same score function as we saw when defining skipgram.
    The equation is almost the same except that since we are predicting all the words
    in the output based on the context, hence u and ν for the different columns (denoted
    by j) needs to be computed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00026.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Turn the score into probabilities using the softmax. Now we need to train this
    model so that the probabilities match the true probabilities of the word, which
    is the one-hot encoding of the actual word:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00027.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The CBOW architecture looks something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00028.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: CBOW implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CBOW implementation is easier to code than skipgram as the `cbow` method is
    pretty much straightforward. For each target word, you take the context and try
    to predict the target word, keeping the context as the input.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence, for the implementation perspective, writing code for CBOW is simpler.
    For each word in the sequence, the same labels lists will be created but this
    list will be the actual target word under focus. The other list is the context
    list which will have the context words depending on the window. Now, once the
    input and output are fixed, we can then yield them so that the model can train
    on it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding model will be in terms of NumPy vectors which can
    be passed to a keras model for batch training, similar to what you saw in the
    *Implementing skipgram* section. Here, `cbow` is the Keras model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'For the CBOW model, you will need to define a input layer first. The embedding
    layer can be the average of the embedding layer, which is then passed on to an
    output layer using the `softmax` function. Hence we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following architecture being built:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00029.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Run the code in the `fasttext skipgram cbow.ipynb` notebook. You will be able
    to compare the vectors created using skipgram and CBOW.
  prefs: []
  type: TYPE_NORMAL
- en: Comparison between skipgram and CBOW
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, you might be wondering which architecture should be used more during the
    actual training of data. The following are some guidelines for differentiating
    between CBOW and skipgram when choosing to train the data:'
  prefs: []
  type: TYPE_NORMAL
- en: Skipgram works well with a small amount of training data. It works well even
    on rare words and phrases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CBOW is faster to train than skipgram. It also has higher accuracy on frequent
    words and phrases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loss functions and optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Choosing a loss function and an optimizing algorithm along with it is one of
    the fundamental strategies of machine learning.  Loss functions are a way of associating
    a cost with the difference between the present model and the actual data distribution.
    The idea is that for specific loss function, optimizing algorithm pair, it would
    be possible to optimize the parameters of the model to make them mimic the real
    data as closely as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Language models that use the neural probabilistic networks are generally trained
    using the maximum likelihood principle. The task is to maximize the probability
    of the next word *w[t]*, which is taken as the target, given the previous words
    h which is the "history". We can model that in terms of the softmax function that
    we will discuss next.
  prefs: []
  type: TYPE_NORMAL
- en: Softmax
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most popular methods for learning parameters of a model is using gradient
    descent. Gradient descent is basically an optimization algorithm that is meant
    for minimizing a function, based on which way the negative gradient points toward.
    In machine learning, the input function that gradient descent acts on is a loss
    function that is decided for the model. The idea is that if we move towards minimizing
    the loss function, the actual model will "learn" the ideal parameters and will
    ideally generalize to out-of-sample or new data to a large extent as well. In
    practice, it has been seen this is generally the case and stochastic gradient,
    which is a variant of gradient descent, has a fast training time as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the gradient descent to be effective, we need such an optimizing function
    that is convex and we want the logarithm of the model''s output to be well behaved
    for gradient-based optimization of the likelihood, going with the **maximum likelihood
    estimation** (**MLE**) principle. Now, consider the fact that taking the logarithm
    of a series of products transforms it to a series of additions, and because the
    likelihood for the whole training dataset is actually the product of the individual
    likelihoods of each sample, it is easier to maximize the log-likelihood as this
    would mean that you are optimizing the sum of the log-likelihood of each sample
    indexed by k:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00030.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we need to chose a suitable function for determining the probabilities,
    given by P in this case. There are some good functions out there that can be used
    and one popular function is the sigmoid function. The sigmoid function looks like
    an S:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00031.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The sigmoid function is best used for binary classification tasks and is used
    in logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: Since we need to obtain the posterior distribution of words, our problem statement
    is more of a multinomial distribution instead of a binary one. Hence, we can chose
    the softmax distribution, which is a generalization of the sigmoid for the multi-class
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'The softmax function calculates the probabilities distribution of the event
    over n different events. The softmax takes a class of values and converts them
    to probabilities with sum 1\. So you can say that it is effectively squashing
    a k-dimensional vector of arbitrary real values to k-dimensional vector of real
    values within the range 0 to 1\. The function is given by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00032.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'You can use the following code to see what the softmax function looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run the previous code in a Jupyter notebook, you should see a graph
    similar to the following. You can also see this in the `softmax function.ipynb`
    notebook under `chapter 3` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00033.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Note that as the values are higher, the probabilities are also higher. This
    is an interesting property of softmax, that is, the reaction to low stimuli is
    a rather uniform distribution and the reaction to high stimuli is probabilities
    that are closer to 0 and 1\. If you are wondering why that is the case, this is
    because of the impact of the exponential function, which focuses on the extreme
    values.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, for a given input word, this function will calculate the probabilities
    of each word over all possible words. If you train using the softmax function,
    the probability associated with the actual word should be the highest:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00034.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, the score function can be considered to be the compatibility of the word
    w[t] with the context h.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we are training this model using the negative log likelihood on the training
    set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00035.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we need to learn our softmax model using gradient descent and hence we
    need to compute the gradient with respect to the input words:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00036.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The update of the parameter models will be in the opposite direction to the
    gradient:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00037.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In our context, let the vocabulary be *V* and the hidden layer size be *N*.
    The units on the adjacent layer are fully connected. The input is a one-hot encoded
    vector, which means for a given word input word context, only one out of the *V*
    units, {*x[1], x[2], ..., x[V]}*, will be 1, and all other units are 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'The weights between the input layer and output layer can be represented by
    a *V* x *N* matrix *W*. Each row of *W* is the *N*-dimensional vector representation
    *v[w]* of the associated word of the input layer. Formally, row *i* of *W* is
    *v[w]^T*. Given a context, assuming *x[k]*=*1* for a specific context word and
    0 otherwise, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00038.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'This is essentially the *k^(th)* row of *W*. Lets call this *ν[wI^T]*. From
    the hidden layer to the output matrix, there is a different weight *W^'' = {w[ij]^''}*,
    which is a *N* x *V* matrix. Using these weights, you can compute a score *u[j]*
    for each word in the vocabulary:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00039.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *ν[w]^''[j]* is the *jth* column of the matrix *W^''*. Now, using the
    softmax equation, we can obtain the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00040.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '*ν[w]* and *ν[w]^''* are two representations of the word *w*.  *ν[w]* comes
    from rows of *W*, which is the input to hidden weight matrix, and *ν[w]^''* comes
    from columns of *W^''*, which is the hidden output matrix. In subsequent analysis,
    we will call *ν[w]* the "input vector" and *ν[w]^''* the "output vector" of the
    word *w*. Considering *u[j]* as the score as described, we can transform the loss
    function to equation (6) to the one as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00041.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Hierarchical softmax
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finding the softmax is highly computationally intensive. For each training
    instance, we have to iterate through every word in the vocabulary and compute
    the softmax. Thus, it is impractical to scale up to large vocabularies and large
    training corpora. To solve this problem, there are two approaches that are used
    in fastText: the hierarchical softmax and the negative sampling approach. We will
    discuss hierarchical softmax in this section and will discuss negative sampling
    in the next section. In both the approaches, the trick is to recognize that we
    don''t need to update all the output vectors per training instance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In hierarchical softmax, a binary tree is computed to represent all the words
    in the vocabulary. The *V* words must be leaf units of the tree. It can be proved
    that there are *V-1* inner units. For each unit, there exists a unique path from
    the root of the tree to the unit, and this path is used to estimate the probability
    of the word represented in the leaf unit:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00042.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Each of the words can be reached by a path from the root through the inner
    nodes, which represent probability mass along the way. Those values are produced
    by the usage of simple sigmoid function as long as the path we are calculating
    is simply the product of those probability mass functions defined with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00043.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'What is *x* in our specific case? It is calculated with the dot product of
    input and output vector representations of the word we are working with:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00044.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *n(w, j)* is the *j*th node on the path from the root to *w*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the hierarchical softmax model, there is no output representation of words.
    Instead, each of the *V - 1* inner units has an output vector *ν[n(w,j)]^''*.
    And the probability of a word being the output word is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00045.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *ch(n)* is the left child of unit *n*; *ν[n(w,j)]^''* is the vector representation
    ("output vector" ) of the inner unit *n(w,j)*; *h* is the output value of the
    hidden layer (in the skipgram model *h = ν[ω]* and in CBOW, ![](img/00046.jpeg));
    ![](img/00047.jpeg) is a special function defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00048.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: To calculate the probability of any output word, we need the probabilities of
    each intermediate node in the path from the root to the output word.
  prefs: []
  type: TYPE_NORMAL
- en: 'We define the probability of going right at an intermediate node as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00049.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Since we are computing a binary tree, the probability of going left will be
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00050.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Theoretically one can use many different types of trees for hierarchical softmax.
    You can randomly generate the tree. Or you can use existing linguistic resources
    such as WordNet. Morin and Benzio used this and showed that there was a 258x improvement
    over the randomly generated tree. But the nodes in the trees built this way generally
    have more than one edge. Another strategy is to learn the hierarchy using a recursive
    partitioning strategy or clustering strategy. The clustering algorithm can be
    a greedy approach, as shown in *Self-organized Hierarchical Softmax* by Yikang
    Shen, Shawn Tan, Christopher Pal, and Aaron Courville. We have another option
    in the form of Huffman codes, which are traditionally used in data compression
    circles. Since we are quite interested in clustering the documents by Nikhil Pawar,
    2012, it is seen that when Huffman encoding is used to encode strings to integers,
    the clustering on the integer instances is much more effective. In word2vec and
    fastText, the Huffman tree is used. An interesting property of Huffman trees is
    that while an inner unit of a binary tree may not always have both children, a
    binary Huffman tree''s inner units always do:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00051.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Tree generated using http://huffman.ooz.ie/?text=abcab
  prefs: []
  type: TYPE_NORMAL
- en: When building a Huffman tree, codes are assigned to the tokens such that the
    length of the code depends on the relative frequency or weight of the token. For
    example, in the previous example, the code for word E is 0000, which is one of
    the highest, and hence you can think that this word has occurred the most number
    of times in the corpus.
  prefs: []
  type: TYPE_NORMAL
- en: The code to build the tree can be found at [https://github.com/facebookresearch/fastText/blob/d72255386b8cd00981f4a83ae346754697a8f4b4/src/model.cc#L279.](https://github.com/facebookresearch/fastText/blob/d72255386b8cd00981f4a83ae346754697a8f4b4/src/model.cc#L279)
  prefs: []
  type: TYPE_NORMAL
- en: You can find the python implementation as part of the `Vocab` class in the method
    `encode_huffman`. For a simpler implementation, you can find the python implementation
    in the `huffman coding.ipynb` notebook in `chapter3` folder of the repository.
  prefs: []
  type: TYPE_NORMAL
- en: For the update equations, the computational complexity per training instance
    reduces from *O(V)* to *O(log(V))*, which is a huge improvement in terms of speed.
    We still roughly have the same number of parameters (*V-1* vectors for the inner
    units as compared to originally *V* output vectors for words).
  prefs: []
  type: TYPE_NORMAL
- en: Google Allo uses hierarchical softmax layer to make their phrase recommendation
    faster.
  prefs: []
  type: TYPE_NORMAL
- en: Negative sampling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An alternative to the hierarchical softmax is **noise contrast estimation**
    (**NCE**), which was introduced by Gutmann and Hyvarinen and applied to language
    modeling by Mnih and Teh. NCE posits that a good model should be able to differentiate
    data from noise by means of logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'While NCE can be shown to approximate the log probability of the softmax, the
    skipgram model is only concerned with the learning high-quality vector representations,
    so we are free to simplify NCE as long as the vector representations retrain their
    quality. We define negative sampling by the following objective:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00052.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: This is used to replace the *log(P(W[O] | W[I]))* term in the skipgram objective.
    Thus, the task is to distinguish the target word *w[O]* from draws from the noise
    distribution *P[n](w)* using logistic regression, where there are *k* negative
    samples for each data sample. In fastText, five negatives are sampled by default.
  prefs: []
  type: TYPE_NORMAL
- en: Subsampling of frequent words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a very large corpus, the most frequent words can easily occur hundreds of
    millions of times, for example, words such as "in", "the" and "a". Such words
    generally provide less information value than the rare words. You can easily see
    that while the fastText model benefits from observing co-occurrences of "France"
    and "Paris", it benefits much less from observing co-occurrences of "France" and
    "the", as nearly every word co-occurs frequently within a sentence with "the".
    The idea can also be applied in the opposite direction. The vector representations
    of frequent words do not change significantly after training on several million
    additional examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'To counter the imbalance between the rare and frequent words, we use a simple
    subsampling approach: each word *w[i]* in the training set is discarded with the
    probability computed by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00053.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, the function *f* is the frequency of the ith word *w* and *t* is a chosen
    threshold and hence a hyperparameter. In fastText, the default value of t is chosen
    to be 0.0001\. The code for this can be found at [https://github.com/facebookresearch/fastText/blob/53dd4c5cefec39f4cc9f988f9f39ab55eec6a02f/src/dictionary.cc#L281.](https://github.com/facebookresearch/fastText/blob/53dd4c5cefec39f4cc9f988f9f39ab55eec6a02f/src/dictionary.cc#L281)
  prefs: []
  type: TYPE_NORMAL
- en: Context definitions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Generally speaking, for a sentence of n words *w[1]*, *w[2]*[, ...,] *w[n]*
    contexts of a word *w[i]* comes from window of size *k* around the word:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00054.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *k* is a parameter. However there are two subtleties:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dynamic window size**: The window size that is being used is dynamic—the
    parameter *k* denotes the maximal window size. For each word in the corpus, a
    window size *k^''* is sampled uniformly from *1*,...,*k*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Effect of subsampling and rare word pruning: **Similar to word2vec, fastText
    has two additional parameters for discarding some of the input words: words appearing
    less frequently than `minCount` are not considered as either words or contexts,
    and in addition, frequent words (as defined by the *-t* parameter) are down sampled.
    Importantly, these words are removed from the text before generating the contexts.
    This has the effect of increasing the effective window size for certain words.
    Subsampling of frequent words should improve the quality of the resultant embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you have taken a look at unsupervised learning in fastText,
    as well as the algorithms and methods that enable it.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will be about how fastText has approached supervised learning
    and you will also learn about how model quantization works in fastText.
  prefs: []
  type: TYPE_NORMAL
