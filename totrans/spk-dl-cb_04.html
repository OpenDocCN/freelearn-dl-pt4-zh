<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Pain Points of Recurrent Neural Networks</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we will cover<span> </span>the following recipes:</p>
<ul>
<li>Introduction to feedforward networks</li>
<li>Sequential workings of RNNs</li>
<li>Paint point #1 – the vanishing gradient problem</li>
<li>Pain point #2 – the<span> </span><span>exploding gradient problem</span></li>
<li>Sequential workings of LSTMs</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p>Recurrent neural networks have proven to be incredibly efficient at tasks involving the learning and prediction of sequential data. However, when it comes to natural language, the question of long-term dependencies comes into play, which is basically remembering the context of a particular conversation, paragraph, or sentence in order to make better predictions in the future. For example, consider a sentence that says:</p>
<p><em>Last year, I happened to visit China. Not only was Chinese food different from the Chinese food available everywhere else in the world, but the people were extremely warm and hospitable too. In my three years of stay in this beautiful country, I managed to pick up and speak very good....</em></p>
<p>If the preceding sentence were fed into a recurrent neural network to predict the next word in the sentence (such as Chinese), the network would find it difficult since it has no memory of the context of the sentence. This is what we mean by long-term dependencies. In order to predict the word Chinese correctly, the network needs to know the context of the sentence as well as remember the fact that I happened to visit China last year. Recurrent neural networks therefore become inefficient at performing such tasks. However, this problem is overcome by<span> </span><strong>Long Short-Term Memory Units</strong><span> </span>(<strong><span>LSTMs</span></strong>), which are capable of remembering long-term dependencies and storing information in the cell state. LSTMs will be discussed later on, but the bulk of this chapter will focus on a basic introduction to Neural Networks, activation functions, Recurrent Networks, some of the main pain points or drawbacks of Recurrent Networks, and finally how these drawbacks may be overcome by the use of LSTMs.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction to feedforward networks</h1>
                </header>
            
            <article>
                
<p><span>To understand recurrent networks, first you have to understand the basics of feedforward networks. </span><span>Both of these networks are named after the way they move information through a series of mathematical operations performed at the nodes of the network. One feeds information in only one direction through every node (never touching a given node twice), while the other cycles it through a loop and feeds it back to the same node (kind of like a feedback loop). It is easily understood how the first kind is called a <strong>feedforward network,</strong> while the latter is recurrent.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>The most important concept while understanding any neural network diagram is the concept of computational graphs. Computational graphs are nothing but the nodes of the neural network connected to each other, and each node performs a particular mathematical function.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p><span>Feedforward neural networks channel the inputs (to the input layer) through a set of computational nodes which are nothing but mathematical operators and activation functions arranged in layers to calculate the network outputs. The output layer is the final layer of the neural network and usually contains linear functions. The layers between the input layer and the output layer are called <strong>hidden layers</strong> and usually contain nonlinear elements or functions:</span></p>
<ol>
<li>The following diagram <span class="packt_screen">(a)</span> <span>shows how nodes are interconnected in feedforward neural networks with many layers:</span></li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d39f7304-059d-40b7-8c8f-e0638930e60b.jpg" style="width:43.50em;height:22.92em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">FeedForward Neural Network</div>
<ol>
<li><span>Feedforward neural networks mainly differ from each other by the type of functions (activation functions) that are used in the hidden-layer nodes. They also differ from each other by the algorithms that are used to optimize the other parameters of the network during training.</span></li>
<li>The relationships between nodes shown in the preceding diagram need not be fully populated for every node; optimization strategies usually start with a large number of hidden nodes and tune the network by eliminating connections, and possibly nodes, as training progresses. It may not be necessary to utilize every node during the training process.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p><span>The neuron is the basic structural element of any neural network. A neuron can be thought of as a simple mathematical function or operator that operates on the input flowing through it to produce an output flowing out of it. The inputs to a neuron are multiplied by the node's weight matrix, summed over all the</span><span> inputs, translated, and passed through an activation function. These are basically matrix operations in mathematics as described here:</span></p>
<ol>
<li>The computational graph representation of a neuron is shown in the preceding diagram <span class="packt_screen">(b)</span><span>.</span></li>
<li>The transfer function for a single neuron or node is written as follows<span>:</span></li>
</ol>
<div class="CDPAlignCenter CDPAlign"><span> <img class="fm-editor-equation" src="assets/b755894e-1200-441f-b0ab-59898c595e85.png" style="width:13.00em;height:4.50em;"/></span></div>
<p style="padding-left: 60px"><span>Here, <em>x</em><sub> <em>i</em> </sub>is the input to the ith node, <em>w</em><sub> <em>i</em> </sub>is the weight term associated with the <em>i</em><sup>th</sup> node, <em>b</em> is the bias which is generally added to prevent overfitting, <em>f</em>(<span class="stix">⋅</span>) is the activation function operating over the inputs flowing into the node, and <em>y</em> is the output from the node.</span></p>
<ol start="3">
<li><span>Neurons with sigmoidal activation functions are commonly used in the hidden layer(s) of the neural network, and the identity function is usually used in the output layer.</span></li>
<li>The activation functions are generally chosen in a manner to ensure the outputs from the node are strictly increasing, smooth (continuous first derivative), or asymptotic.</li>
<li>The following logistic function  is<span> used as a sigmoidal activation function:</span></li>
</ol>
<div class="CDPAlignCenter CDPAlign"> <img class="fm-editor-equation" src="assets/ce8be72e-2f97-46da-bd51-326378f9b278.png" style="width:10.58em;height:3.50em;"/></div>
<ol start="6">
<li>
<p><span>A neural trained using backpropagation algorithm may learn faster if the activation function is antisymmetric, that is, </span><em>f</em><span>(-</span><em>x</em><span>) = -</span><em>f</em><span>(</span><em>x</em><span>) as in the case of the sigmoidal activation function</span><span>. The backpropagation algorithm will be discussed in detail in the following sections of this chapter.</span></p>
</li>
</ol>
<ol start="7">
<li><span>The logistic function, however, is not antisymmetric, but can be made antisymmetric by a simple scaling and shift, resulting in the hyperbolic tangent function which has  a first derivative described by </span><em>f </em><span>'(</span><em>x</em><span>) = 1 - </span><em>f </em><sup>2</sup><span>(</span><em>x</em><span>),  as shown in the following mathematical function:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><span><img class="fm-editor-equation" src="assets/1f3c3908-266c-4400-b025-b3b641a79d06.png" style="width:17.50em;height:3.67em;"/></span></p>
<ol start="8">
<li>
<p><span>The simple form of the sigmoidal function and its derivative allows for the quick and accurate calculation of the gradients needed to optimize the selection of the weights and biases and carry out second-order error analysis.</span></p>
</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p><span>At every neuron/node in the layers of a neural network, a series of matrix operations are performed. A more mathematical way of visualizing the feedforward network is given in the following diagram, which will help you to better understand the operations at each node/neuron:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b4f7b3f7-b9a9-4cc3-8214-1dd45ccd1679.png" style="width:34.42em;height:19.92em;"/></div>
<ol>
<li><span>Intuitively, we can see that the inputs (which are vectors or matrices) are first multiplied by weight matrices. A bias is added to this term and then activated using an activation function (such as ReLU, tanh, sigmoid, threshold, and so on) to produce the output. </span><span>Activation functions are key in ensuring that the network is able to learn linear as well as non-linear functions.</span></li>
<li>This output then flows into the next neuron as its input, and the same set of operations are performed all over again. A number of such neurons combine together to form a layer (which performs a certain function or learns a certain feature of the input vector), and many such layers combine together to form a feedforward neural network that can learn to recognize inputs completely, as shown in the following diagram:</li>
</ol>
<div class="mce-root CDPAlignCenter CDPAlign"><span><br/>
<img class="alignnone size-full wp-image-1319 image-border" src="assets/eefc8906-1179-48e1-9ed9-d04ac3a61f56.png" style="width:74.92em;height:39.00em;"/><br/></span></div>
<ol start="3">
<li><span>Let's suppose our feedforward network has been trained to classify images of dogs and images of cats. Once the network is trained, as shown in the following diagram, it will learn to label images as dog or cat when presented with new images:</span></li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1320 image-border" src="assets/0e66f91a-dd80-4096-8aec-2f815f533387.png" style="width:39.50em;height:17.33em;"/></div>
<ol start="4">
<li>In such networks, there is no relation between the present output and the previous or future outputs.</li>
<li>This means the feedforward network can basically be exposed to any random collection of images and the first image it is exposed to will not necessarily alter how it classifies the second or third images. Therefore, we can say that the output at time step <em>t</em> <span>is independent of the output at time step  </span><em>t - 1</em><span>.</span></li>
<li>Feedforward networks work well in such cases as image classification, where the data is not sequential. Feedforward networks also perform well when used on two related variables such as temperature and location, height and weight, car speed and brand, and so on.</li>
<li>However, there may be cases where the current output is dependent on the outputs at previous time steps (the ordering of data is important).</li>
<li>Consider the scenario of reading a book. Your understanding of the sentences in the book is based on your understanding of all the words in the sentence. It wouldn't be possible to use a feedforward network to predict the next word in a sentence, as the output in such a case would depend on the previous outputs. </li>
</ol>
<ol start="9">
<li>Similarly, there are many cases where the output requires the previous output or some information from the previous outputs (for example, stock market data, NLP, voice recognition, and so on). The feedforward network may be modified as in the following diagram to capture information from previous outputs:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1321 image-border" src="assets/d553d2b8-9636-4209-a475-a7709c6bc69a.png" style="width:39.17em;height:20.92em;"/></div>
<ol start="10">
<li><span>At time step <em>t</em>, the input at <em>t</em> as well as the information from <em>t-1</em> is both provided to the network to obtain the output at time <em>t</em>.</span></li>
<li><span>Similarly, the information from</span> <em>t</em><span> as well as the new input is fed into the network at time step</span> <em>t+1</em><span> to produce the output at</span> <em>t+1</em><span>. The right-hand side of the preceding diagram is a generalized way of representing such a network where the output of the network flows back in as input for future time steps. Such a network</span> is <span>called a</span> <strong>recurrent neural network</strong><span> (</span><strong>RNN</strong><span>).</span></li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p><strong>Activation Function</strong><span>: In artificial neural networks, the activation function of a node decides the kind of output that node produces, given an input or set of inputs. The output <em>y<sub>k</sub></em> is given by the input <em>u<sub>k</sub></em> and bias <em>b<sub>k</sub></em>, which are passed through the activation function <em>φ(.)</em>  as shown in the following expression:</span></p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/453f22b2-8584-4219-876e-a1e06896fc78.png" style="width:10.67em;height:1.83em;"/></div>
<p><span>There are various types of activation functions. The following are the commonly used ones:</span></p>
<ol>
<li><strong>Threshold function</strong>:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/053acc51-b93f-4166-9ed0-f5d279beaa17.png" style="width:10.75em;height:2.92em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1322 image-border" src="assets/9a44b42b-3cab-44d7-a7d4-d473dda6897d.jpg" style="width:20.25em;height:9.67em;"/></div>
<p style="padding-left: 60px">It is clear from the preceding diagram that this kind of  function restricts the output values of neurons to between 0 and 1. This may be useful in many cases. However, this function is non-differentiable, which means it cannot be used to learn non-linearities, which is vital when using the backpropagation algorithm.</p>
<ol start="2">
<li><strong>Sigmoid function</strong>:</li>
</ol>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/997a504a-0af5-4ed6-a7ab-8f11ea140dd4.png" style="width:12.75em;height:3.67em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1323 image-border" src="assets/c8c7d521-a6b3-4bef-9ae9-926e1827dbf6.png" style="width:36.58em;height:16.83em;"/></div>
<p style="padding-left: 60px"><span>The sigmoid function is a logistic function with a lower limit of 0 and an upper limit of 1, as with the threshold function. This activation function is continuous and therefore, also differentiable. In the sigmoid function, the slope parameter of the preceding function is given by α. The function is nonlinear in nature, which is critical in increasing the performance since it is able to accommodate non linearities in the input data unlike regular linear functions. Having non linear capabilities ensure that small changes in the weights and bias causes significant changes in the output of the neuron.</span></p>
<ol start="3">
<li><strong>Hyperbolic Tangent function (tanh)</strong>:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1324 image-border" src="assets/72529d7e-7c5e-43ad-814f-8487b8ca231c.png" style="width:35.25em;height:17.58em;"/></div>
<p class="mce-root" style="padding-left: 60px"><span>This function enables activation functions to range from -1 to +1 instead of between 0 and 1 as in the previous cases.</span></p>
<ol start="4">
<li><strong>Rectified Linear Unit (ReLU) function</strong>: ReLUs are the smooth approximation to the sum of many logistic units, and produce sparse activity vectors. The following is the equation of the function:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e434cc61-8a42-4a32-904f-535621776d8f.png" style="width:18.67em;height:4.67em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1325 image-border" src="assets/a212b63d-3aa8-4b20-931f-745e2d4d6301.jpg" style="width:21.25em;height:16.00em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">ReLU function graph</div>
<p style="padding-left: 60px">In the preceding diagram, softplus <img class="fm-editor-equation" src="assets/af6aae9d-9854-4827-afdd-5f175d48a865.png" style="width:1.00em;height:1.92em;"/>(x) = log ( 1 + e<sup>x</sup>) is the smooth approximation to the rectifier.</p>
<ol start="5">
<li><strong>Maxout function</strong>: This function utilizes a technique known as <strong>"dropout"</strong> and improves the accuracy of the dropout technique's fast approximate model averaging in order to facilitate optimization. </li>
</ol>
<p style="padding-left: 60px">Maxout networks learn not just the relationship between hidden units, but also the activation function of each hidden unit. By actively dropping out hidden units, the network is forced to find other paths to get to the output from a given input during the training process. The following diagram is the graphical depiction of how this works:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/1d788bef-0294-4200-ae1c-2597a552912a.png" style="width:30.00em;height:15.00em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Maxout Network</div>
<p style="padding-left: 60px">The preceding diagram shows the Maxout network with five visible units, three hidden units, and two neurons for each hidden unit. The Maxout function is given by the following equations:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/748c5423-9b57-4857-88fc-e4470c077d46.png" style="width:12.00em;height:1.92em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/3ae85183-c7da-4f19-9e0d-f5d3c8de7156.png" style="width:25.50em;height:3.50em;"/></div>
<p class="mce-root" style="padding-left: 60px">Here <span> W..<sub>ij </sub> is </span>the mean vector of the size of the input obtained by accessing the matrix W ∈  <img class="fm-editor-equation" src="assets/26730fcb-dc57-47ea-a46f-7ef2981a45f8.png" style="width:4.58em;height:1.67em;"/> at the second coordinate <em>i</em> and third coordinate <em>j</em>. The number of intermediate units (<em>k) </em>is called the number of pieces used by the Maxout nets. The following diagram shows how the Maxout function compares to the ReLU and <strong>Parametric Rectified Linear Unit</strong> (<strong>PReLU</strong>) functions:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a0d75714-9de0-455a-a461-1eca6e9a13dc.png" style="width:103.83em;height:25.92em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Graphical comparison of Maxout, ReLU and PReLU function</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sequential workings of RNNs</h1>
                </header>
            
            <article>
                
<p>Recurrent neural networks are a type of artificial neural network designed to recognize and learn patterns in sequences of data. Some of the examples of such sequential data are:</p>
<ul>
<li>Handwriting</li>
<li>Text such as customer reviews, books, source code, and so on</li>
<li>Spoken word / Natural Language</li>
<li>Numerical time series / sensor data</li>
<li>Stock price variation data</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p><span>In recurrent neural networks, the hidden state from the previous time step is fed back into the network at the next time step, as shown in the following diagram:</span></p>
<div class="CDPAlignCenter CDPAlign"><span><img class="alignnone size-full wp-image-1327 image-border" src="assets/dd9b9ab7-9c66-45ee-8200-bc520bbdf0d3.png" style="width:38.50em;height:19.17em;"/><br/></span></div>
<p><span>Basically, the upward facing arrows going into the network represent the inputs (matrices/vectors) to the RNN at each time step, while the upward-facing arrows coming out of the network represent the output of each RNN unit. The horizontal arrows indicate the transfer of information learned in a particular time step (by a particular neuron) onto the next time step.</span></p>
<div class="packt_infobox"><span>M</span>ore information about using RNNs can be found at :<br/>
<a href="https://deeplearning4j.org/usingrnns">https://deeplearning4j.org/usingrnns</a></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p><span>At every node/neuron of a recurrent network, a series of matrix multiplication steps are carried out. The input vector/matrix is multiplied by a weight vector/matrix first, a bias is added to this term, and this is finally passed through an activation function to produce the output (just as in the case of feedforward networks):</span></p>
<ol>
<li><span>The following diagram shows an intuitive and mathematical way of visualizing RNNs in the form of a computational graph:</span></li>
</ol>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/8f28fe47-36f1-491d-a3e9-b4c7920b29f3.png" style="width:104.58em;height:34.67em;"/></div>
<ol start="2">
<li>At the first time step (which is <em>t=0</em>), <em>h</em><sub><em>0</em> </sub>is calculated using the first formula on the right-hand side of the preceding diagram. Since <em>h</em><sup><em>-1</em> </sup>does not exist, the middle term becomes zero.</li>
<li>The input matrix <em>x</em><sub><em>0</em> </sub>is multiplied by the weight matrix <em>w<sub>i</sub></em> and a bias <em>b<sub>h</sub></em><span> </span>is added to this term.</li>
<li>The two preceding matrices are added and then passed through an activation function <em>g<sub>h</sub></em><span> </span>to obtain <em>h<sub>0</sub></em>.</li>
<li>Similarly, <em>y<sub>0</sub></em><span> </span>is calculated using the second equation on the right-hand side of the preceding diagram by multiplying <em>h<sub>0</sub></em><span> </span>with the weight matrix <em>w<sub>y</sub></em>, adding a bias <em>b<sub>y</sub></em><span> </span>to it, and passing it through an activation function <em>g<sub>y</sub></em>.</li>
</ol>
<ol start="6">
<li>At the next time step (which is <em>t=1</em>), <em>h<sup>(t-1)</sup></em><span> </span>does exist. It is nothing but <em>h<sub>0</sub></em>. This term, multiplied with the weight matrix <em>w<sub>R</sub></em><span>, </span>is also provided as the input to the network along with the new input matrix <em>x<sub>1</sub></em>.</li>
<li>This process is repeated over a number of time steps, and the weights, matrices, and biases flow through the entire network over different time steps.</li>
<li>This entire process is executed over one single iteration, which constitutes the forward pass of the network.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>To train feedforward neural networks the most commonly used technique is backpropagation through time. It is a supervised learning method used to reduce the loss function by updating weights and biases in the network after every time step. A number of training cycles (also known as epochs) are executed where the error determined by the loss function is backward propagated by a technique called gradient descent. At the end of each training cycle, the network updates its weights and biases to produce an output which is closer to the desired output, until a sufficiently small error is achieved :</p>
<ol>
<li>The backpropagation algorithm basically implements the following three fundamental steps during every iteration:
<ul>
<li>The forward pass of the input data and calculating the loss function</li>
<li>The computation of gradients and errors</li>
<li>Backpropagation through time and adjustment of weights and biases accordingly</li>
</ul>
</li>
<li><span>After the weighted sum of inputs (passed through an activation function after adding a bias) is fed into the network and an output is obtained, the network immediately compares how different the predicted output is from the actual case (correct output).</span></li>
<li>Next, the error is calculated by the network. This is nothing but the network output subtracted from the actual/correct output.</li>
</ol>
<ol start="4">
<li><span>The next step involves backpropagation through the entire network based on the calculated error. The weights and biases are then updated to notice whether the error increases or decreases.</span></li>
<li><span>The network also remembers whether the error increases by increasing the weights and biases or by decreasing the weights and biases.</span></li>
<li><span>Based on the preceding inferences, the network continues to update the weights and biases during every iteration in a manner such that the error becomes minimal. The following example will make things clearer.</span></li>
<li><span>Consider a simple case of teaching a machine how to double a number, as shown in the following table:</span></li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1329 image-border" src="assets/600d03c8-8815-4d5f-8465-1b4f8eba843f.png" style="width:40.92em;height:7.42em;"/></div>
<ol start="8">
<li>As you can see, by initializing the weights randomly (<em>W = 3</em>), we obtain outputs of 0, 3, 6, and 9.</li>
<li>The error is calculated by subtracting the column of correct outputs from the column of model outputs. The square error is nothing but each error term multiplied by itself. It is usually a better practice to use square error as it eliminates negative values from error terms.</li>
<li>The model then realizes that in order to minimize the error, the weight needs to be updated.</li>
<li class="mce-root"><span>Let's suppose the model updates its weight to</span> <em>W = 4</em> <span>during the next iteration. This would result in the following output:</span></li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1332 image-border" src="assets/00dc3ea3-a4e2-4fd9-b66c-67daf62635fc.png" style="width:40.67em;height:7.42em;"/></div>
<ol start="12">
<li>The model now realizes that the error actually increased by increasing the weight to <em>W = 4</em>. Therefore, the model updates its weight by reducing it to <em>W = 2</em> in its next iteration, which results in the actual/correct output.</li>
<li><span>Note that, in this simple case, the error increases when the weight is increased and reduces when the weight is decreased, as follows:</span></li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1333 image-border" src="assets/08d9f110-c76b-4e1a-8cf5-4e3cdca260b9.png" style="width:38.67em;height:21.92em;"/></div>
<ol start="14">
<li>In an actual neural network, a number of such weight updates are performed during every iteration until the model converges with the actual/correct output.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root">As seen in the preceding case, the error increased when the weight was increased but decreased when the weight was decreased. But this may not always be the case. The network uses the following graph to determine how to update weights and when to stop updating them:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1334 image-border" src="assets/f17c4901-1b95-4398-92e5-7f12a8765452.png" style="width:117.92em;height:62.67em;"/></div>
<ul>
<li>Let the weights be initialized to zero at the beginning of the first iteration. As the network updates its weights by increasing them from point <span class="packt_screen">A</span> to <span class="packt_screen">B</span>, the error rate begins to decrease.</li>
<li>Once the weights reach point <span class="packt_screen">B</span>, the error rate becomes minimal. The network constantly keeps track of the error rate.</li>
<li>On further increasing the weights from point <span class="packt_screen">B</span> towards point <span class="packt_screen">C</span>, the network realizes that the error rate begins to increase again. Thus, the network stops updating its weights and reverts back to the weights at point <span class="packt_screen">B,</span> as they are optimal.</li>
<li>In the next scenario, consider a case where the weights are randomly initialized to some value (let's say, point <span class="packt_screen">C</span>), as shown in the following graph:</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1335 image-border" src="assets/1d3effb7-7d07-42f1-a813-eaa414d61e61.png" style="width:117.50em;height:62.08em;"/></div>
<ul>
<li>On further increasing these random weights, the error also increases ( starting at point <span class="packt_screen">C</span> and moving away from point <span class="packt_screen">B</span>, indicated by the small arrow in the graph).</li>
<li>The network realizes that the error increased and begins to decrease the weights from point <span class="packt_screen">C</span> so that the error decreases (indicated by the long arrow from point <span class="packt_screen">C</span> moving towards point B in the graph). This decrease of weights happens until the error reaches a minimal value (point <span class="packt_screen">B</span> on the graph).</li>
<li>The network continues to further update its weights even after reaching point <span class="packt_screen">B</span> (indicated by the arrow moving away from point <span class="packt_screen">B</span> and towards point <span class="packt_screen">A</span> on the graph). It then realizes that the error is again increasing. As a result, it stops the weight update and reverts back to the weights that gave the minimal error value (which are the weights at point <span class="packt_screen">B</span>).</li>
<li>This is how neural networks perform weight updates after backpropagation. This kind of weight update is momentum-based. It relies on the computed gradients at each neuron of the network during every iteration, as shown in the following diagram:</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1336 image-border" src="assets/8b87bfbc-987b-4a20-8700-a1f650992629.png" style="width:52.58em;height:18.25em;"/></div>
<p>Basically, the gradients are computed for each input with respect to the output every time an input flows into a neuron. The chain rule is used to compute the gradients during the backward pass of backpropagation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>A detailed explanation of the math behind backpropagation can be found at the following links:</p>
<ul>
<li><a href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/">https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/</a></li>
<li><a href="https://becominghuman.ai/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c">https://becominghuman.ai/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5</a></li>
</ul>
<p>Andrej Karpathy's blog has tons of useful information about recurrent neural networks. Here is a link explaining their unreasonable effectiveness:</p>
<ul>
<li><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">http://karpathy.github.io/2015/05/21/rnn-effectiveness/</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pain point #1 – The vanishing gradient problem</h1>
                </header>
            
            <article>
                
<p>Recurrent neural networks are great for tasks involving sequential data. However, they do come with their drawbacks. This section will highlight and discuss one such drawback, known as the <strong>vanishing gradient problem</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>The name vanishing gradient problem stems from the fact that, during the backpropagation step, some of the gradients vanish or become zero. Technically, this means that there is no error term being propagated backward during the backward pass of the network. This becomes a problem when the network gets deeper and more complex.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>This section will describe how the vanishing gradient problem occurs in recurrent neural networks:</p>
<ul>
<li>While using backpropagation, the network first calculates the error, which is nothing but the model output subtracted from the actual output squared (such as the square error).</li>
<li>Using this error, the model then computes the change in error with respect to the change in weights (de/dw).</li>
<li>The computed derivative multiplied by the learning rate<span> </span><img class="fm-editor-equation" src="assets/02520e4e-7b92-446d-a359-0cc7fd1ec5cf.png" style="width:1.17em;height:1.17em;"/><span> </span>gives <img class="fm-editor-equation" src="assets/8a3a2dce-1669-41c3-90fd-ab676c3af592.png" style="width:1.17em;height:1.17em;"/>w, which is nothing but the change in weights. The term<span> </span><img class="fm-editor-equation" src="assets/f70f52a8-c4f3-47b7-b6a9-c7bc77d6b74f.png" style="width:1.17em;height:1.17em;"/>w is added to the original weights to update them to the new weights.</li>
<li>Suppose the value of de/dw (the gradient or rate of change of error with respect to weights) is much less than 1, then that term multiplied by the learning rate <img class="fm-editor-equation inline-image" src="assets/20098d2b-b23b-4c96-93fe-0e39482b627b.png" style="width:1.08em;height:1.08em;"/> (which is always much less than 1) gives a very small, negligible, number which is negligible.</li>
<li>This happens because the weight updates during backpropagation are only accurate for the most recent time step, and this accuracy reduces while backpropagating through the previous time steps and becomes almost insignificant when the weight updates flow through many steps back in time.</li>
<li>There may be certain cases where sentences may be extremely long and the neural network is trying to predict the next word in a sentence. It does so based on the context of the sentence, for which it needs information from many previous time steps (these are called <strong>long-term dependencies</strong>). The number of previous times steps the network needs to backpropagate through increases with the increasing length of sentences. In such cases, the recurrent networks become incapable of remembering information from many time steps in the past and therefore are unable to make accurate predictions.</li>
<li>When such a scenario occurs, the network requires many more complex calculations, as a result of which the number of iterations increases substantially and during which the change in error term vanishes (by reducing over time) and changes in weight (<img class="fm-editor-equation" src="assets/c5361123-15dd-4a54-a3c1-48c41ba7b1d2.png" style="width:1.17em;height:1.17em;"/>w) become negligibly small. As a result, the new or updated weight is almost equal to the previous weight.</li>
<li>Since there is no weight update occurring, the network stops learning or being able to update its weights, which is a problem as this will cause the model to overfit the data.</li>
<li>This entire process is illustrated in the following diagram:</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1340 image-border" src="assets/32031e59-d92e-4da9-b804-82fe2cfc4381.png" style="width:39.58em;height:24.50em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>This section will describe some of the repercussions of the vanishing gradient problem:</p>
<ol>
<li><span>This problem occurs when we train a neural network model using</span><span> some sort of</span> <span>optimization techniques which are gradient based.</span></li>
<li class="graf graf--p graf-after--figure">Generally, adding more<span> </span>hidden layers<span> </span>tends to make the network able to learn more<span> </span>complex arbitrary functions, and thus do a better job in predicting future outcomes. Deep Learning makes a big difference due to the large number of<span> </span>hidden layers<span> </span>it has, ranging from 10 to 200. It is now possible to make sense of complicated sequential data, and perform tasks such as Speech Recognition, Image Classification, Image Captioning, and more.</li>
<li><span>The problem caused by the preceding steps is that, in some cases, the gradients become so small that they almost vanish, which in turn prevents the weights from updating their values during future time steps.</span></li>
<li>In the worst case, it could result in the training process of the network being stopped, which means that the network stops learning the different features it was intended to learn through the training steps.</li>
<li>The main idea behind backpropagation is that it allows us, as researchers, to monitor and understand how machine learning algorithms process and learn various features. When the gradients vanish, it becomes impossible to interpret what is going on with the network, and hence identifying and debugging errors becomes even more of a challenge.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p><span>The following are some of the ways in which the problem of vanishing gradients can be solved:</span></p>
<ul>
<li>One method to overcome this problem to some extent by using the ReLU activation function. <span>It computes the function </span><em><span class="MathJax"><span class="math"><span><span class="mrow"><span class="mi">f</span><span class="mo">(</span><span class="mi">x</span><span class="mo">)</span><span class="mo">=</span><span class="mo">max</span><span class="mo">(</span><span class="mn">0</span><span class="mo">,</span><span class="mi">x</span><span class="mo">) (i.e., t</span></span></span></span></span></em><span>he activation function simply thresholds the lower level of outputs at zero) and prevents the network from producing negative gradients.</span></li>
<li>Another way to overcome this problem is to perform unsupervised training on each layer separately and then fine-tune the entire network through backpropagation, as done by Jürgen Schmidhuber in his study of multi-level hierarchy in neural networks. The link to this paper is provided in the following section.</li>
<li>A third solution to this problem is the use of<span> </span><strong>LSTM</strong><span> </span>(<strong>Long Short-Term Memory</strong>) units or<span> </span><strong>GRUs (Gated Recurrent Units)</strong><span>, </span>which are special types of RNNs.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>The following links provide a more in-depth description of the vanishing gradient problem and also some ways to tackle the issue:</p>
<ul>
<li><a href="https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b">https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b</a></li>
<li><a href="http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/readings/L15%20Exploding%20and%20Vanishing%20Gradients.pdf">http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/readings/L15%20Exploding%20and%20Vanishing%20Gradients.pdf</a></li>
<li><a href="http://people.idsia.ch/~juergen/cvpr2012.pdf">http://people.idsia.ch/~juergen/cvpr2012.pdf</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pain point #2 – The exploding gradient problem</h1>
                </header>
            
            <article>
                
<p>Another drawback of recurrent neural networks is the problem of exploding gradients. This is similar to the vanishing gradient problem but the exact opposite. Sometimes, during backpropagation, the gradients explode to extraordinarily large values. As with the vanishing gradient problem, the problem of exploding gradients occurs when network architectures get deeper.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p><span>The name exploding gradient problem stems from the fact that, during the backpropagation step, some of the gradients vanish or become zero. Technically, this means that there is no error term being propagated backward during the backward pass of the network. This becomes a problem when the network gets deeper and more complex.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>This section will describe the exploding gradient problem in recurrent neural networks:</p>
<ul>
<li>The <span>exploding gradient problem</span> is very similar to the vanishing gradient problem, but just the opposite.</li>
<li>When long-term dependencies arise in recurrent neural networks, the error term is propagated backward through the network sometimes explodes or becomes very large.</li>
<li><span>This error term multiplied by the learning rate results in an extremely large <img class="fm-editor-equation" src="assets/ec6e0c62-a9ca-4280-87e8-d789ac947387.png" style="width:1.17em;height:1.17em;"/>w. This gives rise to new weights that look very different from the previous weights. It is called the exploding gradient problem because the value of the gradient becomes too large.</span></li>
<li><span>The problem of exploding gradients is illustrated in an algorithmic fashion, in the following diagram:</span></li>
</ul>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1341 image-border" src="assets/f498abc3-6230-497a-aa81-2d8083fb592b.png" style="width:78.25em;height:48.83em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Since neural networks use a gradient-based optimization technique to learn features present in data, it is essential that these gradients are preserved in order for the network to calculate an error based on the change in gradients. This section will describe how the exploding gradient problem occurs in recurrent neural networks:</p>
<ul>
<li>While using backpropagation, the network first calculates the error, which is nothing but the model output subtracted from the actual output squared (such as the square error).</li>
<li>Using this error, the model then computes the change in error with respect to the change in weights (de/dw).</li>
<li>The computed derivative multiplied by the learning rate<span> </span><img class="fm-editor-equation" src="assets/f2644c6b-a2bf-4afe-946a-f3449fcf08a7.png" style="width:1.17em;height:1.17em;"/><span> </span>gives <img class="fm-editor-equation" src="assets/0a500dfb-7dc4-4cb8-b8b4-fa4244c67565.png" style="width:1.17em;height:1.17em;"/>w, which is nothing but the change in weights. The term<span> </span><img class="fm-editor-equation" src="assets/f5e5c7a4-64c5-49ff-b331-1110e011cfbb.png" style="width:1.17em;height:1.17em;"/>w is added to the original weights to update them to the new weights.</li>
<li>Suppose the value of de/dw (the gradient or rate of change of error with respect to weights) is greater than 1, then that term multiplied by the learning rate <img class="fm-editor-equation inline-image" src="assets/31f9faa4-34d0-441a-9b0d-8f7fde4012d7.png" style="width:1.08em;height:1.08em;"/> gives a very, very large number that is of no use to the network while trying to optimize weights further, since the weights are no longer in the same range.</li>
<li>This happens because the weight updates during backpropagation are only accurate for the most recent time step, and this accuracy reduces while backpropagating through the previous time steps and becomes almost insignificant when the weight updates flow through many steps back in time.</li>
<li>The number of previous times steps the network needs to backpropagate through increases with the increase in the number of sequences in the input data. In such cases, the recurrent networks become incapable of remembering information from many time steps in the past and therefore are unable to make accurate predictions of future time steps.</li>
<li>When such a scenario occurs, the network requires many more complex calculations, as a result of which the number of iterations increases substantially and during which the change in error term increases beyond 1 and changes in weight (<img class="fm-editor-equation inline-image" src="assets/8e89f7be-6b0a-45b3-812d-7a0bc4274146.png" style="width:1.17em;height:1.17em;"/>w) explode. As a result, the new or updated weight is completely out of range when compared to the previous weight.</li>
<li>Since there is no weight update occurring, the network stops learning or being able to update its weights within a specified range, which is a problem as this will cause the model to overfit the data.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>The following are some of the ways in which the problem of exploding gradients can be solved:</p>
<ul>
<li>Certain gradient clipping techniques can be applied to solve this issue of exploding gradients.</li>
<li>Another way to prevent this is by using truncated Backpropagation Through Time, where instead of starting the backpropagation at the last time step (or output layer), we can choose a smaller time step (say, 15) to start backpropagating. This means that the network will backpropagate through only the last 15 time steps at one instance and learn information related to those 15-time steps only. This is similar to feeding in mini batches of data to the network as it would become far too computationally expensive to compute the gradient over every single element of the dataset in the case of very large datasets.</li>
<li>The final option to prevent the explosion of gradients is by monitoring them and adjusting the learning rate accordingly.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>A more detailed explanation of the vanishing and exploding gradient problems can be found at the following links:</p>
<ul>
<li><a href="http://neuralnetworksanddeeplearning.com/chap5.html">http://neuralnetworksanddeeplearning.com/chap5.html</a></li>
<li><a href="https://www.dlology.com/blog/how-to-deal-with-vanishingexploding-gradients-in-keras/">https://www.dlology.com/blog/how-to-deal-with-vanishingexploding-gradients-in-keras/</a></li>
<li><a href="https://machinelearningmastery.com/exploding-gradients-in-neural-networks/">https://machinelearningmastery.com/exploding-gradients-in-neural-networks/</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sequential working of LSTMs</h1>
                </header>
            
            <article>
                
<p><span><strong>Long Short-Term Memory Unit</strong> (<strong>LSTM</strong>) cells are nothing but slightly more advanced architectures compared to Recurrent Networks. LSTMs can be thought of as a special kind of Recurrent Neural Networks with the capabilities of learning long-term dependencies that exist in sequential data. The main reason behind this is the fact that LSTMs contain memory and are able to store and update information within their cells unlike Recurrent Neural Networks.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>The main components of a Long Short-Term Memory unit are as follows:</p>
<ul>
<li>The input gate</li>
<li>The forget gate</li>
<li>The update gate</li>
</ul>
<p><span>Each of these gates is made up of a sigmoid layer followed by a pointwise multiplication operation. The sigmoid layer outputs numbers between zero and one. These values describe  how much information of each component is allowed to pass through the respective gate. A value of zero means the gate will allow nothing to pass through it, while a value of one means the gate allows all the information to pass through.</span></p>
<p>The best way to understand LSTM cells is through computational graphs, just like in the case of recurrent neural networks.</p>
<p>LSTMs were originally developed by Sepp Hochreiter and Jurgen Schmidhuber in 1997. The following is, link to their published paper:</p>
<ul>
<li><a href="http://www.bioinf.jku.at/publications/older/2604.pdf">http://www.bioinf.jku.at/publications/older/2604.pdf</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>This section will describe the inner components of a single LSTM cell, primarily, the three different gates present inside the cell. A number of such cells stacked together form an LSTM network:</p>
<ol>
<li><span>LSTMs also have a chain-like structure like RNNs. Standard RNNs are basically modules of repeating units like a simple function (for example, tanh).</span></li>
</ol>
<ol start="2">
<li><span>LSTMs have the capability to retain information for long periods of time as compared to RNNs owing to the presence of memory in each unit. This allows them to learn important information during the early stages in a sequence of inputs and also gives it the ability to have a significant impact on the decisions made by the model at the end of each time step.</span></li>
<li><span>By being able to store information right from the early stages of an input sequence, </span><span>LSTMs are actively able to preserve the error that can be backpropagated through time and layers instead of letting that error vanish or explode.</span></li>
<li><span>LSTMs are capable of learning information over many time steps and thus have denser layer architectures by preserving the error which is backpropagated through those layers.</span></li>
<li><span>The cell structures called <strong>"gates"</strong> give the LSTM the ability to retain information, add information or remove information from the <strong>cell state</strong>.</span></li>
<li>The following diagram illustrates the structure of an LSTM. The key feature while trying to  understand LSTMs is in understanding the LSTM network architecture and cell state, which can be visualized here:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1338 image-border" src="assets/2dfbc2de-288c-472f-ab6d-5f045fae0c79.png" style="width:44.92em;height:20.58em;"/></div>
<ol start="7">
<li>In the preceding diagram, <em>x<sub>t</sub></em> and <em>h<sub>t-1</sub></em> are the two inputs to the cell. <em>x</em><sub><em>t</em> </sub>is the input from the current time step, while h<sub>t-1 </sub>is the input from the previous time step (which is the output of the preceding cell during the previous time step). Besides these two inputs, we also have <em>h</em><sub>, </sub>which is the current output (i.e., time step t) from the LSTM cell after performing its operations on the two inputs through its gates.</li>
<li>In the preceding diagram, r<sub>t </sub>represents the output emerging from the input gate, which takes in inputs <em>h</em><sub><em>t-1</em> </sub>and <em>x<sub>t</sub></em>, performs multiplication of these inputs with its weight matrix <em>W<sub>z</sub></em>, and passes them through a sigmoid activation function.</li>
<li>Similarly, the term <em>z<sub>t</sub></em> represents the output emerging from the forget gate. This gate has a set of weight matrices (represented by <em>W<sub>r</sub></em>) which are specific to this particular gate and govern how the gate functions.</li>
<li>Finally, there is <img class="fm-editor-equation inline-image" src="assets/21952021-5706-4316-b91e-71e2c99fd477.png" style="width:1.00em;height:1.83em;"/><sub>t</sub>, which is the output emerging from the update gate. In this case, there are<span> two parts. The first part is a sigmoid layer which is also called the <strong>input gate layer</strong>, and its primary function is deciding which values to update. The next layer is a tanh layer . The primary function of this layer is to create a vector or array containing new values that </span><span> could be added to the cell state.</span></li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>A combination of a number of LSTM cells/units forms an LSTM network. The architecture of such a network is shown in the following diagram:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1339 image-border" src="assets/52895ac7-e1df-4131-a886-f2a72a0c0583.png" style="width:44.50em;height:15.08em;"/></div>
<ol>
<li><span>In the preceding diagram, the full LSTM cell is represented by <em><strong>"A"</strong></em>. The cell takes the current input (<em>x</em></span><em><sub>i</sub></em><span>) of a sequence of inputs, and produces (<em>h</em></span><em><sub>i</sub></em><span>) which is nothing but the output of the current hidden state. This output is then sent to the next LSTM cell as its input.</span></li>
<li><span>An LSTM cell is slightly more complicated than an RNN cell. While the RNN cell has just one function/layer acting on a current input, the LSTM cell has three layers which are the three gates controlling the information flowing through the cell at any given instance in time. </span></li>
<li><span> The cell behaves a lot like the hard disk memory in a computer. The cell, therefore, has the capability to allow the writing, reading and storing of information within its cell state. The cell also makes decisions about what information to store, and when to allow reading, writing, and erasing information. This is facilitated by the gates that open or close accordingly.</span></li>
<li><span> </span><span>The gates present in LSTM cells are analog in contrast to the digital storage systems in today's computers. This means that the gates can only be controlled through an element-wise multiplication through sigmoids, yielding probability values between 0 and 1. A high value will cause the gate to remain open while a low value will cause the gate to remain shut.</span></li>
<li><span>Analog systems have an edge over digital systems when it comes to neural network operations since they are differentiable. This makes analog systems more suitable for tasks like backpropagation which primarily rely on the gradients.</span></li>
<li><span>The gates pass on information or block information or let only a part of the information flow through them based on its strength and importance. The information is filtered at every time step through the sets of weight matrices specific to each gate. Therefore, each gate has complete control over how to act on the information it receives.</span></li>
<li><span>The weight matrices associated with each gate, like the weights that modulate input and hidden states, are adjusted based on the recurrent network's learning process and gradient descent.</span></li>
</ol>
<ol start="8">
<li><span>The first gate is called the <strong>forget gate</strong> and it controls what information is maintained from the previous state. This gate takes the previous cell output (<em>h</em></span><em><sub>t</sub></em><span><em> - 1</em>) as its input along with the current input (<em>x</em></span><em><sub>t</sub></em><span>), and applies a sigmoid activation (</span><img class="fm-editor-equation" src="assets/a15a6578-5902-4ddf-b4af-45fa1aa9bb73.png" style="width:1.00em;height:1.00em;"/><span>) in order to produce and output value between 0 and 1 for each hidden unit. This is followed by the element-wise multiplication with the current state (illustrated by the first operation in the preceding diagram).</span></li>
<li><span>The second gate is called the <strong>update gate</strong> and its primary function is to update the cell state based on the current input. This gate passes the same input as the forget gate's inputs (<em>h</em></span><em><sub>t-1</sub></em><span> and <em>x</em></span><em><sub>t</sub></em><span>) into a sigmoid activation layer </span><span>(</span><img class="fm-editor-equation" src="assets/a15a6578-5902-4ddf-b4af-45fa1aa9bb73.png" style="width:1.00em;height:1.00em;"/>)<span> followed by a tanh activation layer and then performs an element-wise multiplication between these two results. Next, element-wise addition is performed with the result and the current state (illustrated by the second operation in the preceding diagram).</span></li>
<li><span>Finally, there is an output gate which controls what information and how much information gets transferred to the adjoining cell to act as its inputs during the next time step. The current cell state is passed through a tanh activation layer and multiplied element-wise with the cell input (<em>h</em></span><em><sub>t-1</sub></em><span> and <em>x</em></span><em><sub>t</sub></em><span>) after being passed through a sigmoid layer </span><span>(</span><img class="fm-editor-equation" src="assets/a15a6578-5902-4ddf-b4af-45fa1aa9bb73.png" style="width:1.00em;height:1.00em;"/><span>) for this operation.</span></li>
<li><span>The update gate behaves as the filter on what the cell decides to output to the next cell. This output, h<sub>t</sub>, is then passed on to the next LSTM cell as its input, and also to the above layers if many LSTM cells are stacked on top of each other.</span></li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p><span>LSTMs were a big leap forward when compared to what could be accomplished with feedforward networks and Recurrent Neural Networks. One might wonder what the next big step in the near future is, or even what that step might be. A lot researchers do believe "attention" is the next big step when it comes to the field of artificial intelligence. With the amount of data growing vastly with each day it becomes impossible to process every single bit of that data. This is where attention could be a potential game-changer, causing the networks to give their attention only to data or areas which are of high priority or interest and disregard useless information. For example, if an RNN is being used to create an image captioning engine, it will only pick a part of the image to to give its attention to for every word it outputs.</span></p>
<p>The recent (2015) paper by Xu, et al. does exactly this. They explore adding attention to LSTM cells. Reading this paper can be a good place to start learning about the use of attention in neural networks. There have been some good results with using attention for various tasks, and more research is currently being conducted on the subject. The paper by Xu, et al. can be found using the following link:<br/>
<a href="https://arxiv.org/pdf/1502.03044v2.pdf">https://arxiv.org/pdf/1502.03044v2.pdf</a></p>
<p>Attention isn't the only variant to LSTMs. Some of the other active research is based on the utilization of grid LSTMs, as used in the paper by Kalchbrenner, et al., for which the link is at: <a href="https://arxiv.org/pdf/1507.01526v1.pdf">https://arxiv.org/pdf/1507.01526v1.pdf</a>.<a href="https://arxiv.org/pdf/1507.01526v1.pdf"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>Some other useful information and papers related to RNNs and LSTMs in generative networks can be found by visiting the following links:</p>
<ul>
<li><a href="http://www.deeplearningbook.org/contents/rnn.html">http://www.deeplearningbook.org/contents/rnn.html</a></li>
<li><a href="https://arxiv.org/pdf/1502.04623.pdf">https://arxiv.org/pdf/1502.04623.pdf</a></li>
<li><a href="https://arxiv.org/pdf/1411.7610v3.pdf">https://arxiv.org/pdf/1411.7610v3.pdf</a></li>
<li><a href="https://arxiv.org/pdf/1506.02216v3.pdf">https://arxiv.org/pdf/1506.02216v3.pdf</a> </li>
</ul>
<p> </p>


            </article>

            
        </section>
    </body></html>