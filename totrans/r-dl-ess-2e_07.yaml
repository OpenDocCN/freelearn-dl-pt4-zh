- en: Natural Language Processing Using Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter will demonstrate how to use deep learning for **natural language
    processing** (**NLP**). NLP is the processing of human language text. NLP is a
    broad term for a number of different tasks involving text data, which include
    (but are not limited to) the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Document classification**: Classifying documents into different categories
    based on their subject'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Named entity recognition**: Extracting key information from documents, for
    example, people, organizations, and locations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentiment analysis**: Classifying comments, tweets, or reviews as positive
    or negative sentiment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Language translation**: Translating text data from one language to another'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Part of speech tagging**: Assigning the type to each word in a document,
    which is usually used in conjunction with another task'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this chapter, we will look at document classification, which is probably
    the most common NLP technique. This chapter follows a different structure to previous
    chapters, as we will be looking at a single use case (text classification) but
    applying multiple approaches to it. This chapter will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: How to perform text classification using traditional machine learning techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing traditional text classification and deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced deep learning text classification including 1D convolutionals, RNNs,
    LSTMs and GRUs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Document classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will be looking at text classification using Keras. The dataset
    we will use is included in the Keras library. As we have done in previous chapters,
    we will use traditional machine learning techniques to create a benchmark before
    applying a deep learning algorithm. The reason for this is to show how deep learning
    models perform against other techniques.
  prefs: []
  type: TYPE_NORMAL
- en: The Reuters dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use the Reuters dataset, which can be accessed through a function in
    the Keras library. This dataset has 11,228 records with 46 categories. To see
    more information about this dataset, run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Although the Reuters dataset can be accessed from Keras, it is not in a format
    that can be used by other machine learning algorithms. Instead of the actual words,
    the text data is a list of word indices. We will write a short script (`Chapter7/create_reuters_data.R`)
    that downloads the data and the lookup index file and creates a data frame of
    the `y` variable and the text string. We will then save the train and test data
    into two separate files. Here is the first part of the code that creates the file
    with the train data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The second part of the code is similar, it creates the file with the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This creates two files called `../data/reuters.train.tab` and `../data/reuters.test.tab`.
    If we open the first file, this is the first data row. This sentence is a normal
    English sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **y** | **sentence** |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | mcgrath rentcorp said as a result of its december acquisition of space
    co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from
    70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs
    from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from
    12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs
    reuter 3 |'
  prefs: []
  type: TYPE_TB
- en: 'Now that we have the data in tabular format, we can use *traditional* NLP machine
    learning methods to create a classification model. When we merge the train and
    test sets and look at the distribution of the *y* variable, we can see that there
    are 46 classes, but that the number of instances in each class are not the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'For our test set, we will create a binary classification problem. Our task
    will be to identify the news snippets where the classification is 3 from all other
    records. When we change the labels, our *y* distribution changes to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Traditional text classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our first NLP model will use traditional NLP techniques, that is, not deep learning.
    For the rest of this chapter, when we use the term traditional NLP, we mean approaches
    that do not use deep learning. The most used method for NLP in traditional NLP classification
    uses a *bag-of-words* approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will also use a set of hyperparameters and machine learning algorithms to
    maximize accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature generation**: The features can be term frequency, tf-idf, or binary
    flags'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Preprocessing**: We preprocess text data by stemming the words'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Remove stop-words**: We treat the feature creation, stop-words, and stemming
    options as hyperparameters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Machine learning algorithm**: The script applies three machine learning algorithms
    to the data (Naive Bayes, SVM, neural network, and random forest)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We train 48 machine learning algorithms on the data in total, and evaluate
    which model is best. The script for this code is in the `Chapter7/classify_text.R` folder.
    The code does not contain any deep learning models, so feel free to skip it if
    you want. First we load in the necessary libraries and create a function that
    creates a set of text classification models for a combination of hyperparameters
    on multiple machine learning algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have created a sparse data-frame, we will use 4 different machine
    learning algorithms on the data: Naive Bayes, SVM, a neural network model and
    a random forest model. We use 4 machine learning algorithms because, as you see
    below, the code to call a machine learning algorithm is small compared to the
    code needed to create the data in the previous section and the code needed to
    the the NLP. It is almost always a good idea to run multiple machine learning
    algorithms when possible because no machine learning algorithm is consistently
    the best.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We now call the function with different hyperparameters in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'For each combination of hyperparameters, the script saves the best score from
    the four machine learning algorithms in the `best_acc` field. Once the training
    is complete, we can look at the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The results are ordered by best results, so here we can can see that our best
    accuracy overall was `95.24%`. The reason for training so many models is that
    there is no right formula for traditional NLP tasks that's work for most cases,
    so you should try multiple combinations of preprocessing and different algorithms,
    as we have done here. For example, if you searched for an example online on text
    classification, you could find an example that would suggest to use tf-idf and
    naive bayes. Here, we can see that it is one of the worst performers.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning text classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The previous code ran 48 traditional machine learning algorithms over the data
    across a number of different hyperparameters. Now, it is time to see if we can
    find a deep learning model that outperforms them. The first deep learning model
    is in `Chapter7/classify_keras1.R`. The first part of the code loads the data. The
    tokens in the reuters dataset are ranked by how often they occur (in the training
    set) and the `max_features` parameter controls how many distinct tokens will be
    used in the model. We will use all the tokens by setting this to the number of
    entries in the word index. The maxlen parameter controls the length of the input
    sequences to the model, they must all be the same length. If the sequences are
    longer than the maxlen variable, they are truncated, if they are shorter, then
    padding is added to make the length=maxlen. We set this to 250, which means our
    deep learning model expects 250 tokens as input per instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The next section of code builds the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The only thing in this code that we have not seen before is `layer_embedding`. 
    This takes the input and creates an embedding layer, which is a vector of numbers
    for each input token. We will describe word vectors in more detail in the next
    section. Another thing to note is that we don''t preprocess the text or create
    any features – we just feed in the word indices and let the deep learning algorithm
    make sense of it. Here is the output of the script as the model is trained:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Despite the simplicity of the code, we get 94.97% accuracy on the validation
    set after just three epochs, which is only 0.27% less than the best traditional
    NLP approach. Now, it is time to discuss word vectors in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Word vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Instead of representing our text data as a bag of words, deep learning represents
    them as word vectors or embeddings. A vector/embedding is nothing more than a
    series of numbers that represent a word. You may have already heard of popular
    word vectors such as Word2Vec and GloVe. The Word2vec model was invented by Google
    (*Mikolov, Tomas, et al. Efficient estimation of word representations in vector
    space. arXiv preprint arXiv:1301.3781 (2013)*). In their paper, they provide examples
    of how these word vectors have somewhat mysterious and magical properties. If
    you take the vector of the word "*King*", subtract the vector of the word "*Man*",
    add the vector of the word "*Man*", then you get a value close to the vector of
    the word "*Queen"*. Other similarities also exist, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '*vector(''King'') - vector(''Man'') + vector(''Woman'') = vector(''Queen'')*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*vector(''Paris'') - vector(''France'') + vector(''Italy'') = vector(''Rome'')*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If this is the first time you have seen Word2Vec, then you are probably somewhat
    amazed by this. I know I was! These examples imply that word vectors *understand* language,
    so have we solved natural language processing? The answer is no – we are very
    far away from this. Vectors are learned from collections of text documents. In
    fact, the very first layer in our deep learning model is an embedding layer which
    creates a vector space for the words. Let''s look at some of the code from `Chapter7/classify_keras.R`
    again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The value for `max_features` is `30979`, that is, we have `30979` unique features.
    These features are **tokens**, or words. In the traditional text classification,
    we had almost the same number of unique tokens (`30538`). The difference between
    these two numbers is not important; it is due to the different tokenization processes
    used between the two approaches, that is, how the documents were split into tokens.
    The embedding layer has `495664` parameters, which is *30,979 x 16*, that is,
    each unique feature/token is represented by a vector of `16` numbers. The word
    vectors or embeddings learned by deep learning algorithms will have some of the
    characteristics described earlier, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: Synonyms (two words that have the same meaning) will have very similar word
    vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Words from the same semantic collection will be clustered (for example, colors,
    days of the week, makes of cars, and so on)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The vector space between related words can signify the relationship between
    those words (for example, gender for w(King) – w(Queen))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The embedding layer creates word vectors/embeddings based on words and their
    surrounding words. The word vectors end up having these characteristics because
    of a simple fact, which can be summarized by a quote from John Firth, an English
    linguist in 1957:'
  prefs: []
  type: TYPE_NORMAL
- en: '"You shall know a word by the company it keeps."'
  prefs: []
  type: TYPE_NORMAL
- en: The deep learning algorithm learns the vectors for each word by looking at surrounding
    words and therefore learns some of the context. When it sees the word *King*,
    some words near this word may indicate gender, for example, The *King* picked
    up *his* sword. Another sentence could be The *Queen* looked in *her* mirror.
    The word vectors for *King* and *Queen* have a latent gender component that is
    learned from the words surrounding *King* and *Queen* in the data. But it is important
    to realize that the deep learning algorithm has no concept of what gender is,
    or what type of entities it applies to. Even so, word vectors are a huge improvement
    over bag-of-word approaches which have no way of identifying relationships between
    different tokens. Using word vectors also means that we do not have to discard
    sparse terms. Finally, as the number of unique tokens increases, they are much
    more efficient to process than bag-of-words approaches.
  prefs: []
  type: TYPE_NORMAL
- en: We will look at embeddings again in [Chapter 9](e0045e3c-8afd-4e59-be9f-29e652a9a8b1.xhtml), *Anomaly
    Detection and Recommendation Systems*, when we use them in auto-encoders. Now
    that we have seen some traditional machine learning and deep learning approaches
    for solving this problem, it is time to compare them in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing traditional text classification and deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The traditional text classification performed a number of preprocessing steps,
    including word stemming, stop-word processing, and feature generation (tf-idf,
    tf or binary). The deep learning text classification did not need this preprocessing.
    You may have heard various reasons for this previously:'
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning can learn features automatically, so feature creation is not needed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning algorithms for NLP tasks requires far less preprocessing than traditional text
    classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is some truth to this, but this does not answer why we need complex feature
    generation in traditional text classification. A big reason that preprocessing
    is needed in traditional text classification is to overcome a fundamental problem.
  prefs: []
  type: TYPE_NORMAL
- en: For some traditional NLP approaches (for example, classification), text preprocessing
    is not just about creating better features. It is also necessary because the bag-of-words
    representation creates a sparse high-dimensional dataset. Most machine learning
    algorithms have problems with such datasets, meaning that we have to reduce the
    dimensionality before applying machine learning algorithms. Proper preprocessing
    of the text is an essential part of this to ensure that relevant data is not thrown
    away.
  prefs: []
  type: TYPE_NORMAL
- en: 'For traditional text classification, we used an approach called **bag-of-words**.
    This is essentially one-hot encoding each ***token*** (word). Each column represents
    a single token, and the value of each cell is one of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A **tf-idf** (**term frequency, inverse document frequency**) for that token
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The term frequency, that is, the count of how many times that token occurs for
    that document/instance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A binary flag, that is, one if the token is in that document/instance; otherwise,
    it is zero
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You may not have heard of *tf-idf* before. It measures the importance of a token
    by calculating the term frequency (*tf*) of the token in the document (such as
    how many times it occurs in the document) divided by the log of how many times
    it appears in the entire corpus (*idf*). The **corpus** is the entire collection
    of documents. The *tf* part measures how important the token is within a single
    document, and the *idf* measures how unique the token is among all the documents.
    If the token appears many times in the document, but also many times in other
    documents, then it is unlikely to be useful for categorizing documents. If the
    token appears in only a few documents, then it is a potentially valuable feature
    for the classification task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our traditional text classification approach also used *stemming* and processed
    *stop-words*. Indeed, our best result in traditional text classification used
    both approaches. Stemming tries to reduce words to their word stem or root form,
    which reduces the vocabulary size. It also means that words with the same meaning
    but with different verb tenses or noun forms are standardized to the same token.
    Here is an example of stemming. Note that 6/7 of the input words have the same
    output value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Stop-words are common words that appear in most documents for a language. They
    occur so frequently in most documents that they are almost never useful for machine
    learning. The following example shows the list of stop-words for the English language:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The final piece we want to cover in traditional NLP is how it deals with sparse
    terms. Recall from earlier that traditional NLP uses a bag-of-words approach,
    where each unique token gets an individual column. For a large collection of documents,
    there will be thousands of unique tokens, and since most tokens will not appear
    in an individual document, this a very sparse representation, that is, most cells
    are empty. We can check this by looking at taking some of the code from `classify_text.R`,
    modifying it slightly, and looking at the `dtm` and `dtm2` variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We can see that our first document-term matrix (dtm) has 11,228 documents and
    30,538 unique tokens. In this document-term matrix, only 768,265 (0.22%) cells
    have values. Most machine learning algorithms would struggle with such a high-dimensionality sparse
    data frame. If you tried using these machine learning algorithms (for example,
    SVM, random forest, naive bayes) on a data frame with 30,538 dimensions, they
    would fail to run in R (I tried!). This is a known problem in traditional NLP,
    so there is a function (`removeSparseTerms`) in the NLP libraries to remove sparse
    terms from the document-term matrix. This removes columns that have the most empty
    cells. We can see the effect of this, as the second document-term matrix has only
    230 unique tokens and 310,275 (12%) cells have values. This dataset is still relatively
    sparse, but it is in a usable format for machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'This highlights the problem with traditional NLP approaches: the *bag-of-words* approach
    creates a very sparse high-dimensional dataset which is not usable by machine
    learning algorithms. Therefore, you need to remove some of the dimensions, and
    this results in a number of cells with values going from 768,265 to 310,275 in
    our example. We threw away almost 60% of the data before applying any machine
    learning! This also explains why text preprocessing, such as stemming and stop-word
    removal, is used in traditional NLP. Stemming helps to reduce the vocabulary and
    standardize terms by combining variations of many words into one form.'
  prefs: []
  type: TYPE_NORMAL
- en: 'By combining variations, it means they are more likely to survive the culling
    of data. We process stop-words for the opposite reason: if we don''t remove stop-words,
    these terms will probably be kept after removing sparse terms. There are 174 terms
    in the `stopwords()` function in the `tm` package. If the reduced dataset had
    many of these terms, then they would probably not be useful as predictor variables
    due to their commonality throughout the documents.'
  prefs: []
  type: TYPE_NORMAL
- en: It is also worth noting that this is a very small dataset in NLP terms. We only
    have 11,228 documents and 30,538 unique tokens. A larger ***corpus*** (collection
    of text documents) could have half a million unique tokens. In order to reduce
    the number of tokens to something that could be processed in R, we would have
    to throw away a lot more data.
  prefs: []
  type: TYPE_NORMAL
- en: When we use a deep learning approach for NLP, we represent the data as word
    vectors/embeddings rather than using the bag-of-words approach in traditional
    NLP. This is much more efficient, so do not have to preprocess data to remove
    common words, reduce words to a simpler form, or reduce the number of terms before
    applying the deep learning algorithm. The only thing we do have to do is pick
    an embedding size and a max length size for the number of tokens we process for
    each instance. This is needed because deep learning algorithms cannot use variable
    length sequences as inputs to a layer. When instances have more tokens than the
    max length, they are truncated and when instances have less tokens than the max
    length, they are padded.
  prefs: []
  type: TYPE_NORMAL
- en: 'After all of this, you may be wondering why the deep learning algorithm did
    not outperform the traditional NLP approach significantly, if the traditional NLP
    approach throws away 60% of the data. There are a few reasons for this:'
  prefs: []
  type: TYPE_NORMAL
- en: The dataset is small. If we had more data, the deep learning approach would
    improve at a faster rate than the traditional NLP approach.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Certain NLP tasks such as document classification and sentiment analysis depend
    on a very small set of terms. For example, to differentiate between sports news
    and financial news, maybe 50 selected terms would be sufficient to get over 90%
    accuracy. Recall that the function to remove sparse terms in the traditional text
    classification approach – this works because it assumes (correctly) that non-sparse
    terms will be useful features for the machine learning algorithms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We ran 48 machine learning algorithms and only one deep learning approach, which
    was relatively simple! We will soon come across approaches that beat the traditional NLP
    approach.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This book has really only touched the surface of traditional NLP approaches.
    Entire books have been written on the subject. The purpose of looking at these
    approaches is to show how brittle these approaches can be. The deep learning approach
    is much simpler to understand and has far fewer settings. It does not involve
    preprocessing the text or creating features based on weightings such as tf-idf.
    Even so, our first deep learning approach is not very far away from the best model
    out of 48 models in traditional text classification.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced deep learning text classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our basic deep learning model is much less complex than the traditional machine
    learning approach, but its performance is not quite as good. This section looks
    at some advanced techniques for text classification in deep learning. The following
    sections explain a number of different approaches and focus on code examples rather
    than heavy deep explanations. If you are interested in more detail, then look
    at the book *Deep Learning* by Goodfellow, Bengio, and Courville (*Goodfellow,
    Ian, et al. Deep learning. Vol. 1\. Cambridge: MIT Press, 2016.*). Another good
    reference that covers NLP in deep learning is a book by Yoav Goldberg (*Goldberg,
    Yoav. Neural network methods for natural language processing*).'
  prefs: []
  type: TYPE_NORMAL
- en: 1D convolutional neural network model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have seen that the bag-of-words approach in traditional NLP approaches ignores
    sentence structure. Consider applying a sentiment analysis task on the four movie
    reviews in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Id** | **sentence** | **Rating (1=recommended, 0=not recommended)** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | this movie is very good | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | this movie is not good | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | this movie is not very good | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | this movie is not bad | 1 |'
  prefs: []
  type: TYPE_TB
- en: 'If we represent this as a bag of words with term frequency, we will get the
    following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Id** | **bad** | **good** | **is** | **movie** | **not** | **this** | **very**
    |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 | 1 | 1 | 1 | 0 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0 | 1 | 1 | 1 | 1 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0 | 1 | 1 | 1 | 1 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 1 | 0 | 1 | 1 | 1 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: In this simple example, we can see some of the problems with a bag-of-words
    approach, we have lost the relationship between the negation (**not**) and the
    adjectives (**good**, **bad**). To work around this problem, traditional NLP could
    use bigrams, so instead of using single words as tokens, use two words as tokens.
    Now, for the second example, **not good** is a single token, which makes it more
    likely that the machine learning algorithm will pick it up. However, we still
    have a problem with the third example (**not very good**), where we will have
    tokens for **not very** and **very good**. These are still ambiguous, as **not
    very** implies negative sentiment, while **very good** implies positive sentiment.
    We could try higher order n-grams, but this further exacerbates the sparsity problem
    we saw in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Word vectors or embeddings have the same problem. We need some method to handle
    word sequences. Fortunately, there are types of layers in deep learning algorithms
    that can handle sequential data. One that we have already seen is convolutional
    neural networks in [Chapter 5](1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml),* Image
    Classification Using Convolutional Neural Networks*. Recall that these are 2D
    patches that are moved across the image to identify patterns such as a diagonal
    line or an edge. In a similar manner, we can apply a 1D convolutional neural network
    across the word vectors. Here is an example of using a 1D convolutional neural
    network layer for the same text classification problem. The code is in `Chapter7/classify_keras2.R`.
    We are only showing the code for the model architecture, because that is the only
    change from the code in `Chapter7/classify_keras1.R`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that this follows the same pattern that we saw in the image data;
    we have a convolutional layer followed by a max pooling layer. There are 64 convolutional
    layers with a `length=5`, and so these *learn* local patterns in the data. Here
    is the output from the model''s training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This model is an improvement on our previous deep learning model; it gets 95.73%
    accuracy on the fourth epoch. This beats the traditional NLP approach by 0.49%,
    which is a significant improvement. Let's move on to other methods that also look
    to matching sequences. We will start with **recurrent neural networks** (**RNNs**).
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent neural network model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The deep learning networks we have seen so far have no concept of memory. Every
    new piece of information is treated as atomic and has no relation to what has
    already occurred. But, sequences are important in time series and text classification,
    especially sentiment analysis. In the previous section, we saw how word structure
    and order matters, and we used CNNs to resolve this. While this approach worked,
    it does not resolve the problem completely as we still must pick a filter size,
    which limits the range of the layer. Recurrent neural networks are deep learning
    layers which are used to solve this problem. They are networks with feedback loops
    that allow information to flow and therefore are able to *remember* important
    features:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/50985a6f-072d-41b1-ad78-6a456fa9d8f1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.1: A recurrent neural network'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, we can see an example of a recurrent neural network.
    Each piece of information (X[o], X[1], X[2]) is fed into a node which predicts
    *y* variables. The predicted value is also passed to the next node as input, thus
    preserving some sequence information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our first RNN model is in `Chapter7/classify_keras3.R`. We have to change some
    of the parameters for the model: we must decrease the number of features used
    to 4,000, our max length to 100, and drop the most common 100 tokens. We must
    also increase the size of the embedding layer to 32 and run it for 10 epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output from the model''s training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The best validation accuracy was after epoch 7, where we got 93.90% accuracy,
    which is not as good as the CNN model. One of the problems with simple RNN models
    is that it is difficult to maintain context as the gap grows between the different
    pieces of information. Let's move onto a more complex model, that is, the LSTM
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Long short term memory model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LSTMs are designed to learn long-term dependencies. Similar to RNNs, they are
    chained and have four internal neural network layers. They split the state into
    two parts, where one part manages short-term state and the other adds long-term
    state. LSTMs have *gates* which control how *memories* are stored. The input gate
    controls which part of the input should be added to the long-term memory. The
    forget gate controls the part of long-term memory that should be forgotten. The
    final gate, the output gate, controls which part of the long-term memory should
    be in the output. This is a brief description of LSTMs – a good reference for
    more details is [http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for our LSTM model is in `Chapter7/classify_keras4.R`. The parameters
    for the model are max length=150, the size of the embedding layer=32, and the
    model was trained for 10 epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output from the model''s training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The best validation accuracy was after epoch 5, when we got 95.37% accuracy,
    which is a big improvement on the simple RNN model, although still not as good
    as the CNN model. We will cover GRU cells next, which are a similar concept to LSTM.
  prefs: []
  type: TYPE_NORMAL
- en: Gated Recurrent Units model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Gated recurrent units (GRUs)** are similar to LSTM cells but simpler. They
    have one gate that combines the forget and input gates in LSTM, and there is no
    output gate. While GRUs are simpler than LSTMs and therefore quicker to train,
    it is a matter of debate on whether they are better than LSTMs, as the research
    is inconclusive. Therefore, it is recommended to try both, as the results of your
    task may vary. The code for our GRU model is in `Chapter7/classify_keras5.R`.
    The parameters for the model are max length=150, the size of the embedding layer=32,
    and the model was trained for 10 epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output from the model''s training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The best validation accuracy was after epoch 5, when we got 95.90% accuracy,
    which is an improvement on the 95.37% we got with LSTM. In fact, this is the best
    result we have seen so far. In the next section, we will look at bidirectional
    architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Bidirectional LSTM model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We saw in *Figure 7.1* that RNNs (as well as LSTMs and GRUs) are useful because
    they can pass information forwards. But in NLP tasks, it is also useful to look
    backwards. For example, the following two strings have the same meaning:'
  prefs: []
  type: TYPE_NORMAL
- en: I went to Berlin in spring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In spring I went to Berlin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bidirectional LSTMs can pass information backwards as well as forwards. The
    code for our bidirectional LSTM model is in `Chapter7/classify_keras6.R`. The
    parameters for the model are max length=150, the size of the embedding layer=32,
    and the model was trained for 10 epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output from the model''s training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The best validation accuracy was after epoch 4, when we got 95.77% accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Stacked bidirectional model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Bidirectional models are good at picking up information from future states
    that can affect the current state. Stacked bidirectional models allow us to stack
    multiple LSTM/GRU layers in a similar manner to how we stack multiple convolutional
    layers in computer vision tasks. The code for our bidirectional LSTM model is
    in `Chapter7/classify_keras7.R`. The parameters for the model are max length=150,
    the size of the embedding layer=32, and the model was trained for 10 epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output from the model''s training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The best validation accuracy was after epoch 4, when we got 95.59% accuracy,
    which is worse than our bidirectional model, which got 95.77% accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Bidirectional with 1D convolutional neural network model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, the best approaches we have seen are from the 1D convolutional neural
    network model which got 95.73%, and the gated recurrent units model which got
    95.90% accuracy. The following code combines them! The code for our bidirectional
    with 1D convolutional neural network model is in `Chapter7/classify_keras8.R`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The parameters for the model are max length=150, the size of the embedding
    layer=32, and the model was trained for 10 epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output from the model''s training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The best validation accuracy was after epoch 6, when we got 96.04% accuracy,
    which beats all of the previous models.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing the deep learning NLP architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is a summary of all of the models in this chapter, ordered by their sequence
    in this chapter. We can see that the best traditional machine learning approach
    got 95.24%, which was beaten by many of the deep learning approaches. While the
    incremental changes from the best traditional machine learning to the best deep
    learning model may seem small at 0.80%, it reduces our misclassified examples
    by 17%, which is a significant relative change:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model** | **Accuracy** |'
  prefs: []
  type: TYPE_TB
- en: '| Best traditional machine learning approach | 95.24% |'
  prefs: []
  type: TYPE_TB
- en: '| Simple deep learning approach | 94.97% |'
  prefs: []
  type: TYPE_TB
- en: '| 1D convolutional neural network model | 95.73% |'
  prefs: []
  type: TYPE_TB
- en: '| Recurrent neural network model | 93.90% |'
  prefs: []
  type: TYPE_TB
- en: '| Long short term memory model | 95.37% |'
  prefs: []
  type: TYPE_TB
- en: '| Gated recurrent units model | 95.90% |'
  prefs: []
  type: TYPE_TB
- en: '| Bidirectional LSTM model | 95.77% |'
  prefs: []
  type: TYPE_TB
- en: '| Stacked bidirectional model | 95.59% |'
  prefs: []
  type: TYPE_TB
- en: '| Bidirectional with 1D convolutional neural network | 96.04% |'
  prefs: []
  type: TYPE_TB
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We really covered a lot in this chapter! We built a fairly complex traditional
    NLP example that had many hyperparameters, as well as training it on several machine
    learning algorithms. It achieved a reputable result of getting 95.24% accuracy.
    However, when we looked into traditional NLP in more detail, we found that it
    had some major problems: it requires non-trivial feature engineering, it creates
    sparse high-dimensional data frames, and it may require discarding a substantial
    amount of data before machine learning.'
  prefs: []
  type: TYPE_NORMAL
- en: In comparison, the deep learning approach uses word vectors or embeddings, which
    are much more efficient and do not require preprocessing. We ran through a number
    of deep learning approaches, including 1D convolutional layers, Recurrent Neural
    Networks, GRUs, and LSTM. We finally combined the two best previous approaches
    into one approach in our final model to get 96.08% accuracy, compared to 95.24%
    by using traditional NLP.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will develop models using TensorFlow. We will look at
    TensorBoard, which allows us to visualize and debug complex deep learning models.
    We will also look at using TensorFlow estimators, an alternative option for using TensorFlow.
    Then, we will also look at TensorFlow Runs, which automates a lot of the steps
    for hyperparameter tuning. Finally, we will look at options for deploying deep
    learning models.
  prefs: []
  type: TYPE_NORMAL
