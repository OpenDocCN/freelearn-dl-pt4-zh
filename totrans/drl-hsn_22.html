<html><head></head><body>
<div id="sbo-rt-content"><section class="level2 chapterHead" id="chapter-22-multi-agent-rl">
<h1 class="chapterNumber">22</h1>
<h1 class="chapterTitle" id="sigil_toc_id_425">
<span id="x1-41100022"/>Multi-Agent RL
    </h1>
<p>In the last chapter, we discussed discrete optimization problems. In this final chapter, we will introduce multi-agent reinforcement learning (sometimes abbreviated to MARL), a relatively new direction of <span class="cmbx-10x-x-109">reinforcement learning</span> (<span class="cmbx-10x-x-109">RL</span>) and deep RL, which is related to situations when multiple agents communicate in an environment. In real life, such problems appear in auctions, broadband communication networks, Internet of Things, and other scenarios.</p>
<p>In this chapter, we will just take a quick glance at MARL and experiment a bit with simple environments; but, of course, if you find it interesting, there are lots of things you can experiment with. In our experiments, we will use a straightforward approach, with agents sharing the policy that we are optimizing, but the observation will be given from the agent’s standpoint and include information about the other agent’s location. With that simplification, our RL methods will stay the same, and just the environment will require preprocessing and must handle the presence of multiple agents.</p>
<p>More specifically, we will:</p>
<ul>
<li>
<p>Start with an overview of the similarities and differences between the classical single-agent RL problem and MARL</p>
</li>
<li>
<p>Explore the MAgent environment, which was implemented and open sourced by the Geek.AI UK/China research group and later adopted by The Farama Foundation</p>
</li>
<li>
<p>Use MAgent to train models in different environments with several groups of agents</p>
</li>
</ul>
<section class="level3 sectionHead" id="what-is-multi-agent-rl">
<h1 class="heading-1" id="sigil_toc_id_373"> <span id="x1-41200022.1"/>What is multi-agent RL?</h1>
<p>The multi-agent setup is <span id="dx1-412001"/>a natural extension of the familiar RL model that we covered in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109"> </span><a href="ch005.xhtml#x1-190001"><span class="cmti-10x-x-109">1</span></a>. In the classical RL setup, we have one agent communicating with the environment using observations, rewards, and actions. But in some problems that often arise in real life, we have several agents involved in the environment interaction. To give some concrete examples:</p>
<ul>
<li>
<p>A chess game, when our program tries to beat the opponent</p>
</li>
<li>
<p>A market simulation, like product advertisements or price changes, when our actions might lead to counter-actions from other participants</p>
</li>
<li>
<p>Multiplayer games, like Dota 2 or StarCraft II, when the agent needs to control several units competing with other players’ units (in this scenario, several units controlled by a single player might also cooperate to reach the goal)</p>
</li>
</ul>
<p>If other agents are outside of our control, we can treat them as part of the environment and still stick to the normal RL model with the single agent. As you saw in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109"> </span><a href="ch024.xhtml#x1-36400020"><span class="cmti-10x-x-109">20</span></a>, training via self-play is a very powerful technique, which might lead to good policies without much sophistication on the environment side. But in some situations, that’s too limited and not exactly what we want.</p>
<p>In addition, as research shows, a group of simple agents might demonstrate collaborative behavior that is way more complex than expected. Some examples are the OpenAI blog post at <a class="url" href="https://openai.com/blog/emergent-tool-use/"><span class="cmtt-10x-x-109">https://openai.com/blog/emergent-tool-use/</span></a> and the paper <span class="cmti-10x-x-109">Emergent tool use from multi-agent autocurricula </span>by Baker et al. [<span id="x1-412002"/><a href="#">Bak+20</a>] about the “hide-and-seek” game, where a group of agents collaborate and develop more and more sophisticated strategies and counter-strategies to win against another group of agents, for example, “build a fence from available objects” and “use a trampoline to catch agents behind the fence.”</p>
<p>In terms of different ways that agents might communicate, they can be separated into two groups:</p>
<ul>
<li>
<p><span class="cmbx-10x-x-109">Competitive</span>: When two or more agents try to beat each other in order to maximize their reward. The simplest setup is a two-player game, like chess, backgammon, or Atari Pong.</p>
</li>
<li>
<p><span class="cmbx-10x-x-109">Collaborative</span>: When a group of agents needs to use joint efforts to reach some goal.</p>
</li>
</ul>
<p>There are lots of examples that fall into one of these groups, but the most interesting and close to real-life scenarios are normally a mixture of both behaviors. There are tons of examples, starting from some board games that allow you to form allies and going up to modern corporations, where 100% collaboration is assumed, but real life is normally much more complicated than that.</p>
<p>From a theoretical point of view, game theory has quite a developed foundation for both communication forms, but for the sake of brevity, we’re not going to deep dive into the field, which is large and has different terminology. If you’re curious, you can find lots of books and courses that explore it in great depth. To give an example, the minimax algorithm is a well-known result of game theory, and you saw it used in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109"> </span><a href="ch024.xhtml#x1-36400020"><span class="cmti-10x-x-109">20</span></a>.</p>
<p>MARL is a relatively young field, but activity has been growing over time; so, it might be interesting to keep an eye on it.</p>
</section>
<section class="level3 sectionHead" id="getting-started-with-the-environment">
<h1 class="heading-1" id="sigil_toc_id_374"> <span id="x1-41300022.2"/>Getting started with the environment</h1>
<p>Before we jump <span id="dx1-413001"/>into our first MARL example, let’s look at the environment we can use. If you want to play with MARL, your choice is a bit limited. All the environments that come with Gym support only one agent. There are some patches for Atari Pong, to switch it into two-player mode, but they are not standard and are an exception rather than the rule.</p>
<p>DeepMind, together with Blizzard, has made StarCraft II publicly available ( <a class="url" href="https://github.com/deepmind/pysc2"><span class="cmtt-10x-x-109">https://github.com/deepmind/pysc2</span></a>) and makes for a very interesting and challenging environment for experimentation. However, for somebody who is taking their first steps in MARL, it might be too complex. In that regard, I have found the MAgent environment, originally developed by Geek.AI, to be perfectly suitable; it is simple and fast and has minimal dependency, but it still allows you to simulate different multi-agent scenarios for experimentation. It doesn’t provide a Gym-compatible API, but we will implement it on our own.</p>
<p>If you’re interested in MARL, you might also check out the PettingZoo package from The Farama Foundation: <a class="url" href="https://pettingzoo.farama.org"><span class="cmtt-10x-x-109">https://pettingzoo.farama.org</span></a>. It includes more environments and a unified API for communication, but in this chapter, we’re focusing only on the MAgent environment.</p>
<section class="level4 subsectionHead" id="an-overview-of-magent">
<h2 class="heading-2" id="sigil_toc_id_375"> <span id="x1-41400022.2.1"/>An overview of MAgent</h2>
<p>Let’s look at MAgent at a high level. It provides the simulation <span id="dx1-414001"/>of a grid world that 2D agents inhabit. These agents can observe things around them (according to their perception length), move to some distance from their location, and attack other agents around them.</p>
<p>There might be different groups of agents with various characteristics and interaction parameters. For example, the first environment that we will consider is a predator-prey model, where “tigers” hunt “deer” and obtain a reward for that. In the environment configuration, you can specify lots of aspects of the group, like perception, movement, attack distance, the initial health of every agent in the group, how much health they spend on movement and attack, and so on. Aside from the agents, the environment might contain walls that are not crossable by the agents.</p>
<p>The nice thing about MAgent is that it is very scalable, as it is implemented in C++ internally, just exposing the Python interface. This means that the environment can have thousands of agents in the group, providing you with observations and processing the agents’ actions.</p>
</section>
<section class="level4 subsectionHead" id="installing-magent">
<h2 class="heading-2" id="sigil_toc_id_376"> <span id="x1-41500022.2.2"/>Installing MAgent</h2>
<p>As it often <span id="dx1-415001"/>happens, the original version of MAgent hasn’t been maintained for some time. Luckily for us, The Farama Foundation forked the original repo and are currently maintaining it, providing most of the original functionality. Their version is called MAgent2 and the documentation can be found here: <a class="url" href="https://magent2.farama.org/"><span class="cmtt-10x-x-109">https://magent2.farama.org/</span></a>. The GitHub repository is available here: <a class="url" href="https://github.com/Farama-Foundation/magent2"><span class="cmtt-10x-x-109">https://github.com/Farama-Foundation/magent2</span></a>. To install MAgent2, you need to run the following command:</p>
<pre class="lstlisting"><code>pip install magent2==0.3.3</code></pre>
</section>
<section class="level4 subsectionHead" id="setting-up-a-random-environment">
<h2 class="heading-2" id="sigil_toc_id_377"> <span id="x1-41600022.2.3"/>Setting up a random environment</h2>
<p>To quickly <span id="dx1-416001"/>understand the MAgent API and logic, I’ve implemented a simple environment with “tiger” and “deer” agents, where both groups are driven by the random policy. It might not be very interesting from an RL perspective, but it will allow us to quickly learn enough about the API to implement the Gym environment wrapper. The example can be found in <span class="cmtt-10x-x-109">Chapter22/forest</span><span class="cmtt-10x-x-109">_random.py </span>and we’ll walk through it here.</p>
<p>We start with <span class="cmtt-10x-x-109">ForestEnv</span>, defined in <span class="cmtt-10x-x-109">lib/data.py</span>, which defines the environment. This class is inherited from <span class="cmtt-10x-x-109">magent</span><span class="cmtt-10x-x-109">_parallel</span><span class="cmtt-10x-x-109">_env </span>(yes, the name of the class is in lowercase, in contravention of the Python style guide, but that’s how it has been defined in the library), the base class for MAgent environments:</p>
<div class="tcolorbox" id="tcolobox-491">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-577"><code>class ForestEnv(magent_parallel_env, EzPickle): 
    metadata = { 
        "render_modes": ["human", "rgb_array"], 
        "name": "forest_v4", 
        "render_fps": 5, 
    }</code></pre>
</div>
</div>
<p>This class mimics the Gym API, but it is not 100% compatible, so we will need to deal with this in our code later.</p>
<p>In the constructor, we instantiate the <span class="cmtt-10x-x-109">GridWorld </span>class, which works as a Python adapter around the low-level MAgent C++ library API:</p>
<div class="tcolorbox" id="tcolobox-492">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-578"><code>    def __init__(self, map_size: int = MAP_SIZE, max_cycles: int = MAX_CYCLES, 
                 extra_features: bool = False, render_mode: tt.Optional[str] = None, 
                 seed: tt.Optional[int] = None, count_walls: int = COUNT_WALLS, 
                 count_deer: int = COUNT_DEER, count_tigers: int = COUNT_TIGERS): 
        EzPickle.__init__(self, map_size, max_cycles, extra_features, render_mode, seed) 
        env = GridWorld(self.get_config(map_size), map_size=map_size) 
 
        handles = env.get_handles() 
        self.count_walls = count_walls 
        self.count_deer = count_deer 
        self.count_tigers = count_tigers 
 
        names = ["deer", "tiger"] 
        super().__init__(env, handles, names, map_size, max_cycles, [-1, 1], 
                         False, extra_features, render_mode)</code></pre>
</div>
</div>
<p>In the preceding code, we instantiate the <span class="cmtt-10x-x-109">GridWorld </span>class, which implements most of the logic of our environment.</p>
<p>The <span class="cmtt-10x-x-109">GridWorld </span>class is configured by the <span class="cmtt-10x-x-109">Config </span>instance returned by the <span class="cmtt-10x-x-109">get</span><span class="cmtt-10x-x-109">_config </span>function:</p>
<div class="tcolorbox" id="tcolobox-493">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-579"><code>    @classmethod 
    def get_config(cls, map_size: int): 
        # Standard forest config, but deer get reward after every step 
        cfg = forest_config(map_size) 
        cfg.agent_type_dict["deer"]["step_reward"] = 1 
        return cfg</code></pre>
</div>
</div>
<p>This <span id="dx1-416029"/>function uses the <span class="cmtt-10x-x-109">forest</span><span class="cmtt-10x-x-109">_config </span>function, which is imported from the <span class="cmtt-10x-x-109">magent.builtin.config.forest </span>package and tweaks the configuration, adding the reward for deer on every step. This will be important when we start training the deer model, so a reward of 1 on every step will incentivize the agent to live longer.</p>
<p>The rest of the configuration hasn’t been included here as it is largely unchanged and defines lots of details about the environment, including the following:</p>
<ul>
<li>
<p>How many groups of agents do we have in the environment? In our case, we have two groups: “deer” and “tigers.”</p>
</li>
<li>
<p>What are the properties of each group – how far can they see from their location? An example of this could be that the deer can see as far as one cell, but the tigers have can see as far as four. Can they attack others and how far? What is the initial health of each agent? How fast can they recover from damage? There are lots of parameters you can specify.</p>
</li>
<li>
<p>How can they attack other groups and what damage does it do? There is lots of flexibility – for example, you can model the scenario when predators hunt only in pairs (we’ll do this experiment later in the chapter). In our current setup, the situation is simple – any tiger can attack any deer without restrictions.</p>
</li>
</ul>
<p>The last function in the <span class="cmtt-10x-x-109">ForestEnv </span>class is <span class="cmtt-10x-x-109">generate</span><span class="cmtt-10x-x-109">_map</span>, which places walls, deer, and tigers randomly on the map:</p>
<div class="tcolorbox" id="tcolobox-494">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-580"><code>    def generate_map(self): 
        env, map_size = self.env, self.map_size 
        handles = env.get_handles() 
 
        env.add_walls(method="random", n=self.count_walls) 
        env.add_agents(handles[0], method="random", n=self.count_deer) 
        env.add_agents(handles[1], method="random", n=self.count_tigers)</code></pre>
</div>
</div>
<p>Now let’s get to the <span class="cmtt-10x-x-109">forest</span><span class="cmtt-10x-x-109">_random.py </span>source code. In the beginning, we import the <span class="cmtt-10x-x-109">lib.data </span>package and the <span class="cmtt-10x-x-109">VideoRecorder </span>class from Gymnasium:</p>
<div class="tcolorbox" id="tcolobox-495">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-581"><code>from gymnasium.wrappers.monitoring.video_recorder import VideoRecorder 
from lib import data 
 
RENDER_DIR = "render"</code></pre>
</div>
</div>
<p>In <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109"> </span><a href="ch006.xhtml#x1-380002"><span class="cmti-10x-x-109">2</span></a>, we used the <span class="cmtt-10x-x-109">RecordVideo </span>wrapper to capture environment observations automatically, but in the case of MAgent environments, it is not possible, due to different<span id="dx1-416041"/> return values (all methods are returning dictionaries for all agents at once instead of single values). To get around this, we’ll use the <span class="cmtt-10x-x-109">VideoRecorder </span>class to capture videos and write into the <span class="cmtt-10x-x-109">RENDER</span><span class="cmtt-10x-x-109">_DIR</span> directory.</p>
<p>First, we create a <span class="cmtt-10x-x-109">ForestEnv </span>instance and video recorder. An environment object contains the property <span class="cmtt-10x-x-109">agents</span>, which keeps string identifiers for all the agents in the environment. In our case, it will be a list of values like <span class="cmtt-10x-x-109">deer</span><span class="cmtt-10x-x-109">_12 </span>or <span class="cmtt-10x-x-109">tiger</span><span class="cmtt-10x-x-109">_3</span>. With the default configuration, on the map 64 <span class="cmsy-10x-x-109">× </span>64, we have 204 deer agents and 40 tigers, so the <span class="cmtt-10x-x-109">env.agents </span>list has 244 items:</p>
<div class="tcolorbox" id="tcolobox-496">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-582"><code>if __name__ == "__main__": 
    env = data.ForestEnv(render_mode="rgb_array") 
    recorder = VideoRecorder(env, RENDER_DIR + "/forest-random.mp4") 
    sum_rewards = {agent_id: 0.0 for agent_id in env.agents} 
    sum_steps = {agent_id: 0 for agent_id in env.agents}</code></pre>
</div>
</div>
<p>We reset the environment using the <span class="cmtt-10x-x-109">reset() </span>method, but now it returns one value (instead of the two in the Gym API). The returned value is a <span class="cmtt-10x-x-109">dict </span>with agent IDs as keys and observation tensors as values:</p>
<div class="tcolorbox" id="tcolobox-497">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-583"><code>    obs = env.reset() 
    recorder.capture_frame() 
    assert isinstance(obs, dict) 
    print(f"tiger_0: obs {obs[’tiger_0’].shape}, act: {env.action_space(’tiger_0’)}") 
    print(f"deer_0: obs {obs[’deer_0’].shape}, act: {env.action_space(’deer_0’)}\n") 
    step = 0</code></pre>
</div>
</div>
<p>The preceding code produces the following output:</p>
<pre class="lstlisting" id="listing-584"><code>tiger_0: obs (9, 9, 5), act: Discrete(9) 
deer_0: obs (3, 3, 5), act: Discrete(5)</code></pre>
<p>The action space contains five mutually exclusive actions for deer (four directions + a “do nothing” action). Tigers can do the same, but in addition can attack in four directions.</p>
<p>In terms of observations, every tiger gets a 9 <span class="cmsy-10x-x-109">× </span>9 matrix with five different planes of information. Deer are more short-sighted, so their observation is just 3 <span class="cmsy-10x-x-109">× </span>3. The observation always contains the agent in the center, so it shows the grid around this specific agent. The five planes of information are:</p>
<ul>
<li>
<p>Walls: 1 if this cell contains the wall and 0 otherwise</p>
</li>
<li>
<p>Group 1 (the group that the agent belongs to): 1 if the cell contains agents from the agent’s group and 0 otherwise</p>
</li>
<li>
<p>Group 1 health: The relative health of the agent in this cell</p>
</li>
<li>
<p>Group 2 (the group with enemy agents): 1 if there is an enemy in this cell and 0 otherwise</p>
</li>
<li>
<p>Group 2 health: The relative health of the enemy or 0 if nothing is there</p>
</li>
</ul>
<p>If more <span id="dx1-416055"/>groups are configured, the observation will contain more planes in the observation tensor. In addition, MAgent has a “minimap” functionality that adds the “zoomed-out” location of agents of every group. This minimap feature is disabled in my examples, but you can experiment with it to check the effect on the training. Without this feature, every agent sees only a limited range of cells around itself, but minimap allows them to have a more global view of the environment.</p>
<p>Groups 1 and 2 are relative to the agent’s group; so, in the second plane, deer have information about other deer and for tigers, this plane includes other tigers. This makes observations group-independent and allows us to train a single policy for both groups if needed.</p>
<p>Another optional part of the observation is the so-called “extra features,” which includes the agent’s ID, last action, last reward, and normalized position. Concrete details could be found in the MAgent source code, but we’re not going to use this functionality in our examples.</p>
<p>Let’s continue describing our code. We have a loop that is repeated until we have alive agents in the environment. On every iteration, we sample random actions for all the agents and execute them in the environment:</p>
<div class="tcolorbox" id="tcolobox-498">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-585"><code>    while env.agents: 
        actions = {agent_id: env.action_space(agent_id).sample() for agent_id in env.agents} 
        all_obs, all_rewards, all_dones, all_trunc, all_info = env.step(actions) 
        recorder.capture_frame()</code></pre>
</div>
</div>
<p>All values <span id="dx1-416060"/>returned from the <span class="cmtt-10x-x-109">env.step() </span>function are dictionaries with <span class="cmtt-10x-x-109">agent</span><span class="cmtt-10x-x-109">_id</span> as the key. Another very important detail about the MAgent environment is that the set of agents is volatile: agents can disappear from the environment (when they die, for example). In our “forest” environment, tigers are losing 0.1 points of health every step, which could be increased after eating deer. Deer lose health only after an attack and gain it on every step (likely from eating grass).</p>
<p>When the agent dies (a tiger from starvation or a deer from a tiger’s attack), the corresponding entry in the <span class="cmtt-10x-x-109">all</span><span class="cmtt-10x-x-109">_dones </span>dict is set to <span class="cmtt-10x-x-109">True </span>and on the next iteration, the agent disappears from all the dictionaries and the <span class="cmtt-10x-x-109">env.agents </span>list. So, after the death of one agent, the whole episode continues and we need to take this into account during the training.</p>
<p>In the preceding example, the loop is executed until no more agents are alive. As both tigers and deer are behaving randomly (and tigers are losing health at every step), it is very likely that all tigers will die from starvation and the surviving deer will live happily infinitely long. But the environment is configured to automatically remove all deer when no more tigers are left, so our program ends after 30-40 steps.</p>
<p>At the end of the loop, we sum up the reward obtained by agents and track the amount of steps for which they were alive:</p>
<div class="tcolorbox" id="tcolobox-499">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-586"><code>        for agent_id, r in all_rewards.items(): 
            sum_rewards[agent_id] += r 
            sum_steps[agent_id] += 1 
        step += 1</code></pre>
</div>
</div>
<p>After the loop, we show the top 20 agents sorted by their reward:</p>
<div class="tcolorbox" id="tcolobox-500">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-587"><code>    final_rewards = list(sum_rewards.items()) 
    final_rewards.sort(key=lambda p: p[1], reverse=True) 
    for agent_id, r in final_rewards[:20]: 
        print(f"{agent_id}: got {r:.2f} in {sum_steps[agent_id]} steps") 
    recorder.close()</code></pre>
</div>
</div>
<p>The output from this tool might look like this:</p>
<pre class="lstlisting" id="listing-588"><code>$ ./forest_random.py 
tiger_0: obs (9, 9, 5), act: Discrete(9) 
deer_0: obs (3, 3, 5), act: Discrete(5) 
 
tiger_5: got 34.80 in 37 steps 
tiger_37: got 19.70 in 21 steps 
tiger_31: got 19.60 in 21 steps 
tiger_9: got 19.50 in 21 steps 
tiger_24: got 19.40 in 21 steps 
tiger_36: got 19.40 in 21 steps 
tiger_38: got 19.40 in 21 steps 
tiger_1: got 19.30 in 21 steps 
tiger_3: got 19.30 in 21 steps 
tiger_11: got 19.30 in 21 steps 
tiger_12: got 19.30 in 21 steps 
tiger_17: got 19.30 in 21 steps 
tiger_19: got 19.30 in 21 steps 
tiger_26: got 19.30 in 21 steps 
tiger_32: got 19.30 in 21 steps 
tiger_2: got 19.20 in 21 steps 
tiger_8: got 19.20 in 21 steps 
tiger_10: got 19.20 in 21 steps 
tiger_23: got 19.20 in 21 steps 
tiger_25: got 19.20 in 21 steps 
Moviepy - Building video render/forest-random.mp4. 
Moviepy - Writing video render/forest-random.mp4</code></pre>
<p>In my <span id="dx1-416096"/>simulation, one agent (<span class="cmtt-10x-x-109">tiger</span><span class="cmtt-10x-x-109">_5</span>) was especially lucky and lived longer than others. At the end, the program saves a video of the episode. The result of my run is available here: <a class="url" href="https://youtube.com/shorts/pH-Rz9Q4yrI"><span class="cmtt-10x-x-109">https://youtube.com/shorts/pH-Rz9Q4yrI</span></a>.</p>
<p>In <span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109"> </span><a href="#x1-416097r1"><span class="cmti-10x-x-109">22.1</span></a>, two different states are shown: at the beginning of the game and close to the end. Tigers are shown with blue dots (or the darker ones if you’re reading this in grayscale), the red dots are deer, and the gray dots are walls (you can refer to the digital version of the book to see the colors in the screenshot). The attack direction is shown with small black arrows.</p>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_22_01.png" width="600"/> <span id="x1-416097r1"/></p>
<span class="id">Figure 22.1: Two states of the forest environment: at the beginning of the episode (left) and close to the end (right) </span>
</div>
<p>In this <span id="dx1-416098"/>example, both groups of agents were behaving randomly, which is not very interesting. In the next section, we’ll apply a <span class="cmbx-10x-x-109">deep Q-network </span>(<span class="cmbx-10x-x-109">DQN</span>) to improve the tiger’s hunting skills.</p>
</section>
</section>
<section class="level3 sectionHead" id="deep-q-network-for-tigers">
<h1 class="heading-1" id="sigil_toc_id_378"> <span id="x1-41700022.3"/>Deep Q-network for tigers</h1>
<p>Here, we will <span id="dx1-417001"/>apply the DQN model to the tiger group of agents to check whether they can learn how to hunt better. All of the agents share the network, so their behavior will be the same. The deer group will keep random behavior in this example to keep things simple for now; we’ll train them later in the chapter.</p>
<p>The training code can be found in <span class="cmtt-10x-x-109">Chapter22/forest</span><span class="cmtt-10x-x-109">_tigers</span><span class="cmtt-10x-x-109">_dqn.py</span>; it doesn’t differ much from the other DQN versions from the previous chapters.</p>
<section class="level4 subsectionHead" id="understanding-the-code">
<h2 class="heading-2" id="sigil_toc_id_379"> <span id="x1-41800022.3.1"/>Understanding the code</h2>
<p>To make <span id="dx1-418001"/>the <span class="cmtt-10x-x-109">MAgent </span>environment work with our classes, a specialized version of <span class="cmtt-10x-x-109">ExperienceSourceFirstLast </span>was implemented to handle the specifics of the environment. This class is called <span class="cmtt-10x-x-109">MAgentExperienceSourceFirstLast </span>and can be found in <span class="cmtt-10x-x-109">lib/data.py</span>. Let’s check it out to understand how it fits into the rest of the code.</p>
<p>The first class we define is the items produced by our <span class="cmtt-10x-x-109">ExperienceSource</span>. As we discussed in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109"> </span><a href="ch011.xhtml#x1-1070007"><span class="cmti-10x-x-109">7</span></a>, instances of the class <span class="cmtt-10x-x-109">ExperienceFirstLast </span>contain the following fields:</p>
<ul>
<li>
<p><span class="cmtt-10x-x-109">state</span>: Observations from the environment at the current step</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">action</span>: The action we executed</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">reward</span>: The amount of reward we obtained</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">last</span><span class="cmtt-10x-x-109">_state</span>: Observation after executing the action</p>
</li>
</ul>
<p>In a <span id="dx1-418002"/>multi-agent setup, every agent is producing the same set of data, but we also have to be able to identify which group this agent belongs to (in this tiger-deer example, does this experience correspond to the tiger or the deer’s trajectory?). To retain this information, we define a subclass, <span class="cmtt-10x-x-109">ExperienceFirstLastMARL</span>, with a new field keeping the group’s name:</p>
<div class="tcolorbox" id="tcolobox-501">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-589"><code>@dataclass(frozen=True) 
class ExperienceFirstLastMARL(ExperienceFirstLast): 
    group: str 
 
 
class MAgentExperienceSourceFirstLast: 
    def __init__(self, env: magent_parallel_env, agents_by_group: tt.Dict[str, BaseAgent], 
                 track_reward_group: str, env_seed: tt.Optional[int] = None, 
                 filter_group: tt.Optional[str] = None):</code></pre>
</div>
</div>
<p>In the constructor of <span class="cmtt-10x-x-109">MAgentExperienceSourceFirstLast</span>, we pass the following arguments:</p>
<ul>
<li>
<p><span class="cmtt-10x-x-109">magent</span><span class="cmtt-10x-x-109">_parallel</span><span class="cmtt-10x-x-109">_env</span>: The MAgent parallel environment (we experimented with this in the previous section).</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">agents</span><span class="cmtt-10x-x-109">_by</span><span class="cmtt-10x-x-109">_group</span>: The PTAN <span class="cmtt-10x-x-109">BaseAgent </span>object for every group of agents. In our tiger DQN example, tigers will be controlled by a neural network (<span class="cmtt-10x-x-109">ptan.agent.DQNAgent</span>), but deer behave randomly.</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">track</span><span class="cmtt-10x-x-109">_reward</span><span class="cmtt-10x-x-109">_group</span>: The parameter that specifies the group for which we’re tracking the episode’s reward.</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">filter</span><span class="cmtt-10x-x-109">_group</span>: An optional filter for the group for which we want to generate an experience. In our current example, we need only observations from tigers (as we train only tigers), but in the next section, we’ll train a DQN for both tigers and deer, so the filter will be disabled.</p>
</li>
</ul>
<p>In the subsequent constructor code, we store arguments and create two useful mappings for agents: from the agent ID to the group name and back:</p>
<div class="tcolorbox" id="tcolobox-502">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-590"><code>        self.env = env 
        self.agents_by_group = agents_by_group 
        self.track_reward_group = track_reward_group 
        self.env_seed = env_seed 
        self.filter_group = filter_group 
        self.total_rewards = [] 
        self.total_steps = [] 
 
        # forward and inverse map of agent_id -&gt; group 
        self.agent_groups = { 
            agent_id: self.agent_group(agent_id) 
            for agent_id in self.env.agents 
        } 
        self.group_agents = collections.defaultdict(list) 
        for agent_id, group in self.agent_groups.items(): 
            self.group_agents[group].append(agent_id) 
 
    @classmethod 
    def agent_group(cls, agent_id: str) -&gt; str: 
        a, _ = agent_id.split("_", maxsplit=1) 
        return a</code></pre>
</div>
</div>
<p>We also define <span id="dx1-418033"/>a utility method to strip the agent’s numerical ID to get the group name (in our case, it will be <span class="cmtt-10x-x-109">tiger </span>or <span class="cmtt-10x-x-109">deer</span>).</p>
<p>Now comes the main method of the class: the iterator interface, which produces experience items from the environment:</p>
<div class="tcolorbox" id="tcolobox-503">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-591"><code>    def __iter__(self) -&gt; tt.Generator[ExperienceFirstLastMARL, None, None]: 
        # iterate episodes 
        while True: 
            # initial observation 
            cur_obs = self.env.reset(self.env_seed) 
 
            # agent states are kept in groups 
            agent_states = { 
                prefix: [self.agents_by_group[prefix].initial_state() for _ in group] 
                for prefix, group in self.group_agents.items() 
            }</code></pre>
</div>
</div>
<p>Here, we reset the environment in the beginning and create initial states for agents (in case our agents will keep some state, but in this chapter’s examples, they are stateless).</p>
<p>Then, we iterate the episode until we have living agents (the same way we did in the example in the previous section). In this loop, we fill actions in the dictionary, mapping the agent ID to the action. To do this, we use PTAN <span class="cmtt-10x-x-109">BaseAgent </span>instances, which work with batches of observations, so actions will be produced very efficiently for the whole group of agents:</p>
<div class="tcolorbox" id="tcolobox-504">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-592"><code>            episode_steps = 0 
            episode_rewards = 0.0 
            # steps while we have alive agents 
            while self.env.agents: 
                # calculate actions for the whole group and unpack 
                actions = {} 
                for prefix, group in self.group_agents.items(): 
                    gr_obs = [ 
                        cur_obs[agent_id] 
                        for agent_id in group if agent_id in cur_obs 
                    ] 
                    gr_actions, gr_states = self.agents_by_group[prefix]( 
                        gr_obs, agent_states[prefix]) 
                    agent_states[prefix] = gr_states 
                    idx = 0 
                    for agent_id in group: 
                        if agent_id not in cur_obs: 
                            continue 
                        actions[agent_id] = gr_actions[idx] 
                        idx += 1</code></pre>
</div>
</div>
<p>Once we have <span id="dx1-418065"/>actions to be executed, we send them to the environment and obtain dictionaries with new observations, rewards, and done and truncation flags. Then, we generate experience items for every alive agent we currently have:</p>
<div class="tcolorbox" id="tcolobox-505">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-593"><code>                new_obs, rewards, dones, truncs, _ = self.env.step(actions) 
 
                for agent_id, reward in rewards.items(): 
                    group = self.agent_groups[agent_id] 
                    if group == self.track_reward_group: 
                        episode_rewards += reward 
                    if self.filter_group is not None: 
                        if group != self.filter_group: 
                            continue 
                    last_state = new_obs[agent_id] 
                    if dones[agent_id] or truncs[agent_id]: 
                        last_state = None 
                    yield ExperienceFirstLastMARL( 
                        state=cur_obs[agent_id], action=actions[agent_id], 
                        reward=reward, last_state=last_state, group=group 
                    ) 
                cur_obs = new_obs 
                episode_steps += 1</code></pre>
</div>
</div>
<p>At the end of the episode, we remember the number of steps and the average reward obtained by the group:</p>
<div class="tcolorbox" id="tcolobox-506">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-594"><code>            self.total_steps.append(episode_steps) 
            tr_group = self.group_agents[self.track_reward_group] 
            self.total_rewards.append(episode_rewards / len(tr_group))</code></pre>
</div>
</div>
<p>With this class at <span id="dx1-418087"/>hand, our DQN training code stays almost the same as in the single-agent RL case. The full source code of this example can be found in <span class="cmtt-10x-x-109">forest</span><span class="cmtt-10x-x-109">_tigers</span><span class="cmtt-10x-x-109">_dqn.py</span>. Here, I’m going to show only part of the code where PTAN agents and the experience source are created (to illustrate how <span class="cmtt-10x-x-109">MAgentExperienceSourceFirstLast </span>is being used):</p>
<div class="tcolorbox" id="tcolobox-507">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-595"><code>    action_selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=PARAMS.epsilon_start) 
    epsilon_tracker = common.EpsilonTracker(action_selector, PARAMS) 
    tiger_agent = ptan.agent.DQNAgent(net, action_selector, device) 
    deer_agent = data.RandomMAgent(env, env.handles[0]) 
    exp_source = data.MAgentExperienceSourceFirstLast( 
        env, 
        agents_by_group={’deer’: deer_agent, ’tiger’: tiger_agent}, 
        track_reward_group="tiger", 
        filter_group="tiger", 
    ) 
    buffer = ptan.experience.ExperienceReplayBuffer(exp_source, PARAMS.replay_size)</code></pre>
</div>
</div>
<p>As you can see, tigers are controlled by the neural network (which is a very simple two-layer convolution plus a two-layer fully connected net). A group of deer is controlled by a random number generator. The experience replay buffer will be populated only with the tiger experience because of the <span class="cmtt-10x-x-109">filter</span><span class="cmtt-10x-x-109">_group="tiger" </span>argument.</p>
</section>
<section class="level4 subsectionHead" id="training-and-results">
<h2 class="heading-2" id="sigil_toc_id_380"> <span id="x1-41900022.3.2"/>Training and results</h2>
<p>To start<span id="dx1-419001"/> the training, run <span class="cmtt-10x-x-109">./forest</span><span class="cmtt-10x-x-109">_tigers</span><span class="cmtt-10x-x-109">_dqn.py -n run</span><span class="cmtt-10x-x-109">_name --dev cuda</span>. In one hour of training, the tiger’s test reward has reached the best score of 82, which is a significant improvement over the random baseline. Acting randomly, most tigers die after 20 steps and only a few lucky ones can live longer.</p>
<p>Let’s calculate how many deer were eaten to get this score. Initially, each tiger has a health of 10 and spends 0.5 of their health on each step. In total, we have 40 tigers and 204 deer on the map (you can change this amount with command-line arguments). For every eaten deer, the tigers obtain 8 health points, which allows them to survive for an extra 16 steps. For every step, each tiger obtains a reward of 1, so the “excess reward” from the deer eaten by 40 tigers is 82 <span class="cmsy-10x-x-109">⋅ </span>40 <span class="cmsy-10x-x-109">− </span>20 <span class="cmsy-10x-x-109">⋅ </span>40 = 2480. Every deer gives 8 health points, which is converted into an extra 16 steps of a tiger’s life, so the number of deer eaten is 2480<span class="cmmi-10x-x-109">∕</span>16 = 155. So, almost 76% of deer were hunted by the best policy we’ve got. Not bad given that deer are randomly placed on the map and tigers need to get to them to attack.</p>
<p>It’s quite likely that the policy stopped improving just because of the limited view of the tigers. If you are curious, you can enable the minimap in the environment settings and experiment. With more information about the food’s location, it’s likely that the policy could be improved even more.</p>
<p>In <span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109"> </span><a href="#x1-419002r2"><span class="cmti-10x-x-109">22.2</span></a>, the average reward and number of steps during the training is shown. From it, you can see that the main growth was during the first 300 episodes and later, the training progress was almost 0:</p>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_22_02.png" width="600"/> <span id="x1-419002r2"/></p>
<span class="id">Figure 22.2: Average reward (left) and count of steps (right) from training episodes </span>
</div>
<p>However, <span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109"> </span><a href="#x1-419003r3"><span class="cmti-10x-x-109">22.3</span></a> shows the plots for the test reward and steps, which demonstrate that the policy continued to improve even after 300 episodes (<span class="cmsy-10x-x-109">≈ </span>0<span class="cmmi-10x-x-109">.</span>4 hours of training):</p>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_22_03.png" width="600"/> <span id="x1-419003r3"/></p>
<span class="id">Figure 22.3: Average reward (left) and count of steps (right) from test episodes </span>
</div>
<p>The final pair <span id="dx1-419004"/>of charts in <span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109"> </span><a href="#x1-419005r4"><span class="cmti-10x-x-109">22.4</span></a> shows the training loss and epsilon during the training. Both plots are correlated, which indicates that most of the novelty during the training was obtained in the exploration phase (as the loss value is high, which means that new situations arise during training). This might be an indication that better exploration methods might be beneficial for the final policy.</p>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_22_04.png" width="600"/> <span id="x1-419005r4"/></p>
<span class="id">Figure 22.4: Average loss (left) and epsilon (right) during the training </span>
</div>
<p>As usual, besides the training, I implemented a tool to check the models in action. It’s called <span class="cmtt-10x-x-109">forest</span><span class="cmtt-10x-x-109">_tigers</span><span class="cmtt-10x-x-109">_play.py </span>and it loads the trained model and uses it during the episode, producing a video recording of the observations. The video from the best model (with a test score of 82.89) is available here: <a class="url" href="https://www.youtube.com/shorts/ZZf80AHk538"><span class="cmtt-10x-x-109">https://www.youtube.com/shorts/ZZf80AHk538</span></a>. As you can see, tigers’ hunting skills are now significantly better than the random policy: at the end of the episode, just 53 deer were left from the initial 204.</p>
</section>
</section>
<section class="level3 sectionHead" id="collaboration-by-the-tigers">
<h1 class="heading-1" id="sigil_toc_id_381"> <span id="x1-42000022.4"/>Collaboration by the tigers</h1>
<p>The second <span id="dx1-420001"/>experiment that I implemented was designed to make the tigers’ lives more complicated and encourage collaboration between them. The training and play code are the same; the only difference is in the MAgent environment’s configuration.</p>
<p>If you pass the argument <span class="cmtt-10x-x-109">--mode double</span><span class="cmtt-10x-x-109">_attack </span>to the training utility, the environment <span class="cmtt-10x-x-109">data.DoubleAttackEnv </span>will be used. The only difference is the configuration object, which sets additional constraints on tigers’ attacks. In the new setup, they can attack deer only in pairs and have to do this at the same time. A single tiger’s attack doesn’t have any effect. This definitely complicates the training and hunting, as obtaining the reward from eating the deer is now much harder for tigers. To start the training, you can run the same train utility, but with an extra command-line argument:</p>
<pre class="lstlisting"><code>./forest_tigers_dqn.py -n run-name --dev cuda --mode double_attack</code></pre>
<p>Let’s take a look at the results.</p>
<p>In <span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109"> </span><a href="#x1-420002r5"><span class="cmti-10x-x-109">22.5</span></a>, the reward and step plots for the training episodes are shown. As you can see, even after 2 hours of training, the reward is still improving. At the same time, the count of steps in the episode never exceeds 300, which might be an indication that tigers just don’t have nearby deer to eat and die from starvation (it also might just be an internal limit of steps in the environment).</p>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_22_05.png" width="600"/> <span id="x1-420002r5"/></p>
<span class="id">Figure 22.5: Average reward (left) and count of steps (right) from training episodes in double_attack mode </span>
</div>
<p>In contrast to single-tiger hunting mode, the loss during the training is not decreasing (as shown in <span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109"> </span><a href="#x1-420003r6"><span class="cmti-10x-x-109">22.6</span></a>), which might indicate that training hyperparameters could be improved:</p>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_22_06.png" width="600"/> <span id="x1-420003r6"/></p>
<span class="id">Figure 22.6: Average loss during training </span>
</div>
<p>To test the model in action, you can use the same utility as before; just pass it the <span class="cmtt-10x-x-109">--mode double</span><span class="cmtt-10x-x-109">_attack </span>argument. A video recording of the best model I got is available here: <a class="url" href="https://youtu.be/VjGbzP1r7HY"><span class="cmtt-10x-x-109">https://youtu.be/VjGbzP1r7HY</span></a>. As you can see, tigers are now moving in pairs, attacking the deer together.</p>
</section>
<section class="level3 sectionHead" id="training-both-tigers-and-deer">
<h1 class="heading-1" id="sigil_toc_id_382"> <span id="x1-42100022.5"/>Training both tigers and deer</h1>
<p>The next example is the scenario when both tigers and deer are controlled by different DQN models being trained simultaneously. Tigers are rewarded for living longer, which stimulates them to eat more deer, as at every step in the simulation, they lose health points. Deer are also rewarded on every timestamp.</p>
<p>The code is in <span class="cmtt-10x-x-109">forest</span><span class="cmtt-10x-x-109">_both</span><span class="cmtt-10x-x-109">_dqn.py </span>and it is an extension of the previous example. For both groups of agents, we have a separate <span class="cmtt-10x-x-109">DQNAgent </span>class instance, which uses separate neural networks to convert observations into actions. The experience source is the same, but now we’re not filtering on a tiger’s group experience (with the parameter <span class="cmtt-10x-x-109">filter</span><span class="cmtt-10x-x-109">_group=None</span>). Because of this, our replay buffer now contains observations from all the agents in the environment, not just from tigers as in the previous example. During the training, we sample a batch and split examples from deer and tigers into two separate batches to be used for training their networks.</p>
<p>I’m not going to include all the code here, as it differs from the previous example only in small details. If you are curious, you can take a look at the source code in the GitHub repository. <span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109"> </span><a href="#x1-421001r7"><span class="cmti-10x-x-109">22.7</span></a> shows the training reward and steps for tigers. You can see that initially, tigers were able to consistently increase their reward, but later, the growth stopped:</p>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_22_07.png" width="600"/> <span id="x1-421001r7"/></p>
<span class="id">Figure 22.7: Average reward for tigers (left) and count of steps (right) from training episodes </span>
</div>
<p>In the next two plots in <span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109"> </span><a href="#x1-421002r8"><span class="cmti-10x-x-109">22.8</span></a>, the reward for tigers and deer during the testing is shown. There is no clear trend here; both groups are competing and trying to beat their opponent:</p>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_22_08.png" width="600"/> <span id="x1-421002r8"/></p>
<span class="id">Figure 22.8: Test reward for tigers (left) and deer (right) </span>
</div>
<p>As you can<span id="dx1-421003"/> see from <span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109"> </span><a href="#x1-421002r8"><span class="cmti-10x-x-109">22.8</span></a>, deer are way more successful than tigers, which is not surprising, as the speed of both is the same, so the deer just need to move all the time and wait for the tigers to die from starvation. If you want, you can experiment with the environment settings by increasing either the tigers’ speed or the wall density.</p>
<p>As before, it is possible to visualize the learned policies with the utility <span class="cmtt-10x-x-109">forest</span><span class="cmtt-10x-x-109">_both</span><span class="cmtt-10x-x-109">_play.py</span>, but now you need to pass two model files. Here is a video comparing the best model for deer and the best model for tigers: <a class="url" href="https://youtube.com/shorts/vuVL1e26KqY"><span class="cmtt-10x-x-109">https://youtube.com/shorts/vuVL1e26KqY</span></a>. In the video, all deer are just moving to the left of the field. Most likely, tigers can exploit this simple policy for their benefit.</p>
</section>
<section class="level3 sectionHead" id="the-battle-environment">
<h1 class="heading-1" id="sigil_toc_id_383"> <span id="x1-42200022.6"/>The battle environment</h1>
<p>Besides the tiger-deer environment, MAgent contains several other predefined configurations <span id="dx1-422001"/>you can find in the <span class="cmtt-10x-x-109">magent2.builtin.config </span>and <span class="cmtt-10x-x-109">magent2.environment </span>packages. As a final example in this chapter, we’ll take a look at the “battle” configuration, where two groups of agents are fighting each other (without eating, thank goodness). Both agents have health points of 10 and every attack takes 2 health points, so 5 consecutive attacks are required to get the reward for the agent.</p>
<p>You can find the code in <span class="cmtt-10x-x-109">battel</span><span class="cmtt-10x-x-109">_dqn.py</span>. In this setup, one group is behaving randomly and another is using the DQN to improve the policy. Training took two hours and the DQN was able to find a decent policy, but at the end, the training process diverged. In <span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109"> </span><a href="#x1-422002r9"><span class="cmti-10x-x-109">22.9</span></a>, the training and test reward plots are shown:</p>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_22_09.png" width="600"/> <span id="x1-422002r9"/></p>
<span class="id">Figure 22.9: Average reward during training (left) and test (right) in the battle scenario </span>
</div>
<p>The video recording (produced by the tool <span class="cmtt-10x-x-109">battle</span><span class="cmtt-10x-x-109">_play.py</span>) is available here: <a class="url" href="https://youtube.com/shorts/ayfCa8xGY2k"><span class="cmtt-10x-x-109">https://youtube.com/shorts/ayfCa8xGY2k</span></a>. The blue team is random and the red is controlled by the DQN.</p>
</section>
<section class="level3 sectionHead" id="summary-21">
<h1 class="heading-1" id="sigil_toc_id_384"> <span id="x1-42300022.7"/>Summary</h1>
<p>In this chapter, we just touched a bit on the very interesting and dynamic field of MARL, which has several practical applications in trading simulation, communication networks, and others. There are lots of things that you can try on your own using the MAgent environment or other environments (like PySC2).</p>
<p>My congratulations on reaching the end of the book! I hope that the book was useful and you enjoyed reading it as much as I enjoyed gathering the material and writing all the chapters. As a final word, I would like to wish you good luck in this exciting and dynamic area of RL. The domain is developing very rapidly, but with an understanding of the basics, it will become much simpler for you to keep track of the new developments and research in this field.</p>
<p>There are many very interesting topics left uncovered, such as partially observable Markov decision processes (where environment observations don’t fulfill the Markov property) or recent approaches to exploration, such as the count-based methods. There has been a lot of recent activity around multi-agent methods, where many agents need to learn how to coordinate to solve a common problem. I also haven’t mentioned the memory-based RL approach, where your agent can maintain some sort of memory to keep its knowledge and experience. A great deal of effort is being put into increasing the RL sample efficiency, which will ideally be close to human learning performance one day, but this is still a far-off goal at the moment. Of course, it’s not possible to cover the full domain in just one book, because new ideas appear almost every day. However, the goal of this book was to give you a practical foundation in the field, simplifying your own learning of the common methods.</p>
<p>I’d like to end by quoting Volodymyr Mnih’s words from his talk, <span class="cmti-10x-x-109">Recent</span> <span class="cmti-10x-x-109">Advances, Frontiers and Future of Deep RL</span>, from the Deep RL Bootcamp, Berkeley, 2017, which are still very relevant: “The field of deep RL is very new and everything is still exciting. Literally, nothing is solved yet!”</p>
</section>
<section class="level3 likesectionHead" id="leave-a-review">
<h1 class="heading-1" id="sigil_toc_id_15"><span id="x1-17000"/>Leave a Review!</h1>
<p class="normal">Thank you for purchasing this book from Packt Publishing—we hope you enjoyed it! Your feedback is invaluable and helps us improve and grow. Once you’ve completed reading it, please take a moment to leave an <a href="https://packt.link/r/1835882714"><span class="url">Amazon review</span></a>; it will only take a minute, but it makes a big difference for readers like you.</p>
<p>Scan the QR code below to receive a free ebook of your choice.</p>
<div class="center">
<p class="center"><img alt="PIC" height="208" src="../Images/file3.png" width="208"/></p>
</div>
<p class="center"><em class="italic">https://packt.link/NzOWQ</em></p>
</section>
</section>
</div></body></html>