- en: Pain Points of Recurrent Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to feedforward networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sequential workings of RNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Paint point #1 – the vanishing gradient problem'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pain point #2 – the exploding gradient problem'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sequential workings of LSTMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recurrent neural networks have proven to be incredibly efficient at tasks involving
    the learning and prediction of sequential data. However, when it comes to natural
    language, the question of long-term dependencies comes into play, which is basically
    remembering the context of a particular conversation, paragraph, or sentence in
    order to make better predictions in the future. For example, consider a sentence
    that says:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Last year, I happened to visit China. Not only was Chinese food different
    from the Chinese food available everywhere else in the world, but the people were
    extremely warm and hospitable too. In my three years of stay in this beautiful
    country, I managed to pick up and speak very good....*'
  prefs: []
  type: TYPE_NORMAL
- en: If the preceding sentence were fed into a recurrent neural network to predict
    the next word in the sentence (such as Chinese), the network would find it difficult
    since it has no memory of the context of the sentence. This is what we mean by
    long-term dependencies. In order to predict the word Chinese correctly, the network
    needs to know the context of the sentence as well as remember the fact that I
    happened to visit China last year. Recurrent neural networks therefore become
    inefficient at performing such tasks. However, this problem is overcome by **Long
    Short-Term Memory Units** (**LSTMs**), which are capable of remembering long-term
    dependencies and storing information in the cell state. LSTMs will be discussed
    later on, but the bulk of this chapter will focus on a basic introduction to Neural
    Networks, activation functions, Recurrent Networks, some of the main pain points
    or drawbacks of Recurrent Networks, and finally how these drawbacks may be overcome
    by the use of LSTMs.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to feedforward networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To understand recurrent networks, first you have to understand the basics of
    feedforward networks. Both of these networks are named after the way they move
    information through a series of mathematical operations performed at the nodes
    of the network. One feeds information in only one direction through every node
    (never touching a given node twice), while the other cycles it through a loop
    and feeds it back to the same node (kind of like a feedback loop). It is easily
    understood how the first kind is called a **feedforward network,** while the latter
    is recurrent.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most important concept while understanding any neural network diagram is
    the concept of computational graphs. Computational graphs are nothing but the
    nodes of the neural network connected to each other, and each node performs a
    particular mathematical function.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Feedforward neural networks channel the inputs (to the input layer) through
    a set of computational nodes which are nothing but mathematical operators and
    activation functions arranged in layers to calculate the network outputs. The output
    layer is the final layer of the neural network and usually contains linear functions.
    The layers between the input layer and the output layer are called **hidden layers** and
    usually contain nonlinear elements or functions:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram (a) shows how nodes are interconnected in feedforward
    neural networks with many layers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/d39f7304-059d-40b7-8c8f-e0638930e60b.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FeedForward Neural Network
  prefs: []
  type: TYPE_NORMAL
- en: Feedforward neural networks mainly differ from each other by the type of functions
    (activation functions) that are used in the hidden-layer nodes. They also differ
    from each other by the algorithms that are used to optimize the other parameters
    of the network during training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The relationships between nodes shown in the preceding diagram need not be fully
    populated for every node; optimization strategies usually start with a large number
    of hidden nodes and tune the network by eliminating connections, and possibly
    nodes, as training progresses. It may not be necessary to utilize every node during
    the training process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The neuron is the basic structural element of any neural network. A neuron
    can be thought of as a simple mathematical function or operator that operates
    on the input flowing through it to produce an output flowing out of it. The inputs
    to a neuron are multiplied by the node''s weight matrix, summed over all the inputs,
    translated, and passed through an activation function. These are basically matrix
    operations in mathematics as described here:'
  prefs: []
  type: TYPE_NORMAL
- en: The computational graph representation of a neuron is shown in the preceding
    diagram (b).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The transfer function for a single neuron or node is written as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/b755894e-1200-441f-b0ab-59898c595e85.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *x*[ *i* ]is the input to the ith node, *w*[ *i* ]is the weight term associated
    with the *i*^(th) node, *b* is the bias which is generally added to prevent overfitting, *f*(⋅)
    is the activation function operating over the inputs flowing into the node, and *y* is
    the output from the node.
  prefs: []
  type: TYPE_NORMAL
- en: Neurons with sigmoidal activation functions are commonly used in the hidden
    layer(s) of the neural network, and the identity function is usually used in the
    output layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The activation functions are generally chosen in a manner to ensure the outputs
    from the node are strictly increasing, smooth (continuous first derivative), or
    asymptotic.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following logistic function  is used as a sigmoidal activation function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/ce8be72e-2f97-46da-bd51-326378f9b278.png)'
  prefs: []
  type: TYPE_IMG
- en: A neural trained using backpropagation algorithm may learn faster if the activation
    function is antisymmetric, that is, *f*(-*x*) = -*f*(*x*) as in the case of the
    sigmoidal activation function. The backpropagation algorithm will be discussed
    in detail in the following sections of this chapter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The logistic function, however, is not antisymmetric, but can be made antisymmetric
    by a simple scaling and shift, resulting in the hyperbolic tangent function which
    has  a first derivative described by *f *''(*x*) = 1 - *f *²(*x*),  as shown in
    the following mathematical function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/1f3c3908-266c-4400-b025-b3b641a79d06.png)'
  prefs: []
  type: TYPE_IMG
- en: The simple form of the sigmoidal function and its derivative allows for the
    quick and accurate calculation of the gradients needed to optimize the selection
    of the weights and biases and carry out second-order error analysis.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At every neuron/node in the layers of a neural network, a series of matrix
    operations are performed. A more mathematical way of visualizing the feedforward
    network is given in the following diagram, which will help you to better understand
    the operations at each node/neuron:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b4f7b3f7-b9a9-4cc3-8214-1dd45ccd1679.png)'
  prefs: []
  type: TYPE_IMG
- en: Intuitively, we can see that the inputs (which are vectors or matrices) are
    first multiplied by weight matrices. A bias is added to this term and then activated
    using an activation function (such as ReLU, tanh, sigmoid, threshold, and so on)
    to produce the output. Activation functions are key in ensuring that the network
    is able to learn linear as well as non-linear functions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This output then flows into the next neuron as its input, and the same set
    of operations are performed all over again. A number of such neurons combine together
    to form a layer (which performs a certain function or learns a certain feature
    of the input vector), and many such layers combine together to form a feedforward
    neural network that can learn to recognize inputs completely, as shown in the
    following diagram:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/eefc8906-1179-48e1-9ed9-d04ac3a61f56.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s suppose our feedforward network has been trained to classify images
    of dogs and images of cats. Once the network is trained, as shown in the following
    diagram, it will learn to label images as dog or cat when presented with new images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/0e66f91a-dd80-4096-8aec-2f815f533387.png)'
  prefs: []
  type: TYPE_IMG
- en: In such networks, there is no relation between the present output and the previous
    or future outputs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This means the feedforward network can basically be exposed to any random collection
    of images and the first image it is exposed to will not necessarily alter how
    it classifies the second or third images. Therefore, we can say that the output
    at time step *t* is independent of the output at time step  *t - 1*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feedforward networks work well in such cases as image classification, where
    the data is not sequential. Feedforward networks also perform well when used on
    two related variables such as temperature and location, height and weight, car
    speed and brand, and so on.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: However, there may be cases where the current output is dependent on the outputs
    at previous time steps (the ordering of data is important).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Consider the scenario of reading a book. Your understanding of the sentences
    in the book is based on your understanding of all the words in the sentence. It
    wouldn't be possible to use a feedforward network to predict the next word in
    a sentence, as the output in such a case would depend on the previous outputs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Similarly, there are many cases where the output requires the previous output
    or some information from the previous outputs (for example, stock market data,
    NLP, voice recognition, and so on). The feedforward network may be modified as
    in the following diagram to capture information from previous outputs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/d553d2b8-9636-4209-a475-a7709c6bc69a.png)'
  prefs: []
  type: TYPE_IMG
- en: At time step *t*, the input at *t* as well as the information from *t-1* is
    both provided to the network to obtain the output at time *t*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Similarly, the information from *t* as well as the new input is fed into the
    network at time step *t+1* to produce the output at *t+1*. The right-hand side
    of the preceding diagram is a generalized way of representing such a network where
    the output of the network flows back in as input for future time steps. Such a
    network is called a **recurrent neural network** (**RNN**).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Activation Function**: In artificial neural networks, the activation function of
    a node decides the kind of output that node produces, given an input or set of
    inputs. The output *y[k]* is given by the input *u[k]* and bias *b[k]*, which
    are passed through the activation function *φ(.)*  as shown in the following expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/453f22b2-8584-4219-876e-a1e06896fc78.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are various types of activation functions. The following are the commonly
    used ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Threshold function**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/053acc51-b93f-4166-9ed0-f5d279beaa17.png)![](img/9a44b42b-3cab-44d7-a7d4-d473dda6897d.jpg)'
  prefs: []
  type: TYPE_IMG
- en: It is clear from the preceding diagram that this kind of  function restricts
    the output values of neurons to between 0 and 1\. This may be useful in many cases.
    However, this function is non-differentiable, which means it cannot be used to
    learn non-linearities, which is vital when using the backpropagation algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '**Sigmoid function**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/997a504a-0af5-4ed6-a7ab-8f11ea140dd4.png)![](img/c8c7d521-a6b3-4bef-9ae9-926e1827dbf6.png)'
  prefs: []
  type: TYPE_IMG
- en: The sigmoid function is a logistic function with a lower limit of 0 and an upper
    limit of 1, as with the threshold function. This activation function is continuous
    and therefore, also differentiable. In the sigmoid function, the slope parameter
    of the preceding function is given by α. The function is nonlinear in nature,
    which is critical in increasing the performance since it is able to accommodate
    non linearities in the input data unlike regular linear functions. Having non
    linear capabilities ensure that small changes in the weights and bias causes significant
    changes in the output of the neuron.
  prefs: []
  type: TYPE_NORMAL
- en: '**Hyperbolic Tangent function (tanh)**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/72529d7e-7c5e-43ad-814f-8487b8ca231c.png)'
  prefs: []
  type: TYPE_IMG
- en: This function enables activation functions to range from -1 to +1 instead of
    between 0 and 1 as in the previous cases.
  prefs: []
  type: TYPE_NORMAL
- en: '**Rectified Linear Unit (ReLU) function**: ReLUs are the smooth approximation
    to the sum of many logistic units, and produce sparse activity vectors. The following
    is the equation of the function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/e434cc61-8a42-4a32-904f-535621776d8f.png)![](img/a212b63d-3aa8-4b20-931f-745e2d4d6301.jpg)'
  prefs: []
  type: TYPE_IMG
- en: ReLU function graph
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, softplus ![](img/af6aae9d-9854-4827-afdd-5f175d48a865.png)(x)
    = log ( 1 + e^x) is the smooth approximation to the rectifier.
  prefs: []
  type: TYPE_NORMAL
- en: '**Maxout function**: This function utilizes a technique known as **"dropout"** and
    improves the accuracy of the dropout technique''s fast approximate model averaging
    in order to facilitate optimization.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Maxout networks learn not just the relationship between hidden units, but also
    the activation function of each hidden unit. By actively dropping out hidden units,
    the network is forced to find other paths to get to the output from a given input
    during the training process. The following diagram is the graphical depiction
    of how this works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1d788bef-0294-4200-ae1c-2597a552912a.png)'
  prefs: []
  type: TYPE_IMG
- en: Maxout Network
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding diagram shows the Maxout network with five visible units, three
    hidden units, and two neurons for each hidden unit. The Maxout function is given
    by the following equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/748c5423-9b57-4857-88fc-e4470c077d46.png)![](img/3ae85183-c7da-4f19-9e0d-f5d3c8de7156.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here  W..[ij ] is the mean vector of the size of the input obtained by accessing
    the matrix W ∈  ![](img/26730fcb-dc57-47ea-a46f-7ef2981a45f8.png) at the second
    coordinate *i* and third coordinate *j*. The number of intermediate units (*k) *is
    called the number of pieces used by the Maxout nets. The following diagram shows
    how the Maxout function compares to the ReLU and **Parametric Rectified Linear
    Unit** (**PReLU**) functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a0d75714-9de0-455a-a461-1eca6e9a13dc.png)'
  prefs: []
  type: TYPE_IMG
- en: Graphical comparison of Maxout, ReLU and PReLU function
  prefs: []
  type: TYPE_NORMAL
- en: Sequential workings of RNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recurrent neural networks are a type of artificial neural network designed
    to recognize and learn patterns in sequences of data. Some of the examples of
    such sequential data are:'
  prefs: []
  type: TYPE_NORMAL
- en: Handwriting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text such as customer reviews, books, source code, and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spoken word / Natural Language
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Numerical time series / sensor data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stock price variation data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In recurrent neural networks, the hidden state from the previous time step
    is fed back into the network at the next time step, as shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dd9b9ab7-9c66-45ee-8200-bc520bbdf0d3.png)'
  prefs: []
  type: TYPE_IMG
- en: Basically, the upward facing arrows going into the network represent the inputs
    (matrices/vectors) to the RNN at each time step, while the upward-facing arrows
    coming out of the network represent the output of each RNN unit. The horizontal
    arrows indicate the transfer of information learned in a particular time step
    (by a particular neuron) onto the next time step.
  prefs: []
  type: TYPE_NORMAL
- en: 'More information about using RNNs can be found at :'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://deeplearning4j.org/usingrnns](https://deeplearning4j.org/usingrnns)'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At every node/neuron of a recurrent network, a series of matrix multiplication
    steps are carried out. The input vector/matrix is multiplied by a weight vector/matrix
    first, a bias is added to this term, and this is finally passed through an activation
    function to produce the output (just as in the case of feedforward networks):'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows an intuitive and mathematical way of visualizing
    RNNs in the form of a computational graph:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/8f28fe47-36f1-491d-a3e9-b4c7920b29f3.png)'
  prefs: []
  type: TYPE_IMG
- en: At the first time step (which is *t=0*), *h*[*0* ]is calculated using the first
    formula on the right-hand side of the preceding diagram. Since *h*^(*-1* )does
    not exist, the middle term becomes zero.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The input matrix *x*[*0* ]is multiplied by the weight matrix *w[i]* and a bias
    *b[h]* is added to this term.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The two preceding matrices are added and then passed through an activation function
    *g[h]* to obtain *h[0]*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Similarly, *y[0]* is calculated using the second equation on the right-hand
    side of the preceding diagram by multiplying *h[0]* with the weight matrix *w[y]*,
    adding a bias *b[y]* to it, and passing it through an activation function *g[y]*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At the next time step (which is *t=1*), *h^((t-1))* does exist. It is nothing
    but *h[0]*. This term, multiplied with the weight matrix *w[R]*, is also provided
    as the input to the network along with the new input matrix *x[1]*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This process is repeated over a number of time steps, and the weights, matrices,
    and biases flow through the entire network over different time steps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This entire process is executed over one single iteration, which constitutes
    the forward pass of the network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To train feedforward neural networks the most commonly used technique is backpropagation
    through time. It is a supervised learning method used to reduce the loss function
    by updating weights and biases in the network after every time step. A number
    of training cycles (also known as epochs) are executed where the error determined
    by the loss function is backward propagated by a technique called gradient descent.
    At the end of each training cycle, the network updates its weights and biases
    to produce an output which is closer to the desired output, until a sufficiently
    small error is achieved :'
  prefs: []
  type: TYPE_NORMAL
- en: 'The backpropagation algorithm basically implements the following three fundamental
    steps during every iteration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The forward pass of the input data and calculating the loss function
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The computation of gradients and errors
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Backpropagation through time and adjustment of weights and biases accordingly
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: After the weighted sum of inputs (passed through an activation function after
    adding a bias) is fed into the network and an output is obtained, the network
    immediately compares how different the predicted output is from the actual case
    (correct output).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, the error is calculated by the network. This is nothing but the network
    output subtracted from the actual/correct output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The next step involves backpropagation through the entire network based on the
    calculated error. The weights and biases are then updated to notice whether the
    error increases or decreases.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The network also remembers whether the error increases by increasing the weights
    and biases or by decreasing the weights and biases.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Based on the preceding inferences, the network continues to update the weights
    and biases during every iteration in a manner such that the error becomes minimal.
    The following example will make things clearer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Consider a simple case of teaching a machine how to double a number, as shown
    in the following table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/600d03c8-8815-4d5f-8465-1b4f8eba843f.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, by initializing the weights randomly (*W = 3*), we obtain outputs
    of 0, 3, 6, and 9.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The error is calculated by subtracting the column of correct outputs from the
    column of model outputs. The square error is nothing but each error term multiplied
    by itself. It is usually a better practice to use square error as it eliminates
    negative values from error terms.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model then realizes that in order to minimize the error, the weight needs
    to be updated.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s suppose the model updates its weight to *W = 4* during the next iteration.
    This would result in the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00dc3ea3-a4e2-4fd9-b66c-67daf62635fc.png)'
  prefs: []
  type: TYPE_IMG
- en: The model now realizes that the error actually increased by increasing the weight
    to *W = 4*. Therefore, the model updates its weight by reducing it to *W = 2*
    in its next iteration, which results in the actual/correct output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Note that, in this simple case, the error increases when the weight is increased
    and reduces when the weight is decreased, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/08d9f110-c76b-4e1a-8cf5-4e3cdca260b9.png)'
  prefs: []
  type: TYPE_IMG
- en: In an actual neural network, a number of such weight updates are performed during
    every iteration until the model converges with the actual/correct output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As seen in the preceding case, the error increased when the weight was increased
    but decreased when the weight was decreased. But this may not always be the case.
    The network uses the following graph to determine how to update weights and when
    to stop updating them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f17c4901-1b95-4398-92e5-7f12a8765452.png)'
  prefs: []
  type: TYPE_IMG
- en: Let the weights be initialized to zero at the beginning of the first iteration.
    As the network updates its weights by increasing them from point A to B, the error
    rate begins to decrease.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the weights reach point B, the error rate becomes minimal. The network
    constantly keeps track of the error rate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On further increasing the weights from point B towards point C, the network
    realizes that the error rate begins to increase again. Thus, the network stops
    updating its weights and reverts back to the weights at point B, as they are optimal.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the next scenario, consider a case where the weights are randomly initialized
    to some value (let''s say, point C), as shown in the following graph:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/1d3effb7-7d07-42f1-a813-eaa414d61e61.png)'
  prefs: []
  type: TYPE_IMG
- en: On further increasing these random weights, the error also increases ( starting
    at point C and moving away from point B, indicated by the small arrow in the graph).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The network realizes that the error increased and begins to decrease the weights
    from point C so that the error decreases (indicated by the long arrow from point
    C moving towards point B in the graph). This decrease of weights happens until
    the error reaches a minimal value (point B on the graph).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The network continues to further update its weights even after reaching point
    B (indicated by the arrow moving away from point B and towards point A on the
    graph). It then realizes that the error is again increasing. As a result, it stops
    the weight update and reverts back to the weights that gave the minimal error
    value (which are the weights at point B).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is how neural networks perform weight updates after backpropagation. This
    kind of weight update is momentum-based. It relies on the computed gradients at
    each neuron of the network during every iteration, as shown in the following diagram:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/8b87bfbc-987b-4a20-8700-a1f650992629.png)'
  prefs: []
  type: TYPE_IMG
- en: Basically, the gradients are computed for each input with respect to the output
    every time an input flows into a neuron. The chain rule is used to compute the
    gradients during the backward pass of backpropagation.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A detailed explanation of the math behind backpropagation can be found at the
    following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://becominghuman.ai/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5](https://becominghuman.ai/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Andrej Karpathy''s blog has tons of useful information about recurrent neural
    networks. Here is a link explaining their unreasonable effectiveness:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pain point #1 – The vanishing gradient problem'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recurrent neural networks are great for tasks involving sequential data. However,
    they do come with their drawbacks. This section will highlight and discuss one
    such drawback, known as the **vanishing gradient problem**.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The name vanishing gradient problem stems from the fact that, during the backpropagation
    step, some of the gradients vanish or become zero. Technically, this means that
    there is no error term being propagated backward during the backward pass of the
    network. This becomes a problem when the network gets deeper and more complex.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section will describe how the vanishing gradient problem occurs in recurrent
    neural networks:'
  prefs: []
  type: TYPE_NORMAL
- en: While using backpropagation, the network first calculates the error, which is
    nothing but the model output subtracted from the actual output squared (such as
    the square error).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using this error, the model then computes the change in error with respect to
    the change in weights (de/dw).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The computed derivative multiplied by the learning rate ![](img/02520e4e-7b92-446d-a359-0cc7fd1ec5cf.png) gives ![](img/8a3a2dce-1669-41c3-90fd-ab676c3af592.png)w,
    which is nothing but the change in weights. The term ![](img/f70f52a8-c4f3-47b7-b6a9-c7bc77d6b74f.png)w
    is added to the original weights to update them to the new weights.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suppose the value of de/dw (the gradient or rate of change of error with respect
    to weights) is much less than 1, then that term multiplied by the learning rate ![](img/20098d2b-b23b-4c96-93fe-0e39482b627b.png) (which
    is always much less than 1) gives a very small, negligible, number which is negligible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This happens because the weight updates during backpropagation are only accurate
    for the most recent time step, and this accuracy reduces while backpropagating
    through the previous time steps and becomes almost insignificant when the weight
    updates flow through many steps back in time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There may be certain cases where sentences may be extremely long and the neural
    network is trying to predict the next word in a sentence. It does so based on
    the context of the sentence, for which it needs information from many previous
    time steps (these are called **long-term dependencies**). The number of previous
    times steps the network needs to backpropagate through increases with the increasing
    length of sentences. In such cases, the recurrent networks become incapable of
    remembering information from many time steps in the past and therefore are unable
    to make accurate predictions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When such a scenario occurs, the network requires many more complex calculations,
    as a result of which the number of iterations increases substantially and during
    which the change in error term vanishes (by reducing over time) and changes in
    weight (![](img/c5361123-15dd-4a54-a3c1-48c41ba7b1d2.png)w) become negligibly
    small. As a result, the new or updated weight is almost equal to the previous
    weight.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since there is no weight update occurring, the network stops learning or being
    able to update its weights, which is a problem as this will cause the model to
    overfit the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This entire process is illustrated in the following diagram:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/32031e59-d92e-4da9-b804-82fe2cfc4381.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section will describe some of the repercussions of the vanishing gradient
    problem:'
  prefs: []
  type: TYPE_NORMAL
- en: This problem occurs when we train a neural network model using some sort of
    optimization techniques which are gradient based.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generally, adding more hidden layers tends to make the network able to learn
    more complex arbitrary functions, and thus do a better job in predicting future
    outcomes. Deep Learning makes a big difference due to the large number of hidden
    layers it has, ranging from 10 to 200\. It is now possible to make sense of complicated
    sequential data, and perform tasks such as Speech Recognition, Image Classification,
    Image Captioning, and more.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The problem caused by the preceding steps is that, in some cases, the gradients
    become so small that they almost vanish, which in turn prevents the weights from
    updating their values during future time steps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the worst case, it could result in the training process of the network being
    stopped, which means that the network stops learning the different features it
    was intended to learn through the training steps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The main idea behind backpropagation is that it allows us, as researchers, to
    monitor and understand how machine learning algorithms process and learn various
    features. When the gradients vanish, it becomes impossible to interpret what is
    going on with the network, and hence identifying and debugging errors becomes
    even more of a challenge.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are some of the ways in which the problem of vanishing gradients
    can be solved:'
  prefs: []
  type: TYPE_NORMAL
- en: One method to overcome this problem to some extent by using the ReLU activation
    function. It computes the function *f(x)=max(0,x) (i.e., t*he activation function
    simply thresholds the lower level of outputs at zero) and prevents the network
    from producing negative gradients.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another way to overcome this problem is to perform unsupervised training on
    each layer separately and then fine-tune the entire network through backpropagation,
    as done by Jürgen Schmidhuber in his study of multi-level hierarchy in neural
    networks. The link to this paper is provided in the following section.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A third solution to this problem is the use of **LSTM** (**Long Short-Term Memory**)
    units or **GRUs (Gated Recurrent Units)**, which are special types of RNNs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following links provide a more in-depth description of the vanishing gradient
    problem and also some ways to tackle the issue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b](https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/readings/L15%20Exploding%20and%20Vanishing%20Gradients.pdf](http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/readings/L15%20Exploding%20and%20Vanishing%20Gradients.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://people.idsia.ch/~juergen/cvpr2012.pdf](http://people.idsia.ch/~juergen/cvpr2012.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pain point #2 – The exploding gradient problem'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another drawback of recurrent neural networks is the problem of exploding gradients.
    This is similar to the vanishing gradient problem but the exact opposite. Sometimes,
    during backpropagation, the gradients explode to extraordinarily large values.
    As with the vanishing gradient problem, the problem of exploding gradients occurs
    when network architectures get deeper.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The name exploding gradient problem stems from the fact that, during the backpropagation
    step, some of the gradients vanish or become zero. Technically, this means that
    there is no error term being propagated backward during the backward pass of the
    network. This becomes a problem when the network gets deeper and more complex.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section will describe the exploding gradient problem in recurrent neural
    networks:'
  prefs: []
  type: TYPE_NORMAL
- en: The exploding gradient problem is very similar to the vanishing gradient problem,
    but just the opposite.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When long-term dependencies arise in recurrent neural networks, the error term
    is propagated backward through the network sometimes explodes or becomes very
    large.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This error term multiplied by the learning rate results in an extremely large ![](img/ec6e0c62-a9ca-4280-87e8-d789ac947387.png)w.
    This gives rise to new weights that look very different from the previous weights.
    It is called the exploding gradient problem because the value of the gradient
    becomes too large.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The problem of exploding gradients is illustrated in an algorithmic fashion,
    in the following diagram:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/f498abc3-6230-497a-aa81-2d8083fb592b.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since neural networks use a gradient-based optimization technique to learn
    features present in data, it is essential that these gradients are preserved in
    order for the network to calculate an error based on the change in gradients.
    This section will describe how the exploding gradient problem occurs in recurrent
    neural networks:'
  prefs: []
  type: TYPE_NORMAL
- en: While using backpropagation, the network first calculates the error, which is
    nothing but the model output subtracted from the actual output squared (such as
    the square error).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using this error, the model then computes the change in error with respect to
    the change in weights (de/dw).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The computed derivative multiplied by the learning rate ![](img/f2644c6b-a2bf-4afe-946a-f3449fcf08a7.png) gives ![](img/0a500dfb-7dc4-4cb8-b8b4-fa4244c67565.png)w,
    which is nothing but the change in weights. The term ![](img/f5e5c7a4-64c5-49ff-b331-1110e011cfbb.png)w
    is added to the original weights to update them to the new weights.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suppose the value of de/dw (the gradient or rate of change of error with respect
    to weights) is greater than 1, then that term multiplied by the learning rate ![](img/31f9faa4-34d0-441a-9b0d-8f7fde4012d7.png) gives
    a very, very large number that is of no use to the network while trying to optimize
    weights further, since the weights are no longer in the same range.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This happens because the weight updates during backpropagation are only accurate
    for the most recent time step, and this accuracy reduces while backpropagating
    through the previous time steps and becomes almost insignificant when the weight
    updates flow through many steps back in time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of previous times steps the network needs to backpropagate through
    increases with the increase in the number of sequences in the input data. In such
    cases, the recurrent networks become incapable of remembering information from
    many time steps in the past and therefore are unable to make accurate predictions
    of future time steps.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When such a scenario occurs, the network requires many more complex calculations,
    as a result of which the number of iterations increases substantially and during
    which the change in error term increases beyond 1 and changes in weight (![](img/8e89f7be-6b0a-45b3-812d-7a0bc4274146.png)w)
    explode. As a result, the new or updated weight is completely out of range when
    compared to the previous weight.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since there is no weight update occurring, the network stops learning or being
    able to update its weights within a specified range, which is a problem as this
    will cause the model to overfit the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are some of the ways in which the problem of exploding gradients
    can be solved:'
  prefs: []
  type: TYPE_NORMAL
- en: Certain gradient clipping techniques can be applied to solve this issue of exploding
    gradients.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another way to prevent this is by using truncated Backpropagation Through Time,
    where instead of starting the backpropagation at the last time step (or output
    layer), we can choose a smaller time step (say, 15) to start backpropagating.
    This means that the network will backpropagate through only the last 15 time steps
    at one instance and learn information related to those 15-time steps only. This
    is similar to feeding in mini batches of data to the network as it would become
    far too computationally expensive to compute the gradient over every single element
    of the dataset in the case of very large datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final option to prevent the explosion of gradients is by monitoring them
    and adjusting the learning rate accordingly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A more detailed explanation of the vanishing and exploding gradient problems
    can be found at the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://neuralnetworksanddeeplearning.com/chap5.html](http://neuralnetworksanddeeplearning.com/chap5.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.dlology.com/blog/how-to-deal-with-vanishingexploding-gradients-in-keras/](https://www.dlology.com/blog/how-to-deal-with-vanishingexploding-gradients-in-keras/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://machinelearningmastery.com/exploding-gradients-in-neural-networks/](https://machinelearningmastery.com/exploding-gradients-in-neural-networks/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sequential working of LSTMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Long Short-Term Memory Unit** (**LSTM**) cells are nothing but slightly more
    advanced architectures compared to Recurrent Networks. LSTMs can be thought of
    as a special kind of Recurrent Neural Networks with the capabilities of learning
    long-term dependencies that exist in sequential data. The main reason behind this
    is the fact that LSTMs contain memory and are able to store and update information
    within their cells unlike Recurrent Neural Networks.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The main components of a Long Short-Term Memory unit are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The input gate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The forget gate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The update gate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of these gates is made up of a sigmoid layer followed by a pointwise multiplication
    operation. The sigmoid layer outputs numbers between zero and one. These values
    describe  how much information of each component is allowed to pass through the
    respective gate. A value of zero means the gate will allow nothing to pass through
    it, while a value of one means the gate allows all the information to pass through.
  prefs: []
  type: TYPE_NORMAL
- en: The best way to understand LSTM cells is through computational graphs, just
    like in the case of recurrent neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'LSTMs were originally developed by Sepp Hochreiter and Jurgen Schmidhuber in
    1997\. The following is, link to their published paper:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.bioinf.jku.at/publications/older/2604.pdf](http://www.bioinf.jku.at/publications/older/2604.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section will describe the inner components of a single LSTM cell, primarily,
    the three different gates present inside the cell. A number of such cells stacked
    together form an LSTM network:'
  prefs: []
  type: TYPE_NORMAL
- en: LSTMs also have a chain-like structure like RNNs. Standard RNNs are basically
    modules of repeating units like a simple function (for example, tanh).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: LSTMs have the capability to retain information for long periods of time as
    compared to RNNs owing to the presence of memory in each unit. This allows them
    to learn important information during the early stages in a sequence of inputs
    and also gives it the ability to have a significant impact on the decisions made
    by the model at the end of each time step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By being able to store information right from the early stages of an input sequence, LSTMs
    are actively able to preserve the error that can be backpropagated through time
    and layers instead of letting that error vanish or explode.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: LSTMs are capable of learning information over many time steps and thus have
    denser layer architectures by preserving the error which is backpropagated through
    those layers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The cell structures called **"gates"** give the LSTM the ability to retain information,
    add information or remove information from the **cell state**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following diagram illustrates the structure of an LSTM. The key feature
    while trying to  understand LSTMs is in understanding the LSTM network architecture
    and cell state, which can be visualized here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/2dfbc2de-288c-472f-ab6d-5f045fae0c79.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, *x[t]* and *h[t-1]* are the two inputs to the cell.
    *x*[*t* ]is the input from the current time step, while h[t-1 ]is the input from
    the previous time step (which is the output of the preceding cell during the previous
    time step). Besides these two inputs, we also have *h*[, ]which is the current
    output (i.e., time step t) from the LSTM cell after performing its operations
    on the two inputs through its gates.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the preceding diagram, r[t ]represents the output emerging from the input
    gate, which takes in inputs *h*[*t-1* ]and *x[t]*, performs multiplication of
    these inputs with its weight matrix *W[z]*, and passes them through a sigmoid
    activation function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Similarly, the term *z[t]* represents the output emerging from the forget gate.
    This gate has a set of weight matrices (represented by *W[r]*) which are specific
    to this particular gate and govern how the gate functions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, there is ![](img/21952021-5706-4316-b91e-71e2c99fd477.png)[t], which
    is the output emerging from the update gate. In this case, there are two parts.
    The first part is a sigmoid layer which is also called the **input gate layer**,
    and its primary function is deciding which values to update. The next layer is
    a tanh layer . The primary function of this layer is to create a vector or array
    containing new values that  could be added to the cell state.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A combination of a number of LSTM cells/units forms an LSTM network. The architecture
    of such a network is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/52895ac7-e1df-4131-a886-f2a72a0c0583.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, the full LSTM cell is represented by ***"A"***. The
    cell takes the current input (*x**[i]*) of a sequence of inputs, and produces
    (*h**[i]*) which is nothing but the output of the current hidden state. This output
    is then sent to the next LSTM cell as its input.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An LSTM cell is slightly more complicated than an RNN cell. While the RNN cell
    has just one function/layer acting on a current input, the LSTM cell has three
    layers which are the three gates controlling the information flowing through the
    cell at any given instance in time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The cell behaves a lot like the hard disk memory in a computer. The cell, therefore,
    has the capability to allow the writing, reading and storing of information within
    its cell state. The cell also makes decisions about what information to store,
    and when to allow reading, writing, and erasing information. This is facilitated
    by the gates that open or close accordingly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The gates present in LSTM cells are analog in contrast to the digital storage
    systems in today's computers. This means that the gates can only be controlled
    through an element-wise multiplication through sigmoids, yielding probability
    values between 0 and 1\. A high value will cause the gate to remain open while
    a low value will cause the gate to remain shut.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Analog systems have an edge over digital systems when it comes to neural network
    operations since they are differentiable. This makes analog systems more suitable
    for tasks like backpropagation which primarily rely on the gradients.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The gates pass on information or block information or let only a part of the
    information flow through them based on its strength and importance. The information
    is filtered at every time step through the sets of weight matrices specific to
    each gate. Therefore, each gate has complete control over how to act on the information
    it receives.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The weight matrices associated with each gate, like the weights that modulate
    input and hidden states, are adjusted based on the recurrent network's learning
    process and gradient descent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first gate is called the **forget gate** and it controls what information
    is maintained from the previous state. This gate takes the previous cell output
    (*h**[t]** - 1*) as its input along with the current input (*x**[t]*), and applies
    a sigmoid activation (![](img/a15a6578-5902-4ddf-b4af-45fa1aa9bb73.png)) in order
    to produce and output value between 0 and 1 for each hidden unit. This is followed
    by the element-wise multiplication with the current state (illustrated by the
    first operation in the preceding diagram).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The second gate is called the **update gate** and its primary function is to
    update the cell state based on the current input. This gate passes the same input
    as the forget gate's inputs (*h**[t-1]* and *x**[t]*) into a sigmoid activation
    layer (![](img/a15a6578-5902-4ddf-b4af-45fa1aa9bb73.png)) followed by a tanh activation
    layer and then performs an element-wise multiplication between these two results.
    Next, element-wise addition is performed with the result and the current state
    (illustrated by the second operation in the preceding diagram).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, there is an output gate which controls what information and how much
    information gets transferred to the adjoining cell to act as its inputs during
    the next time step. The current cell state is passed through a tanh activation
    layer and multiplied element-wise with the cell input (*h**[t-1]* and *x**[t]*)
    after being passed through a sigmoid layer (![](img/a15a6578-5902-4ddf-b4af-45fa1aa9bb73.png))
    for this operation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The update gate behaves as the filter on what the cell decides to output to
    the next cell. This output, h[t], is then passed on to the next LSTM cell as its
    input, and also to the above layers if many LSTM cells are stacked on top of each
    other.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LSTMs were a big leap forward when compared to what could be accomplished with
    feedforward networks and Recurrent Neural Networks. One might wonder what the
    next big step in the near future is, or even what that step might be. A lot researchers
    do believe "attention" is the next big step when it comes to the field of artificial
    intelligence. With the amount of data growing vastly with each day it becomes
    impossible to process every single bit of that data. This is where attention could
    be a potential game-changer, causing the networks to give their attention only
    to data or areas which are of high priority or interest and disregard useless
    information. For example, if an RNN is being used to create an image captioning
    engine, it will only pick a part of the image to to give its attention to for
    every word it outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The recent (2015) paper by Xu, et al. does exactly this. They explore adding
    attention to LSTM cells. Reading this paper can be a good place to start learning
    about the use of attention in neural networks. There have been some good results
    with using attention for various tasks, and more research is currently being conducted
    on the subject. The paper by Xu, et al. can be found using the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://arxiv.org/pdf/1502.03044v2.pdf](https://arxiv.org/pdf/1502.03044v2.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Attention isn't the only variant to LSTMs. Some of the other active research
    is based on the utilization of grid LSTMs, as used in the paper by Kalchbrenner,
    et al., for which the link is at: [https://arxiv.org/pdf/1507.01526v1.pdf](https://arxiv.org/pdf/1507.01526v1.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Some other useful information and papers related to RNNs and LSTMs in generative
    networks can be found by visiting the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.deeplearningbook.org/contents/rnn.html](http://www.deeplearningbook.org/contents/rnn.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://arxiv.org/pdf/1502.04623.pdf](https://arxiv.org/pdf/1502.04623.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://arxiv.org/pdf/1411.7610v3.pdf](https://arxiv.org/pdf/1411.7610v3.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://arxiv.org/pdf/1506.02216v3.pdf](https://arxiv.org/pdf/1506.02216v3.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
