["```py\npip install magent2==0.3.3\n```", "```py\nclass ForestEnv(magent_parallel_env, EzPickle): \n    metadata = { \n        \"render_modes\": [\"human\", \"rgb_array\"], \n        \"name\": \"forest_v4\", \n        \"render_fps\": 5, \n    }\n```", "```py\n def __init__(self, map_size: int = MAP_SIZE, max_cycles: int = MAX_CYCLES, \n                 extra_features: bool = False, render_mode: tt.Optional[str] = None, \n                 seed: tt.Optional[int] = None, count_walls: int = COUNT_WALLS, \n                 count_deer: int = COUNT_DEER, count_tigers: int = COUNT_TIGERS): \n        EzPickle.__init__(self, map_size, max_cycles, extra_features, render_mode, seed) \n        env = GridWorld(self.get_config(map_size), map_size=map_size) \n\n        handles = env.get_handles() \n        self.count_walls = count_walls \n        self.count_deer = count_deer \n        self.count_tigers = count_tigers \n\n        names = [\"deer\", \"tiger\"] \n        super().__init__(env, handles, names, map_size, max_cycles, [-1, 1], \n                         False, extra_features, render_mode)\n```", "```py\n @classmethod \n    def get_config(cls, map_size: int): \n        # Standard forest config, but deer get reward after every step \n        cfg = forest_config(map_size) \n        cfg.agent_type_dict[\"deer\"][\"step_reward\"] = 1 \n        return cfg\n```", "```py\n def generate_map(self): \n        env, map_size = self.env, self.map_size \n        handles = env.get_handles() \n\n        env.add_walls(method=\"random\", n=self.count_walls) \n        env.add_agents(handles[0], method=\"random\", n=self.count_deer) \n        env.add_agents(handles[1], method=\"random\", n=self.count_tigers)\n```", "```py\nfrom gymnasium.wrappers.monitoring.video_recorder import VideoRecorder \nfrom lib import data \n\nRENDER_DIR = \"render\"\n```", "```py\nif __name__ == \"__main__\": \n    env = data.ForestEnv(render_mode=\"rgb_array\") \n    recorder = VideoRecorder(env, RENDER_DIR + \"/forest-random.mp4\") \n    sum_rewards = {agent_id: 0.0 for agent_id in env.agents} \n    sum_steps = {agent_id: 0 for agent_id in env.agents}\n```", "```py\n obs = env.reset() \n    recorder.capture_frame() \n    assert isinstance(obs, dict) \n    print(f\"tiger_0: obs {obs[’tiger_0’].shape}, act: {env.action_space(’tiger_0’)}\") \n    print(f\"deer_0: obs {obs[’deer_0’].shape}, act: {env.action_space(’deer_0’)}\\n\") \n    step = 0\n```", "```py\ntiger_0: obs (9, 9, 5), act: Discrete(9) \ndeer_0: obs (3, 3, 5), act: Discrete(5)\n```", "```py\n while env.agents: \n        actions = {agent_id: env.action_space(agent_id).sample() for agent_id in env.agents} \n        all_obs, all_rewards, all_dones, all_trunc, all_info = env.step(actions) \n        recorder.capture_frame()\n```", "```py\n for agent_id, r in all_rewards.items(): \n            sum_rewards[agent_id] += r \n            sum_steps[agent_id] += 1 \n        step += 1\n```", "```py\n final_rewards = list(sum_rewards.items()) \n    final_rewards.sort(key=lambda p: p[1], reverse=True) \n    for agent_id, r in final_rewards[:20]: \n        print(f\"{agent_id}: got {r:.2f} in {sum_steps[agent_id]} steps\") \n    recorder.close()\n```", "```py\n$ ./forest_random.py \ntiger_0: obs (9, 9, 5), act: Discrete(9) \ndeer_0: obs (3, 3, 5), act: Discrete(5) \n\ntiger_5: got 34.80 in 37 steps \ntiger_37: got 19.70 in 21 steps \ntiger_31: got 19.60 in 21 steps \ntiger_9: got 19.50 in 21 steps \ntiger_24: got 19.40 in 21 steps \ntiger_36: got 19.40 in 21 steps \ntiger_38: got 19.40 in 21 steps \ntiger_1: got 19.30 in 21 steps \ntiger_3: got 19.30 in 21 steps \ntiger_11: got 19.30 in 21 steps \ntiger_12: got 19.30 in 21 steps \ntiger_17: got 19.30 in 21 steps \ntiger_19: got 19.30 in 21 steps \ntiger_26: got 19.30 in 21 steps \ntiger_32: got 19.30 in 21 steps \ntiger_2: got 19.20 in 21 steps \ntiger_8: got 19.20 in 21 steps \ntiger_10: got 19.20 in 21 steps \ntiger_23: got 19.20 in 21 steps \ntiger_25: got 19.20 in 21 steps \nMoviepy - Building video render/forest-random.mp4\\. \nMoviepy - Writing video render/forest-random.mp4\n```", "```py\n@dataclass(frozen=True) \nclass ExperienceFirstLastMARL(ExperienceFirstLast): \n    group: str \n\nclass MAgentExperienceSourceFirstLast: \n    def __init__(self, env: magent_parallel_env, agents_by_group: tt.Dict[str, BaseAgent], \n                 track_reward_group: str, env_seed: tt.Optional[int] = None, \n                 filter_group: tt.Optional[str] = None):\n```", "```py\n self.env = env \n        self.agents_by_group = agents_by_group \n        self.track_reward_group = track_reward_group \n        self.env_seed = env_seed \n        self.filter_group = filter_group \n        self.total_rewards = [] \n        self.total_steps = [] \n\n        # forward and inverse map of agent_id -> group \n        self.agent_groups = { \n            agent_id: self.agent_group(agent_id) \n            for agent_id in self.env.agents \n        } \n        self.group_agents = collections.defaultdict(list) \n        for agent_id, group in self.agent_groups.items(): \n            self.group_agents[group].append(agent_id) \n\n    @classmethod \n    def agent_group(cls, agent_id: str) -> str: \n        a, _ = agent_id.split(\"_\", maxsplit=1) \n        return a\n```", "```py\n def __iter__(self) -> tt.Generator[ExperienceFirstLastMARL, None, None]: \n        # iterate episodes \n        while True: \n            # initial observation \n            cur_obs = self.env.reset(self.env_seed) \n\n            # agent states are kept in groups \n            agent_states = { \n                prefix: [self.agents_by_group[prefix].initial_state() for _ in group] \n                for prefix, group in self.group_agents.items() \n            }\n```", "```py\n episode_steps = 0 \n            episode_rewards = 0.0 \n            # steps while we have alive agents \n            while self.env.agents: \n                # calculate actions for the whole group and unpack \n                actions = {} \n                for prefix, group in self.group_agents.items(): \n                    gr_obs = [ \n                        cur_obs[agent_id] \n                        for agent_id in group if agent_id in cur_obs \n                    ] \n                    gr_actions, gr_states = self.agents_by_group[prefix]( \n                        gr_obs, agent_states[prefix]) \n                    agent_states[prefix] = gr_states \n                    idx = 0 \n                    for agent_id in group: \n                        if agent_id not in cur_obs: \n                            continue \n                        actions[agent_id] = gr_actions[idx] \n                        idx += 1\n```", "```py\n new_obs, rewards, dones, truncs, _ = self.env.step(actions) \n\n                for agent_id, reward in rewards.items(): \n                    group = self.agent_groups[agent_id] \n                    if group == self.track_reward_group: \n                        episode_rewards += reward \n                    if self.filter_group is not None: \n                        if group != self.filter_group: \n                            continue \n                    last_state = new_obs[agent_id] \n                    if dones[agent_id] or truncs[agent_id]: \n                        last_state = None \n                    yield ExperienceFirstLastMARL( \n                        state=cur_obs[agent_id], action=actions[agent_id], \n                        reward=reward, last_state=last_state, group=group \n                    ) \n                cur_obs = new_obs \n                episode_steps += 1\n```", "```py\n self.total_steps.append(episode_steps) \n            tr_group = self.group_agents[self.track_reward_group] \n            self.total_rewards.append(episode_rewards / len(tr_group))\n```", "```py\n action_selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=PARAMS.epsilon_start) \n    epsilon_tracker = common.EpsilonTracker(action_selector, PARAMS) \n    tiger_agent = ptan.agent.DQNAgent(net, action_selector, device) \n    deer_agent = data.RandomMAgent(env, env.handles[0]) \n    exp_source = data.MAgentExperienceSourceFirstLast( \n        env, \n        agents_by_group={’deer’: deer_agent, ’tiger’: tiger_agent}, \n        track_reward_group=\"tiger\", \n        filter_group=\"tiger\", \n    ) \n    buffer = ptan.experience.ExperienceReplayBuffer(exp_source, PARAMS.replay_size)\n```", "```py\n./forest_tigers_dqn.py -n run-name --dev cuda --mode double_attack\n```"]