["```py\nimport numpy as np\n\ndef coin_mle(data):\n    \"\"\"\n    Returns the learned probability of getting a heads using MLE.\n\n    Parameters\n    ----------\n    data: list, array-like\n        The list of observations. 1 for heads and 0 for tails.\n\n    Returns\n    -------\n    theta: The learned probability of getting a heads.\n    \"\"\"\n    data = np.array(data)\n    n_heads = np.sum(data)\n\n    return n_heads / data.size\n```", "```py\n>>> coin_mle([1, 1, 1, 0, 0])\n0.59999999999999998\n\n>>> coin_mle([1, 1, 1, 0, 0, 0])\n0.5\n\n>>> coin_mle([1, 1, 1, 0, 0, 0, 0])\n0.42857142857142855\n```", "```py\n>>> coin_mle([1, 1, 1])\n1.0\n```", "```py\nimport numpy as np\n\ndef gaussian_mle(data):\n \"\"\"\n Returns the learned parameters of the Normal Distribution using MLE.\n\n Parameters\n ----------\n data: list, array-like\n The list of observed variables.\n\n Returns\n -------\n \\mu: The learned mean of the Normal Distribution.\n \\sigma: The learned standard deviation of the Normal Distribution.\n \"\"\"\n data = np.array(data)\n mu = np.mean(data)\n variance = np.sqrt(np.mean((data - mu)**2))\n\n return mu, variance\n```", "```py\n>>> from numpy.random import normal\n>>> data = normal(loc=1, scale=2, size=10)\n>>> data\narray([ 1.8120102, 2.14363679, 1.49010868, -1.95531206, 1.62449155,\n        1.49345327, 1.48957918, -0.67536313, 4.31506202, 4.24883442])\n\n>>> mu, sigma = gaussian_mle(data)\n>>> mu\n1.5986500906187573\n>>> sigma\n1.805051208889392\n```", "```py\n>>> data = normal(loc=1, scale=2, size=1000)\n>>> data[:10]\narray([ 4.5855015, 1.55162883, -1.61385859, 0.52543984, 0.90247428,\n        3.40717092, 1.4078157, 0.01560836, -1.19409859, -0.01641439])\n\n>>> mu, sigma = gaussian_mle(data)\n>>> mu\n 1.0437186891666821\n>>> sigma\n1.967211026428509\n```", "```py\ndef weather_fit(data):\n    \"\"\"\n    Learn the transition and emission probabilities from the given data\n    for the weather model.\n\n    Parameters\n    ----------\n    data: 2-D list (array-like)\n    Each data point should be a tuple of size 2 with the first element\n    representing the state of *Weather* and the second element representing\n    whether it rained or not.\n    Sunny = 0, Cloudy = 1, Windy = 2\n    Rain = 0, No Rain = 1\n\n    Returns\n    -------\n    transition probability: 2-D array\n    The conditional distribution representing the transition probability \n    of the model.\n    emission probability: 2-D array\n    The conditional distribution representing the emission probability \n    of the model.\n    \"\"\"\n    data = np.array(data)\n\n    transition_counts = np.zeros((3, 3))\n    emission_counts = np.zeros((3, 2))\n\n    for index, datapoint in enumerate(data):\n        if index != len(data)-1:\n            transition_counts[data[index][0], data[index+1][0]] += 1\n        emission_counts[data[index][0], data[index][1]] += 1\n\n    transition_prob = transition_counts / np.sum(transition_counts, axis=0)\n    emission_prob = (emission_counts.T / np.sum(emission_counts.T, axis=0)).T\n\n    return transition_prob, emission_prob\n```", "```py\n>>> import numpy as np\n>>> weather_data = np.random.randint(low=0, high=3, size=1000)\n>>> rain_data = np.random.randint(low=0, high=2, size=1000)\n>>> data = list(zip(weather_data, rain_data))\n>>> transition_prob, emission_prob = weather_fit(data)\n>>> transition_prob\narray([[ 0.3125, 0.38235294, 0.27272727],\n       [ 0.28125, 0.38235294, 0.36363636],\n       [ 0.40625, 0.23529412, 0.36363636]])\n\n>>> emission_prob\narray([[ 0.3125, 0.38235294, 0.27272727],\n       [ 0.28125, 0.38235294, 0.36363636],\n       [ 0.40625, 0.23529412, 0.36363636]])\n\n```", "```py\npip install matplotlib datetime\n```", "```py\nfrom __future__ import print_function\n\nimport datetime\n\nimport numpy as np\nfrom matplotlib import cm, pyplot as plt\nfrom matplotlib.dates import YearLocator, MonthLocator\n\ntry:\n    from matplotlib.finance import quotes_historical_yahoo_ochl\nexcept ImportError:\n    # For Matplotlib prior to 1.5.\n    from matplotlib.finance import (\n        quotes_historical_yahoo as quotes_historical_yahoo_ochl\n    )\n\nfrom hmmlearn.hmm import GaussianHMM\n\nprint(__doc__)\n```", "```py\nquotes = quotes_historical_yahoo_ochl(\n    \"INTC\", datetime.date(1995, 1, 1), datetime.date(2012, 1, 6))\n\n# Unpack quotes\ndates = np.array([q[0] for q in quotes], dtype=int)\nclose_v = np.array([q[2] for q in quotes])\nvolume = np.array([q[5] for q in quotes])[1:]\n\n# Take diff of close value. Note that this makes\n# ``len(diff) = len(close_t) - 1``, therefore, other quantities also\n# need to be shifted by 1.\ndiff = np.diff(close_v)\ndates = dates[1:]\nclose_v = close_v[1:]\n\n# Pack diff and volume for training.\nX = np.column_stack([diff, volume])\n```", "```py\n# Make an HMM instance and execute fit\nmodel = GaussianHMM(n_components=4, covariance_type=\"diag\", n_iter=1000).fit(X)\n\n# Predict the optimal sequence of internal hidden state\nhidden_states = model.predict(X)\n```", "```py\nprint(\"Transition matrix\")\nprint(model.transmat_)\nprint()\n\nprint(\"Means and vars of each hidden state\")\nfor i in range(model.n_components):\n    print(\"{0}th hidden state\".format(i))\n    print(\"mean = \", model.means_[i])\n    print(\"var = \", np.diag(model.covars_[i]))\n    print()\n```", "```py\nTransition matrix\n[[  9.79220773e-01   2.57382344e-15   2.72061945e-03   1.80586073e-02]\n [  1.12216188e-12   7.73561269e-01   1.85019044e-01   4.14196869e-02]\n [  3.25313504e-03   1.12692615e-01   8.83368021e-01   6.86228435e-04]\n [  1.18741799e-01   4.20310643e-01   1.18670597e-18   4.60947557e-01]]\n\nMeans and vars of each hidden state\n0th hidden state\nmean =  [  2.33331888e-02   4.97389989e+07]\nvar =  [  6.97748259e-01   2.49466578e+14]\n\n1st hidden state\nmean =  [  2.12401671e-02   8.81882861e+07]\nvar =  [  1.18665023e-01   5.64418451e+14]\n\n2nd hidden state\nmean =  [  7.69658065e-03   5.43135922e+07]\nvar =  [  5.02315562e-02   1.54569357e+14]\n\n3rd hidden state\nmean =  [ -3.53210673e-01   1.53080943e+08]\nvar =  [  2.55544137e+00   5.88210257e+15]\n```", "```py\nfig, axs = plt.subplots(model.n_components, sharex=True, sharey=True)\ncolours = cm.rainbow(np.linspace(0, 1, model.n_components))\nfor i, (ax, colour) in enumerate(zip(axs, colours)):\n    # Use fancy indexing to plot data in each state.\n    mask = hidden_states == i\n    ax.plot_date(dates[mask], close_v[mask], \".-\", c=colour)\n    ax.set_title(\"{0}th hidden state\".format(i))\n\n    # Format the ticks.\n    ax.xaxis.set_major_locator(YearLocator())\n    ax.xaxis.set_minor_locator(MonthLocator())\n\n    ax.grid(True)\n\nplt.show()\n\n```"]