["```py\nsource(\"nnet_functions.R\")\ndata_sel <- \"bulls_eye\"\n\n........\n\n####################### neural network ######################\nhidden <- 3\nepochs <- 3000\nlr <- 0.5\nactivation_ftn <- \"sigmoid\"\n\ndf <- getData(data_sel) # from nnet_functions\nX <- as.matrix(df[,1:2])\nY <- as.matrix(df$Y)\nn_x=ncol(X)\nn_h=hidden\nn_y=1\nm <- nrow(X)\n```", "```py\n# initialise weights\nset.seed(42)\nweights1 <- matrix(0.01*runif(n_h*n_x)-0.005, ncol=n_x, nrow=n_h)\nweights2 <- matrix(0.01*runif(n_y*n_h)-0.005, ncol=n_h, nrow=n_y)\nbias1 <- matrix(rep(0,n_h),nrow=n_h,ncol=1)\nbias2 <- matrix(rep(0,n_y),nrow=n_y,ncol=1)\n```", "```py\nfor (i in 0:epochs)\n{\n  activation2 <- forward_prop(t(X),activation_ftn,weights1,bias1, weights2,bias2)\n  cost <- cost_f(activation2,t(Y))\n  backward_prop(t(X),t(Y),activation_ftn,weights1,weights2, activation1,activation2)\n  weights1 <- weights1 - (lr * dweights1)\n  bias1 <- bias1 - (lr * dbias1)\n  weights2 <- weights2 - (lr * dweights2)\n  bias2 <- bias2 - (lr * dbias2)\n\n  if ((i %% 500) == 0)\n    print (paste(\" Cost after\",i,\"epochs =\",cost))\n}\n[1] \" Cost after 0 epochs = 0.693147158995952\"\n[1] \" Cost after 500 epochs = 0.69314587328381\"\n[1] \" Cost after 1000 epochs = 0.693116915341439\"\n[1] \" Cost after 1500 epochs = 0.692486724429629\"\n[1] \" Cost after 2000 epochs = 0.687107068792801\"\n[1] \" Cost after 2500 epochs = 0.660418522655335\"\n[1] \" Cost after 3000 epochs = 0.579832913091798\"\n```", "```py\nforward_prop <- function(X,activation_ftn,weights1,bias1,weights2,bias2)\n{\n  # broadcast hack\n  bias1a<-bias1\n  for (i in 2:ncol(X))\n    bias1a<-cbind(bias1a,bias1)\n  bias2a<-bias2\n  for (i in 2:ncol(activation1))\n    bias2a<-cbind(bias2a,bias2)\n\n  Z1 <<- weights1 %*% X + bias1a\n  activation1 <<- activation_function(activation_ftn,Z1)\n  bias2a<-bias2\n  for (i in 2:ncol(activation1))\n    bias2a<-cbind(bias2a,bias2)\n  Z2 <<- weights2 %*% activation1 + bias2a\n  activation2 <<- sigmoid(Z2)\n  return (activation2)\n}\n```", "```py\nactivation_function <- function(activation_ftn,v)\n{\n  if (activation_ftn == \"sigmoid\")\n    res <- sigmoid(v)\n  else if (activation_ftn == \"tanh\")\n    res <- tanh(v)\n  else if (activation_ftn == \"relu\")\n  {\n    v[v<0] <- 0\n    res <- v\n  }\n  else\n    res <- sigmoid(v)\n  return (res)\n}\n```", "```py\ncost_f <- function(activation2,Y)\n{\n  cost = -mean((log(activation2) * Y)+ (log(1-activation2) * (1-Y)))\n  return(cost)\n}\n```", "```py\nbackward_prop <- function(X,Y,activation_ftn,weights1,weights2,activation1,activation2)\n{\n  m <- ncol(Y)\n  derivative2 <- activation2-Y\n  dweights2 <<- (derivative2 %*% t(activation1)) / m\n  dbias2 <<- rowSums(derivative2) / m\n  upd <- derivative_function(activation_ftn,activation1)\n  derivative1 <- t(weights2) %*% derivative2 * upd\n  dweights1 <<- (derivative1 %*% t(X)) / m\n  dbias1 <<- rowSums(derivative1) / m\n}\n```", "```py\nderivative_function <- function(activation_ftn,v)\n{\n  if (activation_ftn == \"sigmoid\")\n   upd <- (v * (1 - v))\n  else if (activation_ftn == \"tanh\")\n   upd <- (1 - (v^2))\n  else if (activation_ftn == \"relu\")\n   upd <- ifelse(v > 0.0,1,0)\n  else\n   upd <- (v * (1 - v))\n  return (upd)\n}\n```", "```py\nlibrary(mxnet)\n?mx.model.FeedForward.create\n```", "```py\nmx.model.FeedForward.create(symbol, X, y = NULL, ctx = NULL,\n begin.round = 1, num.round = 10, optimizer = \"sgd\",\n initializer = mx.init.uniform(0.01), eval.data = NULL,\n eval.metric = NULL, epoch.end.callback = NULL,\n batch.end.callback = NULL, array.batch.size = 128\n ...)\n```", "```py\nset.seed(1234)\nX <- mvrnorm(n = 200, mu = c(0, 0, 0, 0, 0),\n  Sigma = matrix(c(\n    1, .9999, .99, .99, .10,\n    .9999, 1, .99, .99, .10,\n    .99, .99, 1, .99, .10,\n    .99, .99, .99, 1, .10,\n    .10, .10, .10, .10, 1\n  ), ncol = 5))\ny <- rnorm(200, 3 + X %*% matrix(c(1, 1, 1, 1, 0)), .5)\n```", "```py\nm.ols <- lm(y[1:100] ~ X[1:100, ])\nm.lasso.cv <- cv.glmnet(X[1:100, ], y[1:100], alpha = 1)\nplot(m.lasso.cv)\n```", "```py\ncbind(OLS = coef(m.ols),Lasso = coef(m.lasso.cv)[,1])\n               OLS Lasso\n(Intercept)  2.958  2.99\nX[1:100, ]1 -0.082  1.41\nX[1:100, ]2  2.239  0.71\nX[1:100, ]3  0.602  0.51\nX[1:100, ]4  1.235  1.17\nX[1:100, ]5 -0.041  0.00\n```", "```py\nm.ridge.cv <- cv.glmnet(X[1:100, ], y[1:100], alpha = 0)\nplot(m.ridge.cv)\n```", "```py\n> cbind(OLS = coef(m.ols),Lasso = coef(m.lasso.cv)[,1],Ridge = coef(m.ridge.cv)[,1])\n               OLS Lasso   Ridge\n(Intercept)  2.958  2.99  2.9919\nX[1:100, ]1 -0.082  1.41  0.9488\nX[1:100, ]2  2.239  0.71  0.9524\nX[1:100, ]3  0.602  0.51  0.9323\nX[1:100, ]4  1.235  1.17  0.9548\nX[1:100, ]5 -0.041  0.00 -0.0023\n```", "```py\nset.seed(1234)\n## same data as from previous chapter\nif (!file.exists('../data/train.csv'))\n{\n  link <- 'https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/R/data/mnist_csv.zip'\n  if (!file.exists(paste(dataDirectory,'/mnist_csv.zip',sep=\"\")))\n    download.file(link, destfile = paste(dataDirectory,'/mnist_csv.zip',sep=\"\"))\n  unzip(paste(dataDirectory,'/mnist_csv.zip',sep=\"\"), exdir = dataDirectory)\n  if (file.exists(paste(dataDirectory,'/test.csv',sep=\"\")))\n    file.remove(paste(dataDirectory,'/test.csv',sep=\"\"))\n}\n\ndigits.train <- read.csv(\"../data/train.csv\")\n\n## convert to factor\ndigits.train$label <- factor(digits.train$label, levels = 0:9)\n\nsample <- sample(nrow(digits.train), 6000)\ntrain <- sample[1:5000]\ntest <- sample[5001:6000]\n\ndigits.X <- digits.train[train, -1]\ndigits.y <- digits.train[train, 1]\ntest.X <- digits.train[test, -1]\ntest.y <- digits.train[test, 1]\n\n## try various weight decays and number of iterations\n## register backend so that different decays can be\n## estimated in parallel\ncl <- makeCluster(5)\nclusterEvalQ(cl, {source(\"cluster_inc.R\")})\nregisterDoSNOW(cl)\n```", "```py\nset.seed(1234)\ndigits.decay.m1 <- lapply(c(100, 150), function(its) {\n  caret::train(digits.X, digits.y,\n           method = \"nnet\",\n           tuneGrid = expand.grid(\n             .size = c(10),\n             .decay = c(0, .1)),\n           trControl = caret::trainControl(method=\"cv\", number=5, repeats=1),\n           MaxNWts = 10000,\n           maxit = its)\n})\n```", "```py\ndigits.decay.m1[[1]]\nNeural Network \n\n5000 samples\n 784 predictor\n  10 classes: '0', '1', '2', '3', '4', '5', '6', '7', '8', '9' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 4000, 4001, 4000, 3998, 4001 \nResampling results across tuning parameters:\n\n  decay  Accuracy   Kappa\n    0.0     0.56    0.51 \n    0.1     0.56    0.51 \n\nTuning parameter 'size' was held constant at a value of 10\nAccuracy was used to select the optimal model using the\n largest value.\nThe final values used for the model were size = 10 and decay = 0.1.\n```", "```py\ndigits.decay.m1[[2]]\nNeural Network \n\n5000 samples\n 784 predictor\n  10 classes: '0', '1', '2', '3', '4', '5', '6', '7', '8', '9' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 4000, 4002, 3998, 4000, 4000 \nResampling results across tuning parameters:\n\n  decay  Accuracy   Kappa\n    0.0      0.64    0.60 \n    0.1      0.63    0.59 \n\nTuning parameter 'size' was held constant at a value of 10\nAccuracy was used to select the optimal model using the\n largest value.\nThe final values used for the model were size = 10 and decay = 0.\n```", "```py\n## simulated data\nset.seed(1234)\nd <- data.frame(\n x = rnorm(400))\nd$y <- with(d, rnorm(400, 2 + ifelse(x < 0, x + x^2, x + x^2.5), 1))\nd.train <- d[1:200, ]\nd.test <- d[201:400, ]\n\n## three different models\nm1 <- lm(y ~ x, data = d.train)\nm2 <- lm(y ~ I(x^2), data = d.train)\nm3 <- lm(y ~ pmax(x, 0) + pmin(x, 0), data = d.train)\n\n## In sample R2\ncbind(M1=summary(m1)$r.squared,\n M2=summary(m2)$r.squared,M3=summary(m3)$r.squared)\n       M1  M2   M3\n[1,] 0.33 0.6 0.76\n```", "```py\ncor(cbind(M1=fitted(m1),\n M2=fitted(m2),M3=fitted(m3)))\n     M1   M2   M3\nM1 1.00 0.11 0.65\nM2 0.11 1.00 0.78\nM3 0.65 0.78 1.00\n```", "```py\n## generate predictions and the average prediction\nd.test$yhat1 <- predict(m1, newdata = d.test)\nd.test$yhat2 <- predict(m2, newdata = d.test)\nd.test$yhat3 <- predict(m3, newdata = d.test)\nd.test$yhatavg <- rowMeans(d.test[, paste0(\"yhat\", 1:3)])\n\n## correlation in the testing data\ncor(d.test)\n             x     y  yhat1  yhat2 yhat3 yhatavg\nx        1.000  0.44  1.000 -0.098  0.60    0.55\ny        0.442  1.00  0.442  0.753  0.87    0.91\nyhat1    1.000  0.44  1.000 -0.098  0.60    0.55\nyhat2   -0.098  0.75 -0.098  1.000  0.69    0.76\nyhat3    0.596  0.87  0.596  0.687  1.00    0.98\nyhatavg  0.552  0.91  0.552  0.765  0.98    1.00\n```", "```py\n## Fit Models\nnn.models <- foreach(i = 1:4, .combine = 'c') %dopar% {\nset.seed(1234)\n list(nn.train(\n    x = as.matrix(digits.X),\n    y = model.matrix(~ 0 + digits.y),\n    hidden = c(40, 80, 40, 80)[i],\n    activationfun = \"tanh\",\n    learningrate = 0.8,\n    momentum = 0.5,\n    numepochs = 150,\n    output = \"softmax\",\n    hidden_dropout = c(0, 0, .5, .5)[i],\n    visible_dropout = c(0, 0, .2, .2)[i]))\n}\n```", "```py\nnn.yhat <- lapply(nn.models, function(obj) {\n encodeClassLabels(nn.predict(obj, as.matrix(digits.X)))\n })\nperf.train <- do.call(cbind, lapply(nn.yhat, function(yhat) {\n caret::confusionMatrix(xtabs(~ I(yhat - 1) + digits.y))$overall\n }))\ncolnames(perf.train) <- c(\"N40\", \"N80\", \"N40_Reg\", \"N80_Reg\")\noptions(digits = 4)\nperf.train\n                   N40     N80 N40_Reg N80_Reg\nAccuracy        0.9478  0.9622  0.9278  0.9400\nKappa           0.9420  0.9580  0.9197  0.9333\nAccuracyLower   0.9413  0.9565  0.9203  0.9331\nAccuracyUpper   0.9538  0.9673  0.9348  0.9464\nAccuracyNull    0.1126  0.1126  0.1126  0.1126\nAccuracyPValue  0.0000  0.0000  0.0000  0.0000\nMcnemarPValue      NaN     NaN     NaN     NaN\n```", "```py\nnn.yhat.test <- lapply(nn.models, function(obj) {\n encodeClassLabels(nn.predict(obj, as.matrix(test.X)))\n })\n\nperf.test <- do.call(cbind, lapply(nn.yhat.test, function(yhat) {\n caret::confusionMatrix(xtabs(~ I(yhat - 1) + test.y))$overall\n }))\ncolnames(perf.test) <- c(\"N40\", \"N80\", \"N40_Reg\", \"N80_Reg\")\n\nperf.test\n                   N40     N80 N40_Reg N80_Reg\nAccuracy        0.8890  0.8520  0.8980  0.9030\nKappa           0.8765  0.8352  0.8864  0.8920\nAccuracyLower   0.8679  0.8285  0.8776  0.8830\nAccuracyUpper   0.9078  0.8734  0.9161  0.9206\nAccuracyNull    0.1180  0.1180  0.1180  0.1180\nAccuracyPValue  0.0000  0.0000  0.0000  0.0000\nMcnemarPValue      NaN     NaN     NaN     NaN\n```"]