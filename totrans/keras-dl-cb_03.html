<html><head></head><body><div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Optimization for Neural Networks</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">A number of applications in deep learning require optimization problems to be solved. Optimization refers to bringing whatever we are dealing with towards its ultimate state. The problem solved through the use of an optimization process must be supplied with data, providing model constants and parameters in functions, describing the overall objective function along with some constraints.</p>
<p class="calibre4">In this chapter, we will look at the TensorFlow pipeline and various optimization models provided by the TensorFlow library. The list of topics covered are as follows:</p>
<ul class="calibre20">
<li class="calibre21">Optimization basics</li>
<li class="calibre21">Types of optimizers</li>
<li class="calibre21">Gradient descent</li>
<li class="calibre21">Choosing the correct optimizer</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">What is optimization?</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">The process to find maxima or minima is based on constraints. The choice of optimization algorithm for your deep learning model can mean the difference between good results in minutes, hours, and days.</p>
<div class="packt_infobox">Optimization sits at the center of deep learning. Most learning problems reduce to optimization problems. Let's imagine we are solving a problem for some set of data. Using this pre-processed data, we train a model by solving an optimization problem, which optimizes the weights of the model with regards to the chosen loss function and some regularization function.</div>
<p class="calibre4">Hyper parameters of a model play a significant role in the efficient training of a model. Therefore, it is essential to use the different optimization strategies and algorithms to measure appropriate and optimum values of model's hyper parameters, which affect our Model's learning process, and finally the output of a model.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Types of optimizers</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">First, we look at the high-level categories of optimization algorithms and then dive deep into the individual optimizers.</p>
<p class="calibre4"><strong class="calibre7">First order optimization</strong> algorithms minimize or maximize a loss function using its gradient values concerning the parameters. The popularly used First order optimization algorithm is gradient descent. Here, the first order derivative tells us whether the function is decreasing or increasing at a particular point. The first order derivative gives us a line which is tangential to a point on its error surface.</p>
<div class="packt_infobox">The derivative for a function depends on single variables, whereas a gradient for a function depends on multiple variables.</div>
<p class="calibre4"><strong class="calibre7">Second order optimization</strong> algorithms use the second order derivative, which is also known as <strong class="calibre7">Hessian</strong>, to minimize or maximize the given loss function. Here, the Hessian is a matrix of second order partial derivatives. The second derivative is costly to compute. Hence, it's not used much. The second order derivative indicates to us whether the first derivative is increasing or decreasing, giving an idea of functions curvature. The second order derivative gives us with a quadratic surface which touches the shape of the error surface.</p>
<p class="calibre4">The second order derivative is costly to compute, but the advantage of a second order optimization method is that it does not neglect or ignore the curvature of a surface. Also, the stepwise performance is better. The key thing to note while choosing the optimization method is, first-order optimization methods are simple to compute and less time consuming, converging rather fast on large data sets. Second order methods are faster only when the second order derivative is known, and these methods are slower and expensive to compute in terms of both time and memory.</p>
<p class="calibre4">The second order optimization method can, at times, work better than first-order gradient descent methods because second-order methods will never get stuck around paths of slow convergence, that is, around saddle points, whereas gradient descent at times gets stuck and does not converge.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Gradient descent</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">Gradient descent is an algorithm which minimizes functions. A set of parameters defines a function, and the gradient descent algorithm starts with the initial set of param values and iteratively moves toward a set of param values that minimizes the function.</p>
<p class="calibre4">This iterative minimization is achieved using calculus, taking steps in the negative direction of the function gradient, as can be seen in the following diagram:</p>
<div class="mce-root"><img src="Images/e3e8215c-9a54-438f-926c-e840fb8dd465.jpeg" width="665" height="667" class="calibre79"/></div>
<p class="calibre4">Gradient descent is the most successful optimization algorithm. As mentioned earlier, it is used to do weights updates in a neural network so that we minimize the loss function. Let's now talk about an important neural network method called backpropagation, in which we firstly propagate forward and calculate the dot product of inputs with their corresponding weights, and then apply an activation function to the sum of products which transforms the input to an output and adds non linearities to the model, which enables the model to learn almost any arbitrary functional mappings.</p>
<p class="calibre4">Later, we back propagate in the neural network, carrying error terms and updating weights values using gradient descent, as shown in the following graph:</p>
<div class="mce-root"><img src="Images/6408090e-36de-4968-8d52-5868895befb7.png" width="1958" height="1061" class="calibre80"/></div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Different variants of gradient descent</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4"><strong class="calibre7">Standard gradient descent</strong>, also known as <strong class="calibre7">batch gradient descent</strong>, will calculate the gradient of the whole dataset but will perform only one update. Therefore, it can be quite slow and tough to control for datasets which are extremely large and don't fit in the memory. Let's now look at algorithms that can solve this problem.</p>
<p class="calibre4"><strong class="calibre7">Stochastic gradient descent</strong> (<strong class="calibre7">SGD</strong>) performs parameter updates on each training example, whereas mini batch performs an update with <em class="calibre17">n</em> number of training examples in each batch. The issue with SGD is that, due to the frequent updates and fluctuations, it eventually complicates the convergence to the accurate minimum and will keep exceeding due to regular fluctuations. Mini-batch gradient descent comes to the rescue here, which reduces the variance in the parameter update, leading to a much better and stable convergence. SGD and mini-batch are used interchangeably.</p>
<p class="calibre4">Overall problems with gradient descent include choosing a proper learning rate so that we avoid slow convergence at small values, or divergence at larger values and applying the same learning rate to all parameter updates wherein if the data is sparse we might not want to update all of them to the same extent. Lastly, is dealing with saddle points.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Algorithms to optimize gradient descent</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">We will now be looking at various methods for optimizing gradient descent in order to calculate different learning rates for each parameter, calculate momentum, and prevent decaying learning rates.</p>
<p class="calibre4">To solve the problem of high variance oscillation of the SGD, a method called <strong class="calibre7">momentum</strong> was discovered; this accelerates the SGD by navigating along the appropriate direction and softening the oscillations in irrelevant directions. Basically, it adds a fraction of the update vector of the past step to the current update vector. Momentum value is usually set to .9. Momentum leads to a faster and stable convergence with reduced oscillations.</p>
<p class="calibre4"><strong class="calibre7">Nesterov accelerated gradient</strong> explains that as we reach the minima, that is, the lowest point on the curve, momentum is quite high and it doesn't know to slow down at that point due to the large momentum which could cause it to miss the minima entirely and continue moving up. Nesterov proposed that we first make a long jump based on the previous momentum, then calculate the gradient and then make a correction which results in a parameter update. Now, this update prevents us to go too fast and not miss the minima, and makes it more responsive to changes.</p>
<p class="calibre4"><strong class="calibre7">Adagrad</strong> allows the learning rate to adapt based on the parameters. Therefore, it performs large updates for infrequent parameters and small updates for frequent parameters. Therefore, it is very well-suited for dealing with sparse data. The main flaw is that its learning rate is always decreasing and decaying. Problems with decaying learning rates are solved using AdaDelta.</p>
<p class="calibre4"><strong class="calibre7">AdaDelta</strong> solves the problem of decreasing learning rate in AdaGrad. In AdaGrad, the learning rate is computed as one divided by the sum of square roots. At each stage, we add another square root to the sum, which causes the denominator to decrease constantly. Now, instead of summing all prior square roots, it uses a sliding window which allows the sum to decrease.</p>
<p class="calibre4"><strong class="calibre7">Adaptive Moment Estimation</strong> (<strong class="calibre7">Adam</strong>) computes adaptive learning rates for each parameter. Like AdaDelta, Adam not only stores the decaying average of past squared gradients but additionally stores the momentum change for each parameter. Adam works well in practice and is one of the most used optimization methods today.</p>
<p class="calibre4">The following two images (image credit: Alec Radford) show the optimization behavior of optimization algorithms described earlier. We see their behavior on the contours of a loss surface over time. Adagrad, RMsprop, and Adadelta almost quickly head off in the right direction and converge fast, whereas momentum and NAG are headed off-track. NAG is soon able to correct its course due to its improved responsiveness by looking ahead and going to the minimum.</p>
<div class="mce-root"><img src="Images/21078e47-ab9b-4dbd-95a9-aaf1f8dfa1eb.png" width="1240" height="960" class="calibre81"/></div>
<p class="calibre4">The second image displays the behavior of the algorithms at a saddle point. <strong class="calibre7">SGD</strong>, <strong class="calibre7">Momentum</strong>, and <strong class="calibre7">NAG</strong> find it challenging to break symmetry, but slowly they manage to escape the saddle point, whereas <strong class="calibre7">Adagrad</strong>, <strong class="calibre7">Adadelta</strong>, and <strong class="calibre7">RMsprop</strong> head down the negative slope, as can seen from the following image:</p>
<div class="mce-root"><img src="Images/04660bd2-3f81-4c3a-b5bf-94447cf0b8c5.png" width="1258" height="1070" class="calibre82"/></div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Which optimizer to choose</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">In the case that the input data is sparse or if we want fast convergence while training complex neural networks, we get the best results using adaptive learning rate methods. We also don't need to tune the learning rate. For most cases, Adam is usually a good choice.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Optimization with an example</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">Let's take an example of linear regression, where we try to find the best fit for a straight line through a number of data points by minimizing the squares of the distance from the line to each data point. This is why we call it least squares regression. Essentially, we are formulating the problem as an optimization problem, where we are trying to minimize a loss function.</p>
<p class="calibre4">Let's set up input data and look at the scatter plot:</p>
<pre class="calibre26"># input data<br class="calibre2"/>xData = np.arange(100, step=.1)<br class="calibre2"/>yData = xData + 20 * np.sin(xData/10)</pre>
<div class="mce-root"><img src="Images/c03f289d-0fed-49f7-b77b-519f3e7ed240.png" width="822" height="565" class="calibre83"/></div>
<p class="calibre4">Define the data size and batch size:</p>
<pre class="calibre26"># define the data size and batch size<em class="calibre29"><br class="calibre2"/></em>nSamples = 1000<br class="calibre2"/>batchSize = 100</pre>
<p class="calibre4">We will need to resize the data to meet the TensorFlow input format, as follows:</p>
<pre class="calibre26"># resize input for tensorflow<em class="calibre29"><br class="calibre2"/> </em>xData = np.reshape(xData, (nSamples, 1))<br class="calibre2"/> yData = np.reshape(yData, (nSamples, 1)) </pre>
<p class="calibre4">The following scope initializes the <kbd class="calibre18">weights</kbd> and <kbd class="calibre18">bias</kbd>, and describes the linear model and loss function:</p>
<pre class="calibre26"><strong class="calibre3">with </strong>tf.variable_scope(<strong class="calibre3">"linear-regression-pipeline"</strong>):<br class="calibre2"/>     W = tf.get_variable(<strong class="calibre3">"weights"</strong>, (1,1), initializer=tf.random_normal_initializer())<br class="calibre2"/>     b = tf.get_variable(<strong class="calibre3">"bias"</strong>, (1, ), initializer=tf.constant_initializer(0.0))<br class="calibre2"/> <br class="calibre2"/>     <em class="calibre29">#</em> model<br class="calibre2"/><em class="calibre29">     </em>yPred = tf.matmul(X, W) + b<br class="calibre2"/>     <em class="calibre29">#</em> loss function<br class="calibre2"/><em class="calibre29">     </em>loss = tf.reduce_sum((y - yPred)**2/nSamples)</pre>
<p class="calibre4">We then set optimizers for minimizing the loss:</p>
<pre class="calibre26"># set the optimizer<br class="calibre2"/> #optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)<br class="calibre2"/> #optimizer = tf.train.AdamOptimizer(learning_rate=.001).minimize(loss)<br class="calibre2"/> #optimizer = tf.train.AdadeltaOptimizer(learning_rate=.001).minimize(loss)<br class="calibre2"/> #optimizer = tf.train.AdagradOptimizer(learning_rate=.001).minimize(loss)<br class="calibre2"/> #optimizer = tf.train.MomentumOptimizer(learning_rate=.001, momentum=0.9).minimize(loss)<br class="calibre2"/> #optimizer = tf.train.FtrlOptimizer(learning_rate=.001).minimize(loss)<br class="calibre2"/> optimizer = tf.train.RMSPropOptimizer(learning_rate=.001).minimize(loss)  We then select the mini batch and run the optimizers errors = []<br class="calibre2"/> <strong class="calibre3">with </strong>tf.Session() <strong class="calibre3">as </strong>sess:<br class="calibre2"/>     # init variables<br class="calibre2"/>     sess.run(tf.global_variables_initializer())<br class="calibre2"/> <br class="calibre2"/>     <strong class="calibre3">for </strong>_ <strong class="calibre3">in </strong>range(1000):<br class="calibre2"/>         # select mini batch<em class="calibre29"><br class="calibre2"/>         </em>indices = np.random.choice(nSamples, batchSize)<br class="calibre2"/>         xBatch, yBatch = xData[indices], yData[indices]<br class="calibre2"/>         # run optimizer<em class="calibre29"><br class="calibre2"/>         </em>_, lossVal = sess.run([optimizer, loss], feed_dict={X: xBatch, y: yBatch})<br class="calibre2"/>         errors.append(lossVal)<br class="calibre2"/> <br class="calibre2"/> plt.plot([np.mean(errors[i-50:i]) <strong class="calibre3">for </strong>i <strong class="calibre3">in </strong>range(len(errors))])<br class="calibre2"/> plt.show()<br class="calibre2"/> plt.savefig(<strong class="calibre3">"errors.png"</strong>)</pre>
<p class="calibre4">The output of the preceding code is as follows:</p>
<div class="mce-root"><img src="Images/e5d26fc6-cbfa-4d72-bbb5-a894c7135c0f.png" width="779" height="454" class="calibre84"/></div>
<p class="calibre4">We also get a sliding curve, as follows:</p>
<div class="mce-root"><img src="Images/ba681a85-288f-4c46-90ba-092a214c96f1.png" width="674" height="474" class="calibre85"/></div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">In this chapter, we learned the fundamentals of optimization techniques and various types. Optimization is a complicated subject and a lot depends on the nature and size of our data. Also, optimization depends on weight matrices. A lot of these optimizers are trained and tuned for tasks like image classification or predictions. However, for custom or new use cases, we need to perform trial and error to determine the best solution.</p>
<p class="calibre4"/>


            </article>

            
        </section>
    </div>



  </body></html>