["```py\nGAME_ROWS = 6 \nGAME_COLS = 7 \nBITS_IN_LEN = 3 \nPLAYER_BLACK = 1 \nPLAYER_WHITE = 0 \nCOUNT_TO_WIN = 4 \n\nINITIAL_STATE = encode_lists([[]] * GAME_COLS)\n```", "```py\nclass MCTS: \n    def __init__(self, c_puct: float = 1.0): \n        self.c_puct = c_puct \n        # count of visits, state_int -> [N(s, a)] \n        self.visit_count: tt.Dict[int, tt.List[int]] = {} \n        # total value of the stateâ€™s act, state_int -> [W(s, a)] \n        self.value: tt.Dict[int, tt.List[float]] = {} \n        # average value of actions, state_int -> [Q(s, a)] \n        self.value_avg: tt.Dict[int, tt.List[float]] = {} \n        # prior probability of actions, state_int -> [P(s,a)] \n        self.probs: tt.Dict[int, tt.List[float]] = {}\n```", "```py\n def clear(self): \n        self.visit_count.clear() \n        self.value.clear() \n        self.value_avg.clear() \n        self.probs.clear()\n```", "```py\n def find_leaf(self, state_int: int, player: int): \n        states = [] \n        actions = [] \n        cur_state = state_int \n        cur_player = player \n        value = None\n```", "```py\n while not self.is_leaf(cur_state): \n            states.append(cur_state) \n\n            counts = self.visit_count[cur_state] \n            total_sqrt = m.sqrt(sum(counts)) \n            probs = self.probs[cur_state] \n            values_avg = self.value_avg[cur_state]\n```", "```py\n if cur_state == state_int: \n                noises = np.random.dirichlet([0.03] * game.GAME_COLS) \n                probs = [0.75 * prob + 0.25 * noise for prob, noise in zip(probs, noises)] \n            score = [ \n                value + self.c_puct*prob*total_sqrt/(1+count) \n                for value, prob, count in zip(values_avg, probs, counts) \n            ]\n```", "```py\n invalid_actions = set(range(game.GAME_COLS)) - \\ \n                              set(game.possible_moves(cur_state)) \n            for invalid in invalid_actions: \n                score[invalid] = -np.inf \n            action = int(np.argmax(score)) \n            actions.append(action)\n```", "```py\n cur_state, won = game.move(cur_state, action, cur_player) \n            if won: \n                value = -1.0 \n            cur_player = 1-cur_player \n            # check for the draw \n            moves_count = len(game.possible_moves(cur_state)) \n            if value is None and moves_count == 0: \n                value = 0.0 \n\n        return value, cur_state, cur_player, states, actions\n```", "```py\n def is_leaf(self, state_int): \n        return state_int not in self.probs \n\n    def search_batch(self, count, batch_size, state_int, player, net, device=\"cpu\"): \n        for _ in range(count): \n            self.search_minibatch(batch_size, state_int, player, net, device)\n```", "```py\n def search_minibatch(self, count, state_int, player, net, device=\"cpu\"): \n        backup_queue = [] \n        expand_states = [] \n        expand_players = [] \n        expand_queue = [] \n        planned = set() \n        for _ in range(count): \n            value, leaf_state, leaf_player, states, actions = \\ \n                self.find_leaf(state_int, player) \n            if value is not None: \n                backup_queue.append((value, states, actions)) \n            else: \n                if leaf_state not in planned: \n                    planned.add(leaf_state) \n                    leaf_state_lists = game.decode_binary(leaf_state) \n                    expand_states.append(leaf_state_lists) \n                    expand_players.append(leaf_player) \n                    expand_queue.append((leaf_state, states, actions))\n```", "```py\n if expand_queue: \n            batch_v = model.state_lists_to_batch(expand_states, expand_players, device) \n            logits_v, values_v = net(batch_v) \n            probs_v = F.softmax(logits_v, dim=1) \n            values = values_v.data.cpu().numpy()[:, 0] \n            probs = probs_v.data.cpu().numpy()\n```", "```py\n for (leaf_state, states, actions), value, prob in \\ \n                    zip(expand_queue, values, probs): \n                self.visit_count[leaf_state] = [0]*game.GAME_COLS \n                self.value[leaf_state] = [0.0]*game.GAME_COLS \n                self.value_avg[leaf_state] = [0.0]*game.GAME_COLS \n                self.probs[leaf_state] = prob \n                backup_queue.append((value, states, actions))\n```", "```py\n for value, states, actions in backup_queue: \n            cur_value = -value \n            for state_int, action in zip(states[::-1], actions[::-1]): \n                self.visit_count[state_int][action] += 1 \n                self.value[state_int][action] += cur_value \n                self.value_avg[state_int][action] = self.value[state_int][action] / \\ \n                                                    self.visit_count[state_int][action] \n                cur_value = -cur_value\n```", "```py\n def get_policy_value(self, state_int, tau=1): \n        counts = self.visit_count[state_int] \n        if tau == 0: \n            probs = [0.0] * game.GAME_COLS \n            probs[np.argmax(counts)] = 1.0 \n        else: \n            counts = [count ** (1.0 / tau) for count in counts] \n            total = sum(counts) \n            probs = [count / total for count in counts] \n        values = self.value_avg[state_int] \n        return probs, values\n```", "```py\ndef _encode_list_state(dest_np, state_list, who_move): \n    assert dest_np.shape == OBS_SHAPE \n    for col_idx, col in enumerate(state_list): \n        for rev_row_idx, cell in enumerate(col): \n            row_idx = game.GAME_ROWS - rev_row_idx - 1 \n            if cell == who_move: \n                dest_np[0, row_idx, col_idx] = 1.0 \n            else: \n                dest_np[1, row_idx, col_idx] = 1.0 \n\ndef state_lists_to_batch(state_lists, who_moves_lists, device=\"cpu\"): \n    assert isinstance(state_lists, list) \n    batch_size = len(state_lists) \n    batch = np.zeros((batch_size,) + OBS_SHAPE, dtype=np.float32) \n    for idx, (state, who_move) in enumerate(zip(state_lists, who_moves_lists)): \n        _encode_list_state(batch[idx], state, who_move) \n    return torch.tensor(batch).to(device)\n```", "```py\ndef play_game(mcts_stores: tt.Optional[mcts.MCTS | tt.List[mcts.MCTS]], \n              replay_buffer: tt.Optional[collections.deque], net1: Net, net2: Net, \n              steps_before_tau_0: int, mcts_searches: int, mcts_batch_size: int, \n              net1_plays_first: tt.Optional[bool] = None, \n              device: torch.device = torch.device(\"cpu\")): \n    if mcts_stores is None: \n        mcts_stores = [mcts.MCTS(), mcts.MCTS()] \n    elif isinstance(mcts_stores, mcts.MCTS): \n        mcts_stores = [mcts_stores, mcts_stores]\n```", "```py\n state = game.INITIAL_STATE \n    nets = [net1, net2] \n    if net1_plays_first is None: \n        cur_player = np.random.choice(2) \n    else: \n        cur_player = 0 if net1_plays_first else 1 \n    step = 0 \n    tau = 1 if steps_before_tau_0 > 0 else 0 \n    game_history = []\n```", "```py\n result = None \n    net1_result = None \n\n    while result is None: \n        mcts_stores[cur_player].search_batch( \n            mcts_searches, mcts_batch_size, state, \n            cur_player, nets[cur_player], device=device) \n        probs, _ = mcts_stores[cur_player].get_policy_value(state, tau=tau) \n        game_history.append((state, cur_player, probs)) \n        action = np.random.choice(game.GAME_COLS, p=probs)\n```", "```py\n if action not in game.possible_moves(state): \n            print(\"Impossible action selected\") \n        state, won = game.move(state, action, cur_player) \n        if won: \n            result = 1 \n            net1_result = 1 if cur_player == 0 else -1 \n            break \n        cur_player = 1-cur_player \n        # check the draw case \n        if len(game.possible_moves(state)) == 0: \n            result = 0 \n            net1_result = 0 \n            break \n        step += 1 \n        if step >= steps_before_tau_0: \n            tau = 0\n```", "```py\n if replay_buffer is not None: \n        for state, cur_player, probs in reversed(game_history): \n            replay_buffer.append((state, cur_player, probs, result)) \n            result = -result \n\n    return net1_result, step\n```", "```py\n./play.py --cuda -r 10 saves/v2/best\\_* > semi-v2.txt\n```", "```py\nsaves/t1/best_088_39300.dat:     w=1027, l=732, d=1 \nsaves/t1/best_025_09900.dat:     w=1024, l=735, d=1 \nsaves/t1/best_022_08200.dat:     w=1023, l=737, d=0 \nsaves/t1/best_021_08100.dat:     w=1017, l=743, d=0 \nsaves/t1/best_009_03400.dat:     w=1010, l=749, d=1 \nsaves/t1/best_014_04700.dat:     w=1003, l=757, d=0 \nsaves/t1/best_008_02700.dat:     w=998, l=760, d=2 \nsaves/t1/best_010_03500.dat:     w=997, l=762, d=1 \nsaves/t1/best_029_11800.dat:     w=991, l=768, d=1 \nsaves/t1/best_007_02300.dat:     w=980, l=779, d=1\n```", "```py\nsaves/t2/best_069_41500.dat:     w=1023, l=757, d=0 \nsaves/t2/best_070_42200.dat:     w=1016, l=764, d=0 \nsaves/t2/best_066_38900.dat:     w=1005, l=775, d=0 \nsaves/t2/best_071_42600.dat:     w=1003, l=777, d=0 \nsaves/t2/best_059_33700.dat:     w=999, l=781, d=0 \nsaves/t2/best_049_27500.dat:     w=990, l=790, d=0 \nsaves/t2/best_068_41300.dat:     w=990, l=789, d=1 \nsaves/t2/best_048_26700.dat:     w=983, l=796, d=1 \nsaves/t2/best_058_32100.dat:     w=982, l=797, d=1 \nsaves/t2/best_076_45200.dat:     w=982, l=795, d=3\n```", "```py\nsaves/t2/best_059_33700.dat:     w=242, l=138, d=0 \nsaves/t2/best_058_32100.dat:     w=223, l=157, d=0 \nsaves/t2/best_071_42600.dat:     w=217, l=163, d=0 \nsaves/t2/best_068_41300.dat:     w=210, l=170, d=0 \nsaves/t2/best_076_45200.dat:     w=208, l=171, d=1 \nsaves/t2/best_048_26700.dat:     w=202, l=178, d=0 \nsaves/t2/best_069_41500.dat:     w=201, l=179, d=0 \nsaves/t2/best_049_27500.dat:     w=199, l=181, d=0 \nsaves/t2/best_070_42200.dat:     w=197, l=183, d=0 \nsaves/t1/best_021_08100.dat:     w=192, l=188, d=0\n```", "```py\n@dataclass \nclass MuZeroParams: \n    actions_count: int = game.GAME_COLS \n    max_moves: int = game.GAME_COLS * game.GAME_ROWS >> 2 + 1 \n    dirichlet_alpha: float = 0.3 \n    discount: float = 1.0 \n    unroll_steps: int = 5 \n\n    pb_c_base: int = 19652 \n    pb_c_init: float = 1.25 \n\n    dev: torch.device = torch.device(\"cpu\")\n```", "```py\nclass MCTSNode: \n    def __init__(self, prior: float, first_plays: bool): \n        self.first_plays: bool = first_plays \n        self.visit_count = 0 \n        self.value_sum = 0.0 \n        self.prior = prior \n        self.children: tt.Dict[Action, MCTSNode] = {} \n        # node is not expanded, so has no hidden state \n        self.h = None \n        # predicted reward \n        self.r = 0.0\n```", "```py\n @property \n    def is_expanded(self) -> bool: \n        return bool(self.children) \n\n    @property \n    def value(self) -> float: \n        return 0 if not self.visit_count else self.value_sum / self.visit_count\n```", "```py\n def select_child(self, params: MuZeroParams, min_max: MinMaxStats) -> \\ \n            tt.Tuple[Action, \"MCTSNode\"]: \n        max_ucb, best_action, best_node = None, None, None \n        for action, node in self.children.items(): \n            ucb = ucb_value(params, self, node, min_max) \n            if max_ucb is None or max_ucb < ucb: \n                max_ucb = ucb \n                best_action = action \n                best_node = node \n        return best_action, best_node\n```", "```py\ndef ucb_value(params: MuZeroParams, parent: MCTSNode, child: MCTSNode, \n              min_max: MinMaxStats) -> float: \n    pb_c = m.log((parent.visit_count + params.pb_c_base + 1) / \n                 params.pb_c_base) + params.pb_c_init \n    pb_c *= m.sqrt(parent.visit_count) / (child.visit_count + 1) \n    prior_score = pb_c * child.prior \n    value_score = 0.0 \n    if child.visit_count > 0: \n        value_score = min_max.normalize(child.value + child.r) \n    return prior_score + value_score\n```", "```py\n def get_act_probs(self, t: float = 1) -> tt.List[float]: \n        child_visits = sum(map(lambda n: n.visit_count, self.children.values())) \n        p = np.array([(child.visit_count / child_visits) ** (1 / t) \n                      for _, child in sorted(self.children.items())]) \n        p /= sum(p) \n        return list(p)\n```", "```py\n def select_action(self, t: float, params: MuZeroParams) -> Action: \n        act_vals = list(sorted(self.children.keys())) \n\n        if not act_vals: \n            res = np.random.choice(params.actions_count) \n        elif t < 0.0001: \n            res, _ = max(self.children.items(), key=lambda p: p[1].visit_count) \n        else: \n            p = self.get_act_probs(t) \n            res = int(np.random.choice(act_vals, p=p)) \n        return res\n```", "```py\nclass ReprModel(nn.Module): \n    def __init__(self, input_shape: tt.Tuple[int, ...]): \n        super(ReprModel, self).__init__() \n        self.conv_in = nn.Sequential( \n            nn.Conv2d(input_shape[0], NUM_FILTERS, kernel_size=3, padding=1), \n            nn.BatchNorm2d(NUM_FILTERS), \n            nn.LeakyReLU() \n        ) \n        # layers with residual \n        self.conv_1 = nn.Sequential( \n            nn.Conv2d(NUM_FILTERS, NUM_FILTERS, kernel_size=3, padding=1), \n            nn.BatchNorm2d(NUM_FILTERS), \n            nn.LeakyReLU() \n        ) \n        self.conv_2 = nn.Sequential( \n            nn.Conv2d(NUM_FILTERS, NUM_FILTERS, kernel_size=3, padding=1), \n            nn.BatchNorm2d(NUM_FILTERS), \n            nn.LeakyReLU() \n        ) \n        self.conv_3 = nn.Sequential( \n            nn.Conv2d(NUM_FILTERS, NUM_FILTERS, kernel_size=3, padding=1), \n            nn.BatchNorm2d(NUM_FILTERS), \n            nn.LeakyReLU() \n        ) \n        self.conv_4 = nn.Sequential( \n            nn.Conv2d(NUM_FILTERS, NUM_FILTERS, kernel_size=3, padding=1), \n            nn.BatchNorm2d(NUM_FILTERS), \n            nn.LeakyReLU() \n        ) \n        self.conv_5 = nn.Sequential( \n            nn.Conv2d(NUM_FILTERS, NUM_FILTERS, kernel_size=3, padding=1), \n            nn.BatchNorm2d(NUM_FILTERS), \n            nn.LeakyReLU(), \n        ) \n        self.conv_out = nn.Sequential( \n            nn.Conv2d(NUM_FILTERS, 16, kernel_size=1), \n            nn.BatchNorm2d(16), \n            nn.LeakyReLU(), \n            nn.Flatten() \n        ) \n\n        body_shape = (NUM_FILTERS,) + input_shape[1:] \n        size = self.conv_out(torch.zeros(1, *body_shape)).size()[-1] \n        self.out = nn.Sequential( \n            nn.Linear(size, 128), \n            nn.ReLU(), \n            nn.Linear(128, HIDDEN_STATE_SIZE), \n        )\n```", "```py\n def forward(self, x): \n        v = self.conv_in(x) \n        v = v + self.conv_1(v) \n        v = v + self.conv_2(v) \n        v = v + self.conv_3(v) \n        v = v + self.conv_4(v) \n        v = v + self.conv_5(v) \n        c_out = self.conv_out(v) \n        out = self.out(c_out) \n        return out\n```", "```py\nclass PredModel(nn.Module): \n    def __init__(self, actions: int): \n        super(PredModel, self).__init__() \n        self.policy = nn.Sequential( \n            nn.Linear(HIDDEN_STATE_SIZE, 128), \n            nn.ReLU(), \n            nn.Linear(128, actions), \n        ) \n\n        self.value = nn.Sequential( \n            nn.Linear(HIDDEN_STATE_SIZE, 128), \n            nn.ReLU(), \n            nn.Linear(128, 1), \n        ) \n\n    def forward(self, x) -> tt.Tuple[torch.Tensor, torch.Tensor]: \n        return self.policy(x), self.value(x).squeeze(1)\n```", "```py\nclass DynamicsModel(nn.Module): \n    def __init__(self, actions: int): \n        super(DynamicsModel, self).__init__() \n        self.reward = nn.Sequential( \n            nn.Linear(HIDDEN_STATE_SIZE + actions, 128), \n            nn.ReLU(), \n            nn.Linear(128, 1), \n        ) \n\n        self.hidden = nn.Sequential( \n            nn.Linear(HIDDEN_STATE_SIZE + actions, 128), \n            nn.ReLU(), \n            nn.Linear(128, 128), \n            nn.ReLU(), \n            nn.Linear(128, HIDDEN_STATE_SIZE), \n        ) \n\n    def forward(self, h: torch.Tensor, a: torch.Tensor) -> \\ \n            tt.Tuple[torch.Tensor, torch.Tensor]: \n        x = torch.hstack((h, a)) \n        return self.reward(x).squeeze(1), self.hidden(x)\n```", "```py\nclass MuZeroModels: \n    def __init__(self, input_shape: tt.Tuple[int, ...], actions: int): \n        self.repr = ReprModel(input_shape) \n        self.pred = PredModel(actions) \n        self.dynamics = DynamicsModel(actions) \n\n    def to(self, dev: torch.device): \n        self.repr.to(dev) \n        self.pred.to(dev) \n        self.dynamics.to(dev)\n```", "```py\n def sync(self, src: \"MuZeroModels\"): \n        self.repr.load_state_dict(src.repr.state_dict()) \n        self.pred.load_state_dict(src.pred.state_dict()) \n        self.dynamics.load_state_dict(src.dynamics.state_dict()) \n\n    def get_state_dict(self) -> tt.Dict[str, dict]: \n        return { \n            \"repr\": self.repr.state_dict(), \n            \"pred\": self.pred.state_dict(), \n            \"dynamics\": self.dynamics.state_dict(), \n        } \n\n    def set_state_dict(self, d: dict): \n        self.repr.load_state_dict(d[â€™reprâ€™]) \n        self.pred.load_state_dict(d[â€™predâ€™]) \n        self.dynamics.load_state_dict(d[â€™dynamicsâ€™])\n```", "```py\ndef make_expanded_root(player_idx: int, game_state_int: int, params: MuZeroParams, \n                       models: MuZeroModels, min_max: MinMaxStats) -> MCTSNode: \n    root = MCTSNode(1.0, player_idx == 0) \n    state_list = game.decode_binary(game_state_int) \n    state_t = state_lists_to_batch([state_list], [player_idx], device=params.dev) \n    h_t = models.repr(state_t) \n    root.h = h_t[0].cpu().numpy()\n```", "```py\n p_t, v_t = models.pred(h_t) \n    # logits to probs \n    p_t.exp_() \n    probs_t = p_t.squeeze(0) / p_t.sum() \n    probs = probs_t.cpu().numpy() \n    # add dirichlet noise \n    noises = np.random.dirichlet([params.dirichlet_alpha] * params.actions_count) \n    probs = probs * 0.75 + noises * 0.25\n```", "```py\n for a, prob in enumerate(probs): \n        root.children[a] = MCTSNode(prob, not root.first_plays) \n    v = v_t.cpu().item() \n    backpropagate([root], v, root.first_plays, params, min_max) \n    return root\n```", "```py\ndef expand_node(parent: MCTSNode, node: MCTSNode, last_action: Action, \n                params: MuZeroParams, models: MuZeroModels) -> float: \n    h_t = torch.as_tensor(parent.h, dtype=torch.float32, device=params.dev) \n    h_t.unsqueeze_(0) \n    p_t, v_t = models.pred(h_t) \n    a_t = torch.zeros(params.actions_count, dtype=torch.float32, device=params.dev) \n    a_t[last_action] = 1.0 \n    a_t.unsqueeze_(0) \n    r_t, h_next_t = models.dynamics(h_t, a_t) \n    node.h = h_next_t[0].cpu().numpy() \n    node.r = float(r_t[0].cpu().item())\n```", "```py\n p_t.squeeze_(0) \n    p_t.exp_() \n    probs_t = p_t / p_t.sum() \n    probs = probs_t.cpu().numpy() \n    for a, prob in enumerate(probs): \n        node.children[a] = MCTSNode(prob, not node.first_plays) \n    return float(v_t.cpu().item())\n```", "```py\ndef backpropagate(search_path: tt.List[MCTSNode], value: float, first_plays: bool, \n                  params: MuZeroParams, min_max: MinMaxStats): \n    for node in reversed(search_path): \n        node.value_sum += value if node.first_plays == first_plays else -value \n        node.visit_count += 1 \n        value = node.r + params.discount * value \n        min_max.update(value)\n```", "```py\n@torch.no_grad() \ndef run_mcts(player_idx: int, root_state_int: int, params: MuZeroParams, \n             models: MuZeroModels, min_max: MinMaxStats, \n             search_rounds: int = 800) -> MCTSNode: \n    root = make_expanded_root(player_idx, root_state_int, params, models, min_max) \n    for _ in range(search_rounds): \n        search_path = [root] \n        parent_node = None \n        last_action = 0 \n        node = root \n        while node.is_expanded: \n            action, new_node = node.select_child(params, min_max) \n            last_action = action \n            parent_node = node \n            node = new_node \n            search_path.append(new_node) \n        value = expand_node(parent_node, node, last_action, params, models) \n        backpropagate(search_path, value, node.first_plays, params, min_max) \n    return root\n```", "```py\n@dataclass \nclass EpisodeStep: \n    state: int \n    player_idx: int \n    action: int \n    reward: int \n\nclass Episode: \n    def __init__(self): \n        self.steps: tt.List[EpisodeStep] = [] \n        self.action_probs: tt.List[tt.List[float]] = [] \n        self.root_values: tt.List[float] = [] \n\n    def __len__(self): \n        return len(self.steps) \n\n    def add_step(self, step: EpisodeStep, node: MCTSNode): \n        self.steps.append(step) \n        self.action_probs.append(node.get_act_probs()) \n        self.root_values.append(node.value)\n```", "```py\n@torch.no_grad() \ndef play_game( \n        player1: MuZeroModels, player2: MuZeroModels, params: MuZeroParams, \n        temperature: float, init_state: tt.Optional[int] = None \n) -> tt.Tuple[int, Episode]: \n    episode = Episode() \n    state = game.INITIAL_STATE if init_state is None else init_state \n    players = [player1, player2] \n    player_idx = 0 \n    reward = 0 \n    min_max = MinMaxStats()\n```", "```py\n while True: \n        possible_actions = game.possible_moves(state) \n        if not possible_actions: \n            break \n\n        root_node = run_mcts(player_idx, state, params, players[player_idx], min_max) \n        action = root_node.select_action(temperature, params) \n\n        # act randomly on wrong move \n        if action not in possible_actions: \n            action = int(np.random.choice(possible_actions))\n```", "```py\n new_state, won = game.move(state, action, player_idx) \n        if won: \n            if player_idx == 0: \n                reward = 1 \n            else: \n                reward = -1 \n        step = EpisodeStep(state, player_idx, action, reward) \n        episode.add_step(step, root_node) \n        if won: \n            break \n        player_idx = (player_idx + 1) % 2 \n        state = new_state \n    return reward, episode\n```", "```py\ndef sample_batch( \n        episode_buffer: tt.Deque[Episode], batch_size: int, params: MuZeroParams, \n) -> tt.Tuple[ \n    torch.Tensor, tt.Tuple[torch.Tensor, ...], tt.Tuple[torch.Tensor, ...], \n    tt.Tuple[torch.Tensor, ...], tt.Tuple[torch.Tensor, ...], \n]: \n    states = [] \n    player_indices = [] \n    actions = [[] for _ in range(params.unroll_steps)] \n    policy_targets = [[] for _ in range(params.unroll_steps)] \n    rewards = [[] for _ in range(params.unroll_steps)] \n    values = [[] for _ in range(params.unroll_steps)]\n```", "```py\n for episode in np.random.choice(episode_buffer, batch_size): \n        assert isinstance(episode, Episode) \n        ofs = np.random.choice(len(episode) - params.unroll_steps) \n        state = game.decode_binary(episode.steps[ofs].state) \n        states.append(state) \n        player_indices.append(episode.steps[ofs].player_idx)\n```", "```py\n for s in range(params.unroll_steps): \n            full_ofs = ofs + s \n            actions[s].append(episode.steps[full_ofs].action) \n            rewards[s].append(episode.steps[full_ofs].reward) \n            policy_targets[s].append(episode.action_probs[full_ofs]) \n\n            value = 0.0 \n            for step in reversed(episode.steps[full_ofs:]): \n                value *= params.discount \n                value += step.reward \n            values[s].append(value)\n```", "```py\n states_t = state_lists_to_batch(states, player_indices, device=params.dev) \n    res_actions = tuple( \n        torch.as_tensor(np.eye(params.actions_count)[a], \n                        dtype=torch.float32, device=params.dev) \n        for a in actions \n    ) \n    res_policies = tuple( \n        torch.as_tensor(p, dtype=torch.float32, device=params.dev) \n        for p in policy_targets \n    ) \n    res_rewards = tuple( \n        torch.as_tensor(r, dtype=torch.float32, device=params.dev) \n        for r in rewards \n    ) \n    res_values = tuple( \n        torch.as_tensor(v, dtype=torch.float32, device=params.dev) \n        for v in values \n    ) \n    return states_t, res_actions, res_policies, res_rewards, res_values\n```", "```py\n states_t, actions, policy_tgt, rewards_tgt, values_tgt = \\ \n                mu.sample_batch(replay_buffer, BATCH_SIZE, params) \n\n            optimizer.zero_grad() \n            h_t = net.repr(states_t) \n            loss_p_full_t = None \n            loss_v_full_t = None \n            loss_r_full_t = None \n            for step in range(params.unroll_steps): \n                policy_t, values_t = net.pred(h_t) \n                loss_p_t = F.cross_entropy(policy_t, policy_tgt[step]) \n                loss_v_t = F.mse_loss(values_t, values_tgt[step]) \n                # dynamic step \n                rewards_t, h_t = net.dynamics(h_t, actions[step]) \n                loss_r_t = F.mse_loss(rewards_t, rewards_tgt[step]) \n                if step == 0: \n                    loss_p_full_t = loss_p_t \n                    loss_v_full_t = loss_v_t \n                    loss_r_full_t = loss_r_t \n                else: \n                    loss_p_full_t += loss_p_t * 0.5 \n                    loss_v_full_t += loss_v_t * 0.5 \n                    loss_r_full_t += loss_r_t * 0.5 \n            loss_full_t = loss_v_full_t + loss_p_full_t + loss_r_full_t \n            loss_full_t.backward() \n            optimizer.step()\n```", "```py\nsaves/mu-t5-6/best_010_00210.dat:        w=339, l=41, d=0 \nsaves/mu-t5-6/best_015_00260.dat:        w=298, l=82, d=0 \nsaves/mu-t5-6/best_155_02510.dat:        w=287, l=93, d=0 \nsaves/mu-t5-6/best_150_02460.dat:        w=273, l=107, d=0 \nsaves/mu-t5-6/best_140_02360.dat:        w=267, l=113, d=0 \nsaves/mu-t5-6/best_145_02410.dat:        w=266, l=114, d=0 \nsaves/mu-t5-6/best_165_02640.dat:        w=253, l=127, d=0 \nsaves/mu-t5-6/best_005_00100.dat:        w=250, l=130, d=0 \nsaves/mu-t5-6/best_160_02560.dat:        w=236, l=144, d=0 \nsaves/mu-t5-6/best_135_02310.dat:        w=220, l=160, d=0\n```"]