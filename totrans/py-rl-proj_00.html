<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Preface</h1>
                </header>
            
            <article>
                
<p>Reinforcement learning is one of the most exciting and rapidly growing <span>fields in machine learning. This is due to the many novel algorithms developed and incredible results published in recent years.</span></p>
<p class="mce-root">In this book, you will learn about the core concepts of RL including Q-learning, policy gradients, Monte Carlo processes, and several deep reinforcement learning algorithms. As you make your way through the book, you'll work on projects with datasets of various modalities including image, text, and video. You will gain experience in several domains, including gaming, image processing, and physical simulations. You'll explore technologies such as TensorFlow and OpenAI Gym to implement deep learning reinforcement learning algorithms that also predict stock prices, generate natural language, and even build other neural networks.</p>
<p>By the end of this book, you will have hands-on experience with eight reinforcement learning projects, each addressing different topics and/or algorithms. We hope these practical exercises will provide you with better intuition and insight about the field of reinforcement learning and how to apply its algorithms to various problems in real life.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Who this book is for</h1>
                </header>
            
            <article>
                
<p><em>Python Reinforcement Learning Projects</em> is for data analysts, data scientists, and machine learning professionals who have a working knowledge of machine learning techniques and are looking to explore emerging fields within machine learning such as reinforcement learning. Individuals who want to work on hands-on implementation projects will also find this book useful.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What this book covers</h1>
                </header>
            
            <article>
                
<p><a href="b33d2a98-0304-460f-a40c-c35b77f83469.xhtml" target="_blank"/><a href="b33d2a98-0304-460f-a40c-c35b77f83469.xhtml" target="_blank">Chapter 1</a>,<span> </span><em><span class="cdp-organizer-chapter-title"><span class="cdp-organize-title-label">Up and Running with Reinforcement Learning</span></span></em><span>, introduces AI, RL, deep learning, the history/applications of the field, and other relevant topics. It will also provide a high-level overview of fundamental deep learning and TensorFlow concepts, especially those relevant to RL.</span></p>
<p><a href="e3ce4354-b114-4e53-91c4-5d24e0731616.xhtml" target="_blank">Chapter 2</a>, <em><span class="cdp-organizer-chapter-title"><span class="cdp-organize-title-label">Balancing Cart Pole</span></span></em><span>, will have you implement your first RL algorithms in Python and TensorFlow to solve the cart pole balancing problem.</span></p>
<p><a href="0cd8e82e-dbf2-42f6-a525-e8689cace21b.xhtml" target="_blank">Chapter 3</a>,<span> </span><em><span class="cdp-organizer-chapter-title"><span class="cdp-organize-title-label">Playing Atari Games</span></span></em><span>, will get you creating your first deep RL algorithm to play ATARI games.</span></p>
<p><a href="0c223fdb-e360-4b7b-9fc3-f09b81011a45.xhtml" target="_blank">Chapter 4</a>, <em>Simulating Control Tasks</em><span>,</span> provides a brief introduction to actor-critic algorithms for continuous control problems. You will learn how to simulate classic control tasks, look at how to implement basic actor-critic algorithms, and understand the state-of-the-art algorithms for control.</p>
<p><a href="34b7cd13-afac-4f0d-b805-c49b75e3b2f8.xhtml" target="_blank">Chapter 5</a>, <em>Building Virtual Worlds in Minecraft</em><span>, takes the advanced concepts covered in previous chapters and applies them to Minecraft, a game more complex than those found on ATARI.</span></p>
<p><a href="857fa187-2524-4682-ae75-ab6b15e00a50.xhtml" target="_blank">Chapter 6</a>, <em>Learning to Play Go</em><span>, has you building a model that can play Go, the popular Asian board game that is considered one of the world's most complicated games.</span></p>
<p><a href="2b04d0f9-9d53-4b80-a670-3aec735556f8.xhtml" target="_blank">Chapter 7</a>,<span> </span><em><span class="cdp-organizer-chapter-title"><span class="cdp-organize-title-label">Creating a Chatbot</span></span></em><span>, will teach you how to apply deep RL in natural language processing. Our reward function will be a future-looking function, and you will learn how to think in terms of probability when creating this function.</span></p>
<p><a href="a1fdd5bd-1147-46e9-8252-71e5f8c2e469.xhtml" target="_blank">Chapter 8</a>, <em>Generating a Deep Learning Image Classifier</em><span>, introduces one of the latest and most exciting advancements in RL: generating deep learning models using RL. We explore the cutting-edge research produced by Google Brain and implement the algorithms introduced.</span></p>
<p><a href="120121e0-1f17-49d8-852e-d98334fddc98.xhtml" target="_blank"/><a href="120121e0-1f17-49d8-852e-d98334fddc98.xhtml" target="_blank">Chapter 9</a>, <em>Predicting Future Stock Prices</em><span>, discusses building an agent that can predict stock prices.</span></p>
<p><span><a href="4f704a58-467d-4327-b88e-01ba905e7197.xhtml" target="_blank">Chapter 10</a>, <em>Looking Ahead</em></span><span>, concludes the book by discussing some of the real-world applications of reinforcement learning and introducing potential areas of future academic work.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">To get the most out of this book</h1>
                </header>
            
            <article>
                
<p><span>The examples covered in this book can be run on Windows, Ubuntu, or macOS. All the installation instructions are covered. A basic knowledge of Python and machine learning is required. It's preferred that you have GPU hardware, but it's not necessary.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Download the example code files</h1>
                </header>
            
            <article>
                
<p>You can download the example code files for this book from your account at <a href="http://www.packt.com" target="_blank">www.packt.com</a>. If you purchased this book elsewhere, you can visit <a href="http://www.packt.com/support" target="_blank">www.packt.com/support</a> and register to have the files emailed directly to you.</p>
<p>You can download the code files by following these steps:</p>
<ol>
<li>Log in or register at <a href="http://www.packt.com" target="_blank">www.packt.com</a>.</li>
<li>Select the <span class="packt_screen">SUPPORT</span> tab.</li>
<li>Click on <span class="packt_screen">Code Downloads &amp; Errata</span>.</li>
<li>Enter the name of the book in the <span class="packt_screen">Search</span> box and follow the onscreen instructions.</li>
</ol>
<p>Once the file is downloaded, please make sure that you unzip or extract the folder using the latest version of:</p>
<ul>
<li>WinRAR/7-Zip for Windows</li>
<li>Zipeg/iZip/UnRarX for Mac</li>
<li>7-Zip/PeaZip for Linux</li>
</ul>
<p><span>The code bundle for the book is also hosted on GitHub at</span><span> </span><a href="https://github.com/PacktPublishing/Python-Reinforcement-Learning-Projects">https://github.com/PacktPublishing/Python-Reinforcement-Learning-Projects</a><span>. </span><span>In case there's an update to the code, it will be updated on the existing GitHub repository.</span></p>
<p><span>We also have other code bundles from our rich catalog of books and videos available at</span><span> </span><strong><span class="Object"><a href="https://github.com/PacktPublishing/" target="_blank">https://github.com/PacktPublishing/</a></span></strong><span>. Check them out!</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Conventions used</h1>
                </header>
            
            <article>
                
<p>There are a number of text conventions used throughout this book.</p>
<p><kbd>CodeInText</kbd>: <span>Indicates c</span>ode words in text, database table names, folder names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles. <span>Here is an example:</span> "<span>The </span><kbd>gym-minecraft</kbd><span> package has the same interface as other Gym environments</span>."</p>
<p>A block of code is set as follows:</p>
<pre>import logging<br/>import minecraft_py<br/>logging.basicConfig(level=logging.DEBUG)</pre>
<p>Any command-line input or output is written as follows:</p>
<pre><strong>python3 -m pip install gym</strong><br/><strong>python3 -m pip install pygame</strong></pre>
<p><strong>Bold</strong>: Indicates a new term, an important word, or w<span>ords that you see on screen. For example, words in menus or dialog boxes appear in the text like this. Here is an example: "Select <span class="packt_screen">System info</span> from the <span class="packt_screen">Administration</span> panel.</span><span>"</span></p>
<div class="packt_infobox">Warnings or important notes appear like this.</div>
<div class="packt_tip">Tips and tricks appear like this.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Get in touch</h1>
                </header>
            
            <article>
                
<p>Feedback from our readers is always welcome.</p>
<p class="mce-root"><strong>General feedback</strong>: If you have questions about any aspect of this book, <span>mention the book title in the subject of your message and</span> email us at <kbd><span>customercare@packtpub.com</span></kbd>.</p>
<p><strong>Errata</strong>: Although we have taken every care to ensure the accuracy of our content, mistakes do happen. If you have found a mistake in this book, we would be grateful if you would report this to us. Please visit <a href="http://www.packt.com/submit-errata" target="_blank">www.packt.com/submit-errata</a>, selecting your book, clicking on the Errata Submission Form link, and entering the details.</p>
<p><strong>Piracy</strong>: If you come across any illegal copies of our works in any form on the Internet, we would be grateful if you would provide us with the location address or website name. Please contact us at <kbd>copyright@packt.com</kbd> with a link to the material.</p>
<p class="mce-root"><strong>If you are interested in becoming an author</strong>: If there is a topic that you have expertise in and you are interested in either writing or contributing to a book, please visit <a href="http://authors.packtpub.com/" target="_blank">authors.packtpub.com</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reviews</h1>
                </header>
            
            <article>
                
<p>Please leave a review. Once you have read and used this book, why not leave a review on the site that you purchased it from? Potential readers can then see and use your unbiased opinion to make purchase decisions, we at Packt can understand what you think about our products, and our authors can see your feedback on their book. Thank you!</p>
<p>For more information about Packt, please visit <a href="http://www.packt.com/" target="_blank">packt.com</a>.<a href="https://www.packtpub.com/" target="_blank"/></p>


            </article>

            
        </section>
    </body></html>