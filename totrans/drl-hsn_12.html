<html><head></head><body>
<div id="sbo-rt-content"><section class="level2 chapterHead" id="chapter-12-actor-critic-method-a2c-and-a3c">
<h1 class="chapterNumber">12</h1>
<h1 class="chapterTitle" id="sigil_toc_id_415">
<span id="x1-20300012"/>Actor-Critic Method: A2C and A3C
    </h1>
<p>In <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="ch015.xhtml#x1-18200011"><span class="cmti-10x-x-109">11</span></a>, we started to investigate a policy-based alternative to the familiar value-based methods family. In particular, we focused on the method called <span class="cmbx-10x-x-109">REINFORCE </span>and its modification, which uses discounted reward to obtain the gradient of the policy (which gives us the direction in which to improve the policy). Both methods worked well for a small CartPole problem, but for a more complicated Pong environment, we got no convergence.</p>
<p>Here, we will discuss another extension to the vanilla policy gradient method, which magically improves the stability and convergence speed of that method. Despite the modification being only minor, the new method has its own name, <span class="cmbx-10x-x-109">actor-critic</span>, and it‚Äôs one of the most powerful methods in deep <span class="cmbx-10x-x-109">reinforcement</span> <span class="cmbx-10x-x-109">learning </span>(<span class="cmbx-10x-x-109">RL</span>).</p>
<p>In this chapter, we will:</p>
<ul>
<li>
<p>Explore how the baseline impacts statistics and the convergence of gradients</p>
</li>
<li>
<p>Cover an extension of the baseline idea</p>
</li>
<li>
<p>Implement the <span class="cmbx-10x-x-109">advantage actor-critic </span>(<span class="cmbx-10x-x-109">A2C</span>) method and check it on the Pong environment</p>
</li>
<li>
<p>Add asynchronous execution to the A2C method using two different ways: data parallelism and gradient parallelism</p>
</li>
</ul>
<section class="level3 sectionHead" id="variance-reduction">
<h1 class="heading-1" id="sigil_toc_id_182"> <span id="x1-20400012.1"/>Variance reduction</h1>
<p>In the previous <span id="dx1-204001"/>chapter, I briefly mentioned that one of the ways to improve the stability of policy gradient methods is to reduce the variance of the gradient. Now let‚Äôs try to understand why this is important and what it means to reduce the variance. In statistics, variance is the expected square deviation of a random variable from the expected value of that variable:</p>
<div class="math-display">
<img alt="œÄ (a |s) = P[At = a|St = s] " class="math-display" height="26" src="../Images/eq44.png" width="232"/>
</div>
<p>Variance shows us how far values are dispersed from the mean. When variance is high, the random variable can take values that deviate widely from the mean. In the following plot, there is a normal (Gaussian) distribution with the same value for the mean, <span class="cmmi-10x-x-109">Œº </span>= 10, but with different values for the variance.</p>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_12_01.png" width="500"/> <span id="x1-204002r1"/></p>
<span class="id">Figure¬†12.1: The effect of variance on Gaussian distribution </span>
</div>
<p>Now let‚Äôs return to policy gradients. It was stated in the previous chapter that the idea is to increase the probability of good actions and decrease the chance of bad ones. In math notation, our policy gradient was written as <span class="cmsy-10x-x-109">‚àá</span><span class="cmmi-10x-x-109">J </span><span class="cmsy-10x-x-109">‚âà</span><span class="msbm-10x-x-109">ùîº</span>[<span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s,a</span>)<span class="cmsy-10x-x-109">‚àá</span>log <span class="cmmi-10x-x-109">œÄ</span>(<span class="cmmi-10x-x-109">a</span><span class="cmsy-10x-x-109">|</span><span class="cmmi-10x-x-109">s</span>)]. The scaling factor <span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s,a</span>) specifies how much we want to increase or decrease the probability of the action taken in the particular state. In the <span class="cmbx-10x-x-109">REINFORCE </span>method, we used the discounted total reward as the scaling of the gradient. In an attempt to increase <span class="cmbx-10x-x-109">REINFORCE </span>stability, we subtracted the mean reward from the gradient scale.</p>
<p>To understand why this helped, let‚Äôs consider the very simple scenario of an optimization step on which we have three actions with different total discounted rewards: <span class="cmmi-10x-x-109">Q</span><sub><span class="cmr-8">1</span></sub>, <span class="cmmi-10x-x-109">Q</span><sub><span class="cmr-8">2</span></sub>, and <span class="cmmi-10x-x-109">Q</span><sub><span class="cmr-8">3</span></sub>. Now let‚Äôs check what will happen with policy gradients with regard to the relative values of those <span class="cmmi-10x-x-109">Q</span><sub><span class="cmmi-8">s</span></sub>.</p>
<p>As the first example, let both <span class="cmmi-10x-x-109">Q</span><sub><span class="cmr-8">1</span></sub> and <span class="cmmi-10x-x-109">Q</span><sub><span class="cmr-8">2</span></sub> be equal to some small positive number and <span class="cmmi-10x-x-109">Q</span><sub><span class="cmr-8">3</span></sub> be a large negative number. So, actions at the first and second steps led to some small reward, but the third step was not very successful. The resulting <span class="cmbx-10x-x-109">combined </span>gradient for all three steps will try to push our policy far from the action at step three and slightly toward the actions taken at steps one and two, which is a totally reasonable thing to do.</p>
<p>Now let‚Äôs imagine<span id="dx1-204003"/> that our reward is always positive and only the value is different. This corresponds to adding some constant to each of the rewards from the previous example: <span class="cmmi-10x-x-109">Q</span><sub><span class="cmr-8">1</span></sub>, <span class="cmmi-10x-x-109">Q</span><sub><span class="cmr-8">2</span></sub>, and <span class="cmmi-10x-x-109">Q</span><sub><span class="cmr-8">3</span></sub>. In this case, <span class="cmmi-10x-x-109">Q</span><sub><span class="cmr-8">1</span></sub> and <span class="cmmi-10x-x-109">Q</span><sub><span class="cmr-8">2</span></sub> will become large positive numbers and <span class="cmmi-10x-x-109">Q</span><sub><span class="cmr-8">3</span></sub> will have a small positive value. However, our policy update will become different! We will try hard to push our policy toward actions at the first and second steps, and slightly push it toward an action at step three. So, strictly speaking, we are no longer trying to avoid the action taken for step three, despite the fact that the relative rewards are the same.</p>
<p>This dependency of our policy update on the constant added to the reward can slow down our training significantly, as we may require many more samples to <span class="cmti-10x-x-109">average out </span>the effect of such a shift in the policy gradient. Even worse, as our total discounted reward changes over time, with the agent learning how to act better and better, our policy gradient variance can also change. For example, in the Atari Pong environment, the average reward in the beginning is <span class="cmsy-10x-x-109">‚àí</span>21<span class="cmmi-10x-x-109">‚Ä¶</span> <span class="cmsy-10x-x-109">‚àí </span>20, so all the actions look almost equally bad.</p>
<p>To overcome this in the previous chapter, we subtracted the mean total reward from the Q-value and called this mean the <span class="cmbx-10x-x-109">baseline</span>. This trick normalized our policy gradient: in the case of the average reward being <span class="cmsy-10x-x-109">‚àí</span>21, getting a reward of <span class="cmsy-10x-x-109">‚àí</span>20 looks like a win for the agent and it pushes its policy toward the taken actions.</p>
</section>
<section class="level3 sectionHead" id="cartpole-variance">
<h1 class="heading-1" id="sigil_toc_id_183"> <span id="x1-20500012.2"/>CartPole variance</h1>
<p>To check this <span id="dx1-205001"/>theoretical conclusion in practice, let‚Äôs plot our policy gradient variance during the training for both the baseline version and the version without the baseline. The complete example is in <span class="cmtt-10x-x-109">Chapter12/01</span><span class="cmtt-10x-x-109">_cartpole</span><span class="cmtt-10x-x-109">_pg.py</span>, and most of the code is the same as in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="ch015.xhtml#x1-18200011"><span class="cmti-10x-x-109">11</span></a>. The differences in this version are the following:</p>
<ul>
<li>
<p>It now accepts the command-line option <span class="cmtt-10x-x-109">--baseline</span>, which enables the mean subtraction from the reward. By default, no baseline is used.</p>
</li>
<li>
<p>On every training loop, we gather the gradients from the policy loss and use this data to calculate the variance.</p>
</li>
</ul>
<p>To gather only the gradients from the policy loss and exclude the gradients from the entropy bonus added for exploration, we need to calculate the gradients in two stages. Luckily, PyTorch allows this to be done easily. In the following code, only the relevant part of the training loop is included to illustrate the idea:</p>
<div class="tcolorbox" id="tcolobox-249">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-305"><code>        optimizer.zero_grad() 
        logits_v = net(states_v) 
        log_prob_v = F.log_softmax(logits_v, dim=1) 
        log_p_a_v = log_prob_v[range(BATCH_SIZE), batch_actions_t] 
        log_prob_actions_v = batch_scale_v * log_p_a_v 
        loss_policy_v = -log_prob_actions_v.mean()</code></pre>
</div>
</div>
<p>We calculate <span id="dx1-205008"/>the policy loss as before, by calculating the log from the probabilities of taken actions and multiplying it by policy scales (which are the total discounted reward if we are not using the baseline or the total reward minus the baseline).</p>
<p>In the next step, we ask PyTorch to backpropagate the policy loss, calculating the gradients and keeping them in our model‚Äôs buffers:</p>
<div class="tcolorbox" id="tcolobox-250">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-306"><code>        loss_policy_v.backward(retain_graph=True)</code></pre>
</div>
</div>
<p>As we have previously performed <span class="cmtt-10x-x-109">optimizer.zero</span><span class="cmtt-10x-x-109">_grad()</span>, those buffers will contain only the gradients from the policy loss. One tricky thing here is the <span class="cmtt-10x-x-109">retain</span><span class="cmtt-10x-x-109">_graph=True </span>option when we call <span class="cmtt-10x-x-109">backward()</span>. It instructs PyTorch to keep the graph structure of the variables. Normally, this is destroyed by the <span class="cmtt-10x-x-109">backward() </span>call, but in our case, this is not what we want. In general, retaining the graph could be useful when we need to backpropagate the loss multiple times before the call to the optimizer, although this is not a very common situation.</p>
<p>Then, we iterate all parameters from our model (every parameter of our model is a tensor with gradients) and extract their <span class="cmtt-10x-x-109">grad </span>field in a flattened NumPy array:</p>
<div class="tcolorbox" id="tcolobox-251">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-307"><code>        grads = np.concatenate([p.grad.data.numpy().flatten() 
                                for p in net.parameters() 
                                if p.grad is not None])</code></pre>
</div>
</div>
<p>This gives us one long array with all gradients from our model‚Äôs variables. However, our parameter update should take into account not only the policy gradient but also the gradient provided by our entropy bonus. To achieve this, we calculate the entropy loss and call <span class="cmtt-10x-x-109">backward() </span>again. To be able to do this the second time, we need to pass <span class="cmtt-10x-x-109">retain</span><span class="cmtt-10x-x-109">_graph=True</span>.</p>
<p>On the second <span class="cmtt-10x-x-109">backward() </span>call, PyTorch will backpropagate our entropy loss and add the gradients to the internal gradients‚Äô buffers. So, what we now need to do is just ask our optimizer to perform the optimization step using those combined gradients:</p>
<div class="tcolorbox" id="tcolobox-252">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-308"><code>        prob_v = F.softmax(logits_v, dim=1) 
        entropy_v = -(prob_v * log_prob_v).sum(dim=1).mean() 
        entropy_loss_v = -ENTROPY_BETA * entropy_v 
        entropy_loss_v.backward() 
        optimizer.step()</code></pre>
</div>
</div>
<p>Later, the <span id="dx1-205018"/>only thing we need to do is write statistics that we are interested in into TensorBoard:</p>
<div class="tcolorbox" id="tcolobox-253">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-309"><code>        g_l2 = np.sqrt(np.mean(np.square(grads))) 
        g_max = np.max(np.abs(grads)) 
        writer.add_scalar("grad_l2", g_l2, step_idx) 
        writer.add_scalar("grad_max", g_max, step_idx) 
        writer.add_scalar("grad_var", np.var(grads), step_idx)</code></pre>
</div>
</div>
<p>By running this example twice, once with the <span class="cmtt-10x-x-109">--baseline </span>command-line option and once without it, we get a plot of variance of our policy gradient. The following charts show the smoothed reward (average for last 100 episodes) and variance (smoothed with window 20):</p>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_12_02.png" width="600"/> <span id="x1-205024r2"/></p>
<span class="id">Figure¬†12.2: Smoothed reward (left) and variance (right) </span>
</div>
<p>These next two charts show the gradients‚Äô magnitude (L2) and maximum value. All values are smoothed with window 20:</p>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_12_03.png" width="600"/> <span id="x1-205025r3"/></p>
<span class="id">Figure¬†12.3: Gradients‚Äô L2 norm (left) and maximum value (right) </span>
</div>
<p>As you can<span id="dx1-205026"/> see, variance for the version with the baseline is two to three orders of magnitude lower than the version without one, which helps the system to converge faster.</p>
</section>
<section class="level3 sectionHead" id="advantage-actor-critic-a2c">
<h1 class="heading-1" id="sigil_toc_id_184"> <span id="x1-20600012.3"/>Advantage actor-critic (A2C)</h1>
<p>The next step in <span id="dx1-206001"/>reducing the variance is making our baseline state-dependent (which is a good idea, as different states could have very different baselines). Indeed, to decide on the suitability of a particular action in some state, we use the discounted total reward of the action. However, the total reward itself could be represented as a <span class="cmti-10x-x-109">value </span>of the state plus the <span class="cmti-10x-x-109">advantage </span>of the action: <span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s,a</span>) = <span class="cmmi-10x-x-109">V </span>(<span class="cmmi-10x-x-109">s</span>) + <span class="cmmi-10x-x-109">A</span>(<span class="cmmi-10x-x-109">s,a</span>). You saw this in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="ch012.xhtml#x1-1240008"><span class="cmti-10x-x-109">8</span></a>, when we discussed DQN modifications, particularly dueling DQN.</p>
<p>So, why can‚Äôt we use <span class="cmmi-10x-x-109">V </span>(<span class="cmmi-10x-x-109">s</span>) as a baseline? In that case, the scale of our gradient will be just advantage, <span class="cmmi-10x-x-109">A</span>(<span class="cmmi-10x-x-109">s,a</span>), showing how this taken action is better in respect to the average state‚Äôs value. In fact, we can do this, and it is a very good idea for improving the policy gradient method. The only problem here is that we don‚Äôt know the value, <span class="cmmi-10x-x-109">V </span>(<span class="cmmi-10x-x-109">s</span>), of the state that we need to subtract from the discounted total reward, <span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s,a</span>). To solve this, let‚Äôs use <span class="cmti-10x-x-109">another neural network</span>, which will approximate <span class="cmmi-10x-x-109">V </span>(<span class="cmmi-10x-x-109">s</span>) for every observation. To train it, we can exploit the same training procedure we used in DQN methods: we will carry out the Bellman step and then minimize the mean square error to improve <span class="cmmi-10x-x-109">V </span>(<span class="cmmi-10x-x-109">s</span>) approximation.</p>
<p>When we know the value for any state (or at least have some approximation of it), we can use it to calculate the policy gradient and update our policy network to increase probabilities for actions with good advantage values and decrease the chance of actions with bad advantage values. The policy network (which returns a probability distribution of actions) is called the <span class="cmti-10x-x-109">actor</span>, as it tells us what to do. Another network is called <span class="cmti-10x-x-109">critic</span>, as it allows us to understand how good our actions were by returning <span class="cmmi-10x-x-109">V </span>(<span class="cmmi-10x-x-109">s</span>). This improvement is known under a separate name, the <span class="cmbx-10x-x-109">advantage actor-critic</span> <span class="cmbx-10x-x-109">method</span>, which is often abbreviated to A2C. <span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109">¬†</span><a href="#x1-206002r4"><span class="cmti-10x-x-109">12.4</span></a> is an illustration of its architecture:</p>
<div class="minipage">
<p><img alt="PVoalliuceynneett OœÄVbs((((eacrasrci|s)vtt)aoictr))ions " height="300" src="../Images/B22150_12_04.png" width="500"/> <span id="x1-206002r4"/></p>
<span class="id">Figure¬†12.4: The A2C architecture </span>
</div>
<p>In practice, the <span id="dx1-206003"/>policy and value networks partially overlap, mostly due to efficiency and convergence considerations. In this case, the policy and value are implemented as different heads of the network, taking the output from the common body and transforming it into the probability distribution and a single number representing the value of the state.</p>
<p>This helps both networks to share low-level features (such as convolution filters in the Atari agent), but combine them in a different way. The following figure shows this architecture:</p>
<div class="minipage">
<p><img alt="CPVooamllmiuocenynneetntet OœÄVbs((((e(acrasrbci|s)vott)adoictyr))io)ns " height="300" src="../Images/B22150_12_05.png" width="500"/> <span id="x1-206004r5"/></p>
<span class="id">Figure¬†12.5: The A2C architecture with a shared network body </span>
</div>
<p>From a training point of view, we complete these steps:</p>
<ol>
<li>
<div id="x1-206006x1">
<p>Initialize network parameters, <span class="cmmi-10x-x-109">ùúÉ</span>, with random values.</p>
</div>
</li>
<li>
<div id="x1-206008x2">
<p>Play <span class="cmmi-10x-x-109">N </span>steps in the environment, using the current policy, <span class="cmmi-10x-x-109">œÄ</span><sub><span class="cmmi-8">ùúÉ</span></sub>, and saving the state, <span class="cmmi-10x-x-109">s</span><sub><span class="cmmi-8">t</span></sub>, action, <span class="cmmi-10x-x-109">a</span><sub><span class="cmmi-8">t</span></sub>, and reward, <span class="cmmi-10x-x-109">r</span><sub><span class="cmmi-8">t</span></sub>.</p>
</div>
</li>
<li>
<div id="x1-206010x3">
<p>Set <span class="cmmi-10x-x-109">R </span><span class="cmsy-10x-x-109">‚Üê </span>0 if the end of the episode is reached or <span class="cmmi-10x-x-109">V</span> <sub><span class="cmmi-8">ùúÉ</span></sub>(<span class="cmmi-10x-x-109">s</span><sub><span class="cmmi-8">t</span></sub>).</p>
</div>
</li>
<li>
<div id="x1-206012x4">
<p>For <span class="cmmi-10x-x-109">i </span>= <span class="cmmi-10x-x-109">t </span><span class="cmsy-10x-x-109">‚àí </span>1<span class="cmmi-10x-x-109">‚Ä¶</span><span class="cmmi-10x-x-109">t</span><sub><span class="cmmi-8">start</span></sub> (note that steps are processed backward):</p>
<ul>
<li>
<p><span class="cmmi-10x-x-109">R </span><span class="cmsy-10x-x-109">‚Üê</span><span class="cmmi-10x-x-109">r</span><sub><span class="cmmi-8">i</span></sub> + <span class="cmmi-10x-x-109">Œ≥R</span></p>
</li>
<li>
<p>Accumulate the policy gradients:</p>
<div class="math-display">
<img alt="œÄ (a |s) = P[At = a|St = s] " class="math-display" height="23" src="../Images/eq45.png" width="406"/>
</div>
</li>
<li>
<p>Accumulate the value gradients:</p>
<div class="math-display">
<img alt="œÄ (a |s) = P[At = a|St = s] " class="math-display" height="55" src="../Images/eq46.png" width="286"/>
</div>
</li>
</ul>
</div>
</li>
<li>
<div id="x1-206014x5">
<p>Update the network parameters using the accumulated gradients, moving in the direction of the policy gradients, <span class="cmmi-10x-x-109">‚àÇùúÉ</span><sub><span class="cmmi-8">œÄ</span></sub>, and in the opposite direction of the value gradients, <span class="cmmi-10x-x-109">‚àÇùúÉ</span><sub><span class="cmmi-8">v</span></sub>.</p>
</div>
</li>
<li>
<div id="x1-206016x6">
<p>Repeat from step 2 until convergence is reached.</p>
</div>
</li>
</ol>
<p>This <span id="dx1-206017"/>algorithm is just an outline and similar to those that are usually printed in research papers. In practice, several extensions to improve the stability of the method may be used:</p>
<ul>
<li>
<p>An entropy bonus is usually added to improve exploration. It‚Äôs typically written as an entropy value added to the loss function:</p>
<div class="math-display">
<img alt="œÄ (a |s) = P[At = a|St = s] " class="math-display" height="49" src="../Images/eq47.png" width="270"/>
</div>
<p>This function has a minimum when the probability distribution is uniform, so by adding it to the loss function, we push our agent away from being too certain about its actions. The value of <span class="cmmi-10x-x-109">Œ≤ </span>is a hyperparameter scaling the entropy bonus and prioritizing the exploration during the training. Normally, it is constant or linearly decreased during the training.</p>
</li>
<li>
<p>Gradient accumulation is usually implemented as a loss function combining all three components: policy loss, value loss, and entropy loss. You should be careful with signs of these losses, as policy gradients show you the direction of policy improvement, but both the value and entropy losses should be minimized.</p>
</li>
<li>
<p>To improve stability, it‚Äôs worth using several environments, providing you with observations concurrently (when you have multiple environments, your training batch will be created from their observations). We will look at several ways of doing this later in this chapter when we discuss the A3C method.</p>
</li>
</ul>
<p>The version of the preceding method that uses several environments running in parallel is called advantage asynchronous actor-critic, which is also known as A3C. The A3C method will be discussed later, but for now, let‚Äôs implement A2C.</p>
<section class="level4 subsectionHead" id="a2c-on-pong">
<h2 class="heading-2" id="sigil_toc_id_185"> <span id="x1-20700012.3.1"/>A2C on Pong</h2>
<p>In the previous <span id="dx1-207001"/>chapter, you saw a (not very successful) attempt to solve our favorite Pong environment with policy gradient methods. Let‚Äôs try it again with the actor-critic method at hand. The full source code is available in <span class="cmtt-10x-x-109">Chapter12/02</span><span class="cmtt-10x-x-109">_pong</span><span class="cmtt-10x-x-109">_a2c.py</span>.</p>
<p>We start, as usual, by defining hyperparameters (imports are omitted):</p>
<div class="tcolorbox" id="tcolobox-254">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-310"><code>GAMMA = 0.99 
LEARNING_RATE = 0.001 
ENTROPY_BETA = 0.01 
BATCH_SIZE = 128 
NUM_ENVS = 50 
 
REWARD_STEPS = 4 
CLIP_GRAD = 0.1</code></pre>
</div>
</div>
<p>These values are not tuned, which is left as an exercise for the reader. We have one new value here: <span class="cmtt-10x-x-109">CLIP</span><span class="cmtt-10x-x-109">_GRAD</span>. This hyperparameter specifies the threshold for gradient clipping, which basically prevents our gradients from becoming too large at the optimization stage and pushing our policy too far. Clipping is implemented using the PyTorch functionality, but the idea is very simple: if the L2 norm of the gradient is larger than this hyperparameter, then the gradient vector is clipped to this value.</p>
<p>The <span class="cmtt-10x-x-109">REWARD</span><span class="cmtt-10x-x-109">_STEPS </span>hyperparameter determines how many steps ahead we will take to approximate the total discounted reward for every action.</p>
<p>In the policy gradient methods, we used about 10 steps, but in A2C, we will use our value approximation to get a state value for further steps, so it will be fine to decrease the number of steps. The following is our network architecture:</p>
<div class="tcolorbox" id="tcolobox-255">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-311"><code>class AtariA2C(nn.Module): 
    def __init__(self, input_shape: tt.Tuple[int, ...], n_actions: int): 
        super(AtariA2C, self).__init__() 
 
        self.conv = nn.Sequential( 
            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4), 
            nn.ReLU(), 
            nn.Conv2d(32, 64, kernel_size=4, stride=2), 
            nn.ReLU(), 
            nn.Conv2d(64, 64, kernel_size=3, stride=1), 
            nn.ReLU(), 
            nn.Flatten(), 
        ) 
 
        size = self.conv(torch.zeros(1, *input_shape)).size()[-1] 
        self.policy = nn.Sequential( 
            nn.Linear(size, 512), 
            nn.ReLU(), 
            nn.Linear(512, n_actions) 
        ) 
        self.value = nn.Sequential( 
            nn.Linear(size, 512), 
            nn.ReLU(), 
            nn.Linear(512, 1) 
        )</code></pre>
</div>
</div>
<p>It has a <span id="dx1-207035"/>shared convolution body and two heads: the first returns the policy with the probability distribution over our actions and the second head returns one single number, which will approximate the state‚Äôs value. It might look similar to our dueling DQN architecture from <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="ch012.xhtml#x1-1240008"><span class="cmti-10x-x-109">8</span></a>, but our training procedure is different.</p>
<p>The forward pass through the network returns a tuple of two tensors ‚Äì policy and value:</p>
<div class="tcolorbox" id="tcolobox-256">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-312"><code>    def forward(self, x: torch.ByteTensor) -&gt; tt.Tuple[torch.Tensor, torch.Tensor]: 
        xx = x / 255 
        conv_out = self.conv(xx) 
        return self.policy(conv_out), self.value(conv_out)</code></pre>
</div>
</div>
<p>Now we have to discuss a large and important function, which takes the batch of environment transitions and returns three tensors: the batch of states, batch of actions taken, and batch of Q-values calculated using the formula <span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s,a</span>) = <span class="cmex-10x-x-109">‚àë</span> <sub><span class="cmmi-8">i</span><span class="cmr-8">=0</span></sub><sup><span class="cmmi-8">N</span><span class="cmsy-8">‚àí</span><span class="cmr-8">1</span></sup><span class="cmmi-10x-x-109">Œ≥</span><sup><span class="cmmi-8">i</span></sup><span class="cmmi-10x-x-109">r</span><sub><span class="cmmi-8">i</span></sub> + <span class="cmmi-10x-x-109">Œ≥</span><sup><span class="cmmi-8">N</span></sup><span class="cmmi-10x-x-109">V </span>(<span class="cmmi-10x-x-109">s</span><sub><span class="cmmi-8">N</span></sub>). This Q-value will be used in two places: to calculate <span class="cmbx-10x-x-109">mean squared error</span> (<span class="cmbx-10x-x-109">MSE</span>) loss to improve the value approximation in the same way as DQN, and to calculate the advantage of the action.</p>
<div class="tcolorbox" id="tcolobox-257">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-313"><code>def unpack_batch(batch: tt.List[ExperienceFirstLast], net: AtariA2C, 
                 device: torch.device, gamma: float, reward_steps: int): 
    states = [] 
    actions = [] 
    rewards = [] 
    not_done_idx = [] 
    last_states = [] 
    for idx, exp in enumerate(batch): 
        states.append(np.asarray(exp.state)) 
        actions.append(int(exp.action)) 
        rewards.append(exp.reward) 
        if exp.last_state is not None: 
            not_done_idx.append(idx) 
            last_states.append(np.asarray(exp.last_state))</code></pre>
</div>
</div>
<p>In the <span id="dx1-207054"/>beginning, we just walk through our batch of transitions and copy their fields into the lists. Note that the reward value already contains the discounted reward for <span class="cmtt-10x-x-109">REWARD</span><span class="cmtt-10x-x-109">_STEPS</span>, as we use the <span class="cmtt-10x-x-109">ptan.ExperienceSourceFirstLast </span>class. We also need to handle episode-ending situations and remember indices of batch entries for non-terminal episodes.</p>
<p>In the following code, we convert the gathered state and actions into a PyTorch tensor and copy them into the <span class="cmbx-10x-x-109">graphics processing unit </span>(<span class="cmbx-10x-x-109">GPU</span>) if needed:</p>
<div class="tcolorbox" id="tcolobox-258">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-314"><code>    states_t = torch.FloatTensor(np.asarray(states)).to(device) 
    actions_t = torch.LongTensor(actions).to(device)</code></pre>
</div>
</div>
<p>Here, the extra call to <span class="cmtt-10x-x-109">np.asarray() </span>might look redundant, but without it, the performance of tensor creation degrades 5-10x. This is known as <a href="https://github.com/pytorch/pytorch/issues/13918">issue #13918</a> in PyTorch, and at the time of writing, it hasn‚Äôt been solved, so one solution is to pass a single NumPy array instead of a list of arrays.</p>
<p>The rest of the function calculates Q-values, taking into account the terminal episodes:</p>
<div class="tcolorbox" id="tcolobox-259">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-315"><code>    rewards_np = np.array(rewards, dtype=np.float32) 
    if not_done_idx: 
        last_states_t = torch.FloatTensor( 
            np.asarray(last_states)).to(device) 
        last_vals_t = net(last_states_t)[1] 
        last_vals_np = last_vals_v.data.cpu().numpy()[:, 0] 
        last_vals_np *= gamma ** reward_steps 
        rewards_np[not_done_idx] += last_vals_np</code></pre>
</div>
</div>
<p>The preceding code prepares the variable with the last state in our transition chain and queries our network for <span class="cmmi-10x-x-109">V </span>(<span class="cmmi-10x-x-109">s</span>) approximation. Then, this value is multiplied by the discount factor and added to the immediate rewards.</p>
<p>At the end of the function, we pack our Q-values into the tensor and return it:</p>
<div class="tcolorbox" id="tcolobox-260">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-316"><code>    ref_vals_t = torch.FloatTensor(rewards_np).to(device) 
    return states_t, actions_t, ref_vals_t</code></pre>
</div>
</div>
<p>In the following code, you can notice a new way to create environments, the class <span class="cmtt-10x-x-109">gym.vector.SyncVectorEnv</span>, which is being passed a list of lambda functions creating the underlying environments:</p>
<div class="tcolorbox" id="tcolobox-261">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-317"><code>if __name__ == "__main__": 
    parser = argparse.ArgumentParser() 
    parser.add_argument("--dev", default="cpu", help="Device to use, default=cpu") 
    parser.add_argument("--use-async", default=False, action=‚Äôstore_true‚Äô, 
                        help="Use async vector env (A3C mode)") 
    parser.add_argument("-n", "--name", required=True, help="Name of the run") 
    args = parser.parse_args() 
    device = torch.device(args.dev) 
 
    env_factories = [ 
        lambda: ptan.common.wrappers.wrap_dqn(gym.make("PongNoFrameskip-v4")) 
        for _ in range(NUM_ENVS) 
    ] 
    if args.use_async: 
        env = gym.vector.AsyncVectorEnv(env_factories) 
    else: 
        env = gym.vector.SyncVectorEnv(env_factories) 
    writer = SummaryWriter(comment="-pong-a2c_" + args.name)</code></pre>
</div>
</div>
<p>The <span id="dx1-207085"/>class <span class="cmtt-10x-x-109">gym.vector.SyncVectorEnv </span>is provided by Gymnasium and allows wrapping several environments into one single ‚Äúvectorized‚Äù environment. Underlying environments have to have identical action and observation spaces, which allows the vectorized environment to accept a vector of actions and return batches of observations and rewards. You can find more details in the Gymnasium documentation: <a class="url" href="https://gymnasium.farama.org/api/vector/"><span class="cmtt-10x-x-109">https://gymnasium.farama.org/api/vector/</span></a>.</p>
<p>Synchronized vectorized environments (the <span class="cmtt-10x-x-109">SyncVectorEnv </span>class) are almost identical to the optimization we used in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="ch013.xhtml#x1-1600009"><span class="cmti-10x-x-109">9</span></a>, in the section <span class="cmti-10x-x-109">Several</span> <span class="cmti-10x-x-109">environments</span>, where we passed multiple gym environments into the experience source to increase the performance of the DQN training.</p>
<p>But in the case of vectorized environments, a different experience source class has to be used: <span class="cmtt-10x-x-109">VectorExperienceSourceFirstLast</span>, which takes into account vectorization and optimizes the agent application to the observation. From the outside, the interface of this experience source is exactly as before.</p>
<p>The command-line argument <span class="cmtt-10x-x-109">--use-async </span>(which switches our wrapper class from <span class="cmtt-10x-x-109">SyncVectorEnv </span>to <span class="cmtt-10x-x-109">AsyncVectorEnv</span>) is not relevant at the moment ‚Äì we will use it later, when discussing the A3C method.</p>
<p>Then, we create the network, agent, and experience source:</p>
<div class="tcolorbox" id="tcolobox-262">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-318"><code>    net = common.AtariA2C(env.single_observation_space.shape, 
                          env.single_action_space.n).to(device) 
    print(net) 
 
    agent = ptan.agent.PolicyAgent(lambda x: net(x)[0], apply_softmax=True, device=device) 
    exp_source = VectorExperienceSourceFirstLast( 
        env, agent, gamma=GAMMA, steps_count=REWARD_STEPS) 
 
    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE, eps=1e-3)</code></pre>
</div>
</div>
<p>One very <span id="dx1-207095"/>important detail here is passing the <span class="cmtt-10x-x-109">eps </span>parameter to the optimizer. If you‚Äôre familiar with the <span class="cmbx-10x-x-109">Adam </span>algorithm, you may know that epsilon is a small number added to the denominator to prevent zero-division situations. Normally, this value is set to some small number, such as 10<sup><span class="cmsy-8">‚àí</span><span class="cmr-8">8</span></sup> or 10<sup><span class="cmsy-8">‚àí</span><span class="cmr-8">10</span></sup>, but in our case, these values turned out to be too small. I have no mathematically strict explanation for this, but with the default value of epsilon, the method does not converge at all. Very likely, the division to a small value of 10<sup><span class="cmsy-8">‚àí</span><span class="cmr-8">8</span></sup> makes the gradients too large, which turns out to be fatal for training stability.</p>
<p>Another detail is to use <span class="cmtt-10x-x-109">VectorExperienceSourceFirstLast </span>instead of <span class="cmtt-10x-x-109">ExperienceSourceFirstLast</span>. This is required because of the vectorized environment wrapping several normal Atari environments. The vectorized environment also exposes the attributes <span class="cmtt-10x-x-109">single</span><span class="cmtt-10x-x-109">_observation</span><span class="cmtt-10x-x-109">_space </span>and <span class="cmtt-10x-x-109">single</span><span class="cmtt-10x-x-109">_action</span><span class="cmtt-10x-x-109">_space</span>, which are the observation and action spaces of an individual environment.</p>
<p>In the training loop, we use two wrappers:</p>
<div class="tcolorbox" id="tcolobox-263">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-319"><code>    batch = [] 
 
    with common.RewardTracker(writer, stop_reward=18) as tracker: 
        with TBMeanTracker(writer, batch_size=10) as tb_tracker: 
            for step_idx, exp in enumerate(exp_source): 
                batch.append(exp) 
 
                new_rewards = exp_source.pop_total_rewards() 
                if new_rewards: 
                    if tracker.reward(new_rewards[0], step_idx): 
                        break 
 
                if len(batch) &lt; BATCH_SIZE: 
                    continue</code></pre>
</div>
</div>
<p>The first wrapper in this code is already familiar to you: <span class="cmtt-10x-x-109">common.RewardTracker</span>, which computes the mean reward for the last 100 episodes and tells us when this mean reward exceeds the desired threshold. Another wrapper, <span class="cmtt-10x-x-109">TBMeanTracker</span>, is from the PTAN library and is responsible for writing into TensorBoard the mean of the measured parameters for the last 10 steps. This is helpful, as training can take millions of steps and we don‚Äôt want to write millions of points into TensorBoard, but rather write smoothed values every 10 steps.</p>
<p>The next code chunk is responsible for<span id="dx1-207110"/> our calculation of losses, which is the core of the A2C method. First, we unpack our batch using the function we described earlier and ask our network to return the policy and values for this batch:</p>
<div class="tcolorbox" id="tcolobox-264">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-320"><code>                states_t, actions_t, vals_ref_t = common.unpack_batch( 
                    batch, net, device=device, gamma=GAMMA, reward_steps=REWARD_STEPS) 
                batch.clear() 
 
                optimizer.zero_grad() 
                logits_t, value_t = net(states_t)</code></pre>
</div>
</div>
<p>The policy is returned in an unnormalized form, so to convert it into the probability distribution, we need to apply softmax to it. As the policy loss requires the logarithm of the probability distribution, we will use the function <span class="cmtt-10x-x-109">log</span><span class="cmtt-10x-x-109">_softmax</span>, which is more numerically stable than calling <span class="cmtt-10x-x-109">softmax </span>and then <span class="cmtt-10x-x-109">log</span>.</p>
<p>In the value loss part, we calculate the MSE between the value returned by our network and the approximation we performed using the Bellman equation unrolled four steps forward:</p>
<div class="tcolorbox" id="tcolobox-265">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-321"><code>                loss_value_t = F.mse_loss(value_t.squeeze(-1), vals_ref_t)</code></pre>
</div>
</div>
<p>Next, we calculate the policy loss to obtain the policy gradient:</p>
<div class="tcolorbox" id="tcolobox-266">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-322"><code>                log_prob_t = F.log_softmax(logits_t, dim=1) 
                adv_t = vals_ref_t - value_t.detach() 
                log_act_t = log_prob_t[range(BATCH_SIZE), actions_t] 
                log_prob_actions_t = adv_t * log_act_t 
                loss_policy_t = -log_prob_actions_t.mean()</code></pre>
</div>
</div>
<p>The first two steps obtain a log of our policy and calculate the advantage of actions, which is <span class="cmmi-10x-x-109">A</span>(<span class="cmmi-10x-x-109">s,a</span>) = <span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s,a</span>) <span class="cmsy-10x-x-109">‚àí</span><span class="cmmi-10x-x-109">V </span>(<span class="cmmi-10x-x-109">s</span>). The call to <span class="cmtt-10x-x-109">value</span><span class="cmtt-10x-x-109">_t.detach() </span>is important, as we don‚Äôt want to propagate the policy gradient into our value approximation head. Then, we take the log of probability for the actions taken and scale them with the advantage. Our policy gradient loss value will be equal to the negated mean of this scaled log of policy, as the policy gradient directs us toward policy improvement, but loss value is supposed to be minimized.</p>
<p>The last piece of our loss function is entropy loss:</p>
<div class="tcolorbox" id="tcolobox-267">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-323"><code>                prob_t = F.softmax(logits_t, dim=1) 
                entropy_loss_t = ENTROPY_BETA * (prob_t * log_prob_t).sum(dim=1).mean()</code></pre>
</div>
</div>
<p>Entropy loss is equal to the scaled entropy of our policy, taken with the opposite sign (entropy is calculated as <span class="cmmi-10x-x-109">H</span>(<span class="cmmi-10x-x-109">œÄ</span>) = <span class="cmsy-10x-x-109">‚àí</span><span class="cmex-10x-x-109">‚àë</span> <span class="cmmi-10x-x-109">œÄ</span> log <span class="cmmi-10x-x-109">œÄ</span>).</p>
<p>In the following code, we calculate and extract gradients of our policy, which will be used to track the maximum gradient, its variance, and the L2 norm:</p>
<div class="tcolorbox" id="tcolobox-268">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-324"><code>                loss_policy_t.backward(retain_graph=True) 
                grads = np.concatenate([ 
                    p.grad.data.cpu().numpy().flatten() 
                    for p in net.parameters() if p.grad is not None 
                ])</code></pre>
</div>
</div>
<p>As the <span id="dx1-207130"/>final step of our training, we backpropagate the entropy loss and the value loss, clip gradients, and ask our optimizer to update the network:</p>
<div class="tcolorbox" id="tcolobox-269">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-325"><code>                loss_v = entropy_loss_t + loss_value_t 
                loss_v.backward() 
                nn_utils.clip_grad_norm_(net.parameters(), CLIP_GRAD) 
                optimizer.step() 
                loss_v += loss_policy_t</code></pre>
</div>
</div>
<p>At the end of the training loop, we track all of the values that we are going to monitor in TensorBoard:</p>
<div class="tcolorbox" id="tcolobox-270">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-326"><code>                tb_tracker.track("advantage", adv_t, step_idx) 
                tb_tracker.track("values", value_t, step_idx) 
                tb_tracker.track("batch_rewards", vals_ref_t, step_idx) 
                tb_tracker.track("loss_entropy", entropy_loss_t, step_idx) 
                tb_tracker.track("loss_policy", loss_policy_t, step_idx) 
                tb_tracker.track("loss_value", loss_value_t, step_idx) 
                tb_tracker.track("loss_total", loss_v, step_idx) 
                tb_tracker.track("grad_l2", np.sqrt(np.mean(np.square(grads))), step_idx) 
                tb_tracker.track("grad_max", np.max(np.abs(grads)), step_idx) 
                tb_tracker.track("grad_var", np.var(grads), step_idx)</code></pre>
</div>
</div>
<p>There are plenty of values that we need to monitor and we will discuss them in the next section.</p>
</section>
<section class="level4 subsectionHead" id="results-11">
<h2 class="heading-2" id="sigil_toc_id_186"> <span id="x1-20800012.3.2"/>Results</h2>
<p>To start the <span id="dx1-208001"/>training, run <span class="cmtt-10x-x-109">02</span><span class="cmtt-10x-x-109">_pong</span><span class="cmtt-10x-x-109">_a2c.py </span>with the <span class="cmtt-10x-x-109">--dev </span>(for GPU) and <span class="cmtt-10x-x-109">-n</span> options (which provides a name for the run for TensorBoard):</p>
<pre class="lstlisting" id="listing-327"><code>Chapter12$ ./02_pong_a2c.py --dev cuda -n tt 
A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7) 
[Powered by Stella] 
AtariA2C( 
¬†¬†(conv): Sequential( 
¬†¬†¬†(0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4)) 
¬†¬†¬†(1): ReLU() 
¬†¬†¬†(2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2)) 
¬†¬†¬†(3): ReLU() 
¬†¬†¬†(4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1)) 
¬†¬†¬†(5): ReLU() 
¬†¬†¬†(6): Flatten(start_dim=1, end_dim=-1) 
¬†¬†) 
¬†¬†(policy): Sequential( 
¬†¬†¬†(0): Linear(in_features=3136, out_features=512, bias=True) 
¬†¬†¬†(1): ReLU() 
¬†¬†¬†(2): Linear(in_features=512, out_features=6, bias=True) 
¬†¬†) 
¬†¬†(value): Sequential( 
¬†¬†¬†(0): Linear(in_features=3136, out_features=512, bias=True) 
¬†¬†¬†(1): ReLU() 
¬†¬†¬†(2): Linear(in_features=512, out_features=1, bias=True) 
¬†¬†) 
) 
37850: done 1 games, mean reward -21.000, speed 1090.79 f/s 
39250: done 2 games, mean reward -21.000, speed 1111.24 f/s 
39550: done 3 games, mean reward -21.000, speed 1118.06 f/s 
40000: done 4 games, mean reward -21.000, speed 1083.18 f/s 
40300: done 5 games, mean reward -21.000, speed 1141.46 f/s 
40750: done 6 games, mean reward -21.000, speed 1077.44 f/s 
40850: done 7 games, mean reward -21.000, speed 940.09 f/s 
...</code></pre>
<p>As a word of warning, the training process is lengthy. With the original hyperparameters, it requires about 10 million frames to solve, which is about three hours on a GPU.</p>
<p>Later in the <span id="dx1-208034"/>chapter, we‚Äôll check the asynchronous version of the A2C method, which executes the environment in a separate process (which increases both training stability and performance). But first, let‚Äôs focus on our plots in TensorBoard.</p>
<p>The reward dynamics look much better than in the example from the previous chapter:</p>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_12_06.png" width="600"/> <span id="x1-208035r6"/></p>
<span class="id">Figure¬†12.6: Smoothed reward (left) and mean batch values (right) </span>
</div>
<p>The left plot is the mean training episodes reward averaged over the 100 last episodes. The right plot, ‚Äúbatch value,‚Äù shows Q-values approximated using the Bellman equation and an overall positive dynamic in Q approximation. This shows that our training process is improving more or less consistently over time.</p>
<p>The next four charts are related to our loss and include the individual loss components and the total loss:</p>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_12_07.png" width="600"/> <span id="x1-208036r7"/></p>
<span class="id">Figure¬†12.7: Entropy loss (left) and policy loss (right) </span>
</div>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_12_08.png" width="600"/> <span id="x1-208037r8"/></p>
<span class="id">Figure¬†12.8: Value loss (left) and total loss (right) </span>
</div>
<p>Here, we must <span id="dx1-208038"/>note the following:</p>
<ul>
<li>
<p>First, our value loss (<span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109">¬†</span><a href="#x1-208037r8"><span class="cmti-10x-x-109">12.8</span></a>, on the left) is decreasing consistently, which shows that our <span class="cmmi-10x-x-109">V </span>(<span class="cmmi-10x-x-109">s</span>) approximation is improving during the training.</p>
</li>
<li>
<p>The second observation is that our entropy loss (<span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109">¬†</span><a href="#x1-208036r7"><span class="cmti-10x-x-109">12.7</span></a>, on the left) is growing in the middle of the training, but it doesn‚Äôt dominate in the total loss. This basically means that our agent becomes more confident in its actions as the policy becomes less uniform.</p>
</li>
<li>
<p>The last thing to note here is that policy loss (<span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109">¬†</span><a href="#x1-208036r7"><span class="cmti-10x-x-109">12.7</span></a>, on the right) is decreasing most of the time and is correlated to the total loss, which is good, as we are interested in the gradients for our policy first of all.</p>
</li>
</ul>
<p>The last set of plots displays the advantage value and policy gradient metrics:</p>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_12_09.png" width="600"/> <span id="x1-208039r9"/></p>
<span class="id">Figure¬†12.9: Advantage (left) and L2 of gradients (right) </span>
</div>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_12_10.png" width="600"/> <span id="x1-208040r10"/></p>
<span class="id">Figure¬†12.10: Max of gradients (left) and gradients variance (right) </span>
</div>
<p>The <span id="dx1-208041"/>advantage is a scale of our policy gradients, and it equals <span class="cmmi-10x-x-109">Q</span>(<span class="cmmi-10x-x-109">s,a</span>) <span class="cmsy-10x-x-109">‚àí</span><span class="cmmi-10x-x-109">V </span>(<span class="cmmi-10x-x-109">s</span>). We expect it to oscillate around 0 (because, on average, the effect of the single action on the state‚Äôs value shouldn‚Äôt be large), and the chart meets our expectations. The gradient charts demonstrate that our gradients are not too small and not too large. Variance is very small at the beginning of the training (for 2 million frames), but starts to grow later, which means that our policy is changing.</p>
</section>
</section>
<section class="level3 sectionHead" id="asynchronous-advantage-actor-critic-a3c">
<h1 class="heading-1" id="sigil_toc_id_187"> <span id="x1-20900012.4"/>Asynchronous Advantage Actor-Critic (A3C)</h1>
<p>In this <span id="dx1-209001"/>section, we will extend the A2C method. This extension adds true asynchronous environment interaction, and is called <span class="cmbx-10x-x-109">asynchronous advantage</span> <span class="cmbx-10x-x-109">actor-critic </span>(<span class="cmbx-10x-x-109">A3C</span>). This method is one of the most widely used by RL practitioners.</p>
<p>We will take a look at two approaches for adding asynchronous behavior to the basic A2C method: data-level and gradient-level parallelism. They have different resource requirements and characteristics, which makes them applicable to different situations.</p>
<section class="level4 subsectionHead" id="correlation-and-sample-efficiency">
<h2 class="heading-2" id="sigil_toc_id_188"> <span id="x1-21000012.4.1"/>Correlation and sample efficiency</h2>
<p>One of <span id="dx1-210001"/>the approaches to improving the stability of the policy gradient family of methods is using multiple environments in parallel. The reason behind this is the fundamental problem we discussed in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="#"><span class="cmti-10x-x-109">6</span></a>, when we talked about the correlation between samples, which breaks the <span class="cmbx-10x-x-109">independent and identically</span> <span class="cmbx-10x-x-109">distributed </span>(<span class="cmbx-10x-x-109">iid</span>) assumption, which is critical for <span class="cmbx-10x-x-109">stochastic gradient</span> <span class="cmbx-10x-x-109">descent </span>(<span class="cmbx-10x-x-109">SGD</span>) optimization. The negative consequence of such correlation is very high variance in gradients, which means that our training batch contains very similar examples, all of them pushing our network in the same direction. However, this may be totally the wrong direction in the global sense, as all those examples may be from one single lucky or unlucky episode.</p>
<p>With our <span class="cmbx-10x-x-109">deep Q-network (DQN)</span>, we solved the issue by storing a large number of previous states in the replay buffer and sampling our training batch from this buffer. If the buffer is large enough, the random sample from it will be a much better representation of the states‚Äô distribution at large. Unfortunately, this solution won‚Äôt work for policy gradient methods. This is because most of them are on-policy, which means that we have to train on samples generated by our current policy, so remembering old transitions will not be possible anymore. You can try to do this, but the resulting policy gradient will be for the old policy used to generate the samples and not for your current policy that you want to update.</p>
<p>Researchers have focused on this issue for many years. Several ways to address it have been proposed, but the problem is still far from being solved. The most commonly used solution is gathering transitions using several parallel environments, all of them exploiting the current policy. This breaks the correlation within one single episode, as we now train on several episodes obtained from different environments. At the same time, we are still using our current policy. The one very large disadvantage of this is <span class="cmbx-10x-x-109">sample inefficiency</span>, as we basically throw away all the experience that we have obtained after one single training round.</p>
<p>It‚Äôs very simple to compare DQN with policy gradient approaches. For example, for DQN, if we use 1 million samples of a replay buffer and a training batch size of 32 samples for every new frame, every single transition will be used approximately 32 times before it is pushed from the experience replay. For the priority replay buffer, which was discussed in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="ch012.xhtml#x1-1240008"><span class="cmti-10x-x-109">8</span></a>, this number could be much higher, as the sample probability is not uniform. In the case of policy gradient methods, each experience obtained from the environment can be used only once, as our method requires fresh data, so the data efficiency of policy gradient methods could be an order of magnitude lower than the value-based, off-policy methods.</p>
<p>On the other <span id="dx1-210002"/>hand, our A2C agent converged on Pong in 8 million frames, which is just eight times more than 1 million frames for basic DQN in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="#"><span class="cmti-10x-x-109">6</span></a> and <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="ch012.xhtml#x1-1240008"><span class="cmti-10x-x-109">8</span></a>. So, this shows us that policy gradient methods are not completely useless; they‚Äôre just different and have their own specificities that you need to take into account on method selection. If your environment is ‚Äúcheap‚Äù in terms of the agent interaction (the environment is fast, has a low memory footprint, allows parallelization, and so on), policy gradient methods could be a better choice. On the other hand, if the environment is ‚Äúexpensive‚Äù and obtaining a large amount of experience could slow down the training process, the value-based methods could be a smarter way to go.</p>
</section>
<section class="level4 subsectionHead" id="adding-an-extra-a-to-a2c">
<h2 class="heading-2" id="sigil_toc_id_189"> <span id="x1-21100012.4.2"/>Adding an extra ‚ÄúA‚Äù to A2C</h2>
<p>From a practical point of view, communicating with several parallel environments is simple. We already did this in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="ch013.xhtml#x1-1600009"><span class="cmti-10x-x-109">9</span></a> and earlier in the current chapter, but it wasn‚Äôt explicitly stated. In the A2C agent, we passed an array of Gym environments into the <span class="cmtt-10x-x-109">ExperienceSource </span>class, which switched it into round-robin data gathering mode. This means that every time we ask for a transition from the experience source, the class uses the next environment from our array (of course, keeping the state for every environment). This simple approach is equivalent to parallel communication with environments, but with one single difference: communication is not parallel in the strict sense but performed in a serial way. However, samples from our experience source are shuffled. This idea is shown in the following figure:</p>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_12_11.png" width="600"/> <span id="x1-211001r11"/></p>
<span class="id">Figure¬†12.11: An agent training from multiple environments in parallel </span>
</div>
<p>This method works fine and helped us to get convergence in the A2C method, but it is still not perfect in terms of computing resource utilization, as all the processing is done sequentially. Even a modest workstation nowadays has several CPU cores, which can be used for computation, such as training and environment interaction. On the other hand, parallel programming is harder than the traditional paradigm, when you have a clear stream of execution. Luckily, Python is a very expressive and flexible language with lots of third-party libraries, which allows you to do parallel programming without much trouble. We have already seen the example of the <span class="cmtt-10x-x-109">torch.multiprocessing </span>library in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="ch013.xhtml#x1-1600009"><span class="cmti-10x-x-109">9</span></a>, where we parallelized agents‚Äô execution during the DQN training. But there are other higher-level libraries, like <span class="cmtt-10x-x-109">ray</span>, which allow us to parallelize execution of the code, hiding the low-level communication details.</p>
<p>With regard to <span id="dx1-211002"/>actor-critic parallelization, two approaches exist:</p>
<ol>
<li>
<div id="x1-211004x1">
<p><span class="cmbx-10x-x-109">Data parallelism</span>: We can have several processes, each of them communicating with one or more environments and providing us with transitions (<span class="cmmi-10x-x-109">s,r,a,s</span><span class="cmsy-10x-x-109">‚Ä≤</span>). All those samples are gathered together in one single training process, which calculates losses and performs an SGD update. Then, the updated <span class="cmbx-10x-x-109">neural network </span>(<span class="cmbx-10x-x-109">NN</span>) parameters need to be broadcast to all other processes to use in future environment communications. This model is illustrated in <span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109">¬†</span><a href="#x1-211008r12"><span class="cmti-10x-x-109">12.12</span></a>.</p>
</div>
</li>
<li>
<div id="x1-211006x2">
<p><span class="cmbx-10x-x-109">Gradients parallelism</span>: As <span id="dx1-211007"/>the goal of the training process is the calculation of gradients to update our NN, we can have several processes calculating gradients on their own training samples. Then, these gradients can be summed together to perform the SGD update in one process. Of course, updated NN weights also have to be propagated back to all workers to keep data on-policy. This is illustrated in <span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109">¬†</span><a href="#x1-211009r13"><span class="cmti-10x-x-109">12.13</span></a>.</p>
</div>
</li>
</ol>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_12_12.png" width="600"/> <span id="x1-211008r12"/></p>
<span class="id">Figure¬†12.12: The first approach to actor-critic parallelism, based on distributed training samples being gathered </span>
</div>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_12_13.png" width="600"/> <span id="x1-211009r13"/></p>
<span class="id">Figure¬†12.13: The second approach to parallelism, gathering gradients for the model </span>
</div>
<p>The difference <span id="dx1-211010"/>between the two methods might not look very significant from the diagrams, but you need to be aware of the computation cost. The heaviest operation in A2C optimization is the training process, which consists of loss calculation from data samples (forward pass) and the calculation of gradients with respect to this loss. The SGD optimization step is quite lightweight ‚Äì basically, just adding the scaled gradients to the NN‚Äôs weights. By moving the computation of loss and gradients in the second approach (gradient parallelism) from the central process, we eliminated the major potential bottleneck and made the whole process significantly more scalable.</p>
<p>In practice, the choice of the method mainly depends on your resources and your goals. If you have one single optimization problem and lots of distributed computation resources, such as a couple of dozen GPUs spread over several machines in the networks, then gradients parallelism will be the best approach to speed up your training.</p>
<p>However, in the case of one single GPU, both methods will provide a similar performance, but the first approach is generally simpler to implement, as you don‚Äôt need to deal with low-level gradient values. In this chapter, we will compare both methods on our favorite Pong game to see the difference between the approaches and look at PyTorch‚Äôs multiprocessing capabilities.</p>
</section>
<section class="level4 subsectionHead" id="a3c-with-data-parallelism">
<h2 class="heading-2" id="sigil_toc_id_190"> <span id="x1-21200012.4.3"/>A3C with data parallelism</h2>
<p>The first version of<span id="dx1-212001"/> A3C parallelization that we will check (which was outlined in <span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109">¬†</span><a href="#x1-211008r12"><span class="cmti-10x-x-109">12.12</span></a>) has both one main process that carries out training and several child processes communicating with environments and gathering experience to train on.</p>
<p>In fact, we already implemented this version in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="ch013.xhtml#x1-1600009"><span class="cmti-10x-x-109">9</span></a> when we ran several agents in subprocesses when we trained the DQN model (then we got a speed-up of 27% in terms of FPS). In this section, I‚Äôm not going to reimplement the same approach with the A3C method, but rather want to illustrate the ‚Äúpower of libraries.‚Äù</p>
<p>We already briefly mentioned the class <span class="cmtt-10x-x-109">gym.vector.SyncVectorEnv </span>from Gymnasium (it exists only in the Farama fork, not in the original OpenAI Gym) and the PTAN experience source, which supports ‚Äúvectorized‚Äù environments: <span class="cmtt-10x-x-109">VectorExperienceSourceFirstLast</span>. The class <span class="cmtt-10x-x-109">SyncVectorEnv </span>handles wrapped environments sequentially, but there is a drop-in replacement class, <span class="cmtt-10x-x-109">AsyncVectorEnv</span>, which uses <span class="cmtt-10x-x-109">mp.multiprocessing </span>for subenvironments. So, to get the data-parallel version of the A2C method, we just need to replace <span class="cmtt-10x-x-109">SyncVectorEnv </span>with <span class="cmtt-10x-x-109">AsyncVectorEnv </span>and we‚Äôre done.</p>
<p>The code in <span class="cmtt-10x-x-109">Chapter12/02</span><span class="cmtt-10x-x-109">_pong</span><span class="cmtt-10x-x-109">_a2c.py </span>already supports this replacement, which is done by passing the <span class="cmtt-10x-x-109">--use-async </span>command-line option.</p>
<section class="level5 subsubsectionHead" id="results-12">
<h3 class="heading-3" id="sigil_toc_id_397"><span id="x1-213000"/>Results</h3>
<p>The asynchronous <span id="dx1-213001"/>version with 50 environments shows a performance of 2000 FPS, which is a 2x improvement over the sequential version. The following charts compare the performance and reward dynamics of these two versions:</p>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_12_14.png" width="600"/> <span id="x1-213002r14"/></p>
<span class="id">Figure¬†12.14: Comparison of A2C and A3C in terms of reward (left) and speed (right) </span>
</div>
</section>
</section>
<section class="level4 subsectionHead" id="a3c-with-gradient-parallelism">
<h2 class="heading-2" id="sigil_toc_id_191"> <span id="x1-21400012.4.4"/>A3C with gradient parallelism</h2>
<p>The next <span id="dx1-214001"/>approach that we will consider to parallelize A2C implementation will have several child processes, but instead of feeding training data to the central training loop, they will calculate the gradients using their local training data, and send those gradients to the central master process. This process is responsible for combining those gradients (which is basically just summing them) and performing an SGD update on the shared network.</p>
<p>The difference might look minor, but this approach is much more scalable, especially if you have several powerful nodes with multiple GPUs connected to the network. In this case, the central process in the data-parallel model quickly becomes a bottleneck, as the loss calculation and backpropagation are computationally demanding. Gradient parallelization allows for the spreading of the load on several GPUs, performing only a relatively simple operation of gradient combination in a central place.</p>
<section class="level5 subsubsectionHead" id="implementation-9">
<h3 class="heading-3" id="sigil_toc_id_398"><span id="x1-215000"/>Implementation</h3>
<p>The complete <span id="dx1-215001"/>example is in the <span class="cmtt-10x-x-109">Chapter12/03</span><span class="cmtt-10x-x-109">_a3c</span><span class="cmtt-10x-x-109">_grad.py </span>file, and it uses the same <span class="cmtt-10x-x-109">Chapter12/lib/common.py </span>module that we‚Äôve already seen.</p>
<p>As usual, we first define the hyperparameters:</p>
<div class="tcolorbox" id="tcolobox-271">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-328"><code>GAMMA = 0.99 
LEARNING_RATE = 0.001 
ENTROPY_BETA = 0.01 
REWARD_STEPS = 4 
CLIP_GRAD = 0.1 
 
PROCESSES_COUNT = 4 
NUM_ENVS = 8 
GRAD_BATCH = 64 
TRAIN_BATCH = 2 
 
ENV_NAME = "PongNoFrameskip-v4" 
NAME = ‚Äôpong‚Äô 
REWARD_BOUND = 18</code></pre>
</div>
</div>
<p>These are mostly the same as in the previous example, except <span class="cmtt-10x-x-109">BATCH</span><span class="cmtt-10x-x-109">_SIZE </span>is replaced by two parameters: <span class="cmtt-10x-x-109">GRAD</span><span class="cmtt-10x-x-109">_BATCH </span>and <span class="cmtt-10x-x-109">TRAIN</span><span class="cmtt-10x-x-109">_BATCH</span>. The value of <span class="cmtt-10x-x-109">GRAD</span><span class="cmtt-10x-x-109">_BATCH </span>defines the size of the batch used by every child process to compute the loss and get the value of the gradients. The second parameter, <span class="cmtt-10x-x-109">TRAIN</span><span class="cmtt-10x-x-109">_BATCH</span>, specifies how many gradient batches from the child processes will be combined on every SGD iteration. Every entry produced by the child process has the same shape as our network parameters, and we sum up <span class="cmtt-10x-x-109">TRAIN</span><span class="cmtt-10x-x-109">_BATCH </span>values of them together. So, for every optimization step, we use the <span class="cmtt-10x-x-109">TRAIN</span><span class="cmtt-10x-x-109">_BATCH * GRAD</span><span class="cmtt-10x-x-109">_BATCH </span>training samples. As the loss calculation and backpropagation are quite heavy operations, we use a large <span class="cmtt-10x-x-109">GRAD</span><span class="cmtt-10x-x-109">_BATCH </span>to make them more efficient.</p>
<p>Due to this large batch, we should keep <span class="cmtt-10x-x-109">TRAIN</span><span class="cmtt-10x-x-109">_BATCH </span>relatively low to keep our network update on policy.</p>
<p>Now we have two functions ‚Äì <span class="cmtt-10x-x-109">make</span><span class="cmtt-10x-x-109">_env()</span>, which is used to create a <span id="dx1-215016"/>wrapped Pong environment, and <span class="cmtt-10x-x-109">grads</span><span class="cmtt-10x-x-109">_func()</span>, which is much more complicated and implements most of the training logic we normally do in the training loop. As a compensation, the training loop in the main process becomes almost trivial:</p>
<div class="tcolorbox" id="tcolobox-272">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-329"><code>def make_env() -&gt; gym.Env: 
    return ptan.common.wrappers.wrap_dqn(gym.make("PongNoFrameskip-v4")) 
 
 
def grads_func(proc_name: str, net: common.AtariA2C, device: torch.device, 
               train_queue: mp.Queue): 
    env_factories = [make_env for _ in range(NUM_ENVS)] 
    env = gym.vector.SyncVectorEnv(env_factories) 
 
    agent = ptan.agent.PolicyAgent(lambda x: net(x)[0], device=device, apply_softmax=True) 
    exp_source = VectorExperienceSourceFirstLast( 
        env, agent, gamma=GAMMA, steps_count=REWARD_STEPS) 
 
    batch = [] 
    frame_idx = 0 
    writer = SummaryWriter(comment=proc_name)</code></pre>
</div>
</div>
<p>On the creation of the child process, we pass several arguments to the <span class="cmtt-10x-x-109">grads</span><span class="cmtt-10x-x-109">_func() </span>function:</p>
<ul>
<li>
<p>The name of the process, which is used to create the TensorBoard writer. In this example, every child process writes its own TensorBoard dataset.</p>
</li>
<li>
<p>The shared NN.</p>
</li>
<li>
<p>A <span class="cmtt-10x-x-109">torch.device </span>instance, specifying the computation device.</p>
</li>
<li>
<p>The queue used to deliver the calculated gradients to the central process.</p>
</li>
</ul>
<p>Our child <span id="dx1-215033"/>process function looks very similar to the main training loop in the data-parallel version, which is not surprising, as the responsibilities of our child process increased. However, instead of asking the optimizer to update the network, we gather gradients and send them to the queue. The rest of the code is almost the same:</p>
<div class="tcolorbox" id="tcolobox-273">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-330"><code>    with common.RewardTracker(writer, REWARD_BOUND) as tracker: 
        with TBMeanTracker(writer, 100) as tb_tracker: 
            for exp in exp_source: 
                frame_idx += 1 
                new_rewards = exp_source.pop_total_rewards() 
                if new_rewards and tracker.reward(new_rewards[0], frame_idx): 
                    break 
 
                batch.append(exp) 
                if len(batch) &lt; GRAD_BATCH: 
                    continue</code></pre>
</div>
</div>
<p>Up to this point, we‚Äôve gathered the batch with transitions and handled the end-of-episode rewards.</p>
<p>In the next part of the function, we calculate the combined loss from the training data and perform backpropagation of the loss:</p>
<div class="tcolorbox" id="tcolobox-274">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-331"><code>                data = common.unpack_batch(batch, net, device=device, gamma=GAMMA, 
                                           reward_steps=REWARD_STEPS) 
                states_v, actions_t, vals_ref_v = data 
                batch.clear() 
 
                net.zero_grad() 
                logits_v, value_v = net(states_v) 
                loss_value_v = F.mse_loss(value_v.squeeze(-1), vals_ref_v) 
 
                log_prob_v = F.log_softmax(logits_v, dim=1) 
                adv_v = vals_ref_v - value_v.detach() 
                log_p_a = log_prob_v[range(GRAD_BATCH), actions_t] 
                log_prob_actions_v = adv_v * log_p_a 
                loss_policy_v = -log_prob_actions_v.mean() 
 
                prob_v = F.softmax(logits_v, dim=1) 
                ent = (prob_v * log_prob_v).sum(dim=1).mean() 
                entropy_loss_v = ENTROPY_BETA * ent 
 
                loss_v = entropy_loss_v + loss_value_v + loss_policy_v 
                loss_v.backward()</code></pre>
</div>
</div>
<p>In the following code, we send our intermediate values that we‚Äôre going to monitor during the training to TensorBoard:</p>
<div class="tcolorbox" id="tcolobox-275">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-332"><code>                tb_tracker.track("advantage", adv_v, frame_idx) 
                tb_tracker.track("values", value_v, frame_idx) 
                tb_tracker.track("batch_rewards", vals_ref_v, frame_idx) 
                tb_tracker.track("loss_entropy", entropy_loss_v, frame_idx) 
                tb_tracker.track("loss_policy", loss_policy_v, frame_idx) 
                tb_tracker.track("loss_value", loss_value_v, frame_idx) 
                tb_tracker.track("loss_total", loss_v, frame_idx)</code></pre>
</div>
</div>
<p>At the end of the <span id="dx1-215073"/>loop, we need to clip the gradients and extract them from the network‚Äôs parameters into a separate buffer (to prevent them from being corrupted by the next iteration of the loop). Here, we effectively store gradients in the <span class="cmtt-10x-x-109">tensor.grad </span>field for every network parameter. This could be done without bothering with synchronization with other workers, as our network‚Äôs parameters are shared, but the gradients are locally allocated by every process:</p>
<div class="tcolorbox" id="tcolobox-276">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-333"><code>                nn_utils.clip_grad_norm_( 
                    net.parameters(), CLIP_GRAD) 
                grads = [ 
                    param.grad.data.cpu().numpy() if param.grad is not None else None 
                    for param in net.parameters() 
                ] 
                train_queue.put(grads) 
 
    train_queue.put(None)</code></pre>
</div>
</div>
<p>The last line in <span class="cmtt-10x-x-109">grads</span><span class="cmtt-10x-x-109">_func </span>puts <span class="cmtt-10x-x-109">None </span>into the queue, signaling that this child process has reached the <span class="cmti-10x-x-109">game solved </span>state and training should be stopped.</p>
<p>The main process starts with the creation of the network and sharing of its weights:</p>
<div class="tcolorbox" id="tcolobox-277">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-334"><code>if __name__ == "__main__": 
    mp.set_start_method(‚Äôspawn‚Äô) 
    os.environ[‚ÄôOMP_NUM_THREADS‚Äô] = "1" 
    parser = argparse.ArgumentParser() 
    parser.add_argument("--dev", default="cpu", help="Device to use, default=cpu") 
    parser.add_argument("-n", "--name", required=True, help="Name of the run") 
    args = parser.parse_args() 
    device = torch.device(args.dev) 
 
    env = make_env() 
    net = common.AtariA2C(env.observation_space.shape, env.action_space.n).to(device) 
    net.share_memory()</code></pre>
</div>
</div>
<p>Here, as in the previous section, we need to set a start method for <span class="cmtt-10x-x-109">torch.multiprocessing </span>and limit the number of threads started by OpenMP. This is done by setting the environment variable <span class="cmtt-10x-x-109">OMP</span><span class="cmtt-10x-x-109">_NUM</span><span class="cmtt-10x-x-109">_THREADS</span>, which instructs the OpenMP library about the number of threads it can start. OpenMP (<a class="url" href="https://www.openmp.org/"><span class="cmtt-10x-x-109">https://www.openmp.org/</span></a>) is heavily used by the Gym and OpenCV libraries to provide a speed-up on multicore systems, which is a good thing most of the time. By default, the process that uses OpenMP starts a thread for every core in the system. But in our case, the effect from OpenMP is the opposite: as we‚Äôre implementing our own parallelism, by launching several processes, extra threads overload the cores with frequent context switches, which negatively impacts performance. To avoid this, we explicitly limit the amount of threads to one thread. If you want, you can experiment yourself with this parameter. On my system, I experienced a 3-4x performance drop without this environment variable assignment.</p>
<p>Then, we create the communication queue and spawn the required count of child processes:</p>
<div class="tcolorbox" id="tcolobox-278">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-335"><code>    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE, eps=1e-3) 
 
    train_queue = mp.Queue(maxsize=PROCESSES_COUNT) 
    data_proc_list = [] 
    for proc_idx in range(PROCESSES_COUNT): 
        proc_name = f"-a3c-grad_pong_{args.name}#{proc_idx}" 
        p_args = (proc_name, net, device, train_queue) 
        data_proc = mp.Process(target=grads_func, args=p_args) 
        data_proc.start() 
        data_proc_list.append(data_proc)</code></pre>
</div>
</div>
<p>Now we can get to the training loop:</p>
<div class="tcolorbox" id="tcolobox-279">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-336"><code>    batch = [] 
    step_idx = 0 
    grad_buffer = None 
 
    try: 
        while True: 
            train_entry = train_queue.get() 
            if train_entry is None: 
                break</code></pre>
</div>
</div>
<p>The major difference from the data-parallel version of A3C lies in the training loop, which is much simpler here, as child processes have done all the heavy calculations for us. In the beginning of the loop, we handle the <span id="dx1-215114"/>situation when one of the processes has reached the required mean reward (when this happens, we have <span class="cmtt-10x-x-109">None </span>in the queue). In this case, we just exit the loop to stop the training.</p>
<p>We sum gradients together for all the parameters in our network:</p>
<div class="tcolorbox" id="tcolobox-280">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-337"><code>            step_idx += 1 
 
            if grad_buffer is None: 
                grad_buffer = train_entry 
            else: 
                for tgt_grad, grad in zip(grad_buffer, train_entry): 
                    tgt_grad += grad</code></pre>
</div>
</div>
<p>When we have accumulated enough gradient pieces, we convert the sum of the gradients into the PyTorch <span class="cmtt-10x-x-109">FloatTensor </span>and assign them to the <span class="cmtt-10x-x-109">grad </span>field of the network parameters. To average the gradients from different children, we call the optimizer‚Äôs <span class="cmtt-10x-x-109">step() </span>function for every <span class="cmtt-10x-x-109">TRAIN</span><span class="cmtt-10x-x-109">_BATCH </span>gradient obtained. For intermediate steps, we just sum the corresponding gradients together:</p>
<div class="tcolorbox" id="tcolobox-281">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-338"><code>            if step_idx % TRAIN_BATCH == 0: 
                for param, grad in zip(net.parameters(), grad_buffer): 
                    param.grad = torch.FloatTensor(grad).to(device) 
 
                nn_utils.clip_grad_norm_(net.parameters(), CLIP_GRAD) 
                optimizer.step() 
                grad_buffer = None</code></pre>
</div>
</div>
<p>After that, all we need to do is call the optimizer‚Äôs <span class="cmtt-10x-x-109">step() </span>method to update the network parameters using the accumulated gradients.</p>
<p>On the exit <span id="dx1-215129"/>from the training loop, we stop all child processes to make sure that we terminated them, even if <span class="cmtt-10x-x-109">Ctrl + C </span>was pressed to stop the optimization:</p>
<div class="tcolorbox" id="tcolobox-282">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-339"><code>    finally: 
        for p in data_proc_list: 
            p.terminate() 
            p.join()</code></pre>
</div>
</div>
<p>This step is needed to prevent zombie processes from occupying GPU resources.</p>
</section>
<section class="level5 subsubsectionHead" id="results-13">
<h3 class="heading-3" id="sigil_toc_id_399"><span id="x1-216000"/>Results</h3>
<p>This example <span id="dx1-216001"/>can be started the same way as the previous example, and after a while, it should start displaying the speed and mean reward. However, you need to be aware that displayed information is local for every child process, which means that speed, the count of games completed, and the number of frames need to be multiplied by the number of processes. My benchmarks have shown speed to be around 500-600 FPS for every child, which gives 2,000-2,400 FPS in total.</p>
<p>Convergence dynamics are also very similar to the previous version. The total number of observations is about 8<span class="cmmi-10x-x-109">‚Ä¶</span>10 million, which requires about 1.5 hours to complete. The reward chart on the left shows individual processes, but the speed chart on the right shows the sum of all processes. As you can see, gradient parallelism gives slightly higher performance than data parallelism:</p>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_12_15.png" width="600"/> <span id="x1-216002r15"/></p>
<span class="id">Figure¬†12.15: Comparison of A2C and A3C in terms of reward (left) and speed (right) </span>
</div>
</section>
</section>
</section>
<section class="level3 sectionHead" id="summary-11">
<h1 class="heading-1" id="sigil_toc_id_192"> <span id="x1-21700012.5"/>Summary</h1>
<p>In this chapter, you learned about one of the most widely used methods in deep RL: A2C, which wisely combines the policy gradient update with the value of the state approximation. We analyzed the effect of the baseline on the statistics and convergence of gradients. Then, we checked the extension of the baseline idea: A2C, where a separate network head provides us with the baseline for the current state. In addition, we discussed why it is important for policy gradient methods to gather training data from multiple environments, due to their on-policy nature. We also implemented two different approaches to A3C, in order to parallelize and stabilize the training process. Parallelization will come up once again in this book, when we discuss black-box methods (<span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="ch021.xhtml#x1-31100017"><span class="cmti-10x-x-109">17</span></a>).</p>
<p>In the next two chapters, we will take a look at practical problems that can be solved using policy gradient methods, which will wrap up the policy gradient methods part of the book.</p>
</section>
<section class="level3 likesectionHead" id="join-our-community-on-discord-4">
<h1 class="heading-1" id="sigil_toc_id_193"><span id="x1-218000"/>Join our community on Discord</h1>
<p>Read this book alongside other users, Deep Learning experts, and the author himself. Ask questions, provide solutions to other readers, chat with the author via Ask Me Anything sessions, and much more. Scan the QR code or visit the link to join the community. <a class="url" href="https://packt.link/rl"><span class="cmtt-10x-x-109">https://packt.link/rl</span></a></p>
<p><img alt="PIC" height="85" src="../Images/file1.png" width="85"/></p>
</section>
</section>
</div></body></html>