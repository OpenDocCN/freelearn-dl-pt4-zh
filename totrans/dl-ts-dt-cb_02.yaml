- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Getting Started with PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ll explore **PyTorch**, a leading deep learning library
    in Python.
  prefs: []
  type: TYPE_NORMAL
- en: We go over several operations that are useful for understanding how neural networks
    are built using PyTorch. Besides tensor operations, we will also explore how to
    train different types of neural networks. Specifically, we will focus on feedforward,
    recurrent, **long short-term memory**(**LSTM**), and 1D convolutional networks.
  prefs: []
  type: TYPE_NORMAL
- en: In later chapters, we will also cover other types of neural networks, such as
    transformers. Here, we will use synthetic data for demonstrative purposes, which
    will help us showcase both the implementation and theory behind each model.
  prefs: []
  type: TYPE_NORMAL
- en: Upon completing this chapter, you will have gained a robust understanding of
    PyTorch, equipping you with the tools for more advanced deep learning projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Installing PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic operations in PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced operations in PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a simple neural network with PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a feedforward neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a recurrent neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training an LSTM neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a convolutional neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before starting, you will need to ensure that your system meets the following
    technical requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Python 3.9**: You can download Python from [https://www.python.org/downloads/](https://www.python.org/downloads/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'pip (23.3.1) or Anaconda: These are popular package managers for Python. pip
    comes with Python by default. Anaconda can be downloaded from [https://www.anaconda.com/products/distribution](https://www.anaconda.com/products/distribution).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'torch (2.2.0): The main library we will be using for deep learning in this
    chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CUDA (optional)**: If you have a CUDA-capable GPU on your machine, you can
    install a version of PyTorch that supports CUDA. This will enable computations
    on your GPU and can significantly speed up your deep learning experiments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s worth noting that the code presented in this chapter is platform-independent
    and should run on any system with the preceding requirements satisfied.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for this chapter can be found at the following GitHub URL: [https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook](https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook).'
  prefs: []
  type: TYPE_NORMAL
- en: Installing PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To start with PyTorch, we need to install it first. As of the time of writing,
    PyTorch supports Linux, macOS, and Windows platforms. Here, we will guide you
    through the installation process on these operating systems.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`PyTorch` is usually installed via `pip` or Anaconda. We recommend creating
    a new Python environment before installing the library, especially if you will
    be working on multiple Python projects on your system. This is to prevent any
    conflicts between different versions of Python libraries that different projects
    may require.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s see how to install `PyTorch`. We’ll describe how to do this using either
    `pip` or `Anaconda`. We’ll also provide some information about how to use a CUDA
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’re using `pip`, Python’s package manager, you can install PyTorch by
    running the following command in your terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'With the Anaconda Python distribution, you can install PyTorch using the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: If you have a CUDA-capable GPU on your machine, you can install a version of
    `PyTorch` that supports CUDA to enable computations on your GPU. This can significantly
    speed up your deep learning experiments. The PyTorch website provides a tool that
    generates the appropriate installation command based on your needs. Visit the
    PyTorch website, select your preferences (such as OS, package manager, Python
    version, and CUDA version) in the **Quick Start Locally** section, and then copy
    the generated command into your terminal.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After you’ve installed `PyTorch`, you can verify that everything is working
    correctly by opening a Python interpreter and running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This should output the version of `PyTorch` that you installed. Now, you’re
    ready to start using `PyTorch` for deep learning!
  prefs: []
  type: TYPE_NORMAL
- en: In the next sections, we will familiarize ourselves with the basics of `PyTorch`
    and build our first neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Basic operations in PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we start building neural networks with `PyTorch`, it is essential to
    understand the basics of how to manipulate data using this library. In `PyTorch`,
    the fundamental unit of data is the tensor, a generalization of matrices to an
    arbitrary number of dimensions (also known as a multidimensional array).
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A tensor can be a number (a 0D tensor), a vector (a 1D tensor), a matrix (a
    2D tensor), or any multi-dimensional data (a 3D tensor, a 4D tensor, and so on).
    `PyTorch` provides various functions to create and manipulate tensors.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s start by importing `PyTorch`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We can create a tensor in `PyTorch` using various techniques. Let’s start by
    creating tensors from lists:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '`PyTorch` can seamlessly integrate with NumPy, allowing for easy tensor creation
    from `NumPy` arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '`PyTorch` also provides functions to generate tensors with specific values,
    such as zeros or ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: These are commonly used methods in `NumPy` that are also available in `PyTorch`.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we know how to create tensors, let’s look at some basic operations.
    We can perform all standard arithmetic operations on tensors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'You can reshape tensors using the `.``reshape()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This is a brief introduction to tensor operations in `PyTorch`. As you dive
    deeper, you’ll find that `PyTorch` offers various operations to manipulate tensors,
    giving you the flexibility and control needed to implement complex deep learning
    models and algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced operations in PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After exploring basic tensor operations, let’s now dive into more advanced operations
    in `PyTorch`, specifically the linear algebra operations that form the backbone
    of most numerical computations in deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Linear algebra is a subset of mathematics. It deals with vectors, vector spaces,
    and linear transformations between these spaces, such as rotations, scaling, and
    shearing. In the context of deep learning, we deal with high-dimensional vectors
    (tensors), and operations on these vectors play a crucial role in the internal
    workings of models.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s start by revisiting the tensors we created in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The dot product of two vectors is a scalar that measures the vectors’ direction
    and magnitude. In `PyTorch`, we can calculate the dot product of two `1D` tensors
    using the `torch.dot()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Unlike element-wise multiplication, matrix multiplication, also known as the
    dot product, is the operation of multiplying two matrices to produce a new matrix.
    `PyTorch` provides the `torch.mm()` function to perform matrix multiplication:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The transpose of a matrix is a new matrix whose rows are the columns of the
    original matrix and whose columns are the rows. You can compute the transpose
    of a tensor using the `.``T` attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'There are other operations you can perform, such as calculating the determinant
    of a matrix and finding the inverse of a matrix. Let’s look at a couple of these
    operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Note that these two operations are only defined for `2D` tensors (matrices).
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`PyTorch` is a highly optimized library for performing basic and advanced operations,
    particularly linear algebra operations that are crucial in deep learning.'
  prefs: []
  type: TYPE_NORMAL
- en: These operations make `PyTorch` a powerful tool for building and training neural
    networks and performing high-level computations in a more general context. In
    the next section, we will use these building blocks to start constructing deep
    learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Building a simple neural network with PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section will build a simple two-layer neural network from scratch using
    only basic tensor operations to solve a time series prediction problem. We aim
    to demonstrate how one might manually implement a feedforward pass, backpropagation,
    and optimization steps without leveraging `PyTorch`’s predefined layers and optimization
    routines.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We use synthetic data for this demonstration. Suppose we have a simple time
    series data of `100` samples, each with `10` time steps. Our task is to predict
    the next time step based on the previous ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s create a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s start by defining our model parameters and their initial values. Here,
    we are creating a simple two-layer network, so we have two sets of weights and
    biases:'
  prefs: []
  type: TYPE_NORMAL
- en: We use the `requires_grad_()` function to tell `PyTorch` that we want to compute
    gradients with respect to these tensors during the backward pass.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we define our model. For this simple network, we’ll use a sigmoid activation
    function for the hidden layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we’re ready to train our model. Let’s define the learning rate and the
    number of epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This basic code demonstrates the essential parts of a neural network: the forward
    pass, where we compute predictions; the backward pass, where gradients are computed;
    and the update step, where we adjust our weights to minimize the loss.'
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This chapter is focused on exploring the intricacies of the training process
    of a neural network. In future chapters, we’ll show how to train deep neural networks
    without worrying about most of these details.
  prefs: []
  type: TYPE_NORMAL
- en: Training a feedforward neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe walks you through the process of building a feedforward neural network
    using PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Feedforward neural networks, also known as **multilayer perceptrons** (**MLPs**),
    are one of the simplest types of artificial neural networks. The data flows from
    the input layer to the output layer, passing through hidden layers without any
    loop. In this type of neural network, all hidden units in one layer are connected
    to the units of the following layer.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s create a simple feedforward neural network using `PyTorch`. First, we
    need to import the necessary `PyTorch` modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can define a simple feedforward neural network with one hidden layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, `nn.Module` is the base class for all neural network
    modules in `PyTorch`, and our network is a subclass of it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `forward()` method in this class represents the forward pass of the network.
    This is the computation that the network performs when transforming inputs into
    outputs. Here’s a step-by-step explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: The `forward()` method takes an input tensor `x`. This tensor represents the
    input data. Its shape should be compatible with the network’s layers. In this
    case, as the first linear layer (`self.fc1`) expects `10` input features, the
    last dimension of `x` should be `10`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The input tensor is first passed through a linear transformation, represented
    by `self.fc1`. This object is an instance of `PyTorch`’s `nn.Linear` class, and
    it performs a linear transformation that involves multiplying the input data with
    a weight matrix and adding a bias vector. As defined in the `__init__``()` method,
    this layer transforms the 10D space to a 5D space using a linear transformation.
    This reduction is often seen as the neural network “learning” or “extracting”
    features from the input data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output of the first layer is then passed through a `torch.relu()`. This
    is a simple non-linearity that replaces negative values in the tensor with zeros.
    This allows the neural network to model more complex relationships between the
    inputs and the outputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output from the `ReLU``()` function is then passed through another linear
    transformation, `self.fc2`. As before, this object is an instance of `PyTorch`’s
    `nn.Linear` class. This layer reduces the dimensionality of the tensor from `5`
    (the output size of the previous layer) to `1` (the desired output size).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the output of the second linear layer is returned by the `forward()`
    method. This output can then be used for various purposes, such as computing a
    loss for training the network, or as the final output in an inference task (that
    is when the network is used for prediction).
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To train the network, we need a dataset to train on, a loss function, and an
    optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use the same synthetic dataset that we defined for our previous example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use the **mean squared error** (**MSE**) loss for our task, which is
    a common loss function for regression problems. PyTorch provides a built-in implementation
    of this loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use **stochastic gradient descent** (**SGD**) as our optimizer. SGD
    is a type of iterative method for optimizing the objective function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can train our network. We’ll do this for `100` epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: In each epoch, we perform a forward pass, compute the loss, perform a backward
    pass to calculate gradients, and then update our weights.
  prefs: []
  type: TYPE_NORMAL
- en: You have now trained a simple feedforward neural network using `PyTorch`. In
    the upcoming sections, we will dive deeper into more complex network architectures
    and their applications in time series analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Training a recurrent neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Recurrent Neural Networks** (**RNNs**) are a class of neural networks that
    are especially effective for tasks involving sequential data, such as time series
    forecasting and natural language processing.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: RNNs use sequential information by having hidden layers capable of passing information
    from one step in the sequence to the next.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Similar to the feedforward network, we begin by defining our `RNN` class. For
    simplicity, let’s define a single-layer `RNN`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Here, `input_size` is the number of input features per time step, `hidden_size`
    is the number of neurons in the hidden layer, and `output_size` is the number
    of output features. In the `forward()`method, we pass the input `x` and the initial
    hidden state `h0` to the recurrent layer. The RNN returns the output and the final
    hidden state, which we ignore for now. We then take the last output of the sequence
    (`out[:, -1, :]`) and pass it through a fully connected layer to get our final
    output. The hidden states act as the memory of the network, encoding the temporal
    context of the inputs up to the current time step, which is why this type of neural
    network is useful for sequential data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s note some details we used in our code in this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '`x.device`: This refers to the device where the `x` tensor is located. In `PyTorch`,
    tensors can be on the CPU or a GPU, and `.device` is a property that tells you
    where the tensor currently resides. This is particularly important when you are
    running computations on a GPU, as all inputs to a computation must be on the same
    device. In the line of code `h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)`,
    we’re ensuring that the initial hidden state tensor `h0` is on the same device
    as the `x` input tensor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`x.size(0)`: This refers to the size of the `0th` dimension of the tensor `x`.
    In `PyTorch`, `size()` returns the shape of the tensor, and `size(0)` gives the
    size of the first dimension. In the context of this RNN, `x` is expected to be
    a 3D tensor with shape (`batch_size`, `sequence_length`, `num_features`), so `x.size(0)`
    would return the batch size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The training process for RNNs is similar to that of feedforward networks. We’ll
    use the same synthetic dataset, loss function (MSE), and optimizer (SGD) from
    the previous example. However, let’s modify the input data to be 3D, as required
    by the RNN (`batch_size`, `sequence_length`, `num_features`). The three dimensions
    of the input tensor to an RNN represent the following aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: '`batch_size`: This represents the number of sequences in one batch of data.
    In time series terms, you can think of one sample as one sub-sequence (for example,
    the sales of the past five days). So, a batch contains multiple such samples or
    sub-sequences, allowing the model to process and learn from multiple sequences
    simultaneously.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sequence_length`: This is essentially the size of the window you use to look
    at your data. It specifies the number of time steps included in each input sub-sequence.
    For instance, if you’re predicting today’s temperature based on past data, `sequence_length`
    determines how many days back in the past your model looks at each step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_features`: This dimension indicates the number of features (variables)
    in each time step of the data sequence. In the context of time series, a univariate
    series (such as daily temperature at a single location) has one feature per time
    step. In contrast, a multivariate series (such as daily temperature, humidity,
    and wind speed at the same location) has multiple features per time step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s create a synthetic dataset as an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can train our network. We’ll do this for `100` epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Now, we have trained an RNN. This is a big step towards applying these models
    to real-world time series data, which we will discuss in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Training an LSTM neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RNNs suffer from a fundamental problem of “vanishing gradients” where, due to
    the nature of backpropagation in neural networks, the influence of earlier inputs
    on the overall error diminishes drastically as the sequence gets longer. This
    is especially problematic in sequence processing tasks where long-term dependencies
    exist (i.e., future outputs depend on much earlier inputs).
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LSTM networks were introduced to overcome this problem. They use a more complex
    internal structure for each of their cells compared to RNNs. Specifically, an
    LSTM has the ability to decide which information to discard or to store based
    on an internal structure called a cell. This cell uses gates (input, forget, and
    output gates) to control the flow of information into and out of the cell. This
    helps maintain and manipulate the “long-term” information, thereby mitigating
    the vanishing gradient problem.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We begin by defining our `LSTM` class. For simplicity, we’ll define a single-layer
    `LSTM` network. Note that `PyTorch`’s LSTM expects inputs to be 3D in the format
    `batch_size`, `seq_length`, and `num_features`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The `forward()` method is very similar to the one that we introduced earlier
    for RNNs. The main difference resides in the fact that in the RNNs’ case, we initialized
    a single hidden state `h0`, and passed it to the `RNN` layer along with the input
    `x`. In the LSTM, however, you need to initialize both a hidden state `h0` and
    a cell state `c0` because of the internal structure of LSTM cells. These states
    are then passed as a tuple to the `LSTM` layer along with the input `x`.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The training process for LSTM networks is similar to that of feedforward networks
    and RNNs. We’ll use the same synthetic dataset, loss function (MSE), and optimizer
    (SGD) from the previous examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Training a convolutional neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Convolutional neural networks** (**CNNs**) are a class of neural networks
    particularly effective for tasks involving grid-like input data such as images,
    audio spectrograms, and even certain types of time series data.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The central idea of CNNs is to apply a convolution operation on the input data
    with convolutional filters (also known as kernels), which slide over the input
    data to produce output feature maps.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For simplicity, let’s define a single-layer `1D` convolutional neural network,
    which is particularly suited for time series and sequence data. In `PyTorch`,
    we can use the `nn.Conv1d` layer for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: In the `forward` method, we pass the input through a convolutional layer followed
    by a `ReLU``()` activation function and finally pass it through a fully connected
    layer. The `Conv1d` layer expects an input of shape (`batch_size`, `num_channels`,
    and `sequence_length`). Here, `num_channels` refers to the number of input channels
    (equivalent to the number of features in the time series data), and `sequence_length`
    refers to the number of time steps in each sample.
  prefs: []
  type: TYPE_NORMAL
- en: The `Linear` layer will take the output from the `Conv1d` layer and reduce it
    to the desired output size. The input to the `Linear` layer is calculated as `hidden_size*(seq_length-kernel_size+1)`,
    where `hidden_size` is the number of output channels from the `Conv1d` layer,
    and `seq_length-kernel_size+1` is the output sequence length after the convolution
    operation.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The training process for `1D` CNNs is similar to the previous network types.
    We’ll use the same loss function (MSE), and optimizer (SGD), but let’s modify
    the input data to be of size (`batch_size`, `sequence_length`, `num_channels`).
    Recall that the number of channels is equivalent to the number of features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can train our network. We’ll do this for `100` epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we iterate over each epoch. After each training cycle,
    we print the error of the model into the console to monitor the training process.
  prefs: []
  type: TYPE_NORMAL
