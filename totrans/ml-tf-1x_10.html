<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Go Live and Go Big</h1>
                </header>
            
            <article>
                
<p>In this chapter, we are going to learn more about <strong>Amazon Web Services</strong> (<strong>AWS</strong>) and how to create a deep neural network to solve a video action recognition problem. We will show you how to use multiple GPUs for faster training. At the end of the chapter, we will give you a quick overview of Amazon Mechanical Turk Service, which allows us to collect labels and correct the model's results.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Quick look at Amazon Web Services</h1>
                </header>
            
            <article>
                
<p><strong>Amazon Web Services</strong> (<strong>AWS</strong>) is one of the most popular cloud platforms, and was made by Amazon.com. It provides many services, including cloud computing, storage, database services, content delivery, and other functionalities. In this section, we will only focus on virtual server services found on Amazon EC2. Amazon EC2 allows us to create multiple servers that can support the serving of our model and even the training routine. When it comes to serving the model for end users, you can read <a href="b38dd75a-b632-4e7b-b581-202500f4e001.xhtml" target="_blank"><span class="ChapterrefPACKT">Chapter 9</span></a>, <em>Cruise Control - Automation</em>, to learn about TensorFlow Serving. In training, Amazon EC2 has many instance types that we can use. We can use their CPU servers to run our web bot to collect data from the internet. There are several instance types that have multiple NVIDIA GPUs.</p>
<p>Amazon EC2 provides a wide selection of instance types to fit different use cases. The instance types are divided into five categories, as follows:</p>
<ul>
<li>General Purpose</li>
<li>Compute Optimized</li>
<li>Memory Optimized</li>
<li>Storage Optimized</li>
<li>Accelerated Computing Instances</li>
</ul>
<p>The first four categories are best suited to running backend servers. The accelerated computing instances have multiple NVIDIA GPUs that can be used to serve models and train new models with high-end GPUs. There are three types of instances—P2, G2, and F1.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">P2 instances</h1>
                </header>
            
            <article>
                
<p>P2 instances contain high-performance NVIDIA K80 GPUs, each with 2,496 CUDA cores and 12 GB of GPU memory. There are three models of P2, as described in the following table:</p>
<table class="MsoTableGrid">
<tbody>
<tr>
<td>
<p><strong>Model</strong></p>
</td>
<td>
<p><strong>GPUs</strong></p>
</td>
<td>
<p><strong>vCPU</strong></p>
</td>
<td>
<p><strong>Memory (GB)</strong></p>
</td>
<td>
<p><strong>GPU Memory (GB)</strong></p>
</td>
</tr>
<tr>
<td>
<p>p2.xlarge</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>4</p>
</td>
<td>
<p>61</p>
</td>
<td>
<p>12</p>
</td>
</tr>
<tr>
<td>
<p>p2.8xlarge</p>
</td>
<td>
<p>8</p>
</td>
<td>
<p>32</p>
</td>
<td>
<p>488</p>
</td>
<td>
<p>96</p>
</td>
</tr>
<tr>
<td>
<p>p2.16xlarge</p>
</td>
<td>
<p>16</p>
</td>
<td>
<p>64</p>
</td>
<td>
<p>732</p>
</td>
<td>
<p>192</p>
</td>
</tr>
</tbody>
</table>
<p>These models with large GPU memory are best suited for training models. With more GPU memory, we can train the model with a larger batch size and a neural network with lots of parameters.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">G2 instances</h1>
                </header>
            
            <article>
                
<p>G2 instances contain high-performance NVIDIA GPUs, each with 1,536 CUDA cores and 4 GB of GPU memory. There are two models of G2, as described in the following table:</p>
<table class="MsoTableGrid">
<tbody>
<tr>
<td>
<p><strong>Model</strong></p>
</td>
<td>
<p><strong>GPUs</strong></p>
</td>
<td>
<p><strong>vCPU</strong></p>
</td>
<td>
<p><strong>Memory(GB)</strong></p>
</td>
<td>
<p><strong>SSD Storage (GB)</strong></p>
</td>
</tr>
<tr>
<td>
<p>g2.2xlarge</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>8</p>
</td>
<td>
<p>15</p>
</td>
<td>
<p>1 x 60</p>
</td>
</tr>
<tr>
<td>
<p>g2.8xlarge</p>
</td>
<td>
<p>4</p>
</td>
<td>
<p>32</p>
</td>
<td>
<p>60</p>
</td>
<td>
<p>2 x 120</p>
</td>
</tr>
</tbody>
</table>
<p>These models have only 4 GB of GPU memory, so they are limited in training. However, 4 GB of GPU memory is generally enough for serving the model to end users. One of the most important factors is that G2 instances are much cheaper than P2 instances, which allows us to deploy multiple servers under a load balancer for high scalability.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">F1 instances</h1>
                </header>
            
            <article>
                
<p>F1 instances support <strong>field programmable gate arrays</strong> (<strong>FPGAs</strong>). There are two models of F1, as described in the following table:</p>
<table class="MsoTableGrid">
<tbody>
<tr>
<td>
<p><strong>Model</strong></p>
</td>
<td>
<p><strong>GPUs</strong></p>
</td>
<td>
<p><strong>vCPU</strong></p>
</td>
<td>
<p><strong>Memory(GB)</strong></p>
</td>
<td>
<p><strong>SSD Storage (GB)</strong></p>
</td>
</tr>
<tr>
<td>
<p>f1.2xlarge</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>8</p>
</td>
<td>
<p>122</p>
</td>
<td>
<p>470</p>
</td>
</tr>
<tr>
<td>
<p>f1.16xlarge</p>
</td>
<td>
<p>8</p>
</td>
<td>
<p>64</p>
</td>
<td>
<p>976</p>
</td>
<td>
<p>4 x 940</p>
</td>
</tr>
</tbody>
</table>
<p>FPGAs with high memory and computing power are very promising in the field of deep learning. However, TensorFlow and other popular deep learning libraries don't support FPGAs. Therefore, in the next section, we will only cover the prices of P2 and G2 instances.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pricing</h1>
                </header>
            
            <article>
                
<p>Let's explore the pricing of these instances at <a href="https://aws.amazon.com/emr/pricing/"><span class="URLPACKT">https://aws.amazon.com/emr/pricing/</span></a>.</p>
<p>Amazon EC2 offers three pricing options for instances--On-Demand Instance, Reserved Instance, and Spot Instance:</p>
<ul>
<li>On-Demand instance gives you the ability to run the server without disruption. It is suitable if you only want to use the instance for a few days or weeks.</li>
<li>Reserved instance gives you the option to reserve the instance for a one- or three-year term with a significant discount compared to the On-Demand Instance. It is suitable if you want to run the server for production.</li>
<li>Spot instance gives you the option to bid for the server. You can choose the maximum price you are willing to pay per instance hour. This can save you a lot of money. However, these instances can be terminated at any time if someone bids higher than you. It is suitable if your system can handle interruption or if you just want to explore services.</li>
</ul>
<p>Amazon has provided a website to calculate the monthly bill. You can see it at <a href="http://calculator.s3.amazonaws.com/index.html"><span class="URLPACKT">http://calculator.s3.amazonaws.com/index.html</span></a>.</p>
<p>You can click the <span class="packt_screen">Add New Row</span> button and select an instance type.</p>
<p>In the following image, we have selected a <span class="packt_screen">p2.xlarge</span> server. The price for a month is <span class="packt_screen">$658.80</span> at the time of writing:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/c034555a-aa11-4d0c-839e-89085779adfd.png"/></div>
<p>Now click on the <span class="packt_screen">Billing Option</span> column. You will see the price of a reserved instance for a <span class="packt_screen">p2.xlarge</span> server:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/2936c039-f527-4aa6-ae23-724bfbe68789.png"/></div>
<p>There are many other instance types. We suggest that you take a look at the other types and select the server that is best suited to your requirements.</p>
<p>In the next section, we will create a new model that can perform video action recognition with TensorFlow. We will also leverage the training performance using multiple GPUs.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Overview of the application</h1>
                </header>
            
            <article>
                
<p>Human action recognition is a very interesting problem in computer vision and machine learning. There are two popular approaches to this problem,that is,<strong>still image action recognition</strong> and <strong>video action recognition</strong>. In still image action recognition, we can fine-tune a pre-trained model from ImageNet and perform a classification of the actions based on the static image. You can review the previous chapters for more information. In this chapter, we will create a model that can recognize human action from videos. At the end of the chapter, we will show you how to use multiple GPUs to speed up the training process.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Datasets</h1>
                </header>
            
            <article>
                
<p>There are many available datasets that we can use in the training process, as follows:</p>
<ul>
<li>UCF101 (<a href="http://crcv.ucf.edu/data/UCF101.php"><span class="URLPACKT">http://crcv.ucf.edu/data/UCF101.php</span></a>) is an action recognition dataset of realistic action videos with 101 action categories. There are 13,320 videos in total for the 101 action categories, which makes this dataset a great choice for many research papers.</li>
<li>ActivityNet (<a href="http://activity-net.org/"><span class="URLPACKT">http://activity-net.org/</span></a>) is a large-scale dataset for human activity understanding. There are 200 categories with over 648 hours of video. Each category has about 100 videos.</li>
<li>Sports-1M (<a href="http://cs.stanford.edu/people/karpathy/deepvideo/"><span class="URLPACKT">http://cs.stanford.edu/people/karpathy/deepvideo/</span></a>) is another large-scale dataset for sports recognition. There are 1,133,158 videos in total, annotated with 487 sports labels.</li>
</ul>
<p>In this chapter, we will use UCF101 to perform the training process. We also recommend that you try to apply the techniques discussed in this chapter to a large-scale dataset to take full advantage of multiple-GPU training.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing the dataset and input pipeline</h1>
                </header>
            
            <article>
                
<p>The UCF101 dataset contains 101 action categories, such as Basketball shooting, playing guitar, and Surfing. We can download the dataset from <a href="http://crcv.ucf.edu/data/UCF101.php"><span class="URLPACKT">http://crcv.ucf.edu/data/UCF101.php</span></a>.</p>
<p>On the website, you need to download the UCF101 dataset in the file named <kbd>UCF101.rar</kbd>, and the train/test splits for action recognition in the file named <kbd>UCF101TrainTestSplits-RecognitionTask.zip</kbd>. You need to extract the dataset before moving to the next section, where we will perform a pre-processing technique on videos before training.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pre-processing the video for training</h1>
                </header>
            
            <article>
                
<p>UCF101 contains 13,320 video clips with a fixed frame rate and resolution of 25 FPS and 320 x 240 respectively. All video clips are stored in AVI format, so it is not convenient to use them in TensorFlow. Therefore, in this section, we will extract video frames from all the videos into JPEG files. We will only extract video frames at the fixed frame rate of 4 FPS so that we can reduce the input size of the network.</p>
<p>Before we start implementing the code, we need to install the av library from <a href="https://mikeboers.github.io/PyAV/installation.html"><span class="URLPACKT">https://mikeboers.github.io/PyAV/installation.html</span></a>.</p>
<p>First, create a Python package named <kbd>scripts</kbd> in the <kbd>root</kbd> folder. Then, create a new Python file at <kbd>scripts/convert_ucf101.py</kbd>. In the newly created file, add the first code to import and define some parameters, as follows:</p>
<pre style="padding-left: 60px"> import av 
 import os 
 import random 
 import tensorflow as tf 
 from tqdm import tqdm 
 
 FLAGS = tf.app.flags.FLAGS 
 tf.app.flags.DEFINE_string( 
    'dataset_dir', '/mnt/DATA02/Dataset/UCF101', 
    'The folder that contains the extracted content of UCF101.rar' 
 ) 
 
 tf.app.flags.DEFINE_string( 
    'train_test_list_dir',   <br/> '/mnt/DATA02/Dataset/UCF101/ucfTrainTestlist', 
    'The folder that contains the extracted content of  <br/> UCF101TrainTestSplits-RecognitionTask.zip' 
 ) 
 
 tf.app.flags.DEFINE_string( 
    'target_dir', '/home/ubuntu/datasets/ucf101', 
    'The location where all the images will be stored' 
 ) 
 
 tf.app.flags.DEFINE_integer( 
    'fps', 4, 
    'Framerate to export' 
 ) 
 
 def ensure_folder_exists(folder_path): 
    if not os.path.exists(folder_path): 
        os.mkdir(folder_path) 
 
    return folder_path </pre>
<p>In the preceding code, <kbd>dataset_dir</kbd> and <kbd>train_test_list_dir</kbd> are the locations of the folders containing the extracted content of <kbd>UCF101.rar</kbd> and <kbd>UCF101TrainTestSplits-RecognitionTask.zip</kbd> respectively. <kbd>target_dir</kbd> is the folder that all the training images will be stored in. <kbd>ensure_folder_exists</kbd> is a <kbd>utility</kbd> function that creates a folder if it doesn't exist.</p>
<p>Next, let's define the <kbd>main</kbd> function of the Python code:</p>
<pre style="padding-left: 60px"> def main(_): 
    if not FLAGS.dataset_dir: 
        raise ValueError("You must supply the dataset directory with  <br/> --dataset_dir") 
 
    ensure_folder_exists(FLAGS.target_dir) 
    convert_data(["trainlist01.txt", "trainlist02.txt",  <br/> "trainlist03.txt"], training=True) 
    convert_data(["testlist01.txt", "testlist02.txt",  <br/> "testlist03.txt"], training=False) 
 
 if __name__ == "__main__": 
    tf.app.run() </pre>
<p>In the <kbd>main</kbd> function, we create the <kbd>target_dir</kbd> folder and call the <kbd>convert_data</kbd> function which we will create shortly. The <kbd>convert_data</kbd> function takes a list of train/test text files in the dataset and a Boolean called training that indicates whether the text files are for the training process.</p>
<p>Here are some lines from one of the text files:</p>
<pre><strong>ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi 1</strong>
<strong>ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c02.avi 1</strong>
<strong>ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c03.avi 1</strong></pre>
<p>Each line of the text file contains the path to the <kbd>video</kbd> file and the correct label. In this case, we have three video paths from the <kbd>ApplyEyeMakeup</kbd> category, which is the first category in the dataset.</p>
<p>The main idea here is that we read each line of the text files, extract video frames in a JPEG format, and save the location of the extracted files with the corresponding label for further training. Here is the code for the <kbd>convert_data</kbd> function:</p>
<pre style="padding-left: 60px"> def convert_data(list_files, training=False): 
    lines = [] 
    for txt in list_files: 
        lines += [line.strip() for line in  <br/> open(os.path.join(FLAGS.train_test_list_dir, txt))] 
 
    output_name = "train" if training else "test" 
 
    random.shuffle(lines) 
 
    target_dir = ensure_folder_exists(os.path.join(FLAGS.target_dir,  <br/> output_name)) 
    class_index_file = os.path.join(FLAGS.train_test_list_dir,  <br/> "classInd.txt") 
    class_index = {line.split(" ")[1].strip(): int(line.split(" ") <br/> [0]) - 1 for line in open(class_index_file)} 
 
    with open(os.path.join(FLAGS.target_dir, output_name + ".txt"),  <br/> "w") as f: 
        for line in tqdm(lines): 
            if training: 
                filename, _ = line.strip().split(" ") 
            else: 
                filename = line.strip() 
            class_folder, video_name = filename.split("/") 
 
            label = class_index[class_folder] 
            video_name = video_name.replace(".avi", "") 
            target_class_folder =  <br/> ensure_folder_exists(os.path.join(target_dir, class_folder)) 
            target_folder =  <br/> ensure_folder_exists(os.path.join(target_class_folder, video_name)) 
 
            container = av.open(os.path.join(FLAGS.dataset_dir,  <br/>            filename)) 
            frame_to_skip = int(25.0 / FLAGS.fps) 
            last_frame = -1 
            frame_index = 0 
            for frame in container.decode(video=0): 
                if last_frame &lt; 0 or frame.index &gt; last_frame +  <br/>                frame_to_skip: 
                    last_frame = frame.index 
                    image = frame.to_image() 
                    target_file = os.path.join(target_folder,  <br/>                   "%04d.jpg" % frame_index) 
                    image.save(target_file) 
                    frame_index += 1 
            f.write("{} {} {}\n".format("%s/%s" % (class_folder,  <br/>           video_name), label, frame_index)) 
 
    if training: 
        with open(os.path.join(FLAGS.target_dir, "label.txt"), "w")  <br/>        as f: 
            for class_name in sorted(class_index,  <br/>            key=class_index.get): 
                f.write("%s\n" % class_name) </pre>
<p>The preceding code is straightforward. We load the video path from the text files and use the <kbd>av</kbd> library to open the AVI files. Then, we use <kbd>FLAGS.fps</kbd> to control how many frames per second need to be extracted. You can run the <kbd>scripts/convert_ucf101.py</kbd> file using the following command:</p>
<pre><strong>python scripts/convert_ucf101.py</strong></pre>
<p>The total process needs about 30 minutes to convert all the video clips. At the end, the <kbd>target_dir</kbd> folder will contain the following files:</p>
<pre><strong>label.txt  test  test.txt  train  train.txt</strong></pre>
<p>In the <kbd>train.txt</kbd> file, the lines will look like this:</p>
<pre><strong>Punch/v_Punch_g25_c03 70 43</strong>
<strong>Haircut/v_Haircut_g20_c01 33 36</strong>
<strong>BrushingTeeth/v_BrushingTeeth_g25_c02 19 33</strong>
<strong>Nunchucks/v_Nunchucks_g03_c04 55 36</strong>
<strong>BoxingSpeedBag/v_BoxingSpeedBag_g16_c04 17 21</strong></pre>
<p>This format can be understood as follows:</p>
<pre><strong>&lt;Folder location of the video&gt; &lt;Label&gt; &lt;Number of frames in the folder&gt;</strong>  </pre>
<p>There is one thing that you must remember, which is that the labels in <kbd>train.txt</kbd> and <kbd>test.txt</kbd> go from 0 to 100. However, the labels in the UCF101 go from 1 to 101. This is because the <kbd>sparse_softmax_cross_entropy</kbd> function in TensorFlow needs class labels to start from 0.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Input pipeline with RandomShuffleQueue</h1>
                </header>
            
            <article>
                
<p>If you have read <a href="b38dd75a-b632-4e7b-b581-202500f4e001.xhtml"><span class="ChapterrefPACKT">Chapter 9</span></a>, <em>Cruise Control - Automation</em>, you will know that we can use TextLineReader in TensorFlow to simply read the text files line by line and use the line to read the image directly in TensorFlow. However, things get more complex as the data only contains the folder location and the label. Moreover, we only want a subset of frames in one folder. For example, if the number of frames is 30 and we only want 10 frames to train, we will randomize from 0 to 20 and select 10 frames from that point. Therefore, in this chapter, we will use another mechanism to sample the video frames in pure Python and put the selected frame paths into <kbd>RandomShuffleQueue</kbd> for training. We also use <kbd>tf.train.batch_join</kbd> to leverage the training with multiple pre-processing threads.</p>
<p>First, create a new Python file named <kbd>utils.py</kbd> in the <kbd>root</kbd> folder and add the following code:</p>
<pre>def lines_from_file(filename, repeat=False): 
    with open(filename) as handle: 
        while True: 
            try: 
                line = next(handle) 
                yield line.strip() 
            except StopIteration as e: 
                if repeat: 
                    handle.seek(0) 
                else: 
                    raise 
 
if __name__ == "__main__": 
    data_reader = lines_from_file("/home/ubuntu/datasets/ucf101/train.txt", repeat=True) 
 
    for i in range(15): 
        print(next(data_reader)) </pre>
<p>In this code, we create a <kbd>generator</kbd> function named <kbd>lines_from_file</kbd> to read the text files line by line. We also add a <kbd>repeat</kbd> parameter so that the <kbd>generator</kbd> function can read the text from the beginning when it reaches the end of the file.</p>
<p>We have added a main section so you can try to run it to see how the <kbd>generator</kbd> works:</p>
<pre><strong>python utils.py</strong> </pre>
<p>Now, create a new Python file named <kbd>datasets.py</kbd> in the <kbd>root</kbd> folder and add the following code:</p>
<pre style="padding-left: 60px"> import tensorflow as tf 
 import cv2 
 import os 
 import random 
 
 from tensorflow.python.ops import data_flow_ops 
 from utils import lines_from_file 
 
 def sample_videos(data_reader, root_folder, num_samples,  <br/> num_frames): 
    image_paths = list() 
    labels = list() 
    while True: 
        if len(labels) &gt;= num_samples: 
            break 
        line = next(data_reader) 
        video_folder, label, max_frames = line.strip().split(" ") 
        max_frames = int(max_frames) 
        label = int(label) 
        if max_frames &gt; num_frames: 
            start_index = random.randint(0, max_frames - num_frames) 
            frame_paths = list() 
            for index in range(start_index, start_index +  <br/> num_frames): 
                frame_path = os.path.join(root_folder, video_folder,  <br/> "%04d.jpg" % index) 
                frame_paths.append(frame_path) 
            image_paths.append(frame_paths) 
            labels.append(label) 
    return image_paths, labels 
 
 if __name__ == "__main__": 
    num_frames = 5 
    root_folder = "/home/ubuntu/datasets/ucf101/train/" 
    data_reader =  <br/> lines_from_file("/home/ubuntu/datasets/ucf101/train.txt",  <br/> repeat=True) 
 image_paths, labels = sample_videos(data_reader,  <br/> root_folder=root_folder, 
 num_samples=3,  <br/> num_frames=num_frames) 
    print("image_paths", image_paths) 
    print("labels", labels) </pre>
<p>The <kbd>sample_videos</kbd> function is easy to understand. It will receive the <kbd>generator</kbd> object from <kbd>lines_from_file</kbd> function and use the <kbd>next</kbd> function to get the required samples. You can see that we use a <kbd>random.randint</kbd> method to randomize the starting frame position.</p>
<p>You can run the main section to see how the <kbd>sample_videos</kbd> work with the following command:</p>
<pre><strong>python datasets.py</strong></pre>
<p>Up to this point, we have read the dataset text file into the <kbd>image_paths</kbd> and <kbd>labels</kbd> variables, which are Python lists. In the later training routine, we will use a built-in <kbd>RandomShuffleQueue</kbd> in TensorFlow to enqueue <kbd>image_paths</kbd> and <kbd>labels</kbd> into that queue.</p>
<p>Now, we need to create a method that will be used in the training routine to get data from <kbd>RandomShuffleQueue</kbd>, perform pre-processing in multiple threads, and send the data to the <kbd>batch_join</kbd> function to create a mini-batch for training.</p>
<p>In the <kbd>dataset.py</kbd> file, add the following code:</p>
<pre style="padding-left: 60px"> def input_pipeline(input_queue, batch_size=32, num_threads=8,  <br/> image_size=112): 
    frames_and_labels = [] 
    for _ in range(num_threads): 
        frame_paths, label = input_queue.dequeue() 
        frames = [] 
        for filename in tf.unstack(frame_paths): 
            file_contents = tf.read_file(filename) 
            image = tf.image.decode_jpeg(file_contents) 
            image = _aspect_preserving_resize(image, image_size) 
            image = tf.image.resize_image_with_crop_or_pad(image,  <br/>            image_size, image_size) 
            image = tf.image.per_image_standardization(image) 
            image.set_shape((image_size, image_size, 3)) 
            frames.append(image) 
        frames_and_labels.append([frames, label]) 
 
    frames_batch, labels_batch = tf.train.batch_join( 
        frames_and_labels, batch_size=batch_size, 
        capacity=4 * num_threads * batch_size, 
    ) 
    return frames_batch, labels_batch </pre>
<p>In this code, we prepare an array named <kbd>frames_and_labels</kbd> and use a for loop with a <kbd>num_threads</kbd> iteration. This is a very convenient way of adding multi-threading support to the pre-processing process. In each thread, we will call the method <kbd>dequeue</kbd> from the <kbd>input_queue</kbd> to get a <kbd>frame_paths</kbd> and <kbd>label</kbd>. From the <kbd>sample_video</kbd> function in the previous section, we know that <kbd>frame_paths</kbd> is a list of selected video frames. Therefore, we use another for loop to loop through each frame. In each frame, we read, resize, and perform image standardization. This part is similar to the code in <a href="b38dd75a-b632-4e7b-b581-202500f4e001.xhtml" target="_blank"><span class="ChapterrefPACKT">Chapter 9</span></a>, <em>Cruise Control - Automation</em>. At the end of the input pipeline, we add <kbd>frames_and_labels</kbd> with <kbd>batch_size</kbd> parameters. The returned <kbd>frames_batch</kbd> and <kbd>labels_batch</kbd> will be used for a later training routine.</p>
<p>Finally, you should add the following code, which contains the <kbd>_aspect_preserving_resize</kbd> function:</p>
<pre style="padding-left: 60px"> def _smallest_size_at_least(height, width, smallest_side): 
    smallest_side = tf.convert_to_tensor(smallest_side,  <br/> dtype=tf.int32) 
 
    height = tf.to_float(height) 
    width = tf.to_float(width) 
    smallest_side = tf.to_float(smallest_side) 
 
    scale = tf.cond(tf.greater(height, width), 
                    lambda: smallest_side / width, 
                    lambda: smallest_side / height) 
    new_height = tf.to_int32(height * scale) 
    new_width = tf.to_int32(width * scale) 
    return new_height, new_width 
 
 
 def _aspect_preserving_resize(image, smallest_side): 
    smallest_side = tf.convert_to_tensor(smallest_side,  <br/> dtype=tf.int32) 
    shape = tf.shape(image) 
    height = shape[0] 
    width = shape[1] 
    new_height, new_width = _smallest_size_at_least(height, width,  <br/> smallest_side) 
    image = tf.expand_dims(image, 0) 
    resized_image = tf.image.resize_bilinear(image, [new_height,  <br/> new_width], align_corners=False) 
    resized_image = tf.squeeze(resized_image) 
    resized_image.set_shape([None, None, 3]) 
    return resized_image </pre>
<p>This code is the same as what you used in <a href="b38dd75a-b632-4e7b-b581-202500f4e001.xhtml" target="_blank"><span class="ChapterrefPACKT">Chapter 9</span></a>, <em>Cruise Control - Automation</em>.</p>
<p>In the next section, we will create the deep neural network architecture that we will use to perform video action recognitions with 101 categories.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Neural network architecture</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will create a neural network that will take an input of 10 video frames and output the probability over 101 action categories. We will create a neural network based on the conv3d operation in TensorFlow. This network is inspired on the work of D. Tran et al., Learning Spatiotemporal Features with 3D Convolutional Networks. However, we have simplified the model so it is easier to explain in a chapter. We have also used some techniques that are not mentioned by Tran et al., such as batch normalization and dropout.</p>
<p>Now, create a new Python file named <kbd>nets.py</kbd> and add the following code:</p>
<pre style="padding-left: 60px"> import tensorflow as tf 
 from utils import print_variables, print_layers 
 from tensorflow.contrib.layers.python.layers.layers import  <br/> batch_norm 
 def inference(input_data, is_training=False): 
    conv1 = _conv3d(input_data, 3, 3, 3, 64, 1, 1, 1, "conv1") 
    pool1 = _max_pool3d(conv1, 1, 2, 2, 1, 2, 2, "pool1") 
 
    conv2 = _conv3d(pool1, 3, 3, 3, 128, 1, 1, 1, "conv2") 
    pool2 = _max_pool3d(conv2, 2, 2, 2, 2, 2, 2, "pool2") 
     
    conv3a = _conv3d(pool2, 3, 3, 3, 256, 1, 1, 1, "conv3a") 
    conv3b = _conv3d(conv3a, 3, 3, 3, 256, 1, 1, 1, "conv3b") 
    pool3 = _max_pool3d(conv3b, 2, 2, 2, 2, 2, 2, "pool3") 
     
    conv4a = _conv3d(pool3, 3, 3, 3, 512, 1, 1, 1, "conv4a") 
    conv4b = _conv3d(conv4a, 3, 3, 3, 512, 1, 1, 1, "conv4b") 
    pool4 = _max_pool3d(conv4b, 2, 2, 2, 2, 2, 2, "pool4") 
     
    conv5a = _conv3d(pool4, 3, 3, 3, 512, 1, 1, 1, "conv5a") 
    conv5b = _conv3d(conv5a, 3, 3, 3, 512, 1, 1, 1, "conv5b") 
    pool5 = _max_pool3d(conv5b, 2, 2, 2, 2, 2, 2, "pool5") 
 
    fc6 = _fully_connected(pool5, 4096, name="fc6") 
    fc7 = _fully_connected(fc6, 4096, name="fc7") 
    if is_training: 
        fc7 = tf.nn.dropout(fc7, keep_prob=0.5) 
    fc8 = _fully_connected(fc7, 101, name='fc8', relu=False) 
     
    endpoints = dict() 
    endpoints["conv1"] = conv1 
    endpoints["pool1"] = pool1 
    endpoints["conv2"] = conv2 
    endpoints["pool2"] = pool2 
    endpoints["conv3a"] = conv3a 
    endpoints["conv3b"] = conv3b 
    endpoints["pool3"] = pool3 
    endpoints["conv4a"] = conv4a 
    endpoints["conv4b"] = conv4b 
    endpoints["pool4"] = pool4 
    endpoints["conv5a"] = conv5a 
    endpoints["conv5b"] = conv5b 
    endpoints["pool5"] = pool5 
    endpoints["fc6"] = fc6 
    endpoints["fc7"] = fc7 
    endpoints["fc8"] = fc8 
         
    return fc8, endpoints 
 
 if __name__ == "__main__": 
    inputs = tf.placeholder(tf.float32, [None, 10, 112, 112, 3],  <br/> name="inputs") 
    outputs, endpoints = inference(inputs) 
 
    print_variables(tf.global_variables()) 
    print_variables([inputs, outputs]) 
    print_layers(endpoints) </pre>
<p>In the <kbd>inference</kbd> function, we <kbd>call _conv3d</kbd>, <kbd>_max_pool3d</kbd>, and <kbd>_fully_connected</kbd> to create the network. It is not that different to the CNN network for images in previous chapters. At the end of the function, we also create a dictionary named <kbd>endpoints</kbd>, which will be used in the main section to visualize the network architecture.</p>
<p>Next, let's add the code of the <kbd>_conv3d</kbd> and <kbd>_max_pool3d</kbd> functions:</p>
<pre style="padding-left: 60px"> def _conv3d(input_data, k_d, k_h, k_w, c_o, s_d, s_h, s_w, name,  <br/> relu=True, padding="SAME"): 
    c_i = input_data.get_shape()[-1].value 
    convolve = lambda i, k: tf.nn.conv3d(i, k, [1, s_d, s_h, s_w,  <br/> 1], padding=padding) 
    with tf.variable_scope(name) as scope: 
        weights = tf.get_variable(name="weights",  
 shape=[k_d, k_h, k_w, c_i, c_o], 
 regularizer = tf.contrib.layers.l2_regularizer(scale=0.0001), 
                                   <br/> initializer=tf.truncated_normal_initializer(stddev=1e-1,  <br/> dtype=tf.float32)) 
        conv = convolve(input_data, weights) 
        biases = tf.get_variable(name="biases",  
 shape=[c_o], dtype=tf.float32, 
 initializer = tf.constant_initializer(value=0.0)) 
        output = tf.nn.bias_add(conv, biases) 
        if relu: 
            output = tf.nn.relu(output, name=scope.name) 
        return batch_norm(output) 
 
 
 def _max_pool3d(input_data, k_d, k_h, k_w, s_d, s_h, s_w, name,  <br/> padding="SAME"): 
    return tf.nn.max_pool3d(input_data,  
 ksize=[1, k_d, k_h, k_w, 1], 
 strides=[1, s_d, s_h, s_w, 1], padding=padding, name=name) </pre>
<p>This code is similar to the previous chapters. However, we use the built-in <kbd>tf.nn.conv3d</kbd> and <kbd>tf.nn.max_pool3d</kbd> functions instead of <kbd>tf.nn.conv2d</kbd> and <kbd>tf.nn.max_pool3d</kbd> for images. Therefore, we need to add the <kbd>k_d</kbd> and <kbd>s_d</kbd> parameters to give information about the depth of the filters. Moreover, we will need to train this network from scratch without any pre-trained models. So, we need to use the <kbd>batch_norm</kbd> function to add the batch normalization to each layer.</p>
<p>Let's add the code for the fully connected layer:</p>
<pre style="padding-left: 60px"> def _fully_connected(input_data, num_output, name, relu=True): 
    with tf.variable_scope(name) as scope: 
        input_shape = input_data.get_shape() 
        if input_shape.ndims == 5: 
            dim = 1 
            for d in input_shape[1:].as_list(): 
                dim *= d 
            feed_in = tf.reshape(input_data, [-1, dim]) 
        else: 
            feed_in, dim = (input_data, input_shape[-1].value) 
        weights = tf.get_variable(name="weights",  
 shape=[dim, num_output],  
 regularizer = tf.contrib.layers.l2_regularizer(scale=0.0001),                                   <br/> initializer=tf.truncated_normal_initializer(stddev=1e-1,  <br/> dtype=tf.float32)) 
        biases = tf.get_variable(name="biases", 
 shape=[num_output], dtype=tf.float32, 
                                  <br/> initializer=tf.constant_initializer(value=0.0)) 
        op = tf.nn.relu_layer if relu else tf.nn.xw_plus_b 
        output = op(feed_in, weights, biases, name=scope.name) 
        return batch_norm(output) </pre>
<p>This function is a bit different to what we used with images. First, we check that the <kbd>input_shape.ndims</kbd> is equal to 5 instead of 4. Secondly, we add the batch normalization to the output.</p>
<p>Finally, let's open the <kbd>utils.py</kbd> file and add the following <kbd>utility</kbd> functions:</p>
<pre style="padding-left: 60px"> from prettytable import PrettyTable 
 def print_variables(variables): 
    table = PrettyTable(["Variable Name", "Shape"]) 
    for var in variables: 
        table.add_row([var.name, var.get_shape()]) 
    print(table) 
    print("") 
 
 
 def print_layers(layers): 
    table = PrettyTable(["Layer Name", "Shape"]) 
    for var in layers.values(): 
        table.add_row([var.name, var.get_shape()]) 
    print(table) 
    print("") </pre>
<p>Now we can run <kbd>nets.py</kbd> to have a better understanding of the network's architecture:</p>
<pre>    <strong>python nets.py</strong></pre>
<p>In the first part of the console result, you will see a table like this:</p>
<pre>    <strong>+------------------------------------+---------------------+</strong>
    <strong>|           Variable Name            |        Shape        |</strong>
    <strong>+------------------------------------+---------------------+</strong>
    <strong>|          conv1/weights:0           |   (3, 3, 3, 3, 64)  |</strong>
    <strong>|           conv1/biases:0           |        (64,)        |</strong>
    <strong>|       conv1/BatchNorm/beta:0       |        (64,)        |</strong>
    <strong>|   conv1/BatchNorm/moving_mean:0    |        (64,)        |</strong>
    <strong>| conv1/BatchNorm/moving_variance:0  |        (64,)        |</strong>
    <strong>|               ...                  |         ...         |</strong>
    <strong>|           fc8/weights:0            |     (4096, 101)     |</strong>
    <strong>|            fc8/biases:0            |        (101,)       |</strong>
    <strong>|        fc8/BatchNorm/beta:0        |        (101,)       |</strong>
    <strong>|    fc8/BatchNorm/moving_mean:0     |        (101,)       |</strong>
    <strong>|  fc8/BatchNorm/moving_variance:0   |        (101,)       |</strong>
    <strong>+------------------------------------+---------------------+</strong> </pre>
<p>These are the shapes of <kbd>variables</kbd> in the network. As you can see, three <kbd>variables</kbd> that have the text <kbd>BatchNorm</kbd> are added to each layer. These <kbd>variables</kbd> increase the total parameters that the network needs to learn. However, since we will train from scratch, it will be much for harder to train the network without batch normalization. Batch normalization also increases the ability of the network to regularize unseen data.</p>
<p>In the second table of the console, you will see the following table:</p>
<pre>    <strong>+---------------------------------+----------------------+</strong>
    <strong>|          Variable Name          |        Shape         |</strong>
    <strong>+---------------------------------+----------------------+</strong>
    <strong>|             inputs:0            | (?, 10, 112, 112, 3) |</strong>
    <strong>| fc8/BatchNorm/batchnorm/add_1:0 |       (?, 101)       |</strong>
    <strong>+---------------------------------+----------------------+</strong></pre>
<p>These are the shapes of the input and output of the network. As you can see, the input contains 10 video frames of size (112, 112, 3), and the output contains a vector of 101 elements.</p>
<p>In the last table, you will see how the shape of the output at each layer has changed through the network:</p>
<pre>    <strong>+------------------------------------+-----------------------+</strong>
    <strong>|             Layer Name             |         Shape         |</strong>
    <strong>+------------------------------------+-----------------------+</strong>
    <strong>|  fc6/BatchNorm/batchnorm/add_1:0   |       (?, 4096)       |</strong>
    <strong>|  fc7/BatchNorm/batchnorm/add_1:0   |       (?, 4096)       |</strong>
    <strong>|  fc8/BatchNorm/batchnorm/add_1:0   |        (?, 101)       |</strong>
    <strong>|               ...                  |         ...           |</strong>
    <strong>| conv1/BatchNorm/batchnorm/add_1:0  | (?, 10, 112, 112, 64) |</strong>
    <strong>| conv2/BatchNorm/batchnorm/add_1:0  |  (?, 10, 56, 56, 128) |</strong>
    <strong>+------------------------------------+-----------------------+</strong></pre>
<p>In the preceding table, we can see that the output of the <kbd>conv1</kbd> layer has the same size as the input, and the output of the <kbd>conv2</kbd> layer has changed due to the effect of max pooling.</p>
<p>Now, let's create a new Python file named <kbd>models.py</kbd> and add the following code:</p>
<pre style="padding-left: 60px"> import tensorflow as tf 
 
 def compute_loss(logits, labels): 
    labels = tf.squeeze(tf.cast(labels, tf.int32)) 
 
    cross_entropy =  <br/> tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,  <br/> labels=labels) 
    cross_entropy_loss= tf.reduce_mean(cross_entropy) 
    reg_loss =  <br/> tf.reduce_mean(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES <br/> )) 
 
    return cross_entropy_loss + reg_loss, cross_entropy_loss,  <br/> reg_loss 
 
 
 def compute_accuracy(logits, labels): 
    labels = tf.squeeze(tf.cast(labels, tf.int32)) 
    batch_predictions = tf.cast(tf.argmax(logits, 1), tf.int32) 
    predicted_correctly = tf.equal(batch_predictions, labels) 
    accuracy = tf.reduce_mean(tf.cast(predicted_correctly,  <br/>    tf.float32)) 
    return accuracy 
 
 
 def get_learning_rate(global_step, initial_value, decay_steps,  <br/> decay_rate): 
    learning_rate = tf.train.exponential_decay(initial_value,  <br/>    global_step, decay_steps, decay_rate, staircase=True) 
    return learning_rate 
 
 
 def train(total_loss, learning_rate, global_step): 
    optimizer = tf.train.AdamOptimizer(learning_rate) 
    train_op = optimizer.minimize(total_loss, global_step) 
    return train_op </pre>
<p>These functions create the operation to calculate <kbd>loss</kbd>, <kbd>accuracy</kbd>, <kbd>learning rate</kbd>, and perform the train process. This is the same as the previous chapter, so we won't explain these functions.</p>
<p>Now, we have all the functions required to train the network to recognize video actions. In the next section, we will start the training routine on a single GPU and visualize the results on TensorBoard.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training routine with single GPU</h1>
                </header>
            
            <article>
                
<p>In the scripts package, create a new Python file named <kbd>train.py</kbd>. We will start by defining some parameters as follows:</p>
<pre style="padding-left: 60px"> import tensorflow as tf 
 import os 
 import sys 
 from datetime import datetime 
 from tensorflow.python.ops import data_flow_ops 
 
 import nets 
 import models 
 from utils import lines_from_file 
 from datasets import sample_videos, input_pipeline 
 
 # Dataset 
 num_frames = 16 
 train_folder = "/home/ubuntu/datasets/ucf101/train/" 
 train_txt = "/home/ubuntu/datasets/ucf101/train.txt" 
 
 # Learning rate 
 initial_learning_rate = 0.001 
 decay_steps = 1000 
 decay_rate = 0.7 
  
 # Training 
 image_size = 112 
 batch_size = 24 
 num_epochs = 20 
 epoch_size = 28747 
 
 train_enqueue_steps = 100 
 min_queue_size = 1000 
 
 save_steps = 200  # Number of steps to perform saving checkpoints 
 test_steps = 20  # Number of times to test for test accuracy 
 start_test_step = 50 
 
 max_checkpoints_to_keep = 2 
 save_dir = "/home/ubuntu/checkpoints/ucf101" </pre>
<p>These parameters are self-explanatory. Now, we will define some operations for training:</p>
<pre style="padding-left: 60px"> train_data_reader = lines_from_file(train_txt, repeat=True) 
 
 image_paths_placeholder = tf.placeholder(tf.string, shape=(None,  <br/> num_frames), name='image_paths') 
 labels_placeholder = tf.placeholder(tf.int64, shape=(None,),  <br/> name='labels') 
 
 train_input_queue =  <br/> data_flow_ops.RandomShuffleQueue(capacity=10000, 
                                                      <br/> min_after_dequeue=batch_size, 
 dtypes= [tf.string, tf.int64], 
 shapes= [(num_frames,), ()]) 
 
 train_enqueue_op =  <br/> train_input_queue.enqueue_many([image_paths_placeholder,  <br/> labels_placeholder]) 
 
 frames_batch, labels_batch = input_pipeline(train_input_queue,   <br/> batch_size=batch_size, image_size=image_size) 
 
 with tf.variable_scope("models") as scope: 
    logits, _ = nets.inference(frames_batch, is_training=True) 
 
 total_loss, cross_entropy_loss, reg_loss =  <br/> models.compute_loss(logits, labels_batch) 
 train_accuracy = models.compute_accuracy(logits, labels_batch) 
 
 global_step = tf.Variable(0, trainable=False) 
 learning_rate = models.get_learning_rate(global_step,  <br/> initial_learning_rate, decay_steps, decay_rate) 
 train_op = models.train(total_loss, learning_rate, global_step) </pre>
<p>In this code, we get a <kbd>generator</kbd> object from the text file. Then, we create two placeholders for <kbd>image_paths</kbd> and <kbd>labels</kbd>, which will be enqueued to <kbd>RandomShuffleQueue</kbd>. The <kbd>input_pipeline</kbd> function that we created in <kbd>datasets.py</kbd> will receive <kbd>RandomShuffleQueue</kbd> and return a batch of <kbd>frames</kbd> and labels. Finally, we create operations to compute loss, accuracy, and the training operation.</p>
<p>We also want to log the training process and visualize it in TensorBoard. So, we will create some summaries:</p>
<pre style="padding-left: 60px"> tf.summary.scalar("learning_rate", learning_rate) 
 tf.summary.scalar("train/accuracy", train_accuracy) 
 tf.summary.scalar("train/total_loss", total_loss) 
 tf.summary.scalar("train/cross_entropy_loss", cross_entropy_loss) 
 tf.summary.scalar("train/regularization_loss", reg_loss) 
 
 summary_op = tf.summary.merge_all() 
 
 saver = tf.train.Saver(max_to_keep=max_checkpoints_to_keep) 
 time_stamp = datetime.now().strftime("single_%Y-%m-%d_%H-%M-%S") 
 checkpoints_dir = os.path.join(save_dir, time_stamp) 
 summary_dir = os.path.join(checkpoints_dir, "summaries") 
  
 train_writer = tf.summary.FileWriter(summary_dir, flush_secs=10) 
 
 if not os.path.exists(save_dir): 
    os.mkdir(save_dir) 
 if not os.path.exists(checkpoints_dir): 
    os.mkdir(checkpoints_dir) 
 if not os.path.exists(summary_dir): 
    os.mkdir(summary_dir) </pre>
<p><kbd>saver</kbd> and <kbd>train_writer</kbd> will be responsible for saving checkpoints and summaries respectively. Now, let's finish the training process by creating the <kbd>session</kbd> and performing the training loop:</p>
<pre style="padding-left: 60px"> config = tf.ConfigProto() 
 config.gpu_options.allow_growth = True 
 
 with tf.Session(config=config) as sess: 
    coords = tf.train.Coordinator() 
    threads = tf.train.start_queue_runners(sess=sess, coord=coords) 
 
    sess.run(tf.global_variables_initializer()) 
 
    num_batches = int(epoch_size / batch_size) 
 
    for i_epoch in range(num_epochs): 
        for i_batch in range(num_batches): 
            # Prefetch some data into queue 
            if i_batch % train_enqueue_steps == 0: 
                num_samples = batch_size * (train_enqueue_steps + 1) 
 
                image_paths, labels =  <br/> sample_videos(train_data_reader, root_folder=train_folder, 
                                                     <br/> num_samples=num_samples, num_frames=num_frames) 
                print("\nEpoch {} Batch {} Enqueue {}  <br/> videos".format(i_epoch, i_batch, num_samples)) 
 
                sess.run(train_enqueue_op, feed_dict={ 
                    image_paths_placeholder: image_paths, 
                    labels_placeholder: labels 
                }) 
 
            if (i_batch + 1) &gt;= start_test_step and (i_batch + 1) %  <br/> test_steps == 0: 
                _, lr_val, loss_val, ce_loss_val, reg_loss_val,  <br/> summary_val, global_step_val, train_acc_val = sess.run([ 
                    train_op, learning_rate, total_loss,  <br/> cross_entropy_loss, reg_loss, 
                    summary_op, global_step, train_accuracy 
                ]) 
                train_writer.add_summary(summary_val, <br/> global_step=global_step_val) 
  
                print("\nEpochs {}, Batch {} Step {}: Learning Rate  <br/> {} Loss {} CE Loss {} Reg Loss {} Train Accuracy {}".format( 
                    i_epoch, i_batch, global_step_val, lr_val,  <br/> loss_val, ce_loss_val, reg_loss_val, train_acc_val 
                )) 
            else: 
                _ = sess.run(train_op) 
                sys.stdout.write(".") 
                sys.stdout.flush() 
 
          if (i_batch + 1) &gt; 0 and (i_batch + 1) % save_steps ==  0: 
                saved_file = saver.save(sess, 
                                         <br/> os.path.join(checkpoints_dir, 'model.ckpt'), 
                                        global_step=global_step) 
                print("Save steps: Save to file %s " % saved_file) 
 
    coords.request_stop() 
    coords.join(threads) </pre>
<p>This code is very straightforward. We will use the <kbd>sample_videos</kbd> function to get a list of image paths and labels. Then, we will call the <kbd>train_enqueue_op</kbd> operation to add these image paths and labels to <kbd>RandomShuffleQueue</kbd>. After that, the training process can be run by using <kbd>train_op</kbd> without the <kbd>feed_dict</kbd> mechanism.</p>
<p>Now, we can run the training process by calling the following command in the <kbd>root</kbd> folder:</p>
<pre><strong>export PYTHONPATH=.</strong>
<strong>python scripts/train.py</strong></pre>
<p>You may see the <kbd>OUT_OF_MEMORY</kbd> error if your GPU memory isn't big enough for a batch size of 32. In the training process, we created a session with <kbd>gpu_options.allow_growth</kbd> so you can try to change the <kbd>batch_size</kbd> to use your GPU memory effectively.</p>
<p>The training process takes a few hours before it converges. We will take a look at the training process on TensorBoard.</p>
<p>In the directory that you have chosen to save the checkpoints, run the following command:</p>
<pre><strong>tensorboard --logdir .</strong></pre>
<p>Now, open your web browser and navigate to <kbd>http://localhost:6006</kbd>:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/8bef41ea-1d59-44ee-9e19-ff375d4e6768.png"/></div>
<p class="packt_figure">The regularization loss and total loss with one GPU are as follows:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/2a9a5bdf-2a3d-4a94-b80f-5baf06586d7f.png"/></div>
<p>As you can see in these images, the training accuracy took about 10,000 steps to reach 100% accuracy on training data. These 10,000 steps took 6 hours on our machine. It may be different on your configuration.</p>
<p>The training loss is decreasing, and it may reduce if we train longer. However, the training accuracy is almost unchanged after 10,000 steps.</p>
<p>Now, let's move on to the most interesting part of this chapter. We will use multiple GPUs to train and see how that helps.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training routine with multiple GPU</h1>
                </header>
            
            <article>
                
<p>In our experiment, we will use our custom machine instead of Amazon EC2. However, you can achieve the same result on any server with GPUs. In this section, we will use two Titan X GPUs with a batch size of 32 on each GPU. That way, we can compute up to 64 videos in one step, instead of 32 videos in a single GPU configuration.</p>
<p>Now, let's create a new Python file named <kbd>train_multi.py</kbd> in the <kbd>scripts</kbd> package. In this file, add the following code to define some parameters:</p>
<pre style="padding-left: 60px"> import tensorflow as tf 
 import os 
 import sys 
 from datetime import datetime 
 from tensorflow.python.ops import data_flow_ops 
 
 import nets 
 import models 
 from utils import lines_from_file 
 from datasets import sample_videos, input_pipeline 
 
 # Dataset 
 num_frames = 10 
 train_folder = "/home/aiteam/quan/datasets/ucf101/train/" 
 train_txt = "/home/aiteam/quan/datasets/ucf101/train.txt" 
 
 # Learning rate 
 initial_learning_rate = 0.001 
 decay_steps = 1000 
 decay_rate = 0.7 
 
 # Training 
 num_gpu = 2 
 
 image_size = 112 
 batch_size = 32 * num_gpu 
 num_epochs = 20 
 epoch_size = 28747 
 
 train_enqueue_steps = 50 
 
 save_steps = 200  # Number of steps to perform saving checkpoints 
 test_steps = 20  # Number of times to test for test accuracy 
 start_test_step = 50 
 
 max_checkpoints_to_keep = 2 
 save_dir = "/home/aiteam/quan/checkpoints/ucf101" </pre>
<p>These parameters are the same as in the previous <kbd>train.py</kbd> file, except <kbd>batch_size</kbd>. In this experiment, we will use the data parallelism strategy to train with multiple GPUs. Therefore, instead of using 32 for the batch size, we will use a batch size of 64. Then, we will split the batch into two parts; each will be processed by a GPU. After that, we will combine the gradients from the two GPUs to update the weights and biases of the network.</p>
<p>Next, we will use the same operations as before, as follows:</p>
<pre style="padding-left: 60px"> train_data_reader = lines_from_file(train_txt, repeat=True) 
 
 image_paths_placeholder = tf.placeholder(tf.string, shape=(None,  <br/> num_frames), name='image_paths') 
 labels_placeholder = tf.placeholder(tf.int64, shape=(None,),  <br/> name='labels') 
 
 train_input_queue =  <br/> data_flow_ops.RandomShuffleQueue(capacity=10000, 
                                                      <br/> min_after_dequeue=batch_size, 
 dtypes= [tf.string, tf.int64], 
 shapes= [(num_frames,), ()]) 
 
 train_enqueue_op =  <br/> train_input_queue.enqueue_many([image_paths_placeholder,  <br/> labels_placeholder]) 
 
 frames_batch, labels_batch = input_pipeline(train_input_queue,  <br/> batch_size=batch_size, image_size=image_size) 
 
 global_step = tf.Variable(0, trainable=False) 
 learning_rate = models.get_learning_rate(global_step,  <br/> initial_learning_rate, decay_steps, decay_rate) 
 ``` 
 Now, instead of creating a training operation with `models.train`,  <br/> we will create a optimizer and compute gradients in each GPU. 
 ``` 
 optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) 
 
 total_gradients = [] 
 
 frames_batch_split = tf.split(frames_batch, num_gpu) 
 labels_batch_split = tf.split(labels_batch, num_gpu) 
 for i in range(num_gpu): 
    with tf.device('/gpu:%d' % i): 
        with tf.variable_scope(tf.get_variable_scope(), reuse=(i &gt;  <br/> 0)): 
            logits_split, _ = nets.inference(frames_batch_split[i],  <br/> is_training=True) 
            labels_split = labels_batch_split[i] 
            total_loss, cross_entropy_loss, reg_loss =  <br/> models.compute_loss(logits_split, labels_split) 
            grads = optimizer.compute_gradients(total_loss) 
            total_gradients.append(grads) 
            tf.get_variable_scope().reuse_variables() 
 
 with tf.device('/cpu:0'): 
    gradients = models.average_gradients(total_gradients) 
    train_op = optimizer.apply_gradients(gradients, global_step) 
 
    train_accuracy = models.compute_accuracy(logits_split,   <br/> labels_split) </pre>
<p>The gradients will be computed on each GPU and added to a list named <kbd>total_gradients</kbd>. The final gradients will be computed on the CPU using <kbd>average_gradients</kbd>, which we will create shortly. Then, the training operation will be created by calling <kbd>apply_gradients</kbd> on the optimizer.</p>
<p>Now, let's add the following function to the <kbd>models.py</kbd> file in the <kbd>root</kbd> folder to compute the <kbd>average_gradient</kbd>:</p>
<pre style="padding-left: 60px"> def average_gradients(gradients): 
    average_grads = [] 
    for grad_and_vars in zip(*gradients): 
        grads = [] 
        for g, _ in grad_and_vars: 
            grads.append(tf.expand_dims(g, 0)) 
 
        grad = tf.concat(grads, 0) 
        grad = tf.reduce_mean(grad, 0) 
 
        v = grad_and_vars[0][1] 
        grad_and_var = (grad, v) 
        average_grads.append(grad_and_var) 
    return average_grads </pre>
<p>Now, back in the <kbd>train_multi.py</kbd> file, we will create the <kbd>saver</kbd> and <kbd>summaries</kbd> operation to save the <kbd>checkpoints</kbd> and <kbd>summaries</kbd>, like before:</p>
<pre style="padding-left: 60px"> tf.summary.scalar("learning_rate", learning_rate) 
 tf.summary.scalar("train/accuracy", train_accuracy) 
 tf.summary.scalar("train/total_loss", total_loss) 
 tf.summary.scalar("train/cross_entropy_loss", cross_entropy_loss) 
 tf.summary.scalar("train/regularization_loss", reg_loss) 
  
 summary_op = tf.summary.merge_all() 
 
 saver = tf.train.Saver(max_to_keep=max_checkpoints_to_keep) 
 time_stamp = datetime.now().strftime("multi_%Y-%m-%d_%H-%M-%S") 
 checkpoints_dir = os.path.join(save_dir, time_stamp) 
 summary_dir = os.path.join(checkpoints_dir, "summaries") 
 
 train_writer = tf.summary.FileWriter(summary_dir, flush_secs=10) 
 
 if not os.path.exists(save_dir): 
    os.mkdir(save_dir) 
 if not os.path.exists(checkpoints_dir): 
    os.mkdir(checkpoints_dir) 
 if not os.path.exists(summary_dir): 
    os.mkdir(summary_dir) </pre>
<p>Finally, let's add the training loop to train the network:</p>
<pre style="padding-left: 60px"> config = tf.ConfigProto(allow_soft_placement=True) 
 config.gpu_options.allow_growth = True 
 
 sess = tf.Session(config=config) 
 coords = tf.train.Coordinator() 
 threads = tf.train.start_queue_runners(sess=sess, coord=coords) 
  
 sess.run(tf.global_variables_initializer()) 
 
 num_batches = int(epoch_size / batch_size) 
 
 for i_epoch in range(num_epochs): 
    for i_batch in range(num_batches): 
        # Prefetch some data into queue 
        if i_batch % train_enqueue_steps == 0: 
            num_samples = batch_size * (train_enqueue_steps + 1) 
            image_paths, labels = sample_videos(train_data_reader,  <br/> root_folder=train_folder, 
                                                 <br/> num_samples=num_samples, num_frames=num_frames) 
            print("\nEpoch {} Batch {} Enqueue {} <br/> videos".format(i_epoch, i_batch, num_samples)) 
 
            sess.run(train_enqueue_op, feed_dict={ 
                image_paths_placeholder: image_paths, 
                labels_placeholder: labels 
            }) 
 
        if (i_batch + 1) &gt;= start_test_step and (i_batch + 1) %  <br/> test_steps == 0: 
            _, lr_val, loss_val, ce_loss_val, reg_loss_val, <br/> summary_val, global_step_val, train_acc_val = sess.run([ 
                train_op, learning_rate, total_loss, <br/> cross_entropy_loss, reg_loss, 
                summary_op, global_step, train_accuracy 
            ]) 
            train_writer.add_summary(summary_val,  <br/> global_step=global_step_val) 
 
            print("\nEpochs {}, Batch {} Step {}: Learning Rate {} <br/> Loss {} CE Loss {} Reg Loss {} Train Accuracy {}".format( 
                i_epoch, i_batch, global_step_val, lr_val, loss_val, <br/> ce_loss_val, reg_loss_val, train_acc_val 
            )) 
        else: 
            _ = sess.run([train_op]) 
            sys.stdout.write(".") 
            sys.stdout.flush() 
 
        if (i_batch + 1) &gt; 0 and (i_batch + 1) % save_steps == 0: 
            saved_file = saver.save(sess, 
                                    os.path.join(checkpoints_dir,  <br/> 'model.ckpt'), 
                                    global_step=global_step) 
            print("Save steps: Save to file %s " % saved_file) 
 
 coords.request_stop() 
 coords.join(threads) </pre>
<p>The training loop is similar to the previous, except that we have added the <kbd>allow_soft_placement=True</kbd> option to the session configuration. This option will allow TensorFlow to change the placement of <kbd>variables</kbd>, if necessary.</p>
<p>Now we can run the training scripts like before:</p>
<pre><strong>python scripts/train_multi.py</strong></pre>
<p>After a few hours of training, we can look at the TensorBoard to compare the results:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="213" width="548" class=" image-border" src="assets/be48229b-8546-428e-b38f-c7663b088447.png"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="205" width="542" class=" image-border" src="assets/fcb1ec27-d394-49db-b93b-c918b587d0dc.png"/></div>
<p class="packt_figure">Figure 04—Plot on Tensorboard of multiple GPUs training process</p>
<p>As you can see, the training on multiple GPUs achieves 100% accuracy after about 6,000 steps in about four hours on our computer. This almost reduces the training time by half.</p>
<p>Now, let's see how the two training strategies compare:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="208" width="539" class=" image-border" src="assets/c7041563-2279-42b8-b48d-3c5aa41d3289.png"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="202" width="542" class=" image-border" src="assets/aa1e5d15-8f4f-4c60-9a87-7d4e003dbd43.png"/></div>
<p class="packt_figure">Figure 05—Plot on TensorBoard with single and multiple GPUs compared side by side</p>
<p>The orange line is the multiple GPUs result and the blue line is the single GPU result. We can see that the multiple GPUs setup can achieve better results sooner than the single GPU. The different is not very large. However, we can achieve faster training with more and more GPUs. On the P1 instance on Amazon EC2, there are even eight and 16 GPUs. However, the benefit of training on multiple GPUs will be better if we train on large-scale datasets such as ActivityNet or Sports 1M, as the single GPU will take a very long time to converge.</p>
<p>In the next section, we will take a quick look at another Amazon Service, Mechanical Turk.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Overview of Mechanical Turk</h1>
                </header>
            
            <article>
                
<p>Mechanical Turk is a service that allows us to create and manage online human intelligence tasks that will be completed by human workers. There are lots of tasks that humans can do better than computers. Therefore, we can take advantage of this service to support our machine learning system.</p>
<p>You can view this system at <a href="https://www.mturk.com"><span class="URLPACKT">https://www.mturk.com</span></a>. Here is the website of the service:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/623738e4-e367-4e9e-92a0-333b8fe78185.png"/></div>
<p>Here are a couple of examples of tasks that you can use to support your machine learning system:</p>
<ul>
<li><strong>Dataset labeling</strong>: You usually have a lot of unlabeled data, and you can use Mechanical Turk to help you build a consistent ground truth for your machine learning workflow.</li>
<li><strong>Generate dataset</strong>: You can ask the workers to build a large amount of training data. For example, we can ask workers to create text translations or chat sentences for a natural language system. You can ask them to annotate the sentiments of the comments.</li>
</ul>
<p>Beyond labeling, Mechanical Turk can also clean up your messy datasets ready for training, data categorization, and metadata tagging. You can even use this service to have them judge your system output.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>We have taken a look at the Amazon EC2 services to see how many server types we can use. Then, we created a neural network to perform human video action recognition on a single GPU. After that, we applied the data parallelism strategy to speed up the training process. Finally, we had a quick look at the Mechanical Turk service. We hope that you can take advantage of these services to bring your machine learning system to a higher level.</p>


            </article>

            
        </section>
    </body></html>