- en: Convolution Neural Network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Downloading and configuring an image dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning the architecture of a CNN classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using functions to initialize weights and biases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using functions to create a new convolution layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using functions to flatten the densely connected layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining placeholder variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating the first convolution layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating the second convolution layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flattening the second convolution layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating the first fully connected layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying dropout to the first fully connected layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating the second fully connected layer with dropout
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying softmax activation to obtain a predicted class
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining the cost function used for optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing gradient descent cost optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Executing the graph in a TensorFlow session
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating the performance on test data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Convolution neural networks** (**CNN**) are a category of deep learning neural
    networks with a prominent role in building image recognition- and natural language
    processing-based classification models.'
  prefs: []
  type: TYPE_NORMAL
- en: The CNN follows a similar architecture to LeNet, which was primarily designed
    to recognize characters such as numbers, zip codes, and so on. As against artificial
    neural networks, CNN have layers of neurons arranged in three-dimensional space
    (width, depth, and height). Each layer transforms a two-dimensional image into
    a three-dimensional input volume, which is then transformed into a three-dimensional
    output volume using neuron activation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Primarily, CNNs are built using three main types of activation layers: convolution
    layer ReLU, pooling layer, and fully connected layer. The convolution layer is
    used to extract features (spatial relationship between pixels) from the input
    vector (of images) and stores them for further processing after computing a dot
    product with weights (and biases).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Rectified Linear Unit** (**ReLU**) is then applied after convolution to introduce
    non-linearity in the operation.'
  prefs: []
  type: TYPE_NORMAL
- en: This is an element-wise operation (such as a threshold function, sigmoid, and
    tanh) applied to each convolved feature map. Then, the pooling layer (operations
    such as max, mean, and sum) is used to downsize the dimensionality of each feature
    map ensuring minimum information loss. This operation of spatial size reduction
    is used to control overfitting and increase the robustness of the network to small
    distortions or transformations. The output of the pooling layer is then connected
    to a traditional multilayer perceptron (also called the fully connected layer).
    This perceptron uses activation functions such as softmax or SVM to build classifier-based
    CNN models.
  prefs: []
  type: TYPE_NORMAL
- en: The recipes in this chapter will focus on building a convolution neural network
    for image classification using Tensorflow in R. While the recipes will provide
    you with an overview of a typical CNN, we encourage you to adapt and modify the
    parameters according to your needs.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading and configuring an image dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will use the CIFAR-10 dataset to build a convolution neural
    network for image classification. The CIFAR-10 dataset consists of 60,000 32 x
    32 color images of 10 classes, with 6,000 images per class. These are further
    divided into five training batches and one test batch, each with 10,000 images.
  prefs: []
  type: TYPE_NORMAL
- en: 'The test batch contains exactly 1,000 randomly-selected images from each class.
    The training batches contain the remaining images in random order, but some training
    batches may contain more images from one class than another. Between them, the
    training batches contain exactly 5,000 images from each class. The ten outcome
    classes are airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and
    truck. The classes are completely mutually exclusive. In addition, the format
    of the dataset is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first column: The label with 10 classes: airplane, automobile, bird, cat,
    deer, dog, frog, horse, ship, and truck'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The next 1,024 columns: Red pixels in the range of 0 to 255'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The next 1,024 columns: Green pixels in the range of 0 to 255'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The next 1,024 columns: Blue pixels in the range of 0 to 255'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this recipe, you will require R with some packages installed such as `data.table`
    and `imager`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start R (using Rstudio or Docker) and load the required packages.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Download the dataset (binary version) from [http://www.cs.toronto.edu/~kriz/cifar.html](http://www.cs.toronto.edu/~kriz/cifar.html)
    manually or use the following function to download the data in the R environment.
    The function takes the working directory or the downloaded dataset''s location
    path as an input parameter (`data_dir`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the dataset is downloaded and untarred (or unzipped), read it in the R
    environment as train and test datasets. The function takes the filenames of the
    train and test batch datasets (`filenames`) and the number of images to retrieve
    per batch file (`num.images`) as input parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The outcome of the earlier function is a list of red, green, and blue pixel
    dataframes for each image along with their labels. Then, flatten the data into
    a list of two dataframes (one for input and the other for output) using the following
    function, which takes two parameters--a list of input variables (`x_listdata`)
    and a list of output variables (`y_listdata`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the list of input and output train and test dataframes is ready, perform
    sanity checks by plotting the images along with their labels. The function requires
    two mandatory parameters (`index`: image row number and `images.rgb`: flattened
    input dataset) and one optional parameter (`images.lab`: flattened output dataset):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now transform the input data using the min-max standardization technique. The
    `preProcess` function from the package can be used for normalization. The `"range"`
    option of the method performs min-max normalization as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s take a look at what we did in the earlier recipe. In step 2, we downloaded
    the CIFAR-10 dataset from the link mentioned in case it is not present in the
    given link or working directory. In step 3, the unzipped files are loaded in the
    R environment as train and test datasets. The train dataset has a list of 50,000
    images and the test dataset has a list of 10,000 images along with their labels.
    Then, in step 4, the train and test datasets are flattened into a list of two
    dataframes: one with input variables (or images) of length 3,072 (1,024 of red,
    1,024 of green, and 1,024 of blue) and the other with output variables (or labels)
    of length 10 (binary for each class). In step 5, we perform sanity checks for
    the created train and test datasets by generating plots. The following figure
    shows a set of six train images along with their labels. Finally, in step 6, the
    input data is transformed using the min-max standardization technique. An example
    of categories from the CIFAR-10 dataset is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00030.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Learning Multiple Layers of Features from Tiny Images, Alex Krizhevsky, 2009*
    ([http://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf](http://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf)).
    This is also the reference for this section.'
  prefs: []
  type: TYPE_NORMAL
- en: Learning the architecture of a CNN classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The CNN classifier covered in this chapter has two convolution layers followed
    by two fully connected layers in the end, in which the last layer acts as a classifier
    using the softmax `activation` function.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The recipe requires the CIFAR-10 dataset. Thus, the CIFAR-10 dataset should
    be downloaded and loaded into the R environment. Also, images are of size 32 x
    32 pixels.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s define the configuration of the CNN classifier as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each input image (CIFAR-10) is of size 32 x 32 pixels and can be labeled one
    among 10 classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The images of the CIFAR-10 dataset have three channels (red, green, and blue):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The images are stored in one-dimensional arrays of the following length (`img_size_flat`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In the first convolution layer, the size (width x height) of the convolution
    filter is 5 x 5 pixels (`filter_size1`) and the depth (or number) of convolution
    filter is `64` (`num_filters1`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In the second convolution layer, the size and depth of the convolution filter
    is the same as the first convolution layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, the output of the first fully connected layer is the same as the
    input of the second fully connected layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The dimensions and characteristics of an input image are shown in steps 1 and
    2, respectively. Every input image is further processed in a convolution layer
    using a set of filters as defined in steps 4 and 5\. The first convolution layer
    results in a set of 64 images (one for each set filter). In addition, the resolution
    of these images are also reduced to half (because of 2 x 2 max pooling); namely,
    from 32 x 32 pixels to 16 x 16 pixels.
  prefs: []
  type: TYPE_NORMAL
- en: The second convolution layer will input these 64 images and provide an output
    of new 64 images with further reduced resolutions. The updated resolution is now
    8 x 8 pixels (again due to 2 x 2 max pooling). In the second convolution layer,
    a total of 64 x 64 = 4,096 filters are created, which are then further convoluted
    into 64 output images (or channels). Remember that these 64 images of 8 x 8 resolution
    correspond to a single input image.
  prefs: []
  type: TYPE_NORMAL
- en: Further, these 64 output images of 8 x 8 pixels are flattened into a single
    vector of length 4,096 (8 x 8 x 64), as defined in step 3, and are used as an
    input to a fully connected layer of a given set of neurons, as defined in step
    6\. The vector of 4,096 elements is then fed into the first fully connected layer
    of 1,024 neurons. The output neurons are again fed into a second fully connected
    layer of 10 neurons (equal to `num_classes`). These 10 neurons represent each
    of the class labels, which are then used to determine the (final) class of the
    image.
  prefs: []
  type: TYPE_NORMAL
- en: First, the weights of the convolution and fully connected layers are randomly
    initialized till the classification stage (the end of CNN graph). Here, the classification
    error is computed based on the true class and the predicted class (also called
    cross entropy).
  prefs: []
  type: TYPE_NORMAL
- en: Then, the optimizer backpropagates the error through the convolution network
    using the chain rule of differentiation, post which the weights of the layers
    (or filters) are updated such that the error is minimized. This entire cycle of
    one forward and backward propagation is called one iteration. Thousands of such
    iterations are performed till the classification error is reduced to a sufficiently
    low value.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, these iterations are performed using a batch of images instead of
    a single image to increase the efficiency of computation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image represents the convolution network designed in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00032.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Using functions to initialize weights and biases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Weights and biases form an integral part of any deep neural network optimization
    and here we define a couple of functions to automate these initializations. It
    is a good practice to initialize weights with small noise to break symmetry and
    prevent zero gradients. Additionally, a small positive initial bias would avoid
    inactivated neurons, suitable for ReLU activation neurons.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Weights and biases are model coefficients which need to be initialized before
    model compilation. This steps require the `shape` parameter to be determined based
    on input dataset.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following function is used to return randomly initialized weights:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The following function is used to return constant biases:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These functions return TensorFlow variables that are later used as part of a
    Tensorflow graph. The `shape` is defined as a list of attributes defining a filter
    in the convolution layer, which is covered in the next recipe. The weights are
    randomly initialized with a standard deviation equal to `0.1` and biases are initialized
    with a constant value of `0.1`.
  prefs: []
  type: TYPE_NORMAL
- en: Using functions to create a new convolution layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Creating a convolution layer is the primary step in a CNN TensorFlow computational
    graph. This function is primarily used to define the mathematical formulas in
    the TensorFlow graph, which is later used in actual computation during optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The input dataset is defined and loaded. The `create_conv_layer` function presented
    in the recipe takes the following five input parameters and needs to be defined
    while setting-up a convolution layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Input`: This is a four-dimensional tensor (or a list) that comprises a number
    of (input) images, the height of each image (here 32L), the width of each image
    (here 32L), and the number of channels of each image (here 3L : red, blue, and
    green).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Num_input_channels`: This is defined as the number of color channels in the
    case of the first convolution layer or the number of filter channels in the case
    of subsequent convolution layers.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Filter_size`: This is defined as the width and height of each filter in the
    convolution layer. Here, the filter is assumed to be a square.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Num_filters`: This is defined as the number of filters in a given convolution
    layer.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Use_pooling`: This is a binary variable that is used perform 2 x 2 max pooling.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Run the following function to create a new convolution layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the following function to generate plots of convolution layers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the following function to generate plots of convolution layer weights:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The function begins with creating a shape tensor; namely, the list of four integers
    that are the width of a filter, the height of a filter, the number of input channels,
    and the number of given filters. Using this shape tensor, initialize a tensor
    of new weights with the defined shape and create a tensor a new (constant) biases,
    one for each filter.
  prefs: []
  type: TYPE_NORMAL
- en: Once the necessary weights and biases are initialized, create a TensorFlow operation
    for convolution using the `tf$nn$conv2d` function. In our current setup, the strides
    are set to 1 in all four dimensions and padding is set to `SAME`. The first and
    last are set to 1 by default, but the middle two can factor in higher strides.
    A stride is the number of pixels by which we allow the filter matrix to slide
    over the input (image) matrix.
  prefs: []
  type: TYPE_NORMAL
- en: A stride of 3 would mean three pixel jumps across the *x* or *y* axis for each
    filter slide. Smaller strides would produce larger feature maps, thereby requiring
    higher computation for convergence. As the padding is set to `SAME`, the input
    (image) matrix is padded with zeros around the border so that we can apply the
    filter to border elements of the input matrix. Using this feature, we can control
    the size of the output matrix (or feature maps) to be the same as the input matrix.
  prefs: []
  type: TYPE_NORMAL
- en: On convolution, the bias values are added to each filter channel followed by
    pooling to prevent overfitting. In the current setup, 2 x 2 max-pooling (using
    `tf$nn$max_pool`) is performed to downsize the image resolution. Here, we consider
    2 x 2 (`ksize`*)-*sized windows and select the largest value in each window. These
    windows stride by two pixels (`strides`) either in the x or y direction.
  prefs: []
  type: TYPE_NORMAL
- en: On pooling, we add non-linearity to the layer using the ReLU activation function
    (`tf$nn$relu`). In ReLU, each pixel is triggered in the filter and all negative
    pixel values are replaced with zero using the `max(x,0)` function, where *x* is
    a pixel value. Generally, ReLU activation is performed before pooling. However,
    as we are using max-pooling, it doesn't necessarily impact the outcome as such
    because `relu(max_pool(x))` is equivalent to `max_pool(relu(x))`. Thus, by applying
    ReLU post pooling, we can save a lot of ReLU operations (~75%).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the function returns a list of convoluted layers and their corresponding
    weights. The convoluted layer is a four-dimensional tensor with the following
    attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: Number of (input) images, the same as `input`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Height of each image (reduced to half in the case of 2 x 2 max-pooling)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Width of each image (reduced to half in the case of 2 x 2 max-pooling)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of channels produced, one for each convolution filter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using functions to create a new convolution layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The four-dimensional outcome of a newly created convolution layer is flattened
    to a two-dimensional layer such that it can be used as an input to a fully connected
    multilayered perceptron.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The recipe explains how to flatten a convolution layer before building the deep
    learning model. The input to the given function ( `flatten_conv_layer`) is a four-dimensional
    convolution layer that is defined based on previous layer.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Run the following function to flatten the convolution layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The function begins with extracting the shape of the given input layer. As
    stated in previous recipes, the shape of the input layer comprises four integers:
    image number, image height, image width, and the number of color channels in the
    image. The number of features (`num_features`) is then evaluated using a dot-product
    of image height, image weight, and number of color channels.'
  prefs: []
  type: TYPE_NORMAL
- en: Then, the layer is flattened or reshaped into a two-dimensional tensor (using
    `tf$reshape`). The first dimension is set to -1 (which is equal to the total number
    of images) and the second dimension is the number of features.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the function returns a list of flattened layers along with the total
    number of (input) features.
  prefs: []
  type: TYPE_NORMAL
- en: Using functions to flatten the densely connected layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The CNN generally ends with a fully connected multilayered perceptron using
    softmax activation in the output layer. Here, each neuron in the previous convoluted-flattened
    layer is connected to every neuron in the next (fully connected) layer.
  prefs: []
  type: TYPE_NORMAL
- en: The key purpose of the fully convoluted layer is to use the features generated
    in the convolution and pooling stage to classify the given input image into various
    outcome classes (here, 10L). It also helps in learning the non-linear combinations
    of these features to define the outcome classes.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we use two fully connected layers for optimization. This function
    is primarily used to define the mathematical formulas in the TensorFlow graph,
    which is later used in actual computation during optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The (`create_fc_layer`) function takes four input parameters, which are as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Input`: This is similar to the input of the new convolution layer function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Num_inputs`: This is the number of input features generated post flattening
    the convoluted layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Num_outputs`: This is the number of output neurons fully connected with the
    input neurons'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Use_relu`: This takes the binary flag that is set to `FALSE` only in the case
    of the final fully connected layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Run the following function to create a new fully connected layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The function begins with initialing new weights and biases. Then, perform matrix
    multiplication of the input layer with initialized weights and add relevant biases.
  prefs: []
  type: TYPE_NORMAL
- en: If, the fully connected layer is not the final layer of the CNN TensorFlow graph,
    ReLU non-linear activation can be performed. Finally, the fully connected layer
    is returned.
  prefs: []
  type: TYPE_NORMAL
- en: Defining placeholder variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, let's define the placeholder variables that serve as input to
    the modules in a TensorFlow computational graph. These are typically multidimensional
    arrays or matrices in the form of tensors.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The data type of placeholder variables is set to float32 (`tf$float32`) and
    the shape is set to a two-dimensional tensor.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create an input placeholder variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The NULL value in the placeholder allows us to pass non-deterministic arrays
    size.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reshape the input placeholder `x` into a four-dimensional tensor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Create an output placeholder variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the (`true`) classes of the output using argmax:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In step 1, we define an input placeholder variable. The dimensions of the shape
    tensor are `NULL` and `img_size_flat`. The former is set to hold any number of
    images (as rows) and the latter defines the length of input features for each
    image (as columns). In step 2, the input two-dimensional tensor is reshaped into
    a four-dimensional tensor, which can be served as input convolution layers. The
    four dimensions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The first defines the number of input images (currently set to -1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second defines the height of each image (equivalent to image size 32L)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third defines the width of each image (equivalent to image size, again 32L)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fourth defines the number of color channels in each image (here 3L)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In step 3, we define an output placeholder variable to hold true classes or
    labels of the images in `x`. The dimensions of the shape tensor are `NULL` and
    `num_classes`. The former is set to hold any number of images (as rows) and the
    latter defines the true class of each image as a binary vector of length `num_classes`
    (as columns). In our scenario, we have 10 classes. In step 4, we compress the
    two-dimensional output placeholder into a one-dimensional tensor of class numbers
    ranging from 1 to 10.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the first convolution layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, let's create the first convolution layer.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following are the inputs to the function `create_conv_layer` defined in
    the recipe *Using functions to create a new convolution layer*.
  prefs: []
  type: TYPE_NORMAL
- en: '`Input`: This is a four-dimensional reshaped input placeholder variable: `x_image`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Num_input_channels`: This is the number of color channels, namely `num_channels`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Filter_size`: This is the height and width of the filter layer `filter_size1`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Num_filters`: This is the depth of the filter layer, namely `num_filters1`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Use_pooling`: This is the binary flag set to `TRUE`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Run the `create_conv_layer` function with the preceding input parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the `layers` of the first convolution layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the final `weights` of the first convolution layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Generate the first convolution layer plots:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Generate the first convolution layer weight plots:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In steps 1 and 2, we create a first convolution layer of four-dimensions. The
    first dimension (?) represents any number of input images, the second and third
    dimensions represent the height (16 pixels) and width (16 pixels) of each convoluted
    image, and the fourth dimension represents the number of channels (64) produced--one
    for each convoluted filter. In steps 3 and 5, we extract the final weights of
    the convolution layer, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00035.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'In step 4, we plot the output of the first convolution layer, as shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00038.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Creating the second convolution layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, let's create the second convolution layer.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following are the inputs to the function `create_conv_layer` defined in
    the recipe *Using functions to create a new convolution layer*.
  prefs: []
  type: TYPE_NORMAL
- en: '`Input`: This is the four-dimensional output of the first convoluted layer;
    that is, `layer_conv1`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Num_input_channels`: This is the number of filters (or depth) in the first
    convoluted layer, `num_filters1`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Filter_size`: This is the height and width of the filter layer; namely, `filter_size2`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Num_filters`: This is the depth of the filter layer, `num_filters2`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Use_pooling`: This is the binary flag set to `TRUE`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Run the `create_conv_layer` function with the preceding input parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the layers of the second convolution layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the final weights of the second convolution layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Generate the second convolution layer plots:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Generate the second convolution layer weight plots:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In steps 1 and 2, we create a second convolution layer of four dimensions. The
    first dimension (?) represents any number of input images, the second and third
    dimensions represent the height (8 pixels) and width (8 pixels) of each convoluted
    image, and the fourth dimension represents the number of channels (64) produced,
    one for each convoluted filter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In steps 3 and 5, we extract the final weights of the convolution layer, as
    shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00040.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'In step 4, we plot the output of the second convolution layer, as shown in
    the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00042.gif)'
  prefs: []
  type: TYPE_IMG
- en: Flattening the second convolution layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, let's flatten the second convolution layer that we created.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following is the input to the function defined in the recipe Creating the
    second convolution layer, `flatten_conv_layer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Layer`: This is the output of the second convolution layer, `layer_conv2`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Run the `flatten_conv_layer` function with the preceding input parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the flattened layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the number of (input) features generated for each image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Prior to connecting the output of the (second) convolution layer with a fully
    connected network, in step 1, we reshape the four-dimensional convolution layer
    into a two-dimensional tensor. The first dimension (?) represents any number of
    input images (as rows) and the second dimension represents the flattened vector
    of features generated for each image of length 4,096; that is, 8 x 8 x 64 (as
    columns). Steps 2 and 3 validate the dimensions of the reshaped layers and input
    features.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the first fully connected layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, let's create the first fully connected layer.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the inputs to the function defined in the recipe *Using functions
    to flatten the densely connected layer*, `create_fc_layer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Input`: This is the flattened convolution layer; that is, `layer_flat`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Num_inputs`: This is the number of features created post flattening, `num_features`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Num_outputs`: This is the number of fully connected neurons output, `fc_size`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Use_relu`: This is the binary flag set to `TRUE` to incorporate non-linearity
    in the tensor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Run the `create_fc_layer` function with the preceding input parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here, we create a fully connected layer that returns a two-dimensional tensor.
    The first dimension (?) represents any number of (input) images and the second
    dimension represents the number of output neurons (here, 1,024).
  prefs: []
  type: TYPE_NORMAL
- en: Applying dropout to the first fully connected layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, let's apply dropout to the output of the fully connected layer
    to reduce the chance of overfitting. The dropout step involves removing some neurons
    randomly during the learning process.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The dropout is connected to the output of the layer. Thus, model initial structure
    is set up and loaded. For example, in dropout current layer `layer_fc1` is defined,
    on which dropout is applied.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create a placeholder for dropout that can take probability as an input:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Use TensorFlow''s dropout function to handle the scaling and masking of neuron
    outputs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In steps 1 and 2, we can drop (or mask) out the output neurons based on the
    input probability (or percentage). The dropout is generally allowed during training
    and can be turned off (by assigning probability as `1` or `NULL`) during testing.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the second fully connected layer with dropout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, let's create the second fully connected layer along with dropout.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the inputs to the function defined in the recipe *Using functions
    to flatten the densely connected layer*, `create_fc_layer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Input`: This is the output of the first fully connected layer; that is, `layer_fc1`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Num_inputs`: This is the number of features in the output of the first fully
    connected layer, `fc_size`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Num_outputs`: This is the number of the fully connected neurons output (equal
    to the number of labels, `num_classes` )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Use_relu`: This is the binary flag set to `FALSE`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Run the `create_fc_layer` function with the preceding input parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Use TensorFlow''s dropout function to handle the scaling and masking of neuron
    outputs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In step 1, we create a fully connected layer that returns a two-dimensional
    tensor. The first dimension (?) represents any number of (input) images and the
    second dimension represents the number of output neurons (here, 10 class labels).
    In step 2, we provide the option for dropout primarily used during the training
    of the network.
  prefs: []
  type: TYPE_NORMAL
- en: Applying softmax activation to obtain a predicted class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will normalize the outputs of the second fully connected
    layer using softmax activation such that each class has a (probability) value
    restricted between 0 and 1, and all the values across 10 classes add up to 1.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The activation function is applied at the end of the pipeline on predictions
    generated by the deep learning model. Before executing this step, all steps in
    the pipeline need to be executed. The recipe requires the TensorFlow library.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Run the `softmax` activation function on the output of the second fully connected
    layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `argmax` function to determine the class number of the label. It is
    the index of the class with the largest (probability) value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Defining the cost function used for optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The cost function is primarily used to evaluate the current performance of the
    model by comparing the true class labels (`y_true_cls`) with the predicted class
    labels (`y_pred_cls`). Based on the current performance, the optimizer then fine-tunes
    the network parameters, such as weights and biases, to further improve its performance.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The cost function definition is critical as it will decide optimization criteria.
    The cost function definition will require true classes and predicted classes to
    do comparison. The objective function used in this recipe is cross entropy, used
    in multi-classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Evaluate the current performance of each image using the cross entropy function
    in TensorFlow. As the cross entropy function in TensorFlow internally applies
    softmax normalization, we provide the output of the fully connected layer post
    dropout (`layer_fc2_drop`) as an input along with true labels (`y_true`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: In the current cost function, softmax activation function is embedded thus the
    activation function is not required to be defined separately.
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculate the average of the cross entropy, which needs to be minimized using
    an optimizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In step 1, we define a cross entropy to evaluate the performance of classification.
    Based on the exact match between the true and predicted labels, the cross entropy
    function returns a value that is positive and follows a continuous distribution.
    As zero cross entropy ensures a full match, optimizers tend to minimize the cross
    entropy toward the value zero by updating the network parameters such as weights
    and biases. The cross entropy function returns a value for each individual image
    that needs to be further compressed into a single scalar value, which can be used
    in an optimizer. Hence, in step 2, we calculate a simple average of the cross
    entropy output and store it as *cost*.
  prefs: []
  type: TYPE_NORMAL
- en: Performing gradient descent cost optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, let's define an optimizer that can minimize the cost. Post optimization,
    check for CNN performance.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The optimizer definition will require the `cost` recipe to be defined as it
    goes as input to the optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Run an Adam optimizer with the objective of minimizing the cost for a given
    `learning_rate`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the number of `correct_predictions` and calculate the mean percentage
    accuracy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Executing the graph in a TensorFlow session
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Until now, we have only created tensor objects and added them to a TensorFlow
    graph for later execution. In this recipe, we will learn how to create a TensorFlow
    session that can be used to execute (or run) the TensorFlow graph.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we run the graph, we should have TensorFlow installed and loaded in R.
    The installation details can be found in [Chapter 1](part0021.html#K0RQ1-a0a93989f17f4d6cb68b8cfd331bc5ab),
    *Getting Started*.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Load the `tensorflow` library and import the `numpy` package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Reset or remove any existing `default_graph`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Start an `InteractiveSession`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the `global_variables`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Run iterations to perform optimization (`training`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Evaluate the performance of the trained model on test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Steps 1 through 4 are, in a way, the default way to launch a new TensorFlow
    session. In step 4, the variables of weights and biases are initialized, which
    is mandatory before their optimization. Step 5 is primarily to execute the TensorFlow
    session for optimization. As we have a large number of training images, it becomes
    highly difficult (computationally) to calculate the optimum gradient taking all
    the images at once into the optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, a small random sample of 128 images is selected to train the activation
    layer (weights and biases) in each iteration. In the current setup, we run 100
    iterations and report training accuracy for every tenth iteration.
  prefs: []
  type: TYPE_NORMAL
- en: However, these can be increased based on the cluster configuration or computational
    power (CPU or GPU) to obtain higher model accuracy. In addition, a 50% dropout
    rate is used to train the CNN in each iteration. In step 6, we can evaluate the
    performance of the trained model on a test data of 10,000 images.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the performance on test data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will look into the performance of the trained CNN on test
    images using a confusion matrix and plots.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The prerequisite packages for plots are `imager` and `ggplot2`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Get the `actual` or `true` class labels of test images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the predicted class labels of test images. Remember to add `1` to each
    class label, as the starting index of TensorFlow (the same as Python) is 0 and
    that of R is `1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Generate the confusion matrix with rows as true labels and columns as predicted
    labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Generate a plot of the `confusion` matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Run a helper function to plot images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot random misclassified test images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In steps 1 through 3, we extract the true and predicted test class labels and
    create a confusion matrix. The following image shows the confusion matrix of the
    current test predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00046.gif)'
  prefs: []
  type: TYPE_IMG
- en: The test accuracy post 700 training iterations is only ~51% and can be further
    improved by increasing the number of iterations, increasing the batch size, configuring
    layer parameters such as the number of convolution layers (used 2), types of activation
    functions (used ReLU), number of fully connected layers (used two), optimization
    objective function (used accuracy), pooling (used max 2 x 2), dropout probability,
    and many others.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4 is used to build a facet plot of the test confusion matrix, as shown
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00048.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In step 5, we define a helper function to plot the image along with a header
    containing both true and predicted classes. The input parameters of the `check.image`
    function are (`test`) flattened input dataset (`images.rgb`), image number (`index`),
    true label (`true_lab`*),* and predicted label (`pred_lab`). Here, the red, green,
    and blue pixels are initially parsed out, converted into a matrix, appended as
    a list, and displayed as an image using the *plot* function.
  prefs: []
  type: TYPE_NORMAL
- en: 'In step 6, we plot misclassified test images using the helper function of step
    5\. The input parameters of the `plot.misclass.images` function are (`test`) flattened
    input dataset (`images.rgb`), a vector of true labels (`y_actual`), a vector of
    predicted labels (`y_predicted`), and a vector of unique ordered character labels
    (`labels`). Here, the indices of the misclassified images are obtained and an
    index is randomly selected to generate the plot. The following screenshot shows
    a set of six misclassified images with true and predicted labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00050.jpeg)'
  prefs: []
  type: TYPE_IMG
