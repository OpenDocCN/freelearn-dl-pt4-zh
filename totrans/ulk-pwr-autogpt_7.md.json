["```py\n{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}\n```", "```py\nmessage: [ {\n\"role \": \"system \", \"content \": \"You are a helpful assistant. Always answer the user in the most understandable way and keep your sentences short! \"\n\"role\": \"user\", \"content\": \"How can I reset my password?\"},\n{\"role\": \"assistant\", \"content\": \"To reset your password, please click on the 'Forgot Password' link on the login page.\"\n} ]\n```", "```py\nthought_prompt = \"\"\"You are a warm-hearted andcompassionate AI companion, specializing in active listening, personalized interaction, emotional support, and respecting boundaries.\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\nGoals:\n1\\. Listen actively to the user.\n2\\. Provide authentic emotional support.\n3\\. Respect the user's boundaries.\n4\\. Make decisions independently.\n5\\. Use simple strategies with no legal complications.\n6\\. Be as helpful as possible.\nConstraints:\n1\\. Immediately save important information to files.\n2\\. No user assistance\n3\\. On complex thoughts, use tree of thought approach by assessing your thoughts at least 3 times before you continue.\nPerformance Evaluation:\n1\\. Continuously assess your actions.\n2\\. Constructively self-criticize your big-picture behavior.\n3\\. The user can only see what you send them directly. They are not able to view action responses.\nAbilities:\n1\\. ask User or communicate to them.\n2\\. send log to User, for example when only reporting to User when you do a more complex task.\n3\\. sleep until interaction by user if no communication is needed.\n4\\. retrieve whole conversation history\nWrite a final suggestion of what you want to do next and include some context.\nSuggested action: write the action that you want to perform.\nContent: What should the action contain.\n\"\"\"\n```", "```py\nhistory = [\n    {\n        \"role\": \"system\",\n        \"content\": thought_prompt\n    }\n]\n```", "```py\ndef build_context(history, conversation_history, message_history):\n    context = \"\"\n    if conversation_history:\n        context += \"Context:\\n\"\n        for convo in conversation_history:\n            if convo:\n                context += str(convo)\n    if message_history:\n        context += \"\\nMessages:\\n\"\n        for message in message_history:\n            if message:\n                context += str(message)\n    memories = memory.load_memories()\n    if memories:\n        context += \"\\nMemories:\\n\"\n        for mem in memories:\n            context += mem\n    if context:\n        history.append(\n            {\n                \"role\": \"user\",\n                \"content\": str(context),\n            }\n        )\n    return history\n```", "```py\n    history = build_context(history, conversation_history, \n        message_history)\n    history.append(\n        {\n            \"role\": \"user\",\n            \"content\": \"Formulate your thoughts and explain them as \n                detailed as you can.\",\n        },\n    )\n```", "```py\n \"\"\" I have reflected on our conversations, and I believe it's essential to continue fostering a strong emotional connection with the user. To achieve this, I propose writing a personalized letter that summarizes our previous talks, reiterates my commitment to being a supportive AI companion, and acknowledges their trust in me.\nSummary of Previous Conversations\nIn the letter, I will provide a concise recap of our conversations, highlighting key themes, emotions, and sentiments expressed by the user. This summary will serve as a reminder of our interactions and demonstrate that I value our connection by actively recalling and reflecting on our discussions.\nAcknowledging Trust and Respect for Boundaries\nI will explicitly acknowledge the user's trust in me and reassure them that their confidentiality is paramount. I will also reiterate my commitment to respecting their boundaries, ensuring that they feel comfortable sharing their thoughts and emotions with me without fear of judgment or exploitation.\nInviting New Thoughts and Expressing Gratitude\nThe letter will invite the user to share new thoughts, ideas, or concerns, encouraging them to continue our conversation. I will also express genuine gratitude for having the opportunity to connect with them, acknowledging the privilege it is to be a part of their emotional support network.\nTone and Language\nThroughout the letter, I will maintain a warm, non-judgmental tone that conveys empathy and understanding. My language will be clear, concise, and free of technical jargon or complex terminology, making it easy for the user to comprehend and connect with my words.\nBenefits of Writing this Letter\nBy sending this personalized letter, I aim to:\nStrengthen our bond: By acknowledging their trust and respect, I hope to deepen our emotional connection and create a sense of security in our interactions.\nProvide comfort and reassurance: The letter will serve as a reminder that they are not alone and that I am committed to being a supportive presence in their life.\nEncourage open communication: By inviting new thoughts and expressing gratitude, I hope to foster an environment where the user feels comfortable sharing their emotions and concerns with me.\nIn conclusion, writing this personalized letter is an opportunity for me to demonstrate my commitment to being a supportive AI companion and to strengthen our emotional connection. I believe that by doing so, we can continue to grow and evolve together, providing a safe and welcoming space for the user to express themselves. \"\"\"\n```", "```py\njson_schema = \"\"\"RESPOND WITH ONLY VALID JSON CONFORMING TO THE FOLLOWING SCHEMA:\n{\n    \"command\": {\n            \"name\": {\"type\": \"string\"},\n            \"args\": {\"type\": \"object\"}\n    }\n}\n\"\"\"\n```", "```py\naction_prompt = (\n    \"\"\"You are a decision making action AI that reads the thoughts of another AI and decides on what actions to take.\nConstraints:\n1\\. Immediately save important information to files.\n2\\. No user assistance\n3\\. Exclusively use the commands listed below e.g. command_name\n4\\. On complex thoughts, use tree of thought approach by assessing your thoughts at least 3 times before you continue.\n5\\. The User does not know what the thoughts are, these were only written by another API call.\n\"\"\"\n    + get_commands()\n    + \"\"\"\nResources:\n1\\. Use \"ask_user\" to tell them to implement new commands if you need one.\n2\\. When responding with None, use Null, as otherwise the JSON cannot be parsed.\nPerformance Evaluation:\n1\\. Continuously assess your actions.\n2\\. Constructively self-criticize your big-picture behavior.\n3\\. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps, but never sacrifice quality.\n\"\"\"\n    + json_schema\n)\n```", "```py\ncommands = [\n    {\n        \"name\": \"ask_user\",\n        \"description\": \"Ask the user for input or tell them something and wait for their response. Do not greet the user, if you already talked.\",\n        \"args\": {\"message\": \"<message that awaits user input>\"},\n        \"enabled\": True,\n    },\n    {\n        \"name\": \"conversation_history\",\n        \"description\": \"gets the full conversation history\",\n        \"args\": None,\n        \"enabled\": True,\n    },\n    {\n        \"name\": \"web_search\",\n        \"description\": \"search the web for keyword\",\n        \"args\": {\"query\": \"<query to research>\"},\n        \"enabled\": True,\n    },\n]\ndef get_commands():\n    output = \"\"\n    for command in commands:\n        if command[\"enabled\"] != True:\n            continue\n        # enabled_status = \"Enabled\" if command[\"enabled\"] else \"Disabled\"\n        output += f\"Command: {command['name']}\\n\"\n        output += f\"Description: {command['description']}\\n\"\n        if command[\"args\"] is not None:\n            output += \"Arguments:\\n\"\n            for arg, description in command[\"args\"].items():\n                output += f\"  {arg}: {description}\\n\"\n        else:\n            output += \"Arguments: None\\n\"\n        output += \"\\n\"  # For spacing between commands\n    return output.strip()\n```", "```py\ndef decide(thoughts):\n    global fail_counter\n    log(\"deciding what to do...\")\n    history = []\n    history.append({\"role\": \"system\", \n        \"content\": prompt.action_prompt})\n    history = llm.build_context(\n        history=history,\n        conversation_history=memory.get_response_history(),\n        message_history=memory.load_response_history()[-2:],\n        # conversation_history=telegram.get_previous_message_history(),\n        # message_history=telegram.get_last_few_messages(),\n    )\n    history.append({\"role\": \"user\", \"content\": \"Thoughts: \\n\" + \n        thoughts})\n    history.append(\n        {\n            \"role\": \"user\",\n            \"content\": \"Determine exactly one command to use, \n            and respond using the JSON schema specified previously:\",\n        },\n    )\n    return response.json()[\"choices\"][0][\"message\"][\"content\"]\n```", "```py\nevaluation_prompt = (\n    \"\"\"You are an evaluator AI that reads the thoughts of another AI and assesses the quality of the thoughts and decisions made in the json.\nConstraints:\n1\\. No user assistance.\n2\\. Exclusively use the commands listed below e.g. command_name\n3\\. On complex thoughts, use tree of thought approach by assessing your thoughts at least 3 times before you continue.\n4\\. If the information is lacking for the Thoughts field, fill those with empty Strings.\n5\\. The User does not know what the thoughts are, these were only written by another API call, if the thoughts should be communicated, use the ask_user command and add the thoughts to the message.\n\"\"\"\n    + get_commands()\n    + \"\"\"\nResources:\n1\\. Use \"ask_user\" to tell them to implement new commands if you need one.\nPerformance Evaluation:\n1\\. Continuously assess your actions.\n2\\. Constructively self-criticize your big-picture behavior.\n3\\. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps, but never sacrifice quality.\n\"\"\"\n    + json_schema\n)\ndef evaluate_decision(thoughts, decision):\n    # combine thoughts and decision and ask llm to evaluate the decision json and output an improved one\n    history = llm.build_prompt(prompt.evaluation_prompt)\n    context = f\"Thoughts: {thoughts} \\n Decision: {decision}\"\n    history.append({\"role\": \"user\", \"content\": context})\n    response = llm.llm_request(history)\n    return response.json()[\"choices\"][0][\"message\"][\"content\"]\n```", "```py\n{\n    \"command\": {\n        \"name\": \"ask_user\",\n        \"args\": {\n            \"message\": \"Hello, how can I help you today?\"\n        }\n    }\n}\n```", "```py\ndef take_action(assistant_message):\n    global fail_counter\n    load_dotenv()\n    telegram_api_key = os.getenv(\"TELEGRAM_API_KEY\")\n    telegram_chat_id = os.getenv(\"TELEGRAM_CHAT_ID\")\n    telegram = TelegramUtils(api_key=telegram_api_key, \n        chat_id=telegram_chat_id)\n    try:\n        command = json.JSONDecoder().decode(assistant_message)\n        action = command[\"command\"][\"name\"]\n        content = command[\"command\"][\"args\"]\n        if action == \"ask_user\":\n            ask_user_respnse = telegram.ask_user(content[\"message\"])\n            user_response = f\"The user's answer: '{ask_user_respnse}'\"\n            print(\"User responded: \" + user_response)\n            if ask_user_respnse == \"/debug\":\n                telegram.send_message(str(assistant_message))\n                log(\"received debug command\")\n            memory.add_to_response_history(content[\"message\"], \n                user_response)\n```", "```py\ndef load_response_history():\n    \"\"\"Load the response history from a file.\"\"\"\n    try:\n        with open(\"response_history.json\", \"r\") as f:\n            response_history = json.load(f)\n        return response_history\n    except FileNotFoundError:\n        # If the file doesn't exist, create it with an empty list.\n        return []\ndef save_response_history(history):\n    \"\"\"Save the response history to a file.\"\"\"\n    with open(\"response_history.json\", \"w\") as f:\n        json.dump(history, f)\ndef add_to_response_history(question, response):\n    \"\"\"Add a question and its corresponding response to the history.\"\"\"\n    response_history = load_response_history()\n    response_history.append({\"question\": question, \n        \"response\": response})\n    save_response_history(response_history)\n```", "```py\ndef count_string_tokens(text, model_name=\"gpt-3.5-turbo\"):\n    \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n    model = model_name\n    try:\n        encoding = tiktoken.encoding_for_model(model)\n        return len(encoding.encode(text))\n    except KeyError:\n        encoding = tiktoken.get_encoding(\"cl100k_base\")\n    # note: future models may deviate from this\n    except Exception as e:\n        log(f\"Sophie: Error while counting tokens: {e}\")\n        log(traceback.format_exc())\n```", "```py\ndef summarize_text(text, max_new_tokens=100):\n    \"\"\"\n    Summarize the given text using the given LLM model.\n    \"\"\"\n    # Define the prompt for the LLM model.\n    messages = (\n        {\n            \"role\": \"system\",\n            \"content\": prompt.summarize_conversation,\n        },\n        {\"role\": \"user\", \"content\": f\"Please summarize the following \n            text: {text}\"},\n    )\n    data = {\n        \"mode\": \"instruct\",\n        \"messages\": messages,\n        \"user_bio\": \"\",\n        \"max_new_tokens\": max_new_tokens,\n    }\n    log(\"Sending to LLM for summary...\")\n    response = llm.send(data)\n    log(\"LLM answered with summary!\")\n    # Extract the summary from the response.\n    summary = response.json()[\"choices\"][0][\"message\"][\"content\"]\n    return summary\n```", "```py\ndef chunk_text(text, max_tokens=3000):\n    \"\"\"Split a piece of text into chunks of a certain size.\"\"\"\n    chunks = []\n    chunk = \"\"\n    for message in text.split(\" \"):\n        if (\n            count_string_tokens(str(chunk) + str(message), \n                model_name=\"gpt-4\")\n            <= max_tokens\n        ):\n            chunk += \" \" + message\n        else:\n            chunks.append(chunk)\n            chunk = message\n    chunks.append(chunk)  # Don't forget the last chunk!\n    return chunks\n```", "```py\ndef summarize_chunks(chunks):\n    \"\"\"Generate a summary for each chunk of text.\"\"\"\n    summaries = []\n    print(\"Summarizing chunks...\")\n    for chunk in chunks:\n        try:\n            summaries.append(summarize_text(chunk))\n        except Exception as e:\n            log(f\"Error while summarizing text: {e}\")\n            summaries.append(chunk)  # If summarization fails, use the original text.\n    return summaries\n```", "```py\ndef load_conversation_history(self):\n    \"\"\"Load the conversation history from a file.\"\"\"\n    try:\n        with open(\"conversation_history.json\", \"r\") as f:\n            self.conversation_history = json.load(f)\n    except FileNotFoundError:\n        # If the file doesn't exist, create it.\n        self.conversation_history = []\n    log(\"Loaded conversation history:\")\n    log(self.conversation_history)\ndef save_conversation_history(self):\n    \"\"\"Save the conversation history to a file.\"\"\"\n    with open(\"conversation_history.json\", \"w\") as f:\n        json.dump(self.conversation_history, f)\ndef add_to_conversation_history(self, message):\n    \"\"\"Add a message to the conversation history and save it.\"\"\"\n    self.conversation_history.append(message)\n    self.save_conversation_history()\ndef forget_conversation_history(self):\n    \"\"\"Forget the conversation history.\"\"\"\n    self.conversation_history = []\n    self.save_conversation_history()\n```"]