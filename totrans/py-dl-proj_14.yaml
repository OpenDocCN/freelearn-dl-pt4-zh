- en: Develop an Autonomous Agent with Deep R Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Welcome to the chapter on reinforcement learning. In the previous chapters,
    we have worked on solving supervised learning problems. In this chapter, we will
    learn to build and train a deep reinforcement learning model capable of playing
    games.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning is often a new paradigm for deep learning engineers and
    this is why we're using the framework of a game for this training. The business
    use cases that we should be looking out for are typified by process optimization.
    Reinforcement learning is great for gaming, but also applicable in use cases ranging
    from drone control ([https://arxiv.org/pdf/1707.05110.pdf](https://arxiv.org/pdf/1707.05110.pdf))
    and navigation to optimizing file downloads over mobile networks ([http://anrg.usc.edu/www/papers/comsnets_2017.pdf](http://anrg.usc.edu/www/papers/comsnets_2017.pdf)).
  prefs: []
  type: TYPE_NORMAL
- en: 'We will do this with something called deep Q-learning and deep **State-Action-Reward-State-Action**
    (**SARSA**) learning. The idea is that we will build a deep learning model, also
    called an agent in reinforcement learning terms, that interacts with the game
    environment and learns how to play the game while maximizing rewards after several
    attempts at playing. Here is a  diagram illustrating reinforcement learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/599cfdb5-da3b-4f20-aaa2-75dfec68c33c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.1: Reinforcement learning'
  prefs: []
  type: TYPE_NORMAL
- en: For the purpose of this chapter, we will be using the CartPole game from OpenAI
    Gym.
  prefs: []
  type: TYPE_NORMAL
- en: 'What we''ll learn in this chapter is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: How to interact with the Gym toolkit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is Q-learning and SARSA learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coding the RL model and defining hyperparameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building and understanding the training loop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It would be better if you implement the code snippets as you go along in this
    chapter, either in a Jupyter Notebook or any source code editor. This will make
    it easier for you to follow along, as well as understand what each part of the
    code does.
  prefs: []
  type: TYPE_NORMAL
- en: All of the Python and the Jupyter Notebook files for this chapter can be found
    [at https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter14](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter14).
  prefs: []
  type: TYPE_NORMAL
- en: Let's get to the code!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this exercise, we will be using the Gym toolkit from OpenAI for developing
    reinforcement learning models. It supports teaching agents such as CartPole and
    pinball games.
  prefs: []
  type: TYPE_NORMAL
- en: To know more about the Gym toolkit from OpenAI and the games it supports, visit [http://gym.openai.com/](http://gym.openai.com/).
  prefs: []
  type: TYPE_NORMAL
- en: We will also be using the Keras deep learning library, which is a high-level
    neural network API capable of running on top of TensorFlow, Theano, or Cognitive
    Toolkit (CNTK).
  prefs: []
  type: TYPE_NORMAL
- en: To learn more about Keras and its functionalities visit [https://keras.io/](https://keras.io/).
  prefs: []
  type: TYPE_NORMAL
- en: Deep Q-learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this segment, we will implement deep Q-learning with a deep learning model
    built using the Keras deep learning library as the function approximator.
  prefs: []
  type: TYPE_NORMAL
- en: We will start off this segment with a gentle introduction as to how to use the
    Gym module and then move on to understanding what Q-learning is, and finally,
    implement the deep Q-learning. We will be using the CartPole environment from
    OpenAI Gym.
  prefs: []
  type: TYPE_NORMAL
- en: To follow along, refer to the Jupyter Notebook code file for the deep Q-learning
    section at [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter%2015/DQN.ipynb](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter%2015/DQN.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: Importing all of the dependencies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will be using `numpy`, `gym`, `matplotlib`, `keras`, and `tensorflow` packages
    in this segment of the exercise. Here, TensorFlow will be used as the backend
    for Keras. You can install these packages using `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '`deque` is a list-like container with fast appends and pops on either end.'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the CartPole game
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the CartPole game, you will find a pole attached by an unattached joint to
    the cart, which moves on a frictionless track. At the beginning of each game,
    the pole starts in the upright position and the goal is to hold it in the upright
    position as long as possible or for a given number of time steps. You can control
    the CartPole system by applying a force of +1 and -1 (to move the cart either
    to the right or to the left) and prevent the pole from falling over. The game/episode
    ends when the cart moves more than 2.4 units from the center or when the pole
    is more than 45 degrees from the vertical.
  prefs: []
  type: TYPE_NORMAL
- en: Interacting with the CartPole game
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenAI Gym makes it super easy to interact with the game. In this section, we
    will cover how to load, reset, and play the CartPole game.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the game
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s load the `CartPole-v1` game from the `gym` module. It''s very simple.
    All you have to do is feed the `gym.make()` function the name of the game. In our
    case, the game is `CartPole-v1`. Gym then loads the game into your workspace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'It is important that you set `seed` for reproducibility:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s explore how many variables we have in the CartPole game:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6ad4918d-90a9-4b51-bd73-f8e682edde99.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that the CartPole has `4` variables and these are namely the position
    (`x`), velocity (`x_dot`), angular position (`theta`), and the angular velocity
    (`theta_dot`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s explore how many possible responses we have in this game using the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/32179474-c616-4311-bf21-8b2bcb7aad96.png)'
  prefs: []
  type: TYPE_IMG
- en: We see that the CartPole environment has `2` possible responses/buttons, namely
    move left and move right.
  prefs: []
  type: TYPE_NORMAL
- en: Resetting the game
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can reset the game with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The preceding snippet will reset the game and also return you the state (`x`,
    `x_dot`, `theta`, `theta_dot`) of the CartPole after the reset, which will be
    an array of the shape of  (`4`,).
  prefs: []
  type: TYPE_NORMAL
- en: Playing the game
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, once you have reset the game, all there is to do is play. You can feed
    your actions/responses to the game with the use of the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `env.step` function accepts your response/action (move left or right) and
    generates the `new_state`/orientation (x, x_dot, theta, theta_dot) of the CartPole
    system. Along with the new state, the `env.step` function also returns the `reward`,
    which indicates the score you receive for the `action` you just took; `done`,
    which indicates if the game has finished; and `info`, which has system-related
    information.
  prefs: []
  type: TYPE_NORMAL
- en: When the game begins, `done` is set to `False`. Only when the CartPole orientation
    exceeds the game rules will `done` be set to `True`, indicating that either the
    cart moved 2.4 units from the center or the pole was more than 45 degrees from
    the vertical.
  prefs: []
  type: TYPE_NORMAL
- en: As long as every step you take is within the game over limits, the reward for
    that step will be 1 unit, otherwise zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s play the game by making random actions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the Terminal output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fa4390d9-7bf3-4023-9a24-07358b86bbe7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.2: Scores from random actions game'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the CartPole game output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a3a94675-ba98-4953-8e54-f72402e48d24.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.3: Snapshot of the CartPole game that gets displayed on the screen
    when rendered'
  prefs: []
  type: TYPE_NORMAL
- en: '`random.choice` returns a randomly selected item from a non-empty sequence
    such as a list/array.'
  prefs: []
  type: TYPE_NORMAL
- en: Q-learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Q-learning is a policy-based reinforcement learning technique where the goal
    of Q-learning is to learn an optimal policy that helps an agent decide what action
    to take under which circumstances of the environment.
  prefs: []
  type: TYPE_NORMAL
- en: To implement Q-learning, you need to understand what a *Q* function is.
  prefs: []
  type: TYPE_NORMAL
- en: A *Q* function accepts a state and a corresponding action as input and yields
    the total expected reward. It can be expressed as *Q(s, a)*. When at the *s* state,
    an optimal *Q* function indicates to the agent how good of a choice is picking
    an action, *a*.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a single state, *s,* and an action, *a*, *Q(s, a)* can be expressed in
    terms of the *Q* value of the next state, *s''*, given by using the following
    equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f8058bff-e7d2-4eb6-b543-551e2a944f5d.png)'
  prefs: []
  type: TYPE_IMG
- en: This is known as the Bellman equation. It tells us that the maximum reward is
    the sum of the reward the agent received for entering the current state, *s,*
    and the discounted maximum future reward for the next state, *s'*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the pseudocode for the Q-learning algorithm from the book
    *Reinforcement Learning: An Introduction,* by Richard S. Sutton and Andrew G.
    Barto:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a0808dbd-1be2-487f-83de-3e055b19ffb1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.4: Pseudocode for Q-learning'
  prefs: []
  type: TYPE_NORMAL
- en: '*Reinforcement Learning: An Introduction* by Richard S. Sutton and Andrew G.
    Barto *(*[http://incompleteideas.net/book/ebook/the-book.html](http://incompleteideas.net/book/ebook/the-book.html)).'
  prefs: []
  type: TYPE_NORMAL
- en: Defining hyperparameters for Deep Q Learning (DQN)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are some of the hyperparameters defined that we will be using
    throughout the code and are totally configurable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The following are the parameters used:'
  prefs: []
  type: TYPE_NORMAL
- en: '`gamma` : Discount parameter in the Bellman equation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`epsilon_decay`: Multiplier by which you want to discount the value of `epsilon` after
    each episode/game'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`epsilon_min`: Minimum value of `epsilon` beyond which you do not want to decay
    it'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`deque_len`: Size of the `deque` container used to store the training examples
    (state, reward, done, and action)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`target_score`: The average score over 100 epochs that you want the agent to
    score after which you stop the learning process'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`episodes`: Maximum number of games you want the agent to play'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size`: Size of the batch of training data (stored in the `deque` container)
    used to train the agent after each episode'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`optimizer`: Optimizer of choice for training the agent'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`loss`: Loss of choice for training the agent'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experiment with different learning rates, optimizers, batch sizes as well as
    `epsilon_decay` values to see how these factors affect the quality of your model
    and, if you get better results, show it to the deep learning community.
  prefs: []
  type: TYPE_NORMAL
- en: Building the model components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will define all of the functions that go into training
    the reinforcement learning agent. These functions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agent action
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance plot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replay
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training and testing to train and test the agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining the agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's define an agent/function approximator.
  prefs: []
  type: TYPE_NORMAL
- en: The agent is nothing but a simple deep neural network that takes in the state
    (four variables) of the CartPole system and returns the maximum possible reward
    for each of the two actions.
  prefs: []
  type: TYPE_NORMAL
- en: The first, second, and third layers are simple `Dense` layers with 16 neurons
    and with activation as `relu`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final layer is a `Dense` layer with two neurons equal to the number of
    possible `actions`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/11f5fa24-2da6-4ebf-9cdf-0252287c1c2d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.5: Summary of the agent'
  prefs: []
  type: TYPE_NORMAL
- en: Play around with the parameters of the agent to suit the needs of the problem
    you are trying to solve. Try using leaky `relu` in the model if needed.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the agent action
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s define a function that, when called, will return the action that needs
    to be taken for that specific state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: For any value from the uniform distribution (between 0 and 1), less than or
    equal to `epsilon`, the action returned will be `random`. For any value greater
    than `epsilon`, the action chosen will be that predicted by the agent we have
    defined in the preceding code.
  prefs: []
  type: TYPE_NORMAL
- en: The `numpy.random.rand` function generates a random number from a uniform distribution
    over 0 and 1\. `numpy.argmax` returns the index of the maximum value in the sequence.
    `random.randrange` returns a randomly selected item from `range()`.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s define a `deque` object to store the information (`state`, `action`,
    `reward`, and `done`) related to every relevant step we take when playing the
    game. We will then be using the data stored in this `deque` object for training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We have defined the `deque` object to be of a size of `20000`. Once this container
    is filled with 20,000 data points, every new append being made at one end will
    result in popping a data point at the other end. Then, we will end up retaining
    only the latest information over time.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will define a function called `memory`, which, when called during the game,
    will accept the information related to `action`, `state`, `reward`, and `done`
    as input at that time step, and then will store it in the training data `deque`
    container we have defined in the preceding code. You will see that we are storing
    these five variables as a tuple entry at each timestep:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Defining the performance plot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following `performance_plot` function plots the performance of the model
    over time. This function has been placed such that it is only plotted once our
    target of 200 points has been reached. You can also place this function to plot
    the progress after every 100 episodes during training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'A sample plot output of the function (after the goal has been achieved) is
    shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/16cad3a0-7a27-4dd1-b6e6-3cab837c971d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.6: Sample plot output of performance_plot function'
  prefs: []
  type: TYPE_NORMAL
- en: Defining replay
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following `replay` function is called inside the `train` function (defined
    in the next section) at the end of the game for training the agent. It is in this
    function that we define the targets for each state using the *Q* function Bellman
    equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: It is inside this function that we train the agent compiled with mean squared
    error loss to learn to maximize the reward. We have done so because we are predicting
    the numerical value of the reward possible for the two actions. Remember that
    the agent accepts the state as input that is of a shape of 1*4\. The output of
    this agent is of shape 1*2, and it basically contains the expected reward for
    the two possible actions.
  prefs: []
  type: TYPE_NORMAL
- en: So, when an episode ends, we use a batch of data stored in the `deque` container
    to train the agent.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this batch of data, consider the 1^(st) tuple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: For the `state`, we know the `action` that needs to be taken to enter the `new_state` and `reward` for
    doing so. We also have `done`, which indicates whether the `new_state` entered
    is within the game rules.
  prefs: []
  type: TYPE_NORMAL
- en: 'As long as the new state, *s''* ,being entered is within the game rules, that
    is, `done` is `False`, the total `reward` according to the Bellman equation for
    entering the new state *s''* form state *s* by taking an `action` can be written
    in Python as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Output of `model.predict(new_state)[0]` be `[-0.55639267, 0.37972435]`. The `np.amax([-0.55639267,
    0.37972435])` will be `0.37972435`*.*
  prefs: []
  type: TYPE_NORMAL
- en: With the discount/`gamma` as 0.95 and the `reward` as `1`, this gives us the
    following value. The `reward + gamma * np.amax(model.predict(new_state)[0])` end
    us up as `1.36073813587427`.
  prefs: []
  type: TYPE_NORMAL
- en: This is the value of the target defined previously.
  prefs: []
  type: TYPE_NORMAL
- en: Using the model, let's predict the reward for the two possible actions for the
    current state. `target_f = model.predict(state)` will be `[[-0.4597198 0.31523475]]`.
  prefs: []
  type: TYPE_NORMAL
- en: Since we already know the `action` that needs to be taken for the `state`, which
    is `0`, to maximize the reward for the next state, we will set the `reward` at
    index zero of `target_f` equal to the `reward` computed using the Bellman equation,
    which is, `target_f[0][action] = 1.3607381358742714`.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, `target_f` will be equal to `[[1.3607382 0.31523475]]`.
  prefs: []
  type: TYPE_NORMAL
- en: We will use the state as `input` and the `target_f` as the target reward and
    fit the agent/model on it.
  prefs: []
  type: TYPE_NORMAL
- en: This process will be repeated for all of the data points in the batch of training
    data. Also, for each call of the replay function, the value of epsilon is reduced
    by the multiplier epsilon decay.
  prefs: []
  type: TYPE_NORMAL
- en: '`random.sample` samples *n* elements from a population set. `np.amax` returns
    the maximum value in an array.'
  prefs: []
  type: TYPE_NORMAL
- en: Training loop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s put all of the pieces we have formed until now together to implement
    training of the agent using the `train()` function that we have defined here:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the agent by calling the `agent()` function and compile it with the loss
    as `loss` and with the optimizer as `optimizer`, which we have defined in the
    *Defining hyperparameters for Deep Q Learning (DQN)* section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reset the environment and reshape the initial state.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Call the `agent_action` function by passing the `model`, `epsilon`, and `state`
    information and obtain the next action that needs to be taken.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take the action obtained in *Step 3* using the `env.step` function. Store the
    resulting information in the `training_data` deque container by calling the `memory`
    function and passing the required arguments.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign the new state obtained in *Step 4* to the `state` variable and increment
    the time step by 1 unit.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Until done resulting in *Step 4* turns `True`, repeat *Step 3* through *Step
    5*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Call the `replay` function to train the agent on a batch of the training data
    at the end of the episode/game.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Repeat *Step 2* through *Step 7* until the target score has been achieved:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Following code shows the implementation of the `train()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'For the remaining part of this code snippet, please refer to the `DQN.ipynb`
    file here: [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter14/DQN.ipynb](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter14/DQN.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: To view the CartPole game on your screen when training, set the `render` argument
    to `True` inside the `train` function. Also, visualizing the game will slow down
    the training.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following two images are the outputs generated during training of DQN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/531bf89c-4cb8-4708-8727-89a4ec9b6f6f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.7: Scores output when training the agent'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/79840a92-e93b-4271-a9fe-953773276f67.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.8: Plot of scores v/s episodes when training the agent'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that, when training the agent, our target score of `200` points averaging
    over `100` latest episodes was reached at the end of `300` games.
  prefs: []
  type: TYPE_NORMAL
- en: We have been using the epsilon-greedy policy to train the agent. Feel free to
    use other policies listed at [https://github.com/keras-rl/keras-rl/blob/master/rl/policy.py](https://github.com/keras-rl/keras-rl/blob/master/rl/policy.py),
    once you have finished mastering the training of DQN.
  prefs: []
  type: TYPE_NORMAL
- en: It is not always necessary that, when you give a try at training the agent,
    it takes you just 300 games. In some cases, it might even take more than 300\.
    Refer to the notebook at [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter%2015/DQN.ipynb](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter%2015/DQN.ipynb)
    to see the five tries made at training the agent and the number of episodes it
    took to train it.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the DQN model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s test how our trained DQN model performs on new games. The following `test`
    function uses the trained DQN model to play ten games and see whether our average
    target of 200 points will be achieved:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: To view the CartPole game on your screen when testing, set the `render` argument
    to `true` inside the `test` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a9fa9444-0a9e-45dc-ac84-5f4184434e14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.9: Test scores with the trained Q agent'
  prefs: []
  type: TYPE_NORMAL
- en: When the agent is tested on the new 100 CartPole games, it is averaging a score
    of `277.88`.
  prefs: []
  type: TYPE_NORMAL
- en: Remove the threshold of 200 points and aim at training the agent to consistently
    score an average of 450 points or more.
  prefs: []
  type: TYPE_NORMAL
- en: Deep Q-learning scripts in modular form
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The entire script can be split into four modules named `train_dqn.py`, `agent_reply_dqn.py`,
    `test_dqn.py`, and `hyperparameters_dqn.py`. Store these in a folder of your choice,
    for example `chapter_15`. Set `chapter_15` as the project folder in your favorite
    source code editor and just run the `train_dqn.py` file.
  prefs: []
  type: TYPE_NORMAL
- en: The `train_dqn.py` Python file will import functions from all of the other modules
    in places where they are needed for execution.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's walk through the contents of each file.
  prefs: []
  type: TYPE_NORMAL
- en: Module 1 – hyperparameters_dqn.py
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This Python file contains the hyperparameters of the DQN model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Module 2 – agent_replay_dqn.py
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This Python file contains the four functions, namely  `agent()`, `agent_action()`,
    `performance_plot()`, and `replay()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'For the remaining part of this file, please visit here: [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter14/agent_replay_dqn.py](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter14/agent_replay_dqn.py)'
  prefs: []
  type: TYPE_NORMAL
- en: Module 3 – test_dqn.py
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This module contains the `test()` function, which will be called in the `train_dqn.py`
    script to test the performance of the DQN agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Module 4 – train_dqn.py
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this module, we include the `memory()` and  `train()` functions and also
    the calls to train and test the reinforcement learning model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'For the remaining part of this code, please visit here: [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter14/train_dqn.py](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter14/train_dqn.py)'
  prefs: []
  type: TYPE_NORMAL
- en: Deep SARSA learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this segment, we will implement deep SARSA learning with the `keras-rl` library.
    The `keras-rl` library is a simple neural network API that allows simple and easy
    implementation of reinforcement learning models (Q, SARSA, and others). To learn
    more about the `keras-rl` library, visit the documentation at [https://keras-rl.readthedocs.io/en/latest/](https://keras-rl.readthedocs.io/en/latest/).
  prefs: []
  type: TYPE_NORMAL
- en: We will be using the same CartPole environment we have been using so far from
    OpenAI Gym.
  prefs: []
  type: TYPE_NORMAL
- en: A Jupyter Notebook code example for deep SARSA learning can be found at [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter14/Deep%20SARSA.ipynb](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter14/Deep%20SARSA.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: SARSA learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SARSA learning, like Q-learning, is also a policy-based reinforcement learning
    technique. Its goal is to learn an optimal policy, which helps an agent decide
    on the action that needs to be taken under various possible circumstances.
  prefs: []
  type: TYPE_NORMAL
- en: SARSA and Q-learning are very similar to each other, except Q-learning is an
    off-policy algorithm and SARSA is an on-policy algorithm. The Q value learned
    by SARSA is not based on a greedy policy like in Q-learning but is based on the
    action performed under the current  policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a single state, *s,* and an action, *a*, *Q(s, a)* can be expressed in
    terms of the Q value of the next state, *s''* ,and action, *a''*, given by the
    following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b69b5557-639d-477c-b48a-2bb34d6d41ce.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following is the pseudocode for the SARSA learning algorithm from the book, *Reinforcement
    Learning: An Introduction,* by Richard S. Sutton and Andrew G. Barto:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/872c9fb4-5fd1-41ae-a9cb-d92c1112e656.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.10: Pseudocode for SARSA learning'
  prefs: []
  type: TYPE_NORMAL
- en: Importing all of the dependencies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will be using `numpy`, `gym`, `matplotlib`, `keras`, `tensorflow`, and the `keras-rl` package
    in this segment of the exercise. Here, TensorFlow will be used as the backend
    for Keras. You can install these packages with `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Loading the game environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Just like we loaded the game in the DQN segment, we will load the game into
    the workspace and set `seed` for reproducibility:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Defining the agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For deep SARSA learning, we will be using the same agent we used in the Deep
    Q-learning segment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Training the agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Training an agent using the `keras-rl` library is very easy:'
  prefs: []
  type: TYPE_NORMAL
- en: Define the policy you want the training to follow. We will be using the epsilon-greedy
    policy. The equivalent of this in the DQN section would be the agent `action`
    function. To know more about other policies, visit [https://github.com/keras-rl/keras-rl/blob/master/rl/policy.py](https://github.com/keras-rl/keras-rl/blob/master/rl/policy.py).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the agent you would like to use. In this case, the SARSA agent has a lot
    of parameters of which the important ones that need to be defined are `model`,
    `nb_actions`, and `policy`. `model` is the deep learning agent you have defined
    in the preceding code, `nb_actions` is the number of possible actions in the system, and
    `policy` is your preferred choice of policy to train the SARSA agent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We compile the SARSA agent with loss and optimizer of choice.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We fit the SARSA agent by feeding the `.fit` function the arguments environment
    and number of steps to train:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To get complete details on the usage of agents from the `keras-rl` library and
    their parameter definitions, visit this documentation by Keras at [http://keras-rl.readthedocs.io/en/latest/agents/sarsa/#sarsaagent](http://keras-rl.readthedocs.io/en/latest/agents/sarsa/#sarsaagent).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: To view the CartPole game on your screen when training, set visualize argument
    to true inside the `.fit` function. But visualizing the game will slow down the
    training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the scores output when training the SARSA agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e8f1da64-1c3b-407b-bb32-2394bed5a92a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.11: Scores output when training SARSA agent'
  prefs: []
  type: TYPE_NORMAL
- en: Testing the agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once the agent has been trained, we evaluate its performance over 100 new episodes.
    This can be done by calling the `.test` function and feeding the arguments environment
    and number of episodes on which to test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: To view the CartPole game on your screen when testing, set the `visualize` argument
    to `True` inside the `.test` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the output after testing 100 episodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8476eba6-c485-4ca5-96c4-a16d9c20fbce.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Following the the output at the end of the code execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ab9fad31-1de5-4204-b1fe-23c09ea57abd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.12: Test scores with trained SARSA agent'
  prefs: []
  type: TYPE_NORMAL
- en: Deep SARSA learning script in modular form
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For SARSA learning, we have only one script, which implements both the training
    and testing of the SARSA agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The conclusion to the project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This project was to build a deep reinforcement learning model to successfully
    play the game of CartPole-v1 from OpenAI Gym. The use case of this chapter is
    to build a reinforcement learning model on a simple game environment and then
    extend it to other complex games such as Atari.
  prefs: []
  type: TYPE_NORMAL
- en: In the first half of this chapter, we built a deep Q-learning model to play
    the CartPole game. The DQN model during testing scored an average of 277.88 points
    over 100 games.
  prefs: []
  type: TYPE_NORMAL
- en: In the second half of this chapter, we built a deep SARSA learning model (using
    the same epsilon-greedy policy as Q-learning) to play the CartPole game. The SARSA
    model during testing scored an average of 365.67 points over 100 games.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's follow the same technique we have been following in the previous
    chapters for evaluating the performance of the models from the restaurant chain
    point of view.
  prefs: []
  type: TYPE_NORMAL
- en: What are the implications of this score?
  prefs: []
  type: TYPE_NORMAL
- en: An average score of 277.88 with Q-learning means that we have successfully solved
    the game of CartPole as defined on the OpenAI site. It also means that our model
    survives slightly more than half the length of the game with the total game length
    being 500 points.
  prefs: []
  type: TYPE_NORMAL
- en: As regards SARSA learning, on the other hand, an average score of 365.67 with
    Q-learning means that we have successfully solved the game of CartPole as defined
    on the OpenAI site and that our model survives more than 70% the length of the
    game, with the total game length being 500 points.
  prefs: []
  type: TYPE_NORMAL
- en: It is still not a level of performance you should be happy with because the
    goal should not just be to solve the problem but to train a model that is really
    good at scoring a consistent 500 points at each game, so you can see why we'd
    need to continue fine-tuning the models to get the maximum performance possible.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have successfully built a deep reinforcement learning model,
    each with Q-learning and SARSA learning in Keras using the CartPole game from
    OpenAI Gym. We understood Q-learning, SARSA learning, how to interact with game
    environments from Gym, and the function of the agent (deep learning model). We
    defined some key hyperparameters, as well as, in some places, reasoned with why
    we used what we did. Finally, we tested the performance of our reinforcement learning
    on new games and determined that we succeeded in achieving our goals.
  prefs: []
  type: TYPE_NORMAL
