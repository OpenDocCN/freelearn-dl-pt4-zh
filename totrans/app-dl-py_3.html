<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Web Scraping and Interactive Visualizations</h1>
                </header>
            
            <article>
                
<p class="mce-root">So far in this book, we have focused on using Jupyter to build reproducible data analysis pipelines and predictive models. We'll continue to explore these topics in this chapter, but the main focus here is data acquisition. In particular, we will show you how data can be acquired from the web using HTTP requests. This will involve scraping web pages by requesting and parsing HTML. We will then wrap up this chapter by using interactive visualization techniques to explore the data we've collected.</p>
<p class="mce-root">The amount of data available online is huge and relatively easy to acquire. It's also continuously growing and becoming increasingly important. Part of this continual growth is the result of an ongoing global shift from newspapers, magazines, and TV to online content. With customized news feeds available all the time on cell phones, and live-news sources such as Facebook, Reddit, Twitter, and YouTube, it's difficult to imagine the historical alternatives being relevant much longer. Amazingly, this accounts for only some of the increasingly massive amounts of data available online.</p>
<p class="mce-root">With this global shift toward consuming content using HTTP services (blogs, news sites, Netflix, and so on), there are plenty of opportunities to use data-driven analytics. For example, Netflix looks at the movies a user watches and predicts what they will like. This prediction is used to determine the suggested movies that appear. In this chapter, however, we won't be looking at "business-facing" data as such, but instead we will see how the client can leverage the internet as a database. Never before has this amount and variety of data been so easily accessible. We'll use web-scraping techniques to collect data, and then we'll explore it with interactive visualizations in Jupyter.</p>
<p>Interactive visualization is a visual form of data representation, which helps users understand the data using graphs or charts. Interactive visualization helps a developer or analyst present data in a simple form, which can be understood by non-technical personnel too.</p>
<p>By the end of this chapter, you will be able to:</p>
<ul>
<li>Analyze how HTTP requests work</li>
<li>Scrape tabular data from a web page</li>
<li>Build and transform Pandas Data Frames</li>
<li>Create interactive visualizations</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Scraping Web Page Data</h1>
                </header>
            
            <article>
                
<p>In the spirit of leveraging the internet as a database, we can think about acquiring data from web pages either by scraping content or by interfacing with web APIs. Generally, scraping content means getting the computer to read data that was intended to be displayed in a human-readable format. This is in contradistinction to web APIs, where data is delivered in machine-readable formats – the most common being JSON.</p>
<p>In this topic, we will focus on web scraping. The exact process for doing this will depend on the page and desired content. However, as we will see, it's quite easy to scrape anything we need from an HTML page so long as we have an understanding of the underlying concepts and tools. In this topic, we'll use Wikipedia as an example and scrape tabular content from an article. Then, we'll apply the same techniques to scrape data from a page on an entirely separate domain. But fist, we'll take some time to introduce HTTP requests.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction to HTTP Requests</h1>
                </header>
            
            <article>
                
<p>The Hypertext Transfer Protocol, or HTTP for short, is the foundation of data communication for the internet. It defies how a page should be requested and how the response should look. For example, a client can request an Amazon page of laptops for sale, a Google search of local restaurants, or their Facebook feed. Along with the URL, the request will contain the user agent and available browsing cookies among the contents of the <strong>request header</strong>. The user agent tells the server what browser and device the client is using, which is usually used to provide the most user-friendly version of the web page's response. Perhaps they have recently logged in to the web page; such information would be stored in a cookie that might be used to automatically log the user in.</p>
<p>These details of HTTP requests and responses are taken care of under the hood thanks to web browsers. Luckily for us, today the same is true when making requests with high level languages such as Python. For many purposes, the contents of request headers can be largely ignored.</p>
<p class="mce-root"/>
<p>Unless otherwise specified, these are automatically generated in Python when requesting a URL. Still, for the purposes of troubleshooting and understanding the responses yielded by our requests, it's useful to have a foundational understanding of HTTP.</p>
<p>There are many types of HTTP methods, such as GET, HEAD, POST, and PUT. The fist two are used for requesting that data be sent from the server to the client, whereas the last two are used for sending data to the server.</p>
<p><span>These HTTP methods are summarized in the following table: <br/></span></p>
<table style="border-collapse: collapse;width: 88.8781%">
<tbody>
<tr>
<td style="width: 17.6892%"><strong><span>HTTP method</span></strong></td>
<td class="CDPAlignLeft CDPAlign" style="width: 81.3108%"><strong><span>Description</span></strong></td>
</tr>
<tr>
<td style="width: 17.6892%"><span>GET </span></td>
<td style="width: 81.3108%"><span>Retrieves the information from the specified URL</span></td>
</tr>
<tr>
<td style="width: 17.6892%"><span>HEAD</span></td>
<td style="width: 81.3108%"><span>Retrieves the meta information from the HTTP header of the specified </span><span>URL</span></td>
</tr>
<tr>
<td style="width: 17.6892%"><span>POST</span></td>
<td style="width: 81.3108%"><span>Sends the attached information for appending to the resource(s) at the </span><span>specified URL</span></td>
</tr>
<tr>
<td style="width: 17.6892%"><span>PUT</span></td>
<td style="width: 81.3108%"><span>Sends the attached information for replacing the resource(s) at the </span><span>specified URL</span></td>
</tr>
</tbody>
</table>
<p>A GET request is sent each time we type a web page address into our browser and press <em>Enter</em>. For web scraping, this is usually the only HTTP method we are interested in, and it's the only method we'll be using in this chapter.</p>
<p>Once the request has been sent, a variety of response types can be returned from the server. These are labeled with 100-level to 500-level codes, where the fist digit in the code represents the response class. These can be described as follows:</p>
<ul>
<li><span><strong>1xx</strong>: Informational response, for example, server is processing a request. It's uncommon to see this.</span></li>
<li><span><strong>2xx</strong>: Success, for example, page has loaded properly.</span></li>
<li><span><strong>3xx</strong>: Redirection, for example, the requested resource has been moved and we were redirected to a new URL.</span></li>
<li><span><strong>4xx</strong>: Client error, for example, the requested resource does not exist.</span></li>
<li><span><strong>5xx</strong>: Server error, for example, the website server is receiving too much traffic and could not fulfill the request.<br/></span></li>
</ul>
<p>For the purposes of web scraping, we usually only care about the response class, that is, the fist digit of the response code. However, there exist subcategories of responses within each class that offer more granularity on what's going on. For example, a 401 code indicates an unauthorized response, whereas a 404 code indicates a <em>page not found</em> response.</p>
<p>This distinction is noteworthy because a 404 would indicate we've requested a page that does not exist, whereas 401 tells us we need to log in to view the particular resource.</p>
<p>Let's see how HTTP requests can be done in Python and explore some of these topics using the Jupyter Notebook.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Making HTTP Requests in the Jupyter Notebook</h1>
                </header>
            
            <article>
                
<p>Now that we've talked about how HTTP requests work and what type of responses we should expect, let's see how this can be done in Python. We'll use a library called <strong>Requests</strong>, which happens to be the most downloaded external library for Python. It's possible to use Python's built-in tools, such as <kbd>urllib</kbd>, for making HTTP requests, but Requests is far more intuitive, and in fact it's recommended over <kbd>urllib</kbd> in the official Python documentation.</p>
<p>Requests is a great choice for making simple and advanced web requests. It allows for all sorts of customization with respect to headers, cookies, and authorization. It tracks redirects and provides methods for returning specific page content such as JSON. Furthermore, there's an extensive suite of advanced features. However, it does not allow JavaScript to be rendered.</p>
<div class="packt_infobox">Oftentimes, servers return HTML with JavaScript code snippets included, which are automatically run in the browser on load time. When requesting content with Python using Requests, this JavaScript code is visible, but it does not run. Therefore, any elements that would be altered or created by doing so are missing. Often, this does not affect the ability to get the desired information, but in some cases we may need to render the JavaScript in order to scrape the page properly. For doing this, we could use a library like Selenium. This has a similar API to the Requests library, but provides support for rendering JavaScript using web drivers.</div>
<div><span>Let's dive into the following section using the Requests library with Python in a Jupyter Notebook.<br/></span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Handling HTTP requests with Python in a Jupyter Notebook</h1>
                </header>
            
            <article>
                
<ol>
<li>Start the <kbd>NotebookApp</kbd> from the project directory by executing <kbd>jupyter notebook</kbd>. Navigate to the <kbd><em>Chapter-3</em></kbd> directory and open up the <kbd>chapter-3-workbook.ipynb file</kbd>. Find the cell near the top where the packages are loaded and run it.</li>
</ol>
<p style="padding-left: 60px">We are going to request a web page and then examine the response object. There are many different libraries for making requests and many choices for exactly how to do so with each. We'll only use the Requests library, as it provides excellent documentation, advanced features, and a simple API.</p>
<ol start="2">
<li>Scroll down to Subtopic <kbd>Introduction to HTTP requests</kbd> and run the fist cell in that section to import the Requests library. Then, prepare a request by running the cell containing the following code:</li>
</ol>
<pre style="padding-left: 60px"><span>  url = '<a href="https://jupyter.org/">https://jupyter.org/</a>'<br/>  req = requests.Request('GET', url)<br/>  req.headers['User-Agent'] = 'Mozilla/5.0'<br/>  req = req.prepare() </span></pre>
<p style="padding-left: 60px">We use the <kbd>Request class</kbd> to prepare a GET request to the jupyter.org homepage. By specifying the user agent as <kbd>Mozilla/5.0</kbd>, we are asking for a response that would be suitable for a standard desktop browser. Finally, we prepare the request.</p>
<ol start="3">
<li><span>Print the docstring for the "<strong>prepared request</strong>" req, by running the cell containing <kbd>req?</kbd>:<br/></span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e77b864d-d18e-4a67-b60a-1c75504f962e.png" style="width:47.83em;height:22.42em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p style="padding-left: 60px"><span>Looking at its usage, we see how the request can be sent using a session. This is similar to opening a web browser (starting a session) and then requesting a URL.<br/></span></p>
<ol start="4">
<li><span>Make the request and store the response in a variable named page, by running the following code:</span></li>
</ol>
<pre style="padding-left: 60px"><span>  with requests.Session() as sess:<br/>  page = sess.send(req)</span></pre>
<p style="padding-left: 60px"><span>This code returns the HTTP response, as referenced by the page variable. By using the with statement, we initialize a session whose scope is limited to the indented code block. This means we do not have to worry about explicitly closing the session, as it is done automatically.</span></p>
<ol start="5">
<li><span>Run the next two cells in the notebook to investigate the response. The string representation of <kbd>page</kbd> should indicate a 200 status code response. This should agree with the <kbd>status_code</kbd> attribute.</span></li>
<li><span>Save the response text to the page_html variable and take a look at the head of the string with <kbd>page_html[:1000]</kbd> :<br/></span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/76d13a1f-33ff-4387-8142-0a75ef6c31db.png" style="width:65.92em;height:22.42em;"/></p>
<p style="padding-left: 60px">As expected, the response is HTML. We can format this output better with the help of BeautifulSoup, a library which will be used extensively for HTML parsing later in this section.</p>
<ol start="7">
<li><span>Print the head of the formatted HTML by running the following:<br/></span></li>
</ol>
<pre style="padding-left: 60px"><span>  from bs4 import BeautifulSoup<br/>  print(BeautifulSoup(page_html, 'html.parser').prettify()[:1000])<br/></span></pre>
<p style="padding-left: 60px"><span>We import BeautifulSoup and then print the pretty output, where newlines are indented depending on their hierarchy in the HTML structure.<br/></span></p>
<ol start="8">
<li>We can take this a step further and actually display the HTML in Jupyter by using the IPython display module. Do this by running the following code:</li>
</ol>
<pre style="padding-left: 60px"><span>  from IPython.display import HTML<br/>  HTML(page_html) </span></pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/1a80ff73-c7ef-4df5-a3e9-0230db3808bd.png" style="width:61.58em;height:26.25em;"/></p>
<p style="padding-left: 60px">Here, we see the HTML rendered as well as possible, given that no JavaScript code has been run and no external resources have loaded. For example, the images that are hosted on the jupyter.org server are not rendered and we instead see the <kbd>alt text</kbd>: <strong>circle of programming icons</strong>, jupyter logo, and so on.</p>
<ol start="9">
<li>Let's compare this to the live website, which can be opened in Jupyter using an IFrame. Do this by running the following code:</li>
</ol>
<pre style="padding-left: 60px"><span>  from IPython.display import IFrame<br/>  IFrame(src=url, height=800, width=800) </span></pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/8a8f9adc-9993-4134-acd7-85cce986b9cc.png" style="width:30.33em;height:23.08em;"/></p>
<p style="padding-left: 60px">Here, we see the full site rendered, including JavaScript and external resources. In fact, we can even click on the hyperlinks and load those pages in the IFrame, just like a regular browsing session.</p>
<ol start="10">
<li>It's good practice to close the IFrame after using it. This prevents it from eating up memory and processing power. It can be closed by selecting the cell and clicking <strong>Current Outputs</strong> | <strong>Clear</strong> from the <strong>Cell</strong> menu in the Jupyter Notebook.</li>
</ol>
<p style="padding-left: 60px">Recall how we used a prepared request and session to request this content as a string in Python. This is often done using a shorthand method instead. The drawback is that we do not have as much customization of the request header, but that's usually fine.</p>
<ol start="11">
<li><span>Make a request to <a href="http://www.python.org/">http://www.python.org/</a> by running the following code:<br/></span></li>
</ol>
<pre style="padding-left: 60px"><span>  url = '<a href="http://www.python.org">http://www.python.org</a>/'<br/>  page = requests.get(url)<br/>  page<br/>  &lt;Response [200]&gt; </span></pre>
<p style="padding-left: 60px"><span>The string representation of the page (as displayed beneath the cell) should indicate a 200 status code, indicating a successful response.<br/></span></p>
<ol start="12">
<li class="CDPAlignLeft CDPAlign">Run the next two cells. Here, we print the <kbd>url</kbd> and <kbd>history</kbd> attributes of our page.</li>
</ol>
<p class="CDPAlignLeft CDPAlign" style="padding-left: 60px">The URL returned is not what we input; notice the difference? We were redirected from the input URL, <a href="http://www.python.org/">http://www.python.org/</a>, to the secured version of that page, <a href="https://www.python.org/">https://www.python.org/</a>. The difference is indicated by an additional s at the start of the URL, in the protocol. Any redirects are stored in the history attribute; in this case, we find one page in here with status code 301 (permanent redirect), corresponding to the original URL requested.</p>
<p>Now that we're comfortable making requests, we'll turn our attention to parsing the HTML. This can be something of an art, as there are usually multiple ways to approach it, and the best method often depends on the details of the specific HTML in question.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Parsing HTML in the Jupyter Notebook</h1>
                </header>
            
            <article>
                
<p>When scraping data from a web page, after making the request, we must extract the data from the response content. If the content is HTML, then the easiest way to do this is with a high-level parsing library such as Beautiful Soup. This is not to say it's the only way; in principle, it would be possible to pick out the data using regular expressions or Python string methods such as <kbd>split</kbd>, but pursuing either of these options would be an inefficient use of time and could easily lead to errors. Therefore, it's generally frowned upon and instead, the use of a trustworthy parsing tool is recommended.</p>
<p>In order to understand how content can be extracted from HTML, it's important to know the fundamentals of HTML. For starters, HTML stands for <strong>Hyper Text Markup Language</strong>. Like Markdown or XML (<strong>eXtensible Markup Language</strong>), it's simply a language for marking up text.</p>
<p class="mce-root"/>
<p>In HTML, the display text is contained within the content section of HTML elements, where element attributes specify how that element should appear on the page.</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/756dbc5a-2843-4e9d-85bf-d9c189dfacec.png" style="width:27.83em;height:15.58em;"/></p>
<p>Looking at the anatomy of an HTML element, as seen in the preceding picture, we see the content enclosed between start and end tags. In this example, the tags are <kbd>&lt;p&gt;</kbd> for paragraph; other common tag types are <kbd>&lt;div&gt;</kbd> (text block), <kbd>&lt;table&gt;</kbd> (data table), <kbd>&lt;h1&gt;</kbd> (heading), <kbd>&lt;img&gt;</kbd> (image), and <kbd>&lt;a&gt;</kbd> (hyperlinks). Tags have attributes, which can hold important metadata. Most commonly, this metadata is used to specify how the element text should appear on the page. This is where CSS files come into play. The attributes can store other useful information, such as the hyperlink <kbd>href</kbd> in an <kbd>&lt;a&gt;</kbd> tag, which specifies a URL link, or the alternate alt label in an <kbd>&lt;img&gt;</kbd> tag, which specifies the text to display if the image resource cannot be loaded.</p>
<p>Now, let's turn our attention back to the Jupyter Notebook and parse some HTML! Although not necessary when following along with this section, it's very helpful in realworld situations to use the developer tools in Chrome or Firefox to help identify the HTML elements of interest. We'll include instructions for doing this with Chrome in the following section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Parsing HTML with Python in a Jupyter Notebook</h1>
                </header>
            
            <article>
                
<ol>
<li>In <kbd>chapter-3-workbook.ipynb file</kbd>, scroll to the top of Subtopic <kbd>Parsing HTML with Python</kbd>.</li>
</ol>
<p style="padding-left: 60px">In this section, we'll scrape the central bank interest rates for each country, as reported by Wikipedia. Before diving into the code, let's first open up the web page containing this data.</p>
<ol start="2">
<li>Open up the <a href="https://en.wikipedia.org/wiki/List_of_countries_by_central_bank_interest_rates">https://en.wikipedia.org/wiki/List_of_countries_by_ central_bank_interest_rates</a> URL in a web browser. Use Chrome, if possible, as later in this section we'll show you how to view and search the HTML with Chrome's developer tools.</li>
</ol>
<p style="padding-left: 60px">Looking at the page, we see very little content other than a big list of countries and their interest rates. This is the table we'll be scraping.</p>
<ol start="3">
<li><span>Return to the Jupyter Notebook and load the HTML as a Beautiful Soup object so that it can be parsed. Do this by running the following code:<br/></span></li>
</ol>
<pre style="padding-left: 60px"><span>  from bs4 import BeautifulSoup<br/>  soup = BeautifulSoup(page.content, 'html.parser') </span></pre>
<p style="padding-left: 60px">We use Python's default <kbd>html.parser</kbd> as the parser, but third-party parsers such as <kbd>lxml</kbd> may be used instead, if desired.</p>
<p style="padding-left: 60px"><span>Usually, when working with a new object like this Beautiful Soup one, it's a good idea to pull up the docstring by doing</span> <kbd>soup?</kbd><span>. However, in this case, the docstring is not particularly informative. Another tool for exploring Python objects is</span> <kbd>pdir</kbd><span>, which lists all of an object's attributes and methods (this can be installed with</span> <kbd>pip install pdir2</kbd><span>). It's basically a formatted version of Python's built-in</span> <kbd>dir</kbd> <span>function.</span></p>
<ol start="4">
<li><span>Display the attributes and methods for the BeautifulSoup object by running the following code. This will run, regardless of whether or not the <kbd>pdir</kbd> external library is installed:<br/></span></li>
</ol>
<pre style="padding-left: 60px"><span> try:<br/> import pdir<br/> dir = pdir<br/> except:<br/> print('You can install pdir with:\npip install pdir2')<br/> dir(soup) </span></pre>
<p style="padding-left: 60px"><span>Here, we see a list of methods and attributes that can be called on <kbd>soup</kbd>. The most commonly used function is probably <kbd>find_all</kbd>, which returns a list of elements that match the given criteria.<br/></span></p>
<ol start="5">
<li><span>Get the h1 heading for the page with the following code:<br/></span></li>
</ol>
<pre><span>      h1 = soup.find_all('h1')<br/>      h1<br/>      &gt;&gt; [&lt;h1 class="firstHeading" id="firstHeading" lang="en"&gt;<br/>      List of countries by central bank interest rates&lt;/h1&gt;] </span></pre>
<p style="padding-left: 60px"><span>Usually, pages only have one H1 element, so it's obvious that we only find one here.<br/></span></p>
<ol start="6">
<li>Run the next couple of cells. We redefine H1 to the fist (and only) list element with <kbd>h1 = h1[0]</kbd> , and then print out the HTML element attributes with <kbd>h1.attrs</kbd>:</li>
</ol>
<pre>       <span>&gt;&gt; {'class': ['firstHeading'], 'id': 'firstHeading', 'lang': 'en'} </span></pre>
<p style="padding-left: 60px"><span>We see the class and ID of this element, which can both be referenced by CSS code to define the style of this element.<br/></span></p>
<ol start="7">
<li><span>Get the HTML element content (that is, the visible text) by printing <kbd>h1.text</kbd>.<br/></span></li>
<li><span>Get all the images on the page by running the following code:<br/></span></li>
</ol>
<pre><span>      imgs = soup.find_all('img')<br/>      len(imgs)<br/>      &gt;&gt; 91 </span></pre>
<p style="padding-left: 60px"><span>There are lots of images on the page. Most of these are for the country flags.<br/></span></p>
<ol start="9">
<li><span>Print the source of each image by running the following code:<br/></span></li>
</ol>
<pre><span>      [element.attrs['src'] for element in imgs<br/>        if 'src' in element.attrs.keys()] </span></pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/740a50cf-9941-4f4a-a73b-745386209da4.png" style="width:41.17em;height:10.42em;"/></p>
<p style="padding-left: 60px">We use a list comprehension to iterate through the elements, selecting the <kbd>src</kbd> attribute of each (so long as that attribute is actually available).</p>
<p style="padding-left: 60px">Now, let's scrape the table. We'll use Chrome's developer tools to hunt down the element this is contained within.</p>
<ol start="10">
<li>If not already done, open the Wikipedia page we're looking at in Chrome. Then, in the browser, select <span class="packt_screen">Developer Tools</span> from the <kbd>View menu</kbd>. A sidebar will open. The HTML is available to look at from the <kbd>Elements</kbd> tab in <span class="packt_screen">Developer Tools</span>.</li>
<li><span>Select the little arrow in the top left of the tools sidebar. This allows us to hover over the page and see where the HTML element is located, in the Elements section of the sidebar:<br/></span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/225749b6-435f-4729-9879-718d3ea1cba3.png" style="width:32.25em;height:24.17em;"/></p>
<ol start="12">
<li><span>Hover over the body to see how the table is contained within the div that has <kbd>id="bodyContent"</kbd>:<br/></span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/3ccc0aab-d3f5-43ca-ab7a-511cfda86a32.png" style="width:55.67em;height:36.92em;"/></p>
<ol start="13">
<li><span>Select that div by running the following code:<br/></span></li>
</ol>
<pre>     <span>body_content = soup.find('div', {'id': 'bodyContent'}) </span></pre>
<p style="padding-left: 60px"><span>We can now seek out the table within this subset of the full HTML. Usually, tables are organized into headers <kbd>&lt;th&gt;</kbd>, rows <kbd>&lt;tr&gt;</kbd>, and data entries <kbd>&lt;td&gt;</kbd>.<br/></span></p>
<ol start="14">
<li><span>Get the table headers by running the following code:<br/></span></li>
</ol>
<pre style="padding-left: 60px"><span> table_headers = body_content.find_all('th')[:3]<br/> table_headers<br/> &gt;&gt;&gt; [&lt;th&gt;Country or&lt;br/&gt;<br/> currency union&lt;/th&gt;, &lt;th&gt;Central bank&lt;br/&gt;<br/> interest rate (%)&lt;/th&gt;, &lt;th&gt;Date of last&lt;br/&gt;<br/> change&lt;/th&gt;] </span></pre>
<p style="padding-left: 60px"><span>Here, we see three headers. In the content of each is a break element <kbd>&lt;br/&gt;</kbd>, which will make the text a bit more difficult to cleanly parse.<br/></span></p>
<ol start="15">
<li><span>Get the text by running the following code:</span></li>
</ol>
<pre><span>      table_headers = [element.get_text().replace('\n', ' ')<br/>      for element in table_headers]<br/>      table_headers<br/>      &gt;&gt; ['Country or currency union',<br/>      'Central bank interest rate (%)',<br/>      'Date of last change']</span></pre>
<p style="padding-left: 60px"><span>Here, we get the content with the get_text method, and then run the replace string method to remove the newline resulting from the <kbd>&lt;br/&gt;</kbd> element. To get the data, we'll first perform some tests and then scrape all the data in a single cell.<br/></span></p>
<ol start="16">
<li><span>Get the data for each cell in the second &lt;tr&gt; (row) element by running the following code:</span></li>
</ol>
<pre><span>      row_number = 2<br/>      d1, d2, d3 = body_content.find_all('tr')[row_number]\.find_all('td')</span></pre>
<p style="padding-left: 60px"><span>We find all the row elements, pick out the third one, and then find the three data elements inside that.<br/>
Let's look at the resulting data and see how to parse the text from each row.</span></p>
<ol start="17">
<li><span>Run the next couple of cells to print <kbd>d1</kbd> and its <kbd>text</kbd> attribute:<br/></span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/753c4a22-175a-40c9-8557-35c2d040dd2b.png" style="width:65.33em;height:17.83em;"/></p>
<p style="padding-left: 60px">We're getting some undesirable characters at the front. This can be solved by searching for only the text of the <kbd>&lt;a&gt;</kbd> tag.</p>
<ol start="18">
<li><span>Run <kbd>d1.find('a').text</kbd> to return the properly cleaned data for that cell.<br/></span></li>
<li><span>Run the next couple of cells to print <kbd>d2</kbd> and its text. This data appears to be clean enough to convert directly into a flat.<br/></span></li>
<li><span>Run the next couple of cells to print <kbd>d3</kbd> and its text:<br/></span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0a3416ed-8371-407e-a654-8947074319f2.png" style="width:69.00em;height:14.58em;"/></p>
<p style="padding-left: 60px"><span>Similar to <kbd>d1</kbd>, we see that it would be better to get only the <kbd>span</kbd> element's text.<br/></span></p>
<ol start="21">
<li><span>Properly parse the date for this table entry by running the following code:</span></li>
</ol>
<pre><span>       d3.find_all('span')[0].text<br/>       &gt;&gt; '30 June 2016' </span></pre>
<p class="mce-root"/>
<ol start="22">
<li><span>Now, we're ready to perform the full scrape by iterating over the row elements <kbd>&lt;th&gt;</kbd>. Run the following code:</span></li>
</ol>
<pre><span>       data = []<br/>       for i, row in enumerate(body_content.find_all('tr')):<br/>       ...<br/>       ...<br/>       &gt;&gt; Ignoring row 101 because len(data) != 3<br/>       &gt;&gt; Ignoring row 102 because len(data) != 3 </span></pre>
<p style="padding-left: 60px">We iterate over the rows, ignoring any that contain more than three data elements. These rows will not correspond to data in the table we are interested in. Rows that do have three data elements are assumed to be in the table, and we parse the text from these as identified during the testing. T</p>
<p style="padding-left: 60px">The text parsing is done inside a <kbd>try/except</kbd> statement, which will catch any errors and allow this row to be skipped without stopping the iteration. Any rows that raise errors due to this statement should be looked at. The data for these could be recorded manually or accounted for by altering the scraping loop and re-running it. In this case, we'll ignore any errors for the sake of time.</p>
<ol start="23">
<li><span>Print the head of the scraped data list by running print(data[:10]):<br/></span></li>
</ol>
<pre><span>      &gt;&gt; [['Albania', 1.25, '4 May 2016'],<br/>      ['Angola', 16.0, '30 June 2016'],<br/>      ['Argentina', 26.25, '11 April 2017'],<br/>      ['Armenia', 6.0, '14 February 2017'],<br/>      ['Australia', 1.5, '2 August 2016'],<br/>      ['Azerbaijan', 15.0, '9 September 2016'],<br/>      ['Bahamas', 4.0, '22 December 2016'],    <br/>      ['Bahrain', 1.5, '14 June 2017'],<br/>      ['Bangladesh', 6.75, '14 January 2016'],<br/>      ['Belarus', 12.0, '28 June 2017']] </span></pre>
<ol start="24">
<li><span>We'll visualize this data later in the chapter. For now, save the data to a CSV file by running the following code:<br/></span></li>
</ol>
<pre><span>       f_path = '../data/countries/interest-rates.csv'<br/>       with open(f_path, 'w') as f:<br/>         f.write('{};{};{}\n'.format(*table_headers))<br/>         for d in data:<br/>           f.write('{};{};{}\n'.format(*d))</span></pre>
<p style="padding-left: 60px"><span>Note that we are using semicolons to separate the fields.<br/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Activity:Web Scraping with Jupyter Notebooks</h1>
                </header>
            
            <article>
                
<p>We are going to get the population of each country. Then, in the next topic, this will be visualized along with the interest rate data scraped in the previous section.</p>
<p>The page we look at in this activity is available here: <a href="http://www.worldometers.info/world-population/population-by-country/">http://www.worldometers.info/world-population/population-by-country/</a>. Now that we've seen the basics of web scraping, let's apply the same techniques to a new web page and scrape some more data!</p>
<div class="packt_infobox"><span>This page may have changed since this document was created. If this URL no longer leads to a table of country populations, please use this Wikipedia page instead: <a href="https://en.wikipedia.org/wiki/List_of_countries_by_population_(United_Nations)">https://en.wikipedia.org/wiki/List_of_countries_by_population (United_Nations)</a> .<br/></span></div>
<ol>
<li><span>For this page, the data can be scraped using the following code snippet:</span></li>
</ol>
<pre><span>      data = []<br/>      for i, row in enumerate(soup.find_all('tr')):<br/>          row_data = row.find_all('td')<br/>              try:<br/>                  d1, d2, d3 = row_data[1], row_data[5], row_data[6]<br/>                  d1 = d1.find('a').text<br/>                  d2 = float(d2.text)<br/>                  d3 = d3.find_all('span')[1].text.replace('+', '')<br/>              data.append([d1, d2, d3])<br/>          except:<br/>      print('Ignoring row {}'.format(i)) </span></pre>
<p> </p>
<ol start="2">
<li>In the <kbd>chapter-3-workbook.ipynb</kbd> Jupyter Notebook, scroll to <kbd>Activity Web scraping with Python</kbd>.<span><br/></span></li>
</ol>
<ol start="3">
<li>Set the <kbd>url</kbd> variable and load an IFrame of our page in the notebook by running the following code:</li>
</ol>
<pre>      url ='<a href="http://www.worldometers.info/world-population/population-by-country/">http://www.worldometers.info/world-population/<br/>                population-bycountry/</a>'<br/>      IFrame(url, height=300, width=800)</pre>
<ol start="4">
<li>Close the IFrame by selecting the cell and clicking Current Outputs | Clear from the Cell menu in the Jupyter Notebook.</li>
<li>Request the page and load it as a BeautifulSoup object by running the following code:<span><br/></span></li>
</ol>
<pre>       page = requests.get(url)<br/>       soup = BeautifulSoup(page.content,'html.parser') </pre>
<p style="padding-left: 60px">We feed the page content to the <kbd>BeautifulSoup</kbd> constructor. Recall that previously, we used page.text here instead. The difference is that page.content returns the raw binary response content, whereas page.text returns the <kbd>UTF-8</kbd> decoded content. It's usually best practice to pass the bytes object and let <kbd>BeautifulSoup</kbd> decode it, rather than doing it with Requests using <kbd>page.text</kbd>.</p>
<ol start="6">
<li><span>Print the <kbd>H1</kbd> for the page by running the following code:</span></li>
</ol>
<pre><span>      soup.find_all('h1')<br/>      &gt;&gt; [&lt;h1&gt;Countries in the world by population (2017)&lt;/h1&gt;] </span></pre>
<p style="padding-left: 60px"><span>We'll scrape the table by searching for <kbd>&lt;th&gt;</kbd>, <kbd>&lt;tr&gt;</kbd>, and <kbd>&lt;td&gt;</kbd> elements, as in the previous section.</span></p>
<p class="mce-root"/>
<ol start="7">
<li><span>Get and print the table headings by running the following code:<br/></span></li>
</ol>
<pre><span>        table_headers = soup.find_all('th')<br/>        table_headers<br/>        &gt;&gt; [&lt;th&gt;#&lt;/th&gt;,<br/>            &lt;th&gt;Country (or dependency)&lt;/th&gt;,<br/>            &lt;th&gt;Population&lt;br/&gt; (2017)&lt;/th&gt;,<br/>           &lt;th&gt;Yearly&lt;br/&gt; Change&lt;/th&gt;,<br/>            &lt;th&gt;Net&lt;br/&gt; Change&lt;/th&gt;,<br/>            &lt;th&gt;Density&lt;br/&gt; (P/Km²)&lt;/th&gt;,<br/>            &lt;th&gt;Land Area&lt;br/&gt; (Km²)&lt;/th&gt;,<br/>           &lt;th&gt;Migrants&lt;br/&gt; (net)&lt;/th&gt;,<br/>            &lt;th&gt;Fert.&lt;br/&gt; Rate&lt;/th&gt;,<br/>            &lt;th&gt;Med.&lt;br/&gt; Age&lt;/th&gt;,<br/>            &lt;th&gt;Urban&lt;br/&gt; Pop %&lt;/th&gt;,<br/>           &lt;th&gt;World&lt;br/&gt; Share&lt;/th&gt;] </span></pre>
<ol start="8">
<li><span>We are only interested in the fist three columns. Select these and parse the text with the following code:</span></li>
</ol>
<pre><span>      table_headers = table_headers[1:4]<br/>      table_headers = [t.text.replace('\n', '') for t in table_headers]</span></pre>
<p style="padding-left: 60px"><span>After selecting the subset of table headers we want, we parse the text content from each and remove any newline characters.<br/>
Now, we'll get the data. Following the same prescription as the previous section, we'll test how to parse the data for a sample row.</span></p>
<ol start="9">
<li><span>Get the data for a sample row by running the following code:</span></li>
</ol>
<pre><span>     row_number = 2<br/>     row_data = soup.find_all('tr')[row_number]\.find_all('td') </span></pre>
<ol start="10">
<li>How many columns of data do we have? Print the length of <kbd>row_data</kbd> by running <kbd>print(len(row_data))</kbd> .</li>
<li><span>Print the fist elements by running <kbd>print(row_data[:4])</kbd> :</span></li>
</ol>
<pre><span>      &gt;&gt; [&lt;td&gt;2&lt;/td&gt;,</span><br/><span>      &lt;td style="font-weight: bold; font-size:15px; text-align:left"&gt;&lt;a</span><br/><span>      href="/world-population/india-population/"&gt;India&lt;/a&gt;&lt;/td&gt;,</span><br/><span>      &lt;td style="font-weight: bold;"&gt;1,339,180,127&lt;/td&gt;,</span><br/><span>      &lt;td&gt;1.13 %&lt;/td&gt;]</span></pre>
<p style="padding-left: 60px"><span>It's pretty obvious that we want to select list indices 1, 2, and 3. The first data value can be ignored, as it's simply the index.<br/></span></p>
<ol start="12">
<li><span>Select the data elements we're interested in parsing by running the following code:</span></li>
</ol>
<pre><span>      d1, d2, d3 = row_data[1:4] <br/></span></pre>
<ol start="13">
<li><span>Looking at the <kbd>row_data</kbd> output, we can find out how to correctly parse the data.We'll want to select the content of the <kbd>&lt;a&gt;</kbd> element in the fist data element, and then simply get the text from the others. Test these assumptions by running the following code:</span></li>
</ol>
<pre><span>       print(d1.find('a').text)<br/>       print(d2.text)<br/>       print(d3.text)<br/>       &gt;&gt; India<br/>       &gt;&gt; 1,339,180,127<br/>       &gt;&gt; 1.13 % </span></pre>
<p style="padding-left: 60px"><span>Excellent! This looks to be working well. Now, we're ready to scrape the entire table.<br/></span></p>
<ol start="14">
<li><span>Scrape and parse the table data by running the following code:<br/></span></li>
</ol>
<pre><span>      ata = []<br/>      for i, row in enumerate(soup.find_all('tr')):<br/>         try:<br/>             d1, d2, d3 = row.fid_all('td')[1:4]<br/>             d1 = d1.fid('a').text<br/>             d2 = d2.text<br/>             d3 = d3.text<br/>             data.append([d1, d2, d3])<br/>         except:<br/>             print('Error parsing row {}'.format(i))<br/><br/>         &gt;&gt; Error parsing row 0 </span></pre>
<p style="padding-left: 60px"><span>This is quite similar to before, where we try to parse the text and skip the row if there's some error.<br/></span></p>
<ol start="15">
<li><span>Print the head of the scraped data by running print(data[:10]):</span></li>
</ol>
<pre><span>      &gt;&gt; [['China', '1,409,517,397', '0.43 %'],<br/>      ['India', '1,339,180,127', '1.13 %'],<br/>      ['U.S.', '324,459,463', '0.71 %'],<br/>      ['Indonesia', '263,991,379', '1.10 %'],    <br/>      ['Brazil', '209,288,278', '0.79 %'],<br/>      ['Pakistan', '197,015,955', '1.97 %'],<br/>      ['Nigeria', '190,886,311', '2.63 %'],<br/>      ['Bangladesh', '164,669,751', '1.05 %'],<br/>      ['Russia', '143,989,754', '0.02 %'],<br/>      ['Mexico', '129,163,276', '1.27 %']] </span></pre>
<p style="padding-left: 60px">It looks like we have managed to scrape the data! Notice how similar the process was for this table compared to the Wikipedia one, even though this web page is completely different. Of course, it will not always be the case that data is contained within a table, but regardless, we can usually use <kbd>find_all</kbd> as the primary method for parsing.</p>
<ol start="16">
<li>Finally, save the data to a <kbd>CSV file</kbd> for later use. Do this by running the following code:</li>
</ol>
<pre><span>      f_path = '../data/countries/populations.csv'<br/>      with open(f_path, 'w') as f:<br/>        f.write('{};{};{}\n'.format(*table_headers))<br/>        for d in data:<br/>          f.write('{};{};{}\n'.format(*d)) </span></pre>
<p>To summarize, we've seen how Jupyter Notebooks can be used for web scraping. We started this chapter by learning about HTTP methods and status codes. Then, we used the Requests library to actually perform HTTP requests with Python and saw how the Beautiful Soup library can be used to parse the HTML responses.</p>
<p>Our Jupyter Notebook turned out to be a great tool for this type of work. We were able to explore the results of our web requests and experiment with various HTML parsing techniques. We were also able to render the HTML and even load a live version of the web page inside the notebook!</p>
<p>In the next topic of this chapter, we shift to a completely new topic: interactive visualizations. We'll see how to create and display interactive charts right inside the notebook, and use these charts as a way to explore the data we've just collected.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Interactive Visualizations</h1>
                </header>
            
            <article>
                
<p>Visualizations are quite useful as a means of extracting information from a dataset. For example, with a bar graph it's very easy to distinguish the value distribution, compared to looking at the values in a table. Of course, as we have seen earlier in this book, they can be used to study patterns in the dataset that would otherwise be quite difficult to identify. Furthermore, they can be used to help explain a dataset to an unfamiliar party. If included in a blog post, for example, they can boost reader interest levels and be used to break up blocks of text.</p>
<p>When thinking about interactive visualizations, the benefits are similar to static visualizations, but enhanced because they allow for active exploration on the viewer's part. Not only do they allow the viewer to answer questions they may have about the data, they also think of new questions while exploring. This can benefit a separate party such as a blog reader or co-worker, but also a creator, as it allows for easy adhoc exploration of the data in detail, without having to change any code.</p>
<p>In this topic, we'll discuss and show how to use Bokeh to build interactive visualizations in Jupyter. Prior to this, however, we'll briefly revisit pandas Data Frames, which play an important role in doing data visualization with Python.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a DataFrame to Store and Organize Data</h1>
                </header>
            
            <article>
                
<p>As we've seen time and time again in this book, pandas is an integral part of doing data science with Python and Jupyter Notebooks. DataFrames offer a way to organize and store labeled data, but more importantly, pandas provides time saving methods for transforming data within a DataFrame. Examples we have seen in this book include dropping duplicates, mapping dictionaries to columns, applying functions over columns, and filing in missing values.</p>
<p>With respect to visualizations, DataFrames offer methods for creating all sorts of matplotlib graphs, including <kbd>df.plot.barh()</kbd> , <kbd>df.plot.hist()</kbd> , and more. The interactive visualization library Bokeh previously relied on pandas DataFrames for their <em>high-level</em> charts. These worked similar to Seaborn, as we saw earlier in the previous chapter, where a DataFrame is passed to the plotting function along with the specific columns to plot. The most recent version of Bokeh, however, has dropped support for this behavior. Instead, plots are now created in much the same way as matplotlib, where the data can be stored in simple lists or NumPy arrays. The point of this discussion is that DataFrames are not entirely necessary, but still very helpful for organizing and manipulating the data prior to visualization.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building and merging Pandas DataFrames</h1>
                </header>
            
            <article>
                
<p>Let's dive right into an exercise, where we'll continue working on the country data we scraped earlier. Recall that we extracted the central bank interest rates and populations of each country, and saved the results in CSV files. We'll load the data from these files and merge them into a DataFrame, which will then be used as the data source for the interactive visualizations to follow.</p>
<ol>
<li>In the <kbd>chapter-3-workbook.ipynb</kbd> Jupyter Notebook, scroll to the Subtopic <kbd>Building a DataFrame to store and organize data</kbd> .</li>
</ol>
<p style="padding-left: 60px">We are first going to load the data from the <kbd>CSV files</kbd>, so that it's back to the state it was in after scraping. This will allow us to practice building DataFrames from Python objects, as opposed to using the <kbd>pd.read_csv function</kbd>.</p>
<div class="packt_infobox">When using <kbd>pd.read_csv</kbd>, the datatype for each column will be inferred from the string input. On the other hand, when using <kbd>pd.DataFrame</kbd> as we do here, the datatype is instead taken as the type of the input variables. In our case, as will be seen, we read the file and do not bother converting the variables to numeric or date-time until after instantiating the DataFrame.</div>
<ol start="2">
<li><span>Load the CSV files into lists by running the following code:<br/></span></li>
</ol>
<pre><span>        with open('../data/countries/interest-rates.csv', 'r') as f:<br/>          int_rates_col_names = next(f).split(',')<br/>          int_rates = [line.split(',') for line in f.read().splitlines()]<br/></span><span>        with open('../data/countries/populations.csv', 'r') as f:<br/>          </span><span>populations_col_names = next(f).split(',')<br/>          populations = [line.split(',') for line in f.read().splitlines()] </span></pre>
<ol start="3">
<li><span>Check what the resulting lists look like by running the next two cells. We should see an output similar to the following:<br/></span></li>
</ol>
<pre><span>      print(int_rates_col_names)<br/>      int_rates[:5]<br/>      &gt;&gt; ['Country or currency union', 'Central bank interest ...    <br/>      ...<br/>      ['Indonesia', '263', '991', '379', '1.10 %'],    <br/>      ['Brazil', '209', '288', '278', '0.79 %']] </span></pre>
<p style="padding-left: 60px">Now, the data is in a standard Python list structure, just as it was after scraping from the web pages in the previous sections. We're now going to create two DataFrames and merge them, so that all of the data is organized within one object.</p>
<ol start="4">
<li><span>Use the standard DataFrame constructor to create the two DataFrames by running the following code:</span></li>
</ol>
<pre><span>      df_int_rates = pd.DataFrame(int_rates,columns=int_rates_col_names)<br/>      df_populations = pd.DataFrame(populations,<br/>                      columns=populations_col_names) <br/></span></pre>
<p style="padding-left: 60px">This isn't the first time we've used this function in this book. Here, we pass the lists of data (as seen previously) and the corresponding column names. The input data can also be of dictionary type, which can be useful when each column is contained in a separate list.</p>
<p style="padding-left: 60px">Next, we're going to clean up each DataFrame. Starting with the interest rates one, let's print the head and tail, and list the data types.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="5">
<li><span>When displaying the entire DataFrame, the default maximum number of rows is 60 (for version 0.18.1). Let's reduce this to 10 by running the following code:</span></li>
</ol>
<pre><span>      pd.options.display.max_rows = 10 </span></pre>
<ol start="6">
<li><span>Display the head and tail of the interest rates DataFrame by running the following code:</span></li>
</ol>
<pre><span>      df_int_rates </span></pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d1c43f2f-daf2-4866-9dbb-de64f5794494.png" style="width:33.33em;height:24.33em;"/></p>
<ol start="7">
<li><span>Print the data types by running:</span></li>
</ol>
<pre><span>      df_int_rates.dtypes<br/>      &gt;&gt; Country or currency union object<br/>      &gt;&gt; Central bank interest rate (%) object<br/>      &gt;&gt; Date of last change object<br/>      &gt;&gt; dtype: object </span></pre>
<p style="padding-left: 60px">Pandas has assigned each column as a string datatype, which makes sense because the input variables were all strings. We'll want to change these to string, float, and date-time, respectively.</p>
<ol start="8">
<li><span>Convert to the proper datatypes by running the following code:<br/></span></li>
</ol>
<pre><span>        df_int_rates['Central bank interest rate (%)'] = \<br/>        df_int_rates['Central bank interest rate (%)']\<br/>        .astype(float,copy=False)<br/></span><span>        df_int_rates['Date of last change'] = \<br/>        pd.to_datetime(df_int_rates['Date of last change']) </span></pre>
<p style="padding-left: 60px">We use <kbd>astype</kbd> to cast the Interest Rate values as floats, setting <kbd>copy=False</kbd> to save memory. Since the date values are given in such an easy-to-read format, these can be converted simply by using <kbd>pd.to_datetime</kbd>.</p>
<ol start="9">
<li><span>Check the new datatypes of each column by running the following code:</span></li>
</ol>
<pre><span>      df_int_rates.dtypes<br/>        &gt;&gt; Country or currency union             object<br/>        &gt;&gt; Central bank interest rate (%)        float64<br/>        &gt;&gt; Date of last change             datetime64[ns]<br/>        &gt;&gt; dtype: object</span></pre>
<p style="padding-left: 60px"><span>As can be seen, everything is now in the proper format.<br/></span></p>
<ol start="10">
<li><span>Let's apply the same procedure to the other DataFrame. Run the next few cells to repeat the preceding steps for <kbd>df_populations</kbd>:<br/></span></li>
</ol>
<pre><strong>       <span>df_populations </span></strong></pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f705ec84-e2f2-4a1a-b184-8ce45d7a1cdb.png" style="width:28.08em;height:23.33em;"/></p>
<p style="padding-left: 60px"><span>Then, run this code:</span></p>
<pre><span>      df_populations['Population (2017)'] = df_populations['Population<br/>      (2017)']\.str.replace(',', '')\<br/>      .astype(float, copy=False)<br/>      df_populations['Yearly Change'] = df_populations['Yearly Change']\<br/>      .str.rstrip('%')\<br/>      .astype(float, copy=False) </span></pre>
<p style="padding-left: 60px">To cast the numeric columns as a float, we had to first apply some modifications to the strings in this case. We stripped away any commas from the populations and removed the percent sign from the Yearly Change column, using string methods.</p>
<p style="padding-left: 60px">Now, we're going to merge the DataFrames on the country name for each row. Keep in mind that these are still the raw country names as scraped from the web, so there might be some work involved with matching the strings.</p>
<ol start="11">
<li><span>Merge the DataFrames by running the following code:</span></li>
</ol>
<pre><span>       df_merge = pd.merge(df_populations,<br/>         df_int_rates,<br/>         left_on='Country (or dependency)',<br/>         right_on='Country or currency union',<br/>         how='outer'<br/>       df_merge<br/></span></pre>
<p style="padding-left: 60px"><span>We pass the population data in the left DataFrame and the interest rates in the right one, performing an outer match on the country columns. This will result in <kbd>NaN values</kbd> where the two do not overlap.<br/></span></p>
<ol start="12">
<li>For the sake of time, let's just look at the most populated countries to see whether we missed matching any. Ideally, we would want to check everything. Look at the most populous countries by running the following code:</li>
</ol>
<pre>      df_merge.sort_values('Population (2017)', ascending=False)\ .head(10)</pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c00baec8-0c59-4528-8f78-228fd4503cf6.png" style="width:38.00em;height:18.17em;"/></p>
<p style="padding-left: 60px">It looks like U.S. didn't match up. This is because it's listed as United States in the interest rates data. Let's remedy this.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="13">
<li><span>Fix the label for U.S. in the populations table by running the following code:</span></li>
</ol>
<pre><span>      col = 'Country (or dependency)'    <br/>      df_populations.loc[df_populations[col] == 'U.S.'] = 'United States'</span></pre>
<p style="padding-left: 60px"><span>We rename the country for the populations DataFrame with the use of the <kbd>loc</kbd> method to locate that row. Now, let's merge the DataFrames properly.<br/></span></p>
<ol start="14">
<li><span>Re-merge the DataFrames on the country names, but this time use an inner merge to remove the <kbd>NaN</kbd> values:</span></li>
</ol>
<pre><span>      df_merge = pd.merge(df_populations,<br/>                    df_int_rates,<br/>                    left_on='Country (or dependency)',<br/>                    right_on='Country or currency union',<br/>                    how='inner') </span></pre>
<ol start="15">
<li><span>We are left with two identical columns in the merged DataFrame. Drop one of them by running the following code:</span></li>
</ol>
<pre><span>      del df_merge['Country or currency union'] </span></pre>
<ol start="16">
<li><span>Rename the columns by running the following code:</span></li>
</ol>
<pre><span>      name_map = {'Country (or dependency)': 'Country',<br/>          'Population (2017)': 'Population',<br/>          'Central bank interest rate (%)': 'Interest rate'}<br/><br/>      df_merge = df_merge.rename(columns=name_map)     </span></pre>
<p style="padding-left: 60px"><span>We are left with the following merged and cleaned DataFrame:<br/></span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/609d17de-62a7-46c1-9385-af183c54d3a1.png" style="width:39.83em;height:27.58em;"/></p>
<ol start="17">
<li>Now that we have all the data in a nicely organized table, we can move on to the fun part: visualizing it. Let's save this table to a <kbd>CSV file</kbd> for later use, and then move on to discuss how visualizations can be created with Bokeh. Write the merged data to a <kbd>CSV file</kbd> for later use with the following code:</li>
</ol>
<pre>      df_merge.to_csv('../data/countries/merged.csv', index=False)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction to Bokeh</h1>
                </header>
            
            <article>
                
<p>Bokeh is an interactive visualization library for Python. Its goal is to provide similar functionality to <kbd>D3</kbd>, the popular interactive visualization library for JavaScript. Bokeh functions very differently than <kbd>D3</kbd>, which is not surprising given the differences between Python and JavaScript. Overall, it's much simpler and it doesn't allow nearly as much customization as <kbd>D3</kbd> does. This works to its advantage though, as it's much easier to use, and it still boasts an excellent suite of features that we'll explore in this section.</p>
<p>Let's dive right into a quick exercise with the Jupyter Notebook and introduce Bokeh by example.</p>
<p>There is good documentation online for Bokeh, but much of it is outdated. Searching something like <kbd>Bokeh bar plot</kbd> in Google still tends to turn up documentation for legacy modules that no longer exist, for example, the high-level plotting tools that used to be available through <kbd>bokeh.charts</kbd> (prior to version 0.12.0). These are the ones that take pandas DataFrames as input in much the same way that Seaborn plotting functions do. Removing the high-level plotting tools module has simplified Bokeh, and will allow for more focused development going forward. Now, the plotting tools are largely grouped into the bokeh. <kbd>plotting module</kbd>, as will be seen in the next exercise and following activity.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction to interactive visualizations with Bokeh</h1>
                </header>
            
            <article>
                
<p>We'll load the required Bokeh modules and show some simple interactive plots that can be made with Bokeh. Please note that the examples in this book have been designed using version 0.12.10 of Bokeh.</p>
<ol>
<li>In the <kbd>chapter-3-workbook.ipynb</kbd> Jupyter notebook, scroll to Subtopic <kbd>Introduction to Bokeh</kbd>.</li>
<li>Like scikit-learn, Bokeh modules are usually loaded in pieces (unlike pandas, for example, where the whole library is loaded at once). Import some basic plotting modules by running the following code:</li>
</ol>
<pre>      from bokeh.plotting <br/>      import figure, show, output_notebook output_notebook()</pre>
<p style="padding-left: 60px"><span>We need to run <kbd>output_notebook()</kbd> in order to render the interactive visuals within the Jupyter notebook.<br/></span></p>
<ol start="3">
<li><span>Generate random data to plot by running the following code:</span></li>
</ol>
<pre><span>      np.random.seed(30)<br/>      data = pd.Series(np.random.randn(200),<br/>      index=list(range(200)))\<br/>      .cumsum()<br/>      x = data.index<br/>      y = data.values</span></pre>
<p style="padding-left: 60px"><span>The random data is generated using the cumulative sum of a random set of numbers that are distributed about zero. The effect is a trend that looks similar to a stock price time series, for example.</span></p>
<ol start="4">
<li>Plot the data with a line plot in Bokeh by running the following code:</li>
</ol>
<pre>      p = figure(title='Example plot', x_axis_label='x', y_axis_label='y') <br/>      p.line(x, y, legend='Random trend') show(p)</pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f9f51bc7-d14c-435b-b8d2-a069d61cdc52.png" style="width:44.75em;height:44.08em;"/></p>
<p style="padding-left: 60px">We instantiate the figure, as referenced by the variable p, and then plot a line. Running this in Jupyter yields an interactive figure with various options along the right-hand side.</p>
<p style="padding-left: 60px">The top three options (as of version 0.12.10) are <strong>Pan</strong>, <strong>Box Zoom</strong>, and <strong>Wheel Zoom</strong>. Play around with these and experiment with how they work. Use the reset option to re-load the default plot limits.</p>
<ol start="5">
<li>Other plots can be created with the alternative methods of <kbd>figure</kbd>. Draw a scatter plot by running the following code, where we replace <kbd>line</kbd> in the preceding code with <kbd>circle</kbd>:</li>
</ol>
<pre><span>      size = np.random.rand(200) * 5<br/>      p = figure(title='Example plot', x_axis_label='x', y_axis_label='y')<br/>      p.circle(x, y, radius=size, alpha=0.5, legend='Random dots')<br/>      show(p) </span></pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/fa778c9d-5325-479e-8cf2-e7bd42030acb.png" style="width:35.92em;height:36.25em;"/></p>
<p style="padding-left: 60px">Here, we've specified the size of each circle using a random set of numbers.</p>
<p style="padding-left: 60px">A very enticing feature of interactive visualizations is the tooltip. This is a hover tool that allows the user to get information about a point by hovering over it.</p>
<ol start="6">
<li>In order to add this tool, we're going to use a slightly different method for creating the plot. This will require us to import a couple of new libraries. Run the following code:</li>
</ol>
<pre>      p.circle(x, y, radius=size, alpha=0.5, legend='Random dots') show(p)</pre>
<p style="padding-left: 60px">This time, we'll create a data source to pass to the plotting method. This can contain metadata, which can be included in the visualization via the hover tool.</p>
<ol start="7">
<li><span>Create random labels and plot the interactive visualization with a hover tool by running the following code:</span></li>
</ol>
<pre><span>      source = ColumnDataSource(data=dict(<br/>      x=x,<br/>      y=y,<br/>      ...<br/>      ...<br/>      source=source,<br/>        legend='Random dots')<br/>      show(p) </span></pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ad807af2-a484-47dd-a750-662ac88e7a1e.png" style="width:47.25em;height:46.83em;"/></div>
<p style="padding-left: 60px">We define a data source for the plot by passing a dictionary of key/value pairs to the <kbd>ColumnDataSource</kbd> constructor. This source includes the <em>x</em> location, <em>y</em> location, and size of each point, along with the random letter <em>A</em>, <em>B</em>, or <em>C</em> for each point. These random letters are assigned as labels for the hover tool, which will also display the size of each point.</p>
<p style="padding-left: 60px">The <strong>Hover Tool</strong> is then added to the figure, and the data is retrieved from each element through the specific plotting method, which is circle in this case.</p>
<p style="padding-left: 60px">The result is that we are now able to hover over the points and see the data we've selected for the <strong>Hover Tool</strong>!</p>
<p style="padding-left: 60px">We notice, by looking at the toolbar to the right of the plot, that by explicitly including the <strong>Hover Tool</strong>, the others have disappeared. These can be included by manually adding them to the list of tool objects that gets passed to <kbd>bokeh. plotting.figure</kbd>.</p>
<ol start="8">
<li><span>Add pan, zoom, and reset tools to the plot by running the following code:</span></li>
</ol>
<pre><span>     from bokeh.models <br/>     import PanTool, BoxZoomTool, WheelZoomTool, ResetTool<br/>     ...<br/>     ...<br/>        legend='Random dots')<br/>        show(p)</span></pre>
<p style="padding-left: 60px"><span>This code is identical to what was previously shown except for the tools variable, which now references several new tools we've imported from the Bokeh library.<br/></span></p>
<p>We'll stop the introductory exercise here, but we'll continue creating and exploring plots in the following activity.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Activity:Exploring Data with Interactive Visualizations</h1>
                </header>
            
            <article>
                
<p>We'll pick up using Bokeh right where we left off with the previous exercise, except instead of using the randomly generated data seen there, we'll instead use the data we scraped from the web in the fist part of this chapter.</p>
<p>To use Bokeh to create interactive visualizations of our scraped data.</p>
<ol>
<li><span><span>In the <kbd>chapter-3-workbook.ipynb</kbd> file, scroll to the <kbd>Activity: Interactive visualizations with Bokeh section</kbd>.</span></span></li>
<li><span>Load the previously scraped, merged, and cleaned web page data by running the following code:<br/></span></li>
</ol>
<pre><span>      df = pd.read_csv('../data/countries/merged.csv')<br/>      df['Date of last change'] = pd.to_datetime(df['Date of last change']) </span></pre>
<ol start="3">
<li><span>Recall what the data looks like by displaying the DataFrame:<br/></span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/37124811-b721-4d65-80c3-5e483686f8c1.png" style="width:37.08em;height:25.08em;"/></p>
<p style="padding-left: 60px">Whereas in the previous exercise we were interested in learning how Bokeh worked, now we are interested in what this data looks like. In order to explore this dataset, we are going to use interactive visualizations.</p>
<ol start="4">
<li><span>Draw a scatter plot of the population as a function of the interest rate by running the following code:</span></li>
</ol>
<pre><span>      source = ColumnDataSource(data=dict(<br/>          x=df['Interest rate'],<br/>          y=df['Population'],<br/>          desc=df['Country'],<br/>       ))<br/>       hover = HoverTool(tooltips=[<br/>          ('Country', '@desc'),<br/>          ('Interest Rate (%)', '@x'),<br/>          ('Population', '@y')    <br/>       ])<br/>       tools = [hover, PanTool(), BoxZoomTool(), <br/>       WheelZoomTool(), ResetTool()]<br/>          p = figure(tools=tools,<br/>          x_axis_label='Interest Rate (%)',<br/>          y_axis_label='Population')<br/>       p.circle('x', 'y', size=10, alpha=0.5, source=source)<br/>       show(p) </span></pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/1f175063-00f7-4bf9-85b9-b7a19a8eda9d.png" style="width:42.92em;height:42.50em;"/></p>
<p style="padding-left: 60px">This is quite similar to the final examples we looked at when introducing Bokeh in the previous exercise. We set up a customized data source with the x and y coordinates for each point, along with the country name. This country name is passed to the <strong>Hover Tool</strong>, so that it's visible when hovering the mouse over the dot. We pass this tool to the figure, along with a set of other useful tools.</p>
<p class="mce-root"/>
<ol start="5">
<li><span>In the data, we see some clear outliers with high populations. Hover over these to see what they are:<br/></span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/5b5c8b32-c6f6-4abf-8771-4f082bf5c2d0.png" style="width:29.67em;height:10.33em;"/></p>
<p style="padding-left: 60px">We see they belong to India and China. These countries have fairly average interest rates. Let's focus on the rest of the points by using the <strong>Box Zoom</strong> tool to modify the view window size.</p>
<ol start="6">
<li><span>Select the Box Zoom tool and alter the viewing window to better see the majority of the data:<br/></span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6031cccf-3e75-466f-8aff-97e113f9ca58.png" style="width:16.33em;height:9.83em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/98950e43-777f-492e-b146-7505210a94f5.png" style="width:48.00em;height:47.83em;"/></p>
<p style="padding-left: 60px"><span>Explore the points and see how the interest rates compare for various countries.What are the countries with the highest interest rates?:<br/></span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e1e76aa6-a9fc-4d7f-a2b1-bfd83f8d138e.png" style="width:31.83em;height:21.00em;"/></p>
<ol start="7">
<li>Some of the lower population countries appear to have negative interest rates. Select the <strong>Wheel Zoom</strong> tool and use it to zoom in on this region. Use the <strong>Pan</strong> tool to re-center the plot, if needed, so that the negative interest rate samples are in view. Hover over some of these and see what countries they correspond to:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/349b1120-db5a-4ca3-9d4a-5ccfab136c7d.png" style="width:15.67em;height:10.17em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c2f37280-5aa8-43cb-80e7-82efa4d97c50.png" style="width:29.58em;height:19.33em;"/></p>
<p style="padding-left: 60px">Let's re-plot this, adding a color based on the date of last interest rate change. This will be useful to search for relations between the date of last change and the interest rate or population size.</p>
<ol start="8">
<li><span>Add a Year of last change column to the DataFrame by running the following code:</span></li>
</ol>
<pre><span>      def get_year(x):<br/>        year = x.strftime('%Y')<br/>        if year in ['2018', '2017', '2016']:<br/>            return year<br/>      else:         <br/>            return 'Other'<br/>      df['Year of last change'] = df['Date of last change'].apply(get_year)</span></pre>
<p style="padding-left: 60px"><span>We first define a function to group the samples based on year of last change, and then apply that function to the</span> <strong>Date of last change</strong> <span>column. Next, we need to map these values to colors for the visualization.<br/></span></p>
<ol start="9">
<li><span>Create a map to group the last change date into color categories by running the following code:</span></li>
</ol>
<pre><span>      year_to_color = {<br/>      '2018': 'black',<br/>      '2017': 'blue',<br/>      '2016': 'orange',<br/>      'Other':'red'<br/>      } </span></pre>
<p style="padding-left: 60px">Once mapped to the <kbd>Year of last change</kbd> column, this will assign values to colors based on the available categories: 2018, 2017, 2016, and Other. The colors here are standard strings, but they could alternatively by represented by hexadecimal codes.</p>
<ol start="10">
<li><span>Create the colored visualization by running the following code:</span></li>
</ol>
<pre><span>      source = ColumnDataSource(data=dict(<br/>      x=df['Interest rate'],<br/>      ...<br/>      ...<br/>          fill_color='colors', line_color='black',<br/>          legend='label')<br/>      show(p) </span></pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4bcbaf09-268f-44ee-87d1-0eb714ff1705.png" style="width:39.33em;height:38.42em;"/></p>
<p style="padding-left: 60px">There are some technical details that are important here. First of all, we add the colors and labels for each point to the <kbd>ColumnDataSource</kbd>. These are then referenced when plotting the circles by setting the <kbd>fill_color</kbd> and legend arguments.</p>
<p class="mce-root"/>
<ol start="11">
<li><span>Looking for patterns, zoom in on the lower population countries:<br/></span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/be463b9f-63fc-40f4-991e-b9eb1c760eb6.png" style="width:38.42em;height:22.67em;"/></p>
<p style="padding-left: 60px">We can see how the dark dots are more prevalent to the right-hand side of the plot. This indicates that countries that have higher interest rates are more likely to have been recently updated.</p>
<p style="padding-left: 60px">The one data column we have not yet looked at is the year-over-year change in population. Let's visualize this compared to the interest rate and see if there is any trend. We'll also enhance the plot by setting the circle size based on the country population.</p>
<ol start="12">
<li><span>Plot the interest rate as a function of the year-over-year population change by running the following code:<br/></span></li>
</ol>
<pre><span>      source = ColumnDataSource(data=dict(<br/>          x=df['Yearly Change'],<br/>      ...<br/>      ...<br/>      p.circle('x', 'y', size=10, alpha=0.5, source=source,    <br/>      radius='radii')<br/>      show(p) </span></pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/2cae693f-0b3f-454a-9266-560a1d80be75.png" style="width:46.92em;height:46.75em;"/></p>
<p style="padding-left: 60px">Here, we use the square root of the population for the radii, making sure to also scale down the result to a good size for the visualization.</p>
<p style="padding-left: 60px"><span>We see a strong correlation between the year-over-year population change and the interest rate. This correlation is especially strong when we take the population sizes into account, by looking primarily at the bigger circles. Let's add a line of best fit to the plot to illustrate this correlation.</span></p>
<p style="padding-left: 60px">We'll use scikit-learn to create the line of best fit, using the country populations (as visualized in the preceding plot) as weights.</p>
<ol start="13">
<li><span>Determine the line of best fit for the previously plotted relationship by running the following code:<br/></span></li>
</ol>
<pre><span>        from sklearn.linear_model import LinearRegression<br/>        X = df['Yearly Change'].values.reshape(-1, 1)<br/>        y = df['Interest rate'].values<br/>        weights = np.sqrt(df['Population'])/1e5<br/>        lm = LinearRegression()<br/>        lm.fit(X, y, sample_weight=weights)<br/>        lm_x = np.linspace(X.flatten().min(), X.flatten().max(), 50)<br/>        lm_y = lm.predict(lm_x.reshape(-1, 1)) </span></pre>
<p style="padding-left: 60px">The scikit-learn code should be familiar from earlier in this book. As promised, we are using the transformed populations, as seen in the previous plot, as the weights. The line of best fit is then calculated by predicting the linear model values for a range of <em>x</em> values.</p>
<p style="padding-left: 60px">To plot the line, we can reuse the preceding code, adding an extra call to the line module in Bokeh. We'll also have to set a new data source for this line.</p>
<ol start="14">
<li><span>Re-plot the preceding fiure, adding a line of best fi, by running the following code:</span></li>
</ol>
<pre><span>      source = ColumnDataSource(data=dict(<br/>          x=df['Yearly Change'],<br/>          y=df['Interest rate'],<br/>       ...<br/>       ...<br/>       p.line('x', 'y', line_width=2, line_color='red',<br/>          source=lm_source)<br/>          show(p)</span></pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c2c434ec-a839-45e3-874a-dd67617910cc.png" style="width:36.08em;height:35.25em;"/></p>
<p style="padding-left: 60px">For the line source, <kbd>lm_source</kbd>, we include N/A as the country name and population, as these are not applicable values for the line of best fit. As can be seen by hovering over the line, they indeed appear in the tooltip.</p>
<p style="padding-left: 60px">The interactive nature of this visualization gives us a unique opportunity to explore outliers in this dataset, for example, the tiny dot in the lower-right corner.</p>
<ol start="15">
<li><span>Explore the plot by using the zoom tools and hovering over interesting samples.Note the following:<br/></span></li>
</ol>
<ul>
<li style="list-style-type: none">
<ul>
<li><span>Ukraine has an unusually high interest rate, given the low year-over-year population change:</span></li>
</ul>
</li>
<li class="CDPAlignCenter CDPAlign" style="list-style-type: none"><img src="assets/b0800611-c3f6-4fa6-8d1f-c913c9b6f7eb.png" style="width:49.00em;height:36.58em;"/> 
<ul>
<li class="CDPAlignLeft CDPAlign">The small country of Bahrain has an unusually low interest rate, given the high year-over-year population change:</li>
</ul>
</li>
</ul>
<p class="CDPAlignCenter CDPAlign">                 <img src="assets/7a573f2f-33a1-4e1b-8288-c6995b23a7f7.png" style="width:49.92em;height:32.92em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we scraped web page tables and then used interactive visualizations to study the data.</p>
<p>We started by looking at how HTTP requests work, focusing on GET requests and their response status codes. Then, we went into the Jupyter Notebook and made HTTP requests with Python using the Requests library. We saw how Jupyter can be used to render HTML in the notebook, along with actual web pages that can be interacted with. After making requests, we saw how Beautiful Soup can be used to parse text from the HTML, and used this library to scrape tabular data.</p>
<p>After scraping two tables of data, we stored them in pandas DataFrames. The fist table contained the central bank interest rates for each country and the second table contained the populations. We combined these into a single table that was then used to create interactive visualizations.</p>
<p>Finally, we used Bokeh to render interactive visualizations in Jupyter. We saw how to use the Bokeh API to create various customized plots and made scatter plots with specific interactive abilities such as zoom, pan, and hover. In terms of customization, we explicitly showed how to set the point radius and color for each data sample. Furthermore, when using Bokeh to explore the scraped population data, the tooltip was utilized to show country names and associated data when hovering over the points.</p>
<p>Congratulations for completing this introductory course on data science using Jupyter Notebooks! Regardless of your experience with Jupyter and Python coming into the book, you've learned some useful and applicable skills for practical data science!</p>


            </article>

            
        </section>
    </body></html>