- en: Automated Image Captioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned about building an object detection and classification
    model, which was really exciting. But in this chapter, we are going to do something
    even more impressive by combining current state-of-the-art techniques in both computer
    visionandnatural language processing to form a complete image description approach
    ([https://www.cs.cmu.edu/~afarhadi/papers/sentence.pdf](https://www.cs.cmu.edu/~afarhadi/papers/sentence.pdf)). This
    will be responsible for constructing computer-generated natural descriptions of
    any provided images.
  prefs: []
  type: TYPE_NORMAL
- en: Our team has been asked to build this model to generate natural language descriptions
    of images to be used as the core intelligence of a company that wants to help
    the visually impaired take advantage of the explosion of photo sharing that's
    done on the web. It's exciting to think that this deep learning technology could
    have the power to effectively bring image content alive to this community. People
    who are likely to enjoy the outcome of our work are those who are visually impaired
    from birth right up to our aging population. Each of these user types and many
    more could use an image captioning bot that could be based on the model in this
    project so that they can keep up with family by knowing the content of posted
    images, for example.
  prefs: []
  type: TYPE_NORMAL
- en: With this in mind, let's look at the deep learning engineering that we need
    to do. The idea is to replace the encoder (RNN layer) in an encoder-decoder architecture
    with a deep **convolutional neural network** (**CNN**) trained to classify objects
    in images.
  prefs: []
  type: TYPE_NORMAL
- en: Normally, the CNN's last layer is the softmax layer, which assigns the probability
    that each object might be in the image. But if we remove that softmax layer from
    CNN, we can feed the CNN's rich encoding of the image into the decoder (language
    generation RNN) designed to produce phrases. We can then train the whole system
    directly on images and their captions, so it maximizes the likelihood that the
    descriptions it produces best match the training descriptions for each image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the small illustration of the **Auto Image Captioning Model**. In the
    top left corner is the **Encoder-Decoder** architecture for sequence-to-sequence
    model which is combined with the **Object Detection model** as shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ec12594b-416d-4a3e-a4c0-db121bec1eae.png)'
  prefs: []
  type: TYPE_IMG
- en: In this implementation, we will be using a pretrained Inception-v3 model as
    a feature extractor in an encoder trained on the ImageNet dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's import all of the dependencies that we will need to build an auto-captioning
    model.
  prefs: []
  type: TYPE_NORMAL
- en: All of the Python files and the Jupyter Notebooks for this chapter can be found
    at [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter11](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter11).
  prefs: []
  type: TYPE_NORMAL
- en: Initialization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this implementation, we need a TensorFlow version greater than or equal
    to 1.9 and we will also enable the eager execution ([https://www.tensorflow.org/guide/eager](https://www.tensorflow.org/guide/eager))
    mode, which will help us use the debug the code more effectively. Here is the
    code for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Download and prepare the MS-COCO dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are going to use the MS-COCO dataset ([http://cocodataset.org/#home](http://cocodataset.org/#home)) to
    train our model. This dataset contains more than 82,000 images, each of which
    has been annotated with at least five different captions. The following code will
    download and extract the dataset automatically:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This involves a large download ahead. We'll use the training set; it's a 13
    GB file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following will be the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'For this example, we''ll select a subset of 40,000 captions and use these and
    the corresponding images to train our model. As always, captioning quality will
    improve if you choose to use more data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the data preparation is completed, we will have all of the image path
    stored in the `img_name_vector` list variable, and the associated captions are
    stored in `train_caption,` as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8f149cda-f700-48c4-94ef-ebaaa0e549c7.png)'
  prefs: []
  type: TYPE_IMG
- en: Data preparation for a deep CNN encoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, we will use Inception-v3 (pretrained on ImageNet) to classify each image.
    We will extract features from the last convolutional layer. We will create a helper
    function that will transform the input image to the format that is expected by
    Inception-v3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Now let's initialize the Inception-v3 model and load the pretrained ImageNet
    weights. To do so, we'll create a `tf.keras` model where the output layer is the
    last convolutional layer in the Inception-v3 architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'While creating the `keras` model, you can see a parameter called `include_top=False`
    that indicates whether to include the fully connected layer at the top of the
    network or not:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: So, the `image_features_extract_model` is our deep CNN encoder, which is responsible
    for learning the features from the given image.
  prefs: []
  type: TYPE_NORMAL
- en: Performing feature extraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we will pre-process each image with the deep CNN encoder and dump the output
    to the disk:'
  prefs: []
  type: TYPE_NORMAL
- en: We will load the images in batches using the `load_image()` helper function
    that we created before
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will feed the images into the encoder to extract the features
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Dump the features as a `numpy` array:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Data prep for a language generation (RNN) decoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first step is to pre-process the captions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will perform a few basic pre-processing steps on the captions, such as the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we'll tokenize the captions (for example, by splitting on spaces). This
    will help us to build a vocabulary of all the unique words in the data (for example,
    "playing", "football", and so on).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we'll limit the vocabulary size to the top 5,000 words to save memory.
    We'll replace all other words with the token `unk` (for unknown). You can obviously
    optimize that according to the use case.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we will create a word --> index mapping and vice versa.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will then pad all sequences to be the same length as the longest one.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is the code for that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'So, the end result will be an array of a sequence of integers, as shown in
    the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/15faa166-9e04-4e32-b06f-28c4c2650c4d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we will split the data into training and validation samples using an 80:20
    split ratio:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Setting up the data pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our images and captions are ready! Next, let''s create a `tf.data` dataset
    ([https://www.tensorflow.org/api_docs/python/tf/data/Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset))
    to use for training our model. Now we will prepare the pipeline for an image and
    the text model by performing transformations and batching on them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Defining the captioning model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The model architecture we are using to build the auto captioning is inspired
    by the *Show, Attend and Tell* paper ([https://arxiv.org/pdf/1502.03044.pdf](https://arxiv.org/pdf/1502.03044.pdf)).
    The features that we extracted from the lower convolutional layer of Inception-v3
    gave us a vector of a shape of (8, 8, 2048). Then, we squash that to a shape of
    (64, 2048).
  prefs: []
  type: TYPE_NORMAL
- en: 'This vector is then passed through the CNN encoder, which consists of a single
    fully connected layer. The RNN (GRU in our case) attends over the image to predict
    the next word:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Attention
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we will define the attention mechanism popularly known as Bahdanau attention
    ([https://arxiv.org/pdf/1409.0473.pdf](https://arxiv.org/pdf/1409.0473.pdf)). We
    will need the features from the CNN encoder of a shape of (`batch_size`, `64`,
    `embedding_dim`). This attention mechanism will return the context vector and
    the attention weights over the time axis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: CNN encoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now let''s define the CNN encoder that will be the single, fully connected
    layer followed by the ReLU activation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: RNN decoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here, we will define the RNN decoder which will take the encoded features from
    the encoder. The features are fed into the attention layer, which is concatenated
    with the input embedding vector. Then, the concatenated vector is passed into
    the GRU module, which is further passed through two fully connected layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Loss function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are using the `Adam` optimizer to train the model and masking the loss calculated
    for the `<PAD>` key:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Training the captioning model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let's train the model. The first thing we need to do is to extract the
    features stored in the respective `.npy` files and then pass those features through
    the CNN encoder.
  prefs: []
  type: TYPE_NORMAL
- en: The encoder output, hidden state (initialized to 0) and the decoder input (which
    is the start token) are passed to the decoder. The decoder returns the predictions
    and the decoder hidden state.
  prefs: []
  type: TYPE_NORMAL
- en: The decoder hidden state is then passed back into the model and the predictions
    are used to calculate the loss. While training, we use the **teacher forcing technique**
    to decide the next input to the decoder.
  prefs: []
  type: TYPE_NORMAL
- en: Teacher forcing is the technique where the target word is passed as the next
    input to the decoder. This technique helps to learn the correct sequence or correct
    statistical properties for the sequence, quickly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final step is to calculate the gradient and apply it to the optimizer and
    backpropagate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c5a47375-df57-44ad-99fd-06addbe5908f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After performing the training process over few epochs lets plot the `Epoch`
    vs `Loss` graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c859ad39-eb10-4791-9641-d43a4bc2a54a.png)'
  prefs: []
  type: TYPE_IMG
- en: The loss vs Epoch plot during training process
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the captioning model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The evaluation function is similar to the training loop, except we don't use
    teacher forcing here. The input to the decoder at each time step is its previous
    predictions, along with the hidden state and the encoder output.
  prefs: []
  type: TYPE_NORMAL
- en: 'A few key points to remember while making predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: Stop predicting when the model predicts the end token
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Store the attention weights for every time step
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s define the `evaluate()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, let''s create a `helper` function to visualize the attention points that
    predict the words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9edc8d8c-9af0-4bf5-a0df-1fc5f89145f1.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/ccb51a85-bbc0-4031-8da8-bfaa6b5d147e.png)'
  prefs: []
  type: TYPE_IMG
- en: Deploying the captioning model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now let's deploy the complete module as a RESTful service. To do so, we will
    write an inference code that loads the latest checkpoint and makes the prediction
    on the given image.
  prefs: []
  type: TYPE_NORMAL
- en: Look into the `inference.py` file in the repository. All the code is similar
    to the training loop except we don't use teacher forcing here. The input to the
    decoder at each time step is its previous predictions, along with the hidden state
    and the encoder output.
  prefs: []
  type: TYPE_NORMAL
- en: 'One important part is to load the model in memory for which we are using the
    `tf.train.Checkpoint()` method, which loads all of the learned weights for `optimizer`,
    `encoder`, `decoder` into the memory. Here is the code for that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'So, we will create an `evaluate()` function, which defines the prediction loop. To
    make sure that the prediction ends after certain words, we will stop predicting
    when the model predicts the end token, `<end>`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s use this `evaluate()` function in our web application code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following command in the Terminal to run the web app:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we request the API, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We should get our caption predicted, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8becf540-8d9a-4cd4-89b5-dba2bd53f48e.png)'
  prefs: []
  type: TYPE_IMG
- en: Make sure to train the model on the large image to get better predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Voila! We just deployed the state-of-the-art automatic captioning module.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this implementation, we used a pretrained Inception-v3 model as a feature
    extractor in an encoder trained on the ImageNet dataset as part of a deep learning
    solution. This solution combines current state-of-the-art techniques in both *computer
    vision *and* natural language processing,* to form a complete image description approach ([https://www.cs.cmu.edu/~afarhadi/papers/sentence.pdf](https://www.cs.cmu.edu/~afarhadi/papers/sentence.pdf)) able
    to construct computer-generated natural descriptions of any provided images. We've
    effectively broken the barrier between images and language with this trained model
    and we've provided a technology that could be used as part of an application,
    helping the visually impaired enjoy the benefits of the megatrend of photo sharing!
    Great work!
  prefs: []
  type: TYPE_NORMAL
