- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Including Node Features with Vanilla Neural Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用香草神经网络在节点特征中包含节点特征
- en: 'So far, the only type of information we’ve considered is the graph topology.
    However, graph datasets tend to be richer than a mere set of connections: nodes
    and edges can also have features to represent scores, colors, words, and so on.
    Including this additional information in our input data is essential to produce
    the best embeddings possible. In fact, this is something natural in machine learning:
    node and edge features have the same structure as a tabular (non-graph) dataset.
    This means that traditional techniques can be applied to this data, such as neural
    networks.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们考虑的唯一类型的信息是图拓扑结构。然而，图形数据集往往比单纯的连接更丰富：节点和边缘也可以具有特征，用于表示分数、颜色、单词等等。将这些附加信息包含在我们的输入数据中对于产生最佳嵌入至关重要。事实上，这在机器学习中是自然而然的：节点和边缘特征具有与表格（非图形）数据集相同的结构。这意味着可以将传统技术应用于这些数据，例如神经网络。
- en: 'In this chapter, we will introduce two new graph datasets: `Cora` and `Facebook
    Page-Page`. We will see how **Vanilla Neural Networks** perform on node features
    only by considering them as tabular datasets. We will then experiment to include
    topological information in our neural networks. This will give us our first GNN
    architecture: a simple model that considers both node features and edges. Finally,
    we’ll compare the performance of the two architectures and obtain one of the most
    important results of this book.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍两个新的图形数据集：`Cora` 和 `Facebook Page-Page`。我们将看到**香草神经网络**如何仅将其视为表格数据集来处理节点特征。然后，我们将尝试在神经网络中包含拓扑信息。这将给我们带来我们第一个GNN架构：一个简单的模型，同时考虑节点特征和边缘。最后，我们将比较这两种架构的性能，并得到本书中最重要的结果之一。
- en: By the end of this chapter, you will master the implementation of vanilla neural
    networks and vanilla GNNs in PyTorch. You will be able to embed topological features
    into the node representations, which is the basis of every GNN architecture. This
    will allow you to greatly improve the performance of your models by transforming
    tabular datasets into graph problems.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本章结束时，您将掌握在PyTorch中实现香草神经网络和香草GNN的方法。您将能够将拓扑特征嵌入节点表示中，这是每个GNN架构的基础。这将允许您通过将表格数据集转换为图问题来大大提高模型的性能。
- en: 'In this chapter, we’ll cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Introducing graph datasets
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍图形数据集
- en: Classifying nodes with vanilla neural networks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用香草神经网络对节点进行分类
- en: Classifying nodes with vanilla graph neural networks
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用香草图神经网络对节点进行分类
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: All the code examples from this chapter can be found on GitHub at [https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter05](https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter05).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有代码示例都可以在GitHub上找到：[https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter05](https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter05)。
- en: Installation steps required to run the code on your local machine can be found
    in the *Preface* of this book.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的*前言*中可以找到运行代码所需的安装步骤。
- en: Introducing graph datasets
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍图形数据集
- en: 'The graph datasets we’re going to use in this chapter are richer than Zachary’s
    Karate Club: they have more nodes, more edges, and include node features. In this
    section, we will introduce them to give us a good understanding of these graphs
    and how to process them with PyTorch Geometric. Here are the two datasets we will
    use:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章中将使用的图形数据集比Zachary's Karate Club更丰富：它们具有更多的节点、更多的边缘，并包含节点特征。在本节中，我们将介绍它们，以便更好地理解这些图形及如何使用PyTorch
    Geometric处理它们。以下是我们将使用的两个数据集：
- en: The `Cora` dataset
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Cora` 数据集'
- en: The `Facebook` `Page-Page` dataset
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Facebook` `Page-Page` 数据集'
- en: 'Let’s start with the smaller one: the popular `Cora` dataset.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从较小的一个开始：流行的`Cora`数据集。
- en: The Cora dataset
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Cora 数据集
- en: Introduced by Sen et al. in 2008 [1], `Cora` (no license) is the most popular
    dataset for node classification in the scientific literature. It represents a
    network of 2,708 publications, where each connection is a reference. Each publication
    is described as a binary vector of 1,433 unique words, where `0` and `1` indicate
    the absence or presence of the corresponding word, respectively. This representation
    is also called a binary **bag of words** in natural language processing. Our goal
    is to classify each node into one of seven categories.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 由 Sen 等人于 2008 年提出 [1]，`Cora`（无许可）是科学文献中最流行的节点分类数据集。它代表了一个包含 2,708 篇出版物的网络，其中每个连接代表一个参考文献。每篇出版物用一个包含
    1,433 个独特单词的二进制向量表示，其中 `0` 和 `1` 分别表示对应单词的缺失或存在。这种表示方法也被称为自然语言处理中的二进制 **词袋**。我们的目标是将每个节点分类到七个类别中的一个。
- en: 'Regardless of the type of data, visualization is always an important step to
    getting a good grasp of the problem we face. However, graphs can quickly become
    too big to visualize using Python libraries such as `networkx`. This is why dedicated
    tools have been developed specifically for graph data visualization. In this book,
    we utilize two of the most popular ones: **yEd Live** ([https://www.yworks.com/yed-live/](https://www.yworks.com/yed-live/))
    and **Gephi** ([https://gephi.org/](https://gephi.org/)).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 无论数据类型如何，可视化始终是理解我们面临问题的一个重要步骤。然而，图形很快就会变得太大，无法使用像 `networkx` 这样的 Python 库进行可视化。这就是为什么专门的工具被开发出来，专门用于图数据的可视化。在本书中，我们使用了两款最流行的工具：**yEd
    Live** ([https://www.yworks.com/yed-live/](https://www.yworks.com/yed-live/))
    和 **Gephi** ([https://gephi.org/](https://gephi.org/))。
- en: The following figure is a plot of the `Cora` dataset made with yEd Live. You
    can see nodes corresponding to papers in orange and connections between them in
    green. Some papers are so interconnected that they form clusters. These clusters
    should be easier to classify than poorly connected nodes.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图是使用 yEd Live 绘制的 `Cora` 数据集。你可以看到，节点以橙色表示对应的论文，绿色表示它们之间的连接。一些论文的互联性如此强，以至于它们形成了聚类。这些聚类应该比连接较差的节点更容易分类。
- en: '![Figure 5.1 – The Cora dataset visualized with yEd Live](img/B19153_05_001.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.1 – 使用 yEd Live 可视化的 Cora 数据集](img/B19153_05_001.jpg)'
- en: Figure 5.1 – The Cora dataset visualized with yEd Live
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1 – 使用 yEd Live 可视化的 Cora 数据集
- en: 'Let’s import it and analyze its main characteristics with PyTorch Geometric.
    This library has a dedicated class to download the dataset and return a relevant
    data structure. We assume here that PyTorch Geometric has already been installed:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们导入它并使用 PyTorch Geometric 分析它的主要特征。这个库有一个专门的类来下载数据集并返回相关的数据结构。我们假设这里已经安装了 PyTorch
    Geometric：
- en: 'We import the `Planetoid` class from PyTorch Geometric:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从 PyTorch Geometric 导入 `Planetoid` 类：
- en: '[PRE0]'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We download it using this class:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用这个类下载它：
- en: '[PRE1]'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`Cora` only has one graph we can store in a dedicated `data` variable:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Cora` 只有一个图，我们可以将其存储在一个专门的 `data` 变量中：'
- en: '[PRE2]'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let’s print information about the dataset in general:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们打印出有关数据集的一般信息：
- en: '[PRE3]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This gives us the following output:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这给我们带来了以下输出：
- en: '[PRE4]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can also get detailed information thanks to dedicated functions from PyTorch
    Geometric:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以通过 PyTorch Geometric 的专门函数获得详细信息：
- en: '[PRE5]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This is the result of the previous block:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是前一个代码块的结果：
- en: '[PRE6]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The first output confirms the information about the number of nodes, features,
    and classes. The second one gives more insights into the graph itself: edges are
    undirected, every node has neighbors, and the graph doesn’t have any self-loop.
    We could test other properties using PyTorch Geometric’s utils functions, but
    we wouldn’t learn anything new in this example.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个输出确认了关于节点数量、特征和类别的信息。第二个输出则提供了更多关于图形本身的见解：边是无向的，每个节点都有邻居，而且图中没有自环。我们可以使用
    PyTorch Geometric 的 utils 函数测试其他属性，但在这个例子中不会学到任何新东西。
- en: 'Now that we know more about `Cora`, let’s see one that is more representative
    of the size of real-world social networks: the `Facebook` `Page-Page` dataset.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了更多关于 `Cora` 的信息，让我们来看一个更具代表性的，体现现实世界社交网络规模的数据集：`Facebook` `Page-Page`
    数据集。
- en: The Facebook Page-Page dataset
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Facebook Page-Page 数据集
- en: 'This dataset was introduced by Rozemberczki et al. in 2019 [2]. It was created
    using the Facebook Graph API in November 2017\. In this dataset, each of the 22,470
    nodes represents an official Facebook page. Pages are connected when there are
    mutual likes between them. Node features (128-dim vectors) are created from textual
    descriptions written by the owners of these pages. Our goal is to classify each
    node into one of four categories: politicians, companies, television shows, and
    governmental organizations.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集由 Rozemberczki 等人在 2019 年提出[2]。它是通过 Facebook Graph API 于 2017 年 11 月创建的。在这个数据集中，22,470
    个节点中的每一个代表一个官方 Facebook 页面。当两个页面之间有互相点赞时，它们会被连接。节点特征（128 维向量）是通过这些页面的所有者编写的文本描述创建的。我们的目标是将每个节点分类到四个类别之一：政治人物、公司、电视节目和政府组织。
- en: 'The `Facebook Page-Page` dataset is similar to the previous one: it’s a social
    network with a node classification task. However, there are three major differences
    with `Cora`:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`Facebook 页面-页面`数据集与前一个数据集相似：它是一个具有节点分类任务的社交网络。然而，它与`Cora`有三大不同之处：'
- en: The number of nodes is much higher (2,708 versus 22,470)
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点的数量要高得多（2,708 与 22,470）
- en: The dimensionality of the node features decreased dramatically (from 1,433 to
    128)
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点特征的维度大幅度降低（从 1,433 降到 128）
- en: The goal is to classify each node into four categories instead of seven (which
    is easier since there are fewer options)
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标是将每个节点分类为四个类别，而不是七个类别（这更容易，因为选项更少）。
- en: 'The following figure is a visualization of the dataset using Gephi. First,
    nodes with few connections have been filtered out to improve performance. The
    size of the remaining nodes depends on their number of connections, and their
    color indicates the category they belong to. Finally, two layouts have been applied:
    Fruchterman-Reingold and ForceAtlas2.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 下图是使用 Gephi 可视化的数据集。首先，少量连接的节点已被过滤掉，以提高性能。剩余节点的大小取决于它们的连接数量，颜色表示它们所属的类别。最后，应用了两种布局：Fruchterman-Reingold
    和 ForceAtlas2。
- en: '![Figure 5.2 – The Facebook Page-Page dataset visualized with Gephi](img/B19153_05_002.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.2 – 使用 Gephi 可视化的 Facebook 页面-页面数据集](img/B19153_05_002.jpg)'
- en: Figure 5.2 – The Facebook Page-Page dataset visualized with Gephi
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 – 使用 Gephi 可视化的 Facebook 页面-页面数据集
- en: 'We can import the `Facebook Page-Page` dataset the same way we did it for `Cora`:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以以与`Cora`相同的方式导入`Facebook 页面-页面`数据集：
- en: 'We import the `FacebookPagePage` class from PyTorch Geometric:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从 PyTorch Geometric 导入`FacebookPagePage`类：
- en: '[PRE7]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We download it using this class:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用这个类下载它：
- en: '[PRE8]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We store the graph in a dedicated `data` variable:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将图存储在一个专用的`data`变量中：
- en: '[PRE9]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let’s print information about the dataset in general:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们打印一下关于数据集的一般信息：
- en: '[PRE10]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This gives us the following output:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这会给我们以下输出：
- en: '[PRE11]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The same dedicated functions can be applied here:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同样的专用函数可以在这里应用：
- en: '[PRE12]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This is the result of the previous block:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这是上一部分的结果：
- en: '[PRE13]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Unlike `Cora`, `Facebook Page-Page` doesn’t have training, evaluation, and
    test masks by default. We can arbitrarily create masks with the `range()` function:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与`Cora`不同，`Facebook 页面-页面`数据集默认没有训练、评估和测试掩码。我们可以使用`range()`函数随意创建掩码：
- en: '[PRE14]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Alternatively, PyTorch Geometric offers a transform function to calculate random
    masks when the dataset is loaded:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，PyTorch Geometric 提供了一个转换函数，用来在加载数据集时计算随机掩码：
- en: '[PRE15]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The first output confirms the number of nodes and classes we saw in the description
    of the dataset. The second output tells us that this graph has `self` loops: some
    pages are connected to themselves. This is surprising but, in practice, it will
    not matter, as we’re going to see soon.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个输出确认了我们在数据集描述中看到的节点和类别的数量。第二个输出告诉我们，这个图包含了`self`环路：一些页面与自己连接。这虽然让人惊讶，但实际上它不会影响结果，因为我们很快就会看到。
- en: These are the two graph datasets we will use in the next section, to compare
    the performance of a Vanilla Neural Network to the performance of our first GNN.
    Let’s implement them step by step.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们将在下一部分中使用的两个图数据集，用来比较普通神经网络与我们第一个图神经网络（GNN）的性能。让我们一步步实现它们。
- en: Classifying nodes with vanilla neural networks
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用普通神经网络进行节点分类
- en: 'Compared to Zachary’s Karate Club, these two datasets include a new type of
    information: node features. They provide additional information about the nodes
    in a graph, such as a user’s age, gender, or interests in a social network. In
    a vanilla neural network (also called **multilayer perceptron**), these embeddings
    are directly used in the model to perform downstream tasks such as node classification.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will consider node features as a regular tabular dataset.
    We will train a simple neural network on this dataset to classify our nodes. Note
    that this architecture does not take into account the topology of the network.
    We will try to fix this issue in the next section and compare our results.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: 'The tabular dataset of node features can be easily accessed through the `data`
    object we created. First, I would like to convert this object into a regular pandas
    DataFrame by merging `data.x` (containing the node features) and `data.y` (containing
    the class label of each node among seven classes). In the following, we will use
    the `Cora` dataset:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This gives us the following dataset:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **0** | **1** | **…** | **1432** | **label** |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
- en: '| **0** | 0 | 0 | … | 0 | 3 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
- en: '| **1** | 0 | 0 | … | 0 | 4 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
- en: '| **…** | … | … | … | … | … |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
- en: '| **2707** | 0 | 0 | … | 0 | 3 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
- en: Figure 5.3 – Tabular representation of the Cora dataset (without topological
    information)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: If you’re familiar with machine learning, you probably recognize a typical dataset
    with data and labels. We can develop a simple `data.x` with the labels provided
    by `data.y`.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create our own MLP class with four methods:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '`__init__()` to initialize an instance'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`forward()` to perform the forward pass'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fit()` to train the model'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`test()` to evaluate it'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Before we can train our model, we must define the main metric. There are several
    metrics for multiclass classification problems: accuracy, F1 score, **Area Under
    the Receiver Operating Characteristic Curve** (**ROC AUC**) score, and so on.
    For this work, let’s implement a simple accuracy, which is defined as the fraction
    of correct predictions. It is not the best metric for multiclass classification,
    but it is simpler to understand. Feel free to replace it with your metric of choice:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, we can start the actual implementation. We don’t need PyTorch Geometric
    to implement the MLP in this section. Everything can be done in regular PyTorch
    with the following steps:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: 'We import the required classes from PyTorch:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We create a new class called `MLP`, which will inherit all the methods and
    properties from `torch.nn.Module`:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The `__init__()` method has three arguments (`dim_in`, `dim_h`, and `dim_out`)
    for the number of neurons in the input, hidden, and output layers, respectively.
    We also define two linear layers:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The `forward()` method performs the forward pass. The input is fed to the first
    linear layer with a **Rectified Linear Unit** (**ReLU**) activation function,
    and the result is passed to the second linear layer. We return the log softmax
    of this final result for classification:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The `fit()` method is in charge of the training loop. First, we initialize
    a loss function and an optimizer that will be used during the training process:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'A regular PyTorch training loop is then implemented. We use our `accuracy()`
    function on top of the loss function:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'In the same loop, we plot the loss and accuracy for training and evaluation
    data every 20 epochs:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The `test()` method evaluates the model on the test set and returns the accuracy
    score:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Now that our class is complete, we can create, train, and test an instance of
    MLP.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: 'We have two datasets, so we need a model dedicated to `Cora` and another one
    for `Facebook Page-Page`. First, let’s train an MLP on `Cora`:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: 'We create an MLP model and print it to check that our layers are correct:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This gives us the following output:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Good, we get the right number of features. Let’s train this model for 100 epochs:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Here are the metrics that are printed in the training loop:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Finally, we can evaluate its performance in terms of accuracy with the following
    lines:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We obtain the following accuracy score on test data:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We repeat the same process for the `Facebook Page-Page` dataset, and here is
    the output we obtain:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Even though these datasets are similar in some aspects, we can see that the
    accuracy scores we obtain are vastly different. This will make an interesting
    comparison when we combine node features and network topology in the same model.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Classifying nodes with vanilla graph neural networks
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Instead of directly introducing well-known GNN architectures, let’s try to build
    our own model to understand the thought process behind GNNs. First, we need to
    go back to the definition of a simple linear layer.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: A basic neural network layer corresponds to a linear transformation ![](img/Formula_B19153_05_001.png),
    where ![](img/Formula_B19153_05_002.png) is the input vector of node ![](img/Formula_B19153_05_005.png)
    and ![](img/Formula_B19153_05_003.png) is the weight matrix. In PyTorch, this
    equation can be implemented with the `torch.mm()` function, or with the `nn.Linear`
    class that adds other parameters such as biases.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 'With our graph datasets, the input vectors are node features. It means that
    nodes are completely separate from each other. This is not enough to capture a
    good understanding of the graph: like a pixel in an image, the context of a node
    is essential to understand it. If you look at a group of pixels instead of a single
    one, you can recognize edges, patterns, and so on. Likewise, to understand a node,
    you need to look at its neighborhood.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s call ![](img/Formula_B19153_05_004.png) the set of neighbors of node
    ![](img/Formula_B19153_05_0051.png). Our **graph linear layer** can be written
    as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B19153_05_006.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
- en: You can imagine several variants of this equation. For instance, we could have
    a weight matrix ![](img/Formula_B19153_05_007.png) dedicated to the central node,
    and another one ![](img/Formula_B19153_05_008.png) for the neighbors. Note that
    we cannot have a weight matrix per neighbor, as this number can change from node
    to node.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: We’re talking about neural networks, so we can’t apply the previous equation
    to each node. Instead, we perform matrix multiplications that are much more efficient.
    For instance, the equation of the linear layer can be rewritten as ![](img/Formula_B19153_05_009.png),
    where ![](img/Formula_B19153_05_010.png) is the input matrix.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, the adjacency matrix ![](img/Formula_B19153_05_011.png) contains
    the connections between every node in the graph. Multiplying the input matrix
    by this adjacency matrix will directly sum up the neighboring node features. We
    can add `self` loops to the adjacency matrix so that the central node is also
    considered in this operation. We call this updated adjacency matrix ![](img/Formula_B19153_05_012.png).
    Our graph linear layer can be rewritten as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B19153_05_013.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
- en: 'Let’s test this layer by implementing it in PyTorch Geometric. We’ll then be
    able to use it as a regular layer to build a GNN:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create a new class, which is a subclass of `torch.nn.Module`:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This class takes two parameters, `dim_in` and `dim_out`, for the number of
    features of the input and the output, respectively. We add a basic linear transformation
    without bias:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We perform two operations – the linear transformation, and then the multiplication
    with the adjacency matrix ![](img/Formula_B19153_05_014.png):'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Before we can create our vanilla GNN, we need to convert the edge index from
    our dataset (`data.edge_index`) in coordinate format to a dense adjacency matrix.
    We also need to include `self` loops; otherwise, the central nodes won’t be taken
    into account in their own embeddings.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'This is easily implemented with the `to_den_adj()` and `torch.eye()` functions:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Here is what the adjacency matrix looks like:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Unfortunately, we only see zeros in this tensor because it’s a sparse matrix.
    A more detailed print would show a few connections between nodes (represented
    by ones). Now that we have our dedicated layer and the adjacency matrix, the implementation
    of the vanilla GNN is very similar to that of the MLP.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 'We create a new class with two vanilla graph linear layers:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We perform the same operations with our new layers, which take the adjacency
    matrix we previously calculated as an additional input:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The `fit()` and `test()` methods work in the exact same way:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We can create, train, and evaluate our model with the following lines:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'This gives us the following output:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We replicate the same training process with the `Facebook Page-Page` dataset.
    In order to obtain comparable results, the same experiment is repeated 100 times
    for each model on each dataset. The following table summarizes the results:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **MLP** | **GNN** |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
- en: '| **Cora** | 53.47%(±1.81%) | 74.98%(±1.50%) |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
- en: '| **Facebook** | 75.21%(±0.40%) | 84.85%(±1.68%) |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
- en: Figure 5.4 – Summary of accuracy scores with a standard deviation
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the MLP has poor accuracy on `Cora`. It performs better on the
    `Facebook Page-Page` dataset but is still surpassed by our vanilla GNN in both
    cases. These results show the importance of including topological information
    in node features. Instead of a tabular dataset, our GNN considers the entire neighborhood
    of each node, which leads to a 10-20% boost in terms of accuracy in these examples.
    This architecture is still crude, but it gives us a guideline to refine it and
    build even better models.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we learned about the missing link between vanilla neural networks
    and GNNs. We built our own GNN architecture using our intuition and a bit of linear
    algebra. We explored two popular graph datasets from the scientific literature
    to compare our two architectures. Finally, we implemented them in PyTorch and
    evaluated their performance. The result is clear: even our intuitive version of
    a GNN completely outperforms the MLP on both datasets.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 'In [*Chapter 6*](B19153_06.xhtml#_idTextAnchor074), *Normalizing Embeddings
    with Graph Convolutional Networks*, we refine our vanilla GNN architecture to
    correctly normalize its inputs. This graph convolutional network model is an incredibly
    efficient baseline we’ll keep using in the rest of the book. We will compare its
    results on our two previous datasets and introduce a new interesting task: node
    regression.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad,
    “Collective Classification in Network Data”, AIMag, vol. 29, no. 3, p. 93, Sep.
    2008\. Available: [https://ojs.aaai.org//index.php/aimagazine/article/view/2157](https://ojs.aaai.org//index.php/aimagazine/article/view/2157)'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] B. Rozemberczki, C. Allen, and R. Sarkar, Multi-Scale Attributed Node Embedding.
    arXiv, 2019\. doi: 10.48550/ARXIV.1909.13021\. Available: [https://arxiv.org/abs/1909.13021](https://arxiv.org/abs/1909.13021)'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
