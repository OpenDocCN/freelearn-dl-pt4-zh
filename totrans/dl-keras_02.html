<html><head></head><body>
        <section>

            <header>
                <h1 class="header-title">Keras Installation and API</h1>
            </header>

            <article>
                
<p>In the previous chapter, we discussed the basic principles of neural networks and provided a few examples of nets that are able to recognize MNIST handwritten numbers.</p>
<p>This chapter explains how to install Keras, Theano, and TensorFlow. Step by step, we will look at how to get the environment working and move from intuition to working nets in very little time. Then we will discuss how to install on a dockerized infrastructure based on containers, and in the cloud with Google GCP, Amazon AWS, and Microsoft Azure. In addition to that, we will present an overview of Keras APIs, and some commonly useful operations such as loading and saving neural networks' architectures and weights, early stopping, history saving, checkpointing, and interactions with TensorBoard and Quiver. Let us start.</p>
<p>By the end of this chapter, we will have covered the following topics:</p>
<ul>
<li>Installing and configuring Keras</li>
<li>Keras architecture</li>
</ul>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Installing Keras</h1>
            </header>

            <article>
                
<p>In the sections to follow, we will show how to install Keras on multiple platforms.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Step 1 — install some useful dependencies</h1>
            </header>

            <article>
                
<p>First, we install the <kbd>numpy</kbd> package, which provides support for large, multidimensional arrays and matrices as well as high-level mathematical functions. Then we install <kbd>scipy</kbd>, a library used for scientific computation. After that, it might be appropriate to install <kbd>scikit-learn</kbd>, a package considered the Python Swiss army knife for machine learning. In this case, we will use it for data exploration. Optionally, it could be useful to install <kbd>pillow</kbd>, a library useful for image processing, and <kbd>h5py</kbd>, a library useful for data serialization used by Keras for model saving. A single command line is enough for installing what is needed. Alternatively, one can install Anaconda Python, which will automatically install <kbd>numpy</kbd>, <kbd>scipy</kbd>, <kbd>scikit-learn</kbd>, <kbd>h5py</kbd>, <kbd>pillow</kbd>, and a lot of other libraries that are needed for scientific computing (for more information, refer to: <span><em>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</em>, by S. Ioffe and C. Szegedy, <a href="https://arxiv.org/abs/1502.03167" target="_blank">arXiv.org/abs/1502.03167</a></span><span>, 2015</span>). You can find the packages available in Anaconda Python at <a href="https://docs.continuum.io/anaconda/pkg-docs" target="_blank">https://docs.continuum.io/anaconda/pkg-docs</a>. <span>The following screenshot shows how to install the packages for our work:</span></p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" height="290" src="assets/image_02_001.png" width="685"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Step 2 — install Theano</h1>
            </header>

            <article>
                
<p>We can use <kbd>pip</kbd> to install Theano, as shown in the following screenshot:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/image_02_002.png"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Step 3 — install TensorFlow</h1>
            </header>

            <article>
                
<p>Now we can install TensorFlow using the instructions found on the TensorFlow website at <a href="https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#pip-installation" target="_blank">https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#pip-installation</a>. Again, we simply use <kbd>pip</kbd> for installing the correct package, as shown in the following screenshot. For instance, if we need to use GPUs, it is important to pick the appropriate package:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/image_02_003.png"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Step 4 — install Keras</h1>
            </header>

            <article>
                
<p>Now we can simply install Keras and start testing the installed environment. Pretty simple; let's use <kbd>pip</kbd> again, as shown in this screenshot:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" height="203" src="assets/image_02_004.png" width="659"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Step 5 — testing Theano, TensorFlow, and Keras</h1>
            </header>

            <article>
                
<p>Now let's test the environment. First let's look at how to define the sigmoid function in Theano. As you see, it is very simple; we just write the mathematical formula and compute the function element-wise on a matrix. Just run the Python Shell and write the code as shown in the following screenshot to get the result:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" height="146" src="assets/image_02_005.png" width="680"/></div>
<p>So, Theano works. Let's test TensorFlow by simply importing the MNIST dataset as shown in the following screenshot. We have already seen, in <a href="c2484fb4-248d-49ed-8166-06aff812e5e9.xhtml" target="_blank">Chapter 1</a>, <em>Neural Networks Foundations</em>, a few working examples of the Keras network:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" height="252" src="assets/image_02_006.png" width="686"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Configuring Keras</h1>
            </header>

            <article>
                
<p>Keras has a very minimalist configuration file. Let's load it with a <kbd>vi</kbd> session. The parameters are very simple:</p>
<table class="a" style="width: 578px;height: 277px">
<tbody>
<tr>
<td><strong>Parameters</strong></td>
<td><strong>Values</strong></td>
</tr>
<tr>
<td><kbd>image_dim_ordering</kbd></td>
<td>Can be either <kbd>tf</kbd> for the TensorFlow image ordering or <kbd>th</kbd> for Theano image ordering</td>
</tr>
<tr>
<td><kbd>epsilon</kbd></td>
<td>The <kbd>epsilon</kbd> value used during computation</td>
</tr>
<tr>
<td><kbd>floatx</kbd></td>
<td>Can be either <kbd>float32</kbd> or <kbd>float64</kbd></td>
</tr>
<tr>
<td><kbd>backend</kbd></td>
<td>Can be either <kbd>tensorflow</kbd> or <kbd>theano</kbd></td>
</tr>
</tbody>
</table>
<p>The <kbd>image_dim_ordering</kbd> of <kbd>th</kbd> value gives you a somewhat non-intuitive dimension ordering for images (depth, width, and height), instead of (width, height, and depth), for <kbd>tf</kbd>. The following are the default parameters in my machine:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/image_02_007.png"/></div>
<div class="packt_infobox">If you install a GPU-enabled TensorFlow version, then Keras will automatically use your configured GPU when TensorFlow is selected as the backend.</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Installing Keras on Docker</h1>
            </header>

            <article>
                
<p>One of the easiest ways to get started with TensorFlow and Keras is running in a Docker container. A convenient solution is to use a predefined Docker image for deep learning created by the community that contains all the popular DL frameworks (TensorFlow, Theano, Torch, Caffe, and so on). Refer to the GitHub repository at <a href="https://github.com/saiprashanths/dl-docker" target="_blank">https://github.com/saiprashanths/dl-docker</a> for the code files. Assuming that you already have Docker up and running (for more information, refer to <a href="https://www.docker.com/products/overview" target="_blank">https://www.docker.com/products/overview</a>), installing it is pretty simple and is shown as follows:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/image_02_008.png"/></div>
<p class="packt_figure CDPAlignLeft CDPAlign">The following screenshot, says something like, after getting the image from Git, we build the Docker image:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/image_02_009.png"/></div>
<p class="packt_figure CDPAlignLeft CDPAlign">In this screenshot, we see how to run it:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/image_02_010.png"/></div>
<p>From within the container, it is possible to activate support for Jupyter Notebooks (for more information, refer to <a href="http://jupyter.org/" target="_blank">http://jupyter.org/</a>):</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/image_02_011.png"/></div>
<p>Access it directly from the host machine on port:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/image_02_012.png"/></div>
<p>It is also possible to access TensorBoard (for more information, refer to <a href="https://www.tensorflow.org/how_tos/summaries_and_tensorboard/" target="_blank">https://www.tensorflow.org/how_tos/summaries_and_tensorboard/</a>) with the help of the command in the screenshot that follows, which is discussed in the next section:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/image_02_013.png"/></div>
<div class="packt_figure CDPAlignLeft CDPAlign">
<p>After running the preceding command, you will be redirected to the following page:</p>
<p><img class="image-border" src="assets/image_02_014.png"/></p>
</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Installing Keras on Google Cloud ML</h1>
            </header>

            <article>
                
<p>Installing Keras on Google Cloud is very simple. First, we can install Google Cloud (for the downloadable file, refer to <a href="https://cloud.google.com/sdk/" target="_blank">https://cloud.google.com/sdk/</a>)<em>,</em> a command-line interface for Google Cloud Platform; then we can use CloudML, a managed service that enables us to easily build machine, learning models with TensorFlow. Before using Keras, let's use Google Cloud with TensorFlow to train an MNIST example available on GitHub. The code is local and training happens in the cloud:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/image_02_015.png"/></div>
<p class="packt_figure CDPAlignLeft CDPAlign">In the following screenshot, you can see how to run a training session:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/image_02_016.png"/></div>
<p>We can use TensorBoard to show how cross-entropy decreases across iterations:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/image_02_017.png"/></div>
<p class="packt_figure CDPAlignLeft CDPAlign">In the next screenshot, we see the graph of cross-entropy:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" height="277" src="assets/image_02_018.png" width="537"/></div>
<p>Now, if we want to use Keras on the top of TensorFlow, we simply download the Keras source from PyPI (for the downloadable file, refer to <a href="https://pypi.Python.org/pypi/Keras/1.2.0" target="_blank">https://pypi.Python.org/pypi/Keras/1.2.0</a> or later versions) and then directly use Keras as a CloudML package solution, as in the following example:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" height="251" src="assets/image_02_019.png" width="534"/></div>
<p>Here, <kbd>trainer.task2.py</kbd> is an example script:</p>
<pre>
from keras.applications.vgg16 import VGG16<br/>from keras.models import Model<br/>from keras.preprocessing import image<br/>from keras.applications.vgg16 import preprocess_input<br/>import numpy as np<br/><br/># pre-built and pre-trained deep learning VGG16 model<br/>base_model = VGG16(weights='imagenet', include_top=True)<br/>for i, layer in enumerate(base_model.layers):<br/>  print (i, layer.name, layer.output_shape)
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Installing Keras on Amazon AWS</h1>
            </header>

            <article>
                
<p>Installing TensorFlow and Keras on Amazon is very simple. Indeed, it is possible to use a prebuilt AMI named <kbd>TFAMI.v3</kbd> that is open and free (for more information, refer to <a href="https://github.com/ritchieng/tensorflow-aws-ami" target="_blank">https://github.com/ritchieng/tensorflow-aws-ami</a>), shown as follows:</p>
<p><img class="image-border" src="assets/image_02_020.png"/></p>
<p>This AMI runs TensorFlow in less than five minutes and supports TensorFlow, Keras, OpenAI Gym, and all dependencies. As of January 2017, it supports the following:</p>
<ul>
<li>TensorFlow 0.12 <a href="https://github.com/tensorflow/tensorflow/tree/9d66dae6fc5d1b964a03498ddabb97a78a999015"/></li>
<li>Keras 1.1.0</li>
<li>TensorLayer 1.2.7</li>
<li>CUDA 8.0</li>
<li>CuDNN 5.1</li>
<li>Python 2.7</li>
<li>Ubuntu 16.04</li>
</ul>
<p><span>In addition,</span> <kbd>TFAMI.v3</kbd> <span>works on P2 computing instances (for more information, refer to </span><a href="https://aws.amazon.com/ec2/instance-types/#p2" target="_blank">https://aws.amazon.com/ec2/instance-types/#p2</a><span>), as shown in the following screenshot:</span></p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/image_02_021.png"/></div>
<p>Some features of P2 instances are as follows:</p>
<ul>
<li>Intel Xeon E5-2686v4 (Broadwell) processors</li>
<li>NVIDIA K80 GPUs, each with 2,496 parallel cores and 12 GB of GPU memory</li>
<li>Supports peer-to-peer GPU communication</li>
<li>Provides enhanced networking (for more information, refer to <a href="https://aws.amazon.com/ec2/faqs/#What_networking_capabilities_are_included_in_this_feature" target="_blank">https://aws.amazon.com/ec2/faqs/#What_networking_capabilities_are_included_in_this_feature</a>) with 20 Gbps of aggregate network bandwidth</li>
</ul>
<p>The <kbd>TFAMI.v3</kbd> also works on G2 computing instances (for more information, refer to <a href="https://aws.amazon.com/ec2/instance-types/#g2" target="_blank">https://aws.amazon.com/ec2/instance-types/#g2</a>). <span>Some features of G2 instances are as follows</span>:</p>
<ul>
<li>Intel Xeon E5-2670 (Sandy Bridge) processors</li>
<li>NVIDIA GPUs, each with 1,536 CUDA cores and 4 GB of video memory</li>
</ul>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Installing Keras on Microsoft Azure</h1>
            </header>

            <article>
                
<p>One way to install Keras on Azure is to install the support for Docker and then get a containerized version of TensorFlow plus Keras. Online, it is also possible to find a detailed set of instructions on how to install Keras and TensorFlow with Docker, but this is essentially what we have seen already in a previous section (for more information, refer to <a href="https://blogs.msdn.microsoft.com/uk_faculty_connection/2016/09/26/tensorflow-on-docker-with-microsoft-azure/" target="_blank">https://blogs.msdn.microsoft.com/uk_faculty_connection/2016/09/26/tensorflow-on-docker-with-microsoft-azure/</a>).</p>
<p>If you use Theano as the only backend, then Keras can run with just a click by loading a pre-built package available on Cortana Intelligence Gallery (for more information, refer to <a href="https://gallery.cortanaintelligence.com/Experiment/Theano-Keras-1" target="_blank">https://gallery.cortanaintelligence.com/Experiment/Theano-Keras-1</a>).<br/>
The following sample shows how to import Theano and Keras into Azure ML directly as a ZIP file and use them in the Execute Python Script module. This example is due to Hai Ning (for more information, refer to <a href="https://goo.gl/VLR25o" target="_blank">https://goo.gl/VLR25o</a>), and it essentially runs the Keras code within the <kbd>azureml_main()</kbd> method:</p>
<pre>
# The script MUST contain a function named azureml_main<br/># which is the entry point for this module.<br/><br/># imports up here can be used to<br/>import pandas as pd<br/>import theano<br/>import theano.tensor as T<br/>from theano import function<br/>from keras.models import Sequential<br/>from keras.layers import Dense, Activation<br/>import numpy as np<br/># The entry point function can contain up to two input arguments:<br/>#   Param&lt;dataframe1&gt;: a pandas.DataFrame<br/>#   Param&lt;dataframe2&gt;: a pandas.DataFrame<br/>def azureml_main(dataframe1 = None, dataframe2 = None):<br/>    # Execution logic goes here<br/>    # print('Input pandas.DataFrame #1:rnrn{0}'.format(dataframe1))<br/><br/>    # If a zip file is connected to the third input port is connected,<br/>    # it is unzipped under ".Script Bundle". This directory is added<br/>    # to sys.path. Therefore, if your zip file contains a Python file<br/>    # mymodule.py you can import it using:<br/>    # import mymodule<br/>    model = Sequential()<br/>    model.add(Dense(1, input_dim=784, activation="relu"))<br/>    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])<br/>    data = np.random.random((1000,784))<br/>    labels = np.random.randint(2, size=(1000,1))<br/>    model.fit(data, labels, nb_epoch=10, batch_size=32)<br/>    model.evaluate(data, labels)<br/><br/>    return dataframe1,
</pre>
<p>In this screenshot, you see an example use of Microsoft Azure ML to run Theano and Keras:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" height="415" src="assets/image_02_022.png" width="504"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Keras API</h1>
            </header>

            <article>
                
<p>Keras has a modular, minimalist, and easy extendable architecture. Francois Chollet, the author of Keras, says:</p>
<div class="packt_quote">The library was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research.</div>
<p>Keras defines high-level neural networks running on top of either TensorFlow (for more information, refer to <a href="https://github.com/tensorflow/tensorflow" target="_blank">https://github.com/tensorflow/tensorflow</a>) or Theano (<span>for more information, refer to </span><a href="https://github.com/Theano/Theano" target="_blank">https://github.com/Theano/Theano</a>). In details:</p>
<ul>
<li><strong>Modularity</strong>: A model is either a sequence or a graph of standalone modules that can be combined together like LEGO blocks for building neural networks. Namely, the library predefines a very large number of modules implementing different types of neural layers, cost functions, optimizers, initialization schemes, activation functions, and regularization schemes.</li>
<li><strong>Minimalism</strong>: The library is implemented in Python and each module is kept short and self-describing.</li>
<li><strong>Easy extensibility</strong>: The library can be extended with new functionalities, as we will describe in <a href="9384823c-eb58-4a0f-91e7-1a5508eeb520.xhtml" target="_blank">Chapter 7</a>, <span><em>Additional Deep Learning Models</em>.</span></li>
</ul>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Getting started with Keras architecture</h1>
            </header>

            <article>
                
<p>In this section, we review the most important Keras components used for defining neural networks. First, we define what a tensor is, then we discuss different ways of composing predefined modules, and we conclude with an overview of the ones most commonly used.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">What is a tensor?</h1>
            </header>

            <article>
                
<p>Keras uses either Theano or TensorFlow to perform very efficient computations on tensors. But what is a tensor anyway? A tensor is nothing but a multidimensional array or matrix. Both the backends are capable of efficient symbolic computations on tensors, which are the fundamental building blocks for creating neural networks.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Composing models in Keras</h1>
            </header>

            <article>
                
<p>There are two ways of composing models in Keras. They are as follows:</p>
<ul>
<li>Sequential composition</li>
<li>Functional composition</li>
</ul>
<p>Let us take a look at each one in detail.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Sequential composition</h1>
            </header>

            <article>
                
<p>The first one is the sequential composition, where different predefined models are stacked together in a linear pipeline of layers similar to a stack or a queue. In <a href="c2484fb4-248d-49ed-8166-06aff812e5e9.xhtml">Chapter 1</a>, <em>Neural Networks Foundations</em>, we saw a few examples of sequential pipelines. For instance:</p>
<pre>
model = Sequential()<br/>model.add(Dense(N_HIDDEN, input_shape=(784,)))<br/>model.add(Activation('relu'))<br/>model.add(Dropout(DROPOUT))<br/>model.add(Dense(N_HIDDEN))<br/>model.add(Activation('relu'))<br/>model.add(Dropout(DROPOUT))<br/>model.add(Dense(nb_classes))<br/>model.add(Activation('softmax'))<br/>model.summary()
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Functional composition</h1>
            </header>

            <article>
                
<p>The second way of composing modules is via the functional API, where it is possible to define complex models, such as directed acyclic graphs, models with shared layers, or multi-output models. We will see such examples in <a href="9384823c-eb58-4a0f-91e7-1a5508eeb520.xhtml">Chapter 7</a>, <em>Additional Deep Learning Models</em>.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">An overview of predefined neural network layers</h1>
            </header>

            <article>
                
<p>Keras has a number of prebuilt layers. Let us review the most commonly used ones and highlight in which chapter these layers are mostly used.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Regular dense</h1>
            </header>

            <article>
                
<p>A dense model is a fully connected neural network layer. We have already seen examples of usage in <a href="c2484fb4-248d-49ed-8166-06aff812e5e9.xhtml">Chapter 1</a>, <em>Neural Networks Foundations</em>. Here is the prototype with a definition of the parameters:</p>
<pre>
keras.layers.core.Dense(units, activation=<span class="hljs-keyword">None</span>, use_bias=<span class="hljs-keyword">True</span>, kernel_initializer=<span class="hljs-string">'glorot_uniform'</span>, bias_initializer=<span class="hljs-string">'zeros'</span>, kernel_regularizer=<span class="hljs-keyword">None</span>, bias_regularizer=<span class="hljs-keyword">None</span>, activity_regularizer=<span class="hljs-keyword">None</span>, kernel_constraint=<span class="hljs-keyword">None</span>, bias_constraint=<span class="hljs-keyword">None)</span>
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Recurrent neural networks — simple, LSTM, and GRU</h1>
            </header>

            <article>
                
<p>Recurrent neural networks are a class of neural networks that exploit the sequential nature of their input. Such inputs could be a text, a speech, time series, and anything else where the occurrence of an element in the sequence is dependent on the elements that appeared before it. We will discuss simple, LSTM, and GRU recurrent neural networks in <a href="57a694a6-93f4-4eec-9fbf-e4eafd2d6824.xhtml">Chapter 6</a>, <em>Recurrent Neural Network — RNN</em>. Here you can see some prototypes with a definition of the parameters:</p>
<pre>
keras.layers.recurrent.Recurrent(return_sequences=<span class="hljs-keyword">False</span>, go_backwards=<span class="hljs-keyword">False</span>, stateful=<span class="hljs-keyword">False</span>, unroll=<span class="hljs-keyword">False</span>, implementation=<span class="hljs-number">0</span>)<br/><br/>keras.layers.recurrent.SimpleRNN(units, activation=<span class="hljs-string">'tanh'</span>, use_bias=<span class="hljs-keyword">True</span>, kernel_initializer=<span class="hljs-string">'glorot_uniform'</span>, recurrent_initializer=<span class="hljs-string">'orthogonal'</span>, bias_initializer=<span class="hljs-string">'zeros'</span>, kernel_regularizer=<span class="hljs-keyword">None</span>, recurrent_regularizer=<span class="hljs-keyword">None</span>, bias_regularizer=<span class="hljs-keyword">None</span>, activity_regularizer=<span class="hljs-keyword">None</span>, kernel_constraint=<span class="hljs-keyword">None</span>, recurrent_constraint=<span class="hljs-keyword">None</span>, bias_constraint=<span class="hljs-keyword">None</span>, dropout=<span class="hljs-number">0.0</span>, recurrent_dropout=<span class="hljs-number">0.0)<br/></span><br/>keras.layers.recurrent.GRU(units, activation=<span class="hljs-string">'tanh'</span>, recurrent_activation=<span class="hljs-string">'hard_sigmoid'</span>, use_bias=<span class="hljs-keyword">True</span>, kernel_initializer=<span class="hljs-string">'glorot_uniform'</span>, recurrent_initializer=<span class="hljs-string">'orthogonal'</span>, bias_initializer=<span class="hljs-string">'zeros'</span>, kernel_regularizer=<span class="hljs-keyword">None</span>, recurrent_regularizer=<span class="hljs-keyword">None</span>, bias_regularizer=<span class="hljs-keyword">None</span>, activity_regularizer=<span class="hljs-keyword">None</span>, kernel_constraint=<span class="hljs-keyword">None</span>, recurrent_constraint=<span class="hljs-keyword">None</span>, bias_constraint=<span class="hljs-keyword">None</span>, dropout=<span class="hljs-number">0.0</span>, recurrent_dropout=<span class="hljs-number">0.0</span>)<br/><br/>keras.layers.recurrent.LSTM(units, activation=<span class="hljs-string">'tanh'</span>, recurrent_activation=<span class="hljs-string">'hard_sigmoid'</span>, use_bias=<span class="hljs-keyword">True</span>, kernel_initializer=<span class="hljs-string">'glorot_uniform'</span>, recurrent_initializer=<span class="hljs-string">'orthogonal'</span>, bias_initializer=<span class="hljs-string">'zeros'</span>, unit_forget_bias=<span class="hljs-keyword">True</span>, kernel_regularizer=<span class="hljs-keyword">None</span>, recurrent_regularizer=<span class="hljs-keyword">None</span>, bias_regularizer=<span class="hljs-keyword">None</span>, activity_regularizer=<span class="hljs-keyword">None</span>, kernel_constraint=<span class="hljs-keyword">None</span>, recurrent_constraint=<span class="hljs-keyword">None</span>, bias_constraint=<span class="hljs-keyword">None</span>, dropout=<span class="hljs-number">0.0</span>, recurrent_dropout=<span class="hljs-number">0.0</span>)
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Convolutional and pooling layers</h1>
            </header>

            <article>
                
<p>ConvNets are a class of neural networks using convolutional and pooling operations for progressively learning rather sophisticated models based on progressive levels of abstraction. This learning via progressive abstraction resembles vision models that have evolved over millions of years inside the human brain. People called it <em>deep</em> with 3-5 layers a few years ago, and now it has gone up to 100-200. We will discuss convolutional neural networks in <a href="4be2a04a-4545-4051-bcd9-32764d21f0f2.xhtml">Chapter 3</a>, <em>Deep Learning with ConvNets</em>. Here are some prototypes with a definition of the parameters:</p>
<pre>
keras.layers.convolutional.Conv1D(filters, kernel_size, strides=<span class="hljs-number">1</span>, padding=<span class="hljs-string">'valid'</span>, dilation_rate=<span class="hljs-number">1</span>, activation=<span class="hljs-keyword">None</span>, use_bias=<span class="hljs-keyword">True</span>, kernel_initializer=<span class="hljs-string">'glorot_uniform'</span>, bias_initializer=<span class="hljs-string">'zeros'</span>, kernel_regularizer=<span class="hljs-keyword">None</span>, bias_regularizer=<span class="hljs-keyword">None</span>, activity_regularizer=<span class="hljs-keyword">None</span>, kernel_constraint=<span class="hljs-keyword">None</span>, bias_constraint=<span class="hljs-keyword">None</span>)<br/><br/>keras.layers.convolutional.Conv2D(filters, kernel_size, strides=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=<span class="hljs-string">'valid'</span>, data_format=<span class="hljs-keyword">None</span>, dilation_rate=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), activation=<span class="hljs-keyword">None</span>, use_bias=<span class="hljs-keyword">True</span>, kernel_initializer=<span class="hljs-string">'glorot_uniform'</span>, bias_initializer=<span class="hljs-string">'zeros'</span>, kernel_regularizer=<span class="hljs-keyword">None</span>, bias_regularizer=<span class="hljs-keyword">None</span>, activity_regularizer=<span class="hljs-keyword">None</span>, kernel_constraint=<span class="hljs-keyword">None</span>, bias_constraint=<span class="hljs-keyword">None</span>)<br/><br/>keras.layers.pooling.MaxPooling1D(pool_size=<span class="hljs-number">2</span>, strides=<span class="hljs-keyword">None</span>, padding=<span class="hljs-string">'valid'</span>)<br/><br/>keras.layers.pooling.MaxPooling2D(pool_size=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), strides=<span class="hljs-keyword">None</span>, padding=<span class="hljs-string">'valid'</span>, data_format=<span class="hljs-keyword">None</span>)
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Regularization</h1>
            </header>

            <article>
                
<p>Regularization is a way to prevent overfitting. We have already seen examples of usage in <a href="c2484fb4-248d-49ed-8166-06aff812e5e9.xhtml" target="_blank">Chapter 1</a>, <em>Neural Networks Foundations</em>. Multiple layers have parameters for regularization.  The following is the list of regularization parameters commonly used for dense, and convolutional modules:</p>
<ul>
<li><kbd>kernel_regularizer</kbd>: Regularizer function applied to the weight matrix</li>
<li><kbd>bias_regularizer</kbd>: Regularizer function applied to the bias vector</li>
<li><kbd>activity_regularizer</kbd>: Regularizer function applied to the output of the layer (its activation)</li>
</ul>
<p>In addition is possible to use Dropout for regularization and that is frequently a very effective choice</p>
<pre>
keras.layers.core.Dropout(rate, noise_shape=<span class="hljs-keyword">None</span>, seed=<span class="hljs-keyword">None</span>)
</pre>
<p>Where:</p>
<ul>
<li><kbd>rate</kbd>: It is a float between 0 and 1 which represents the fraction of the input units to drop</li>
<li><kbd>noise_shape</kbd>: It is a 1D integer tensor which represents the shape of the binary dropout mask that will be multiplied with the input</li>
<li><kbd>seed</kbd>: It is a integer which is used use as random seed</li>
</ul>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Batch normalization</h1>
            </header>

            <article>
                
<p>Batch normalization (for more information, refer to <a href="https://www.colwiz.com/cite-in-google-docs/cid=f20f9683aaf69ce" target="_blank">https://www.colwiz.com/cite-in-google-docs/cid=f20f9683aaf69ce</a>) is a way to accelerate learning and generally achieve better accuracy. We will look at examples of usage in <a href="a67ea944-b1a6-48a3-b8aa-4e698166c0eb.xhtml" target="_blank">Chapter 4</a>, <em>Generative Adversarial Networks and WaveNet</em>, when we discuss GANs. Here is the prototype with a definition of the parameters:</p>
<pre>
keras.layers.normalization.BatchNormalization(axis=-<span class="hljs-number">1</span>, momentum=<span class="hljs-number">0.99</span>, epsilon=<span class="hljs-number">0.001</span>, center=<span class="hljs-keyword">True</span>, scale=<span class="hljs-keyword">True</span>, beta_initializer=<span class="hljs-string">'zeros'</span>, gamma_initializer=<span class="hljs-string">'ones'</span>, moving_mean_initializer=<span class="hljs-string">'zeros'</span>, moving_variance_initializer=<span class="hljs-string">'ones'</span>, beta_regularizer=<span class="hljs-keyword">None</span>, gamma_regularizer=<span class="hljs-keyword">None</span>, beta_constraint=<span class="hljs-keyword">None</span>, gamma_constraint=<span class="hljs-keyword">None</span>)
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">An overview of predefined activation functions</h1>
            </header>

            <article>
                
<p>Activation includes commonly used functions such as s<span>igmoid</span>, linear, h<span>yperbolic tangent</span>, and ReLU. We have seen a few examples of activation functions in <a href="c2484fb4-248d-49ed-8166-06aff812e5e9.xhtml" target="_blank">Chapter 1</a>, <em>Neural Networks Foundations</em>, and more examples will be presented in the next chapters. The following diagrams are examples of sigmoid, linear, hyperbolic tangent, and ReLU activation functions:</p>
<table class="a0">
<tbody>
<tr>
<td>
<p><strong>Sigmoid</strong></p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" height="191" src="assets/image_02_023.png" width="307"/></div>
</td>
<td>
<p><strong>Linear</strong></p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" height="191" src="assets/image_02_024.png" width="307"/></div>
</td>
</tr>
<tr>
<td>
<p><strong>Hyperbolic tangent</strong></p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" height="191" src="assets/image_02_025.png" width="307"/></div>
</td>
<td>
<p><strong>ReLU</strong></p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" height="191" src="assets/image_02_026.png" width="307"/></div>
</td>
</tr>
</tbody>
</table>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">An overview of losses functions</h1>
            </header>

            <article>
                
<p>Losses functions (or objective functions, <span>or optimization score function; for more information, refer to <a href="https://keras.io/losses/" target="_blank">https://keras.io/losses/</a>)</span> can be classified into four categories:</p>
<ul>
<li>Accuracy which is used for classification problems. There are multiple choices: <kbd>binary_accuracy</kbd> (mean accuracy rate across all predictions for binary classification problems), <kbd>categorical_accuracy</kbd> (mean accuracy rate across all predictions for multiclass classification problems), <kbd>sparse_categorical_accuracy</kbd> (useful for sparse targets), and <kbd>top_k_categorical_accuracy</kbd> (success when the target class is within the <kbd>top_k</kbd> predictions provided).</li>
<li>Error loss, which measures the difference between the values predicted and the values actually observed. There are multiple choices: <kbd>mse</kbd> (mean square error between predicted and target values), <kbd>rmse</kbd> (root square error between predicted and target values), <kbd>mae</kbd> (mean absolute error between predicted and target values), <kbd>mape</kbd> (mean percentage error between predicted and target values), and <kbd>msle</kbd> (mean squared logarithmic error between predicted and target values).</li>
<li>Hinge loss, which is generally used for training classifiers. There are two versions: <em>hinge</em> defined as <img height="13" src="assets/image_02_027.png" width="135"/> and <em>squared hinge</em> defined as the the squared value of the hinge loss.</li>
<li>Class loss is used to calculate the cross-entropy for classification problems. There are multiple versions, including binary cross-entropy (for more information, refer to <a href="https://en.wikipedia.org/wiki/Cross_entropy" target="_blank">https://en.wikipedia.org/wiki/Cross_entropy</a>), and categorical cross-entropy.</li>
</ul>
<p>We have seen a few examples of objective functions in <a href="c2484fb4-248d-49ed-8166-06aff812e5e9.xhtml" target="_blank">Chapter 1</a>, <em>Neural Networks Foundations</em>, and more examples will be presented in the next chapters.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">An overview of metrics</h1>
            </header>

            <article>
                
<p>A metric function (for more information, refer to <a href="https://keras.io/metrics/" target="_blank">https://keras.io/metrics/</a>) is similar to an objective function. The only difference is that the results from evaluating a metric are not used when training the model. We have seen a few examples of metrics in <a href="c2484fb4-248d-49ed-8166-06aff812e5e9.xhtml" target="_blank">Chapter 1</a>, <em>Neural Networks Foundations</em>, and more examples will be presented in the next chapters.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">An overview of optimizers</h1>
            </header>

            <article>
                
<p>Optimizers include SGD, RMSprop, and Adam. We have seen a few examples of optimizers in <a href="c2484fb4-248d-49ed-8166-06aff812e5e9.xhtml" target="_blank">Chapter 1</a>, <em>Neural Networks Foundations</em>, and more examples (Adagrad and Adadelta; for more information, refer to <a href="https://keras.io/optimizers/" target="_blank">https://keras.io/optimizers/</a>) will be presented in the next chapters.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Some useful operations</h1>
            </header>

            <article>
                
<p>Here we report some utility operations that can be carried out with Keras APIs. The goal is to facilitate the creation of networks, the training process, and the saving of intermediate results.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Saving and loading the weights and the architecture of a model</h1>
            </header>

            <article>
                
<p>Model architectures can be easily saved and loaded as follows:</p>
<pre>
# save as JSON json_string = model.to_json()<br/># save as YAML yaml_string = model.to_yaml() <br/># model reconstruction from JSON: from keras.models import model_from_json model = model_from_json(json_string) # model reconstruction from YAML model = model_from_yaml(yaml_string)
</pre>
<p>Model parameters (weights) can be easily saved and loaded as follows:</p>
<pre>
from keras.models import load_model model.save('my_model.h5')<br/># creates a HDF5 file 'my_model.h5' del model<br/># deletes the existing model<br/># returns a compiled model<br/># identical to the previous one model = load_model('my_model.h5')
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Callbacks for customizing the training process</h1>
            </header>

            <article>
                
<p>The training process can be stopped when a metric has stopped improving by using an appropriate <kbd>callback</kbd>:</p>
<pre>
keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0,  <br/>patience=0, verbose=0, mode='auto')
</pre>
<p>Loss history can be saved by defining a <kbd>callback</kbd> like the following:</p>
<pre>
class LossHistory(keras.callbacks.Callback):     def on_train_begin(self, logs={}):         self.losses = []     def on_batch_end(self, batch, logs={}):         self.losses.append(logs.get('loss')) model = Sequential() model.add(Dense(10, input_dim=784, init='uniform')) model.add(Activation('softmax')) model.compile(loss='categorical_crossentropy', optimizer='rmsprop') history = LossHistory() model.fit(X_train,Y_train, batch_size=128, nb_epoch=20,  <br/>verbose=0, callbacks=[history]) print history.losses
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Checkpointing</h1>
            </header>

            <article>
                
<p>Checkpointing is a process that saves a snapshot of the application's state at regular intervals, so the application can be restarted from the last saved state in case of failure. This is useful during training of deep learning models, which can often be a time-consuming task. The state of a deep learning model at any point in time is the weights of the model at that time. Keras saves these weights in HDF5 format (for more information, refer to <a href="https://www.hdfgroup.org/" target="_blank">https://www.hdfgroup.org/</a>) and provides checkpointing using its callback API.</p>
<p>Some scenarios where checkpointing can be useful include the following:</p>
<ul>
<li>If you want the ability to restart from your last checkpoint after your AWS Spot instance (<span>for more information, refer to <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/how-spot-instances-work.html" target="_blank">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/how-spot-instances-work.html</a></span>) or Google preemptible virtual machine (<span>for more information, refer to <a href="https://cloud.google.com/compute/docs/instances/preemptible" target="_blank">https://cloud.google.com/compute/docs/instances/preemptible</a></span>) is unexpectedly terminated</li>
<li>If you want to stop training, perhaps to test your model on test data, then continue training from the last checkpoint</li>
<li>If you want to retain the best version (by some metric such as validation loss) as it trains over multiple epochs</li>
</ul>
<p>The first and second scenarios can be handled by saving a checkpoint after each epoch, which is handled by the default usage of the <kbd>ModelCheckpoint</kbd> callback. The following code illustrates how to add checkpointing during training of your deep learning model in Keras:</p>
<pre>
from __future__ import division, print_function <br/>from keras.callbacks import ModelCheckpoint <br/>from keras.datasets import mnist <br/>from keras.models import Sequential <br/>from keras.layers.core import Dense, Dropout <br/>from keras.utils import np_utils <br/>import numpy as np <br/>import os <br/><br/>BATCH_SIZE = 128 <br/>NUM_EPOCHS = 20 <br/>MODEL_DIR = "/tmp" <br/><br/>(Xtrain, ytrain), (Xtest, ytest) = mnist.load_data() <br/>Xtrain = Xtrain.reshape(60000, 784).astype("float32") / 255 <br/>Xtest = Xtest.reshape(10000, 784).astype("float32") / 255 <br/>Ytrain = np_utils.to_categorical(ytrain, 10) <br/>Ytest = np_utils.to_categorical(ytest, 10) <br/>print(Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape) <br/><br/>model = Sequential() <br/>model.add(Dense(512, input_shape=(784,), activation="relu")) <br/>model.add(Dropout(0.2)) <br/>model.add(Dense(512, activation="relu")) <br/>model.add(Dropout(0.2)) <br/>model.add(Dense(10, activation="softmax")) <br/><br/>model.compile(optimizer="rmsprop", loss="categorical_crossentropy", <br/>              metrics=["accuracy"]) <br/><br/># save best model <br/>checkpoint = ModelCheckpoint( <br/>    filepath=os.path.join(MODEL_DIR, "model-{epoch:02d}.h5")) <br/>model.fit(Xtrain, Ytrain, batch_size=BATCH_SIZE, nb_epoch=NUM_EPOCHS, <br/>          validation_split=0.1, callbacks=[checkpoint])
</pre>
<p>The third scenario involves monitoring a metric, such as validation accuracy or loss, and only saving a checkpoint if the current metric is better than the previously saved checkpoint. Keras provides an additional parameter, <kbd>save_best_only</kbd>, which needs to be set to <kbd>true</kbd> when instantiating the checkpoint object in order to support this functionality.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Using TensorBoard and Keras</h1>
            </header>

            <article>
                
<p>Keras provides a callback for saving your training and test metrics, as well as activation histograms for the different layers in your model:</p>
<pre>
keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0,  <br/>write_graph=True, write_images=False)
</pre>
<p>Saved data can then be visualized with TensorBoad launched at the command line:</p>
<pre>
tensorboard --logdir=/full_path_to_your_logs
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Using Quiver and Keras</h1>
            </header>

            <article>
                
<p>In <a href="4be2a04a-4545-4051-bcd9-32764d21f0f2.xhtml" target="_blank">Chapter 3</a>, <em>Deep Learning with ConvNets</em>, we will discuss ConvNets, which are an advanced deep learning technique for dealing with images. Here we give a preview of Quiver (for more information, refer to <a href="https://github.com/jakebian/quiver" target="_blank">https://github.com/jakebian/quiver</a>), a tool useful for visualizing ConvNets features in an interactive way. The installation is pretty simple, and after that Quiver can be used with one single line:</p>
<pre>
<strong>pip install quiver_engine </strong><br/><br/><strong>from quiver_engine import server     server.launch(model)</strong>
</pre>
<p>This will launch the visualization at <kbd>localhost:5000</kbd>. Quiver allows you to visually inspect a neural network, as in the following example:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" height="338" src="assets/image_02_028.png" width="686"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Summary</h1>
            </header>

            <article>
                
<p>In this chapter, we discussed how to install Theano, TensorFlow, and Keras on the following:</p>
<ul>
<li>Your local machine</li>
<li>A dockerized infrastructure based on containers</li>
<li>In the cloud with Google GCP, Amazon AWS, and Microsoft Azure</li>
</ul>
<p>In addition to that, we looked at a few modules defining Keras APIs and some commonly useful operations such as loading and saving neural networks' architectures and weights, early stopping, history saving, checkpointing, interactions with TensorBoard, and interactions with Quiver.</p>
<p>In the next chapter, we will introduce the concept of convolutional networks a fundamental innovation in deep learning which has been used with success in multiple domains from text, to video, to speech going well beyond the initial image processing domain where they were originally conceived.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </body></html>