<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Tuning and Optimizing Models</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the last two chapters, we trained deep learning models for classification, regression, and image recognition tasks. In this chapter, we will discuss some important issues in regard to managing deep learning projects. While this chapter may seem somewhat theoretical, if any of the issues discussed are not correctly managed, it can derail your deep learning project. We will look at how to choose evaluation metrics and how to create an estimate of how well a deep learning model will perform before you begin modeling. Next, we will move onto <span>data distribution</span><span> and the mistakes often made in splitting data into correct partitions for training. </span><span>Many machine learning projects fail in production use because the</span> data distribution is different to what the model was trained with. We will look at data augmentation, a valuable method to enhance your model's accuracy. Finally, we will discuss hyperparameters and learn how to tune them.</p>
<p class="mce-root">In this chapter, we will be looking at the following topics:</p>
<ul>
<li>Evaluation metrics and evaluating performance</li>
<li><span>Data preparation</span></li>
<li><span>Data pre-processing</span></li>
<li><span>Data augmentation</span></li>
<li><span>Tuning hyperparameters</span></li>
<li>Use case—interpretability</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluation metrics and evaluating performance</h1>
                </header>
            
            <article>
                
<p>This section will discuss how to set up a deep learning project and what evaluation metrics to select. We will look at how to select evaluation criteria and how to decide when the model is approaching optimal performance. We will also discuss how all deep learning models tend to overfit and how to manage the bias/variance tradeoff. This will give guidelines on what to do when models have low accuracy.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Types of evaluation metric</h1>
                </header>
            
            <article>
                
<p>Different <span>evaluation metrics are used for categorization and regression tasks. For categorization, accuracy is the most commonly used evaluation metric. However, accuracy is only valid if the cost of errors is the same for all classes, which is not always the case. For example, in medical diagnosis, the cost of a false negative will be much higher than the cost of a false positive. A false negative in this case says that the person is not sick when they are, and a delay in diagnosis can have serious, perhaps fatal, consequences. On the other hand, a false positive is saying that the person is sick when they are not, which is upsetting for that person but is not life threatening.</span></p>
<p><span>This issue is compounded when you have imbalanced datasets, that is, when one class is much more common than the other. Going back to our medical diagnosis example, if only 1% of people who get tested actually have the disease, then a machine learning algorithm can get 99% accuracy by just declaring that nobody has the disease. In this case, you can look at other metrics rather than accuracy. One such metric that is useful for imbalanced datasets is the F1 evaluation metric, which is a weighted average of precision and recall. The formula for the F1 score is as follows:</span></p>
<p class="mce-root"><em>F1 = 2 * (precision * recall) / (precision + recall)</em></p>
<p class="mce-root">The formulas for precision and recall are as follows:</p>
<p class="mce-root"><em>precision = true_positives / (true_positives + false_positives)</em><br/>
<em>recall = true_positives / (true_positives + false_negatives)</em></p>
<p>For regression, you have a choice of evaluation metrics: MAE, MSE, and RMSE. <strong>MAE</strong>, or <strong>Mean Absolute Error</strong>, is the simplest; it is just the average of the absolute difference between the actual value and the predicted value. The advantage of MAE is that it is easily understood; if MAE is 3.5, then the difference between the predicted value and the actual value is 3.5 on average. <strong>MSE</strong>, or <strong>Mean Squared Error</strong>, is the average of the squared error, that is, it takes the <span>difference between the actual value and the predicted value, squares it, and then takes the average of those values. The advantage of using MSE over MAE is that it penalizes errors according to their severity. If the difference between the actual value and the predicted value for two rows was 2 and 5, then the MSE would put more weight on the second example because the error is larger. <strong>RMSE</strong>, or <strong>Root Mean Squared Error</strong>, is the square root of MSE. The advantage of using MSE is that it puts the error term back into units that are comparable to the actual values. For regression tasks, RMSE is usually the preferred metric.</span></p>
<p><span>For more information on metrics in MXNet, see </span><a href="https://mxnet.incubator.apache.org/api/python/metric/metric.html">https://mxnet.incubator.apache.org/api/python/metric/metric.html</a>. </p>
<p><span>For more information on metrics in Keras, see </span><a href="https://keras.io/metrics/">https://keras.io/metrics/</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating performance</h1>
                </header>
            
            <article>
                
<p>We have explored a few deep learning models in earlier chapters. We <span>got an accuracy rate of 98.36% in our image classification task on the <kbd>MNIST</kbd> dataset in <a href="1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml">Chapter 5</a></span>, <em>Image Classification Using Convolutional Neural Networks.</em><span> For the binary classification task (predicting which customers will return in the next 14 days) in <a href="28315a07-2bf0-45c8-8e6f-0e4f01616ca3.xhtml">Chapter 4</a>,<em> Training Deep Prediction Models</em>, we got an accuracy rate of 77.88%</span><span>. But what does this actually mean and how do we evaluate the performance of a deep learning model?</span></p>
<p>The obvious starting point in evaluating whether your deep learning model has good predictive capability is by comparing it to other models. <span>The <kbd>MNIST</kbd> dataset is used in a lot of </span>benchmarks for deep learning research, so we know that there are models that achieve 99.5% accuracy. Therefore, our model is OK, but not great. In the <em>D</em><span><em>ata augmentation</em></span> <span>section </span>in this chapter, we will improve our model significantly, from 98.36%<span> accuracy to </span><span>98.95% accuracy, by augmenting our data with new images created by making changes to the existing image data. In general, for image classification tasks anything less than 95% accuracy probably indicates a problem with your deep learning model. Either the model is not designed correctly or you do not have enough data for your task.</span></p>
<p>Our binary classification model only had 77.54% accuracy, which is much less than the image classification task. So, is it a terrible model? Not really; it is still a useful model. We also have some benchmarks from other machine learning models such as random forest and xgboost that we ran on a small section of the data. We also saw that we got an increase in accuracy when we moved from a model with 3,900 rows to a deeper model with 390,000 rows. This highlights that deep learning models improve with more data.</p>
<p>One step you can do to evaluate your model's performance is to see if more data will increase accuracy significantly. The data can be acquired from more training data, or from data augmentation, which we will see later. You can use learning curves t<span>o evaluate if this will help with performance. To c</span>reate a learning curve, you train a series of machine learning models with increasing sizes, for example, 10,000 rows to 200,000 rows in steps of 1,000 rows. For each step, run <kbd>5</kbd> different machine learning models to smooth the results and plot average accuracy by the sample size. Here is the pseudocode to perform this task:</p>
<pre class="mce-root">For k=10000 to 200000 step 1000<br/>   For n=1 to 5<br/>       [sample] = Take k rows from dataset<br/>       Split [sample] into train (80%) / test (20%)<br/>       Run ML (DT) algorithm<br/>       Calculate Accuracy on test<br/>       Save accuracy value<br/>Plot k, avg(Accuracy)</pre>
<p class="mce-root">Here is an example of a learning curve plot for similar task to the churn problem:</p>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref"><img class="alignnone size-full wp-image-592 image-border" src="assets/f5c76faa-0ab1-4117-a821-8d943a43ec49.png" style="width:44.08em;height:21.92em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Figure 6.1: An example of a learning curve which plots accuracy by data size</div>
<p class="mce-root">In this case, accuracy is in a very narrow range and stabilizes as the # instances increase. Therefore, for this algorithm and hyperparameter choice, adding more data will not increase accuracy significantly.</p>
<p>If we get a learning curve that is flat like in this example, then adding more data to the existing model will not increase accuracy. We could try to improve our performance by either changing the model architecture or by adding more features. We discussed some options for this in <a href="1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml">Chapter 5</a>, <em>Image Classification Using Convolutional Neural Networks</em>.</p>
<p>Going back to our binary classification model, let's consider how we could we use it in production. Recall that this model is trying to predict if customers will return in the next <em>x</em> days. Here is the confusion matrix from that model again:</p>
<pre>      Predicted<br/>Actual     0     1<br/> 0     10714  4756<br/> 1      3870 19649</pre>
<p>If we look at how the model performs for each class, we get a different accuracy rates:</p>
<ul>
<li>For <kbd>Actual=0</kbd>, we get <em>10714 / (10714 + 4756) = 69.3%</em> <span>values correct. This is called specificity or the true negative rate.</span></li>
<li><span>For <kbd>Actual=1</kbd>, we get <em>19649 / (3466</em></span><span><em> + 19649) = 85.0%</em> values correct. This is called </span>sensitivity or the true positive rate.</li>
</ul>
<p>For this use case, <span>sensitivity is probably more of a concern than specificity. If I were a senior manager, I would be more interested in knowing which customers were predicted to return but did not. This group could be sent offers to entice them back. Here is how a senior manager might use this model, assuming that the model is built to predict whether a person comes in from September 1 to September 14. On September 15, we get the preceding confusion matrix. How should a manager allocate his/her limited marketing budget?</span></p>
<ul>
<li><span>I can see that I got 4,756 customers who were predicted not to return but actually did. This is good, but I cannot really act on this. I can attempt to send offers to the 10,135 who did not return, but since my model already predicted that they would not return, I would expect the response rate to be low.</span></li>
<li>The 3,870 <span>customers who were predicted to return but did not are more interesting. These people should be sent offers to entice them back before their change in behavior becomes permanent. This represents only 9.9% of my customer base, so by only sending offers to these customers, I am not diluting my budget by sending offers to a large contingent of my customers.</span></li>
</ul>
<p><span>The prediction model should not be used in isolation; other metrics should be combined with it to develop a marketing strategy. F</span>or example, <strong>customer lifetime value</strong> (<strong>CLV</strong>), which measures the expected future revenue for a customer minus the cost to re-acquire that customer, could be combined with the prediction model. By using a prediction model and CLV together, we can prioritize customers that are likely to return by their predicted future value.</p>
<p>To summarize this section, it is all too easy to get obsessed with optimizing evaluation metrics, especially if you are new to the field. As a data scientist, you should always remember that optimizing evaluation metrics on a machine learning task is not the ultimate goal—it is just a proxy for improving some part of the business. You must be able to link the results of your machine learning model back to a business use case. In some cases, for example, digit recognition in the <span><kbd>MNIST</kbd> dataset, </span>there is a direct link between your evaluation metrics and your business case. But sometimes it is not so obvious, and you need help to work with the business in finding out how to use the results of your analysis to maximize the benefits to the company.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data preparation</h1>
                </header>
            
            <article>
                
<p>Machine learning is about training a model to generalize on the cases it sees so that it can make predictions on unseen data. Therefore, the data used to train the deep learning model should be similar to the data that the model sees in production. However, at an early product stage, <span>you may have little or no data to train a model, so what can you do?</span> For example, a mobile app could include a machine learning model that predicts the subject of image taken by the mobile camera. When the app is being written, there may not be enough data to train the model using a deep learning network. One approach would be to augment the dataset with <span>images from other sources to train the deep learning network. However, you need to know how to manage this and how to deal with the uncertainty it introduces. Another approach is transfer learning, which we will cover in <a href="94299ae0-c3fc-4f1d-97a8-5e8b85b260e9.xhtml">Chapter 11</a>, <em>The Next Level in Deep Learning</em>.</span></p>
<p><span>Another difference between deep learning and traditional machine learning is the size of the datasets. This can affect the ratios used to split data between </span>train/test—the recommended guidelines for splitting data into <span>70/30 or 80/20 splits for </span>machine learning need to be revised for training deep learning models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Different data distributions</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>In previous chapters, w</span>e used the MNIST dataset for classification tasks. While this dataset contains handwritten digits, the data is not representative of real-life data. In <a href="1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml">Chapter 5</a>, <em>Image Classification Using Convolutional Neural Networks,</em> we visualized some of the digits, if you go back and look at these images, it is clear that these images are in a standard format:</p>
<ul>
<li>There are all grayscale</li>
<li>The images are all 28 x 28</li>
<li>The images all appear to have at border of at least <span>1 pixel</span></li>
<li>The images are all of the same scale, that is, each image takes up most of the image</li>
<li>There is very little distortion, since the border is black and the foreground is white</li>
<li>Images are the <em>right way up</em>, that is, we do not have any major rotations</li>
</ul>
<p>The original use case for the MNIST dataset is to recognize 5 digit postcodes on letters. Let's suppose we train a model on the 60,000 images in the MNIST dataset and wish to use it in a production environment to recognize postcodes from letters and packages. Here are the steps a production system must go through before any deep learning can be applied:</p>
<ul>
<li>Scan the letters</li>
<li>Find the postcode section</li>
<li>Split the postcode digits into 5 different regions (one per digit)</li>
</ul>
<p>In any one of these data transformation steps, additional data bias could occur. If we used the <em>clean</em> MNIST data to train a model and then tried to predict the <em>biased</em> transformed data, then our model may not work that well. Examples of how bias could affect the production data include the following:</p>
<ul>
<li><span>Correctly locating the postcode is a difficult problem in itself</span></li>
<li>The letters will have backgrounds and foregrounds of different colors and contrasts, and so converting them to grayscale may not be consistent depending on the type of letter and pen used on the letter / package</li>
<li>The results from the scanning<span> processes </span>may vary because of different hardware and software being used—this is a ongoing problem in applying deep learning to medical image data</li>
<li>Finally, the <span>difficultly in </span>splitting the postcode into 5 different regions depends on the letter and pen used, as well as the quality of the preceding steps</li>
</ul>
<p>In this example, the distribution of the data <span>used to train and the estimate model's performance is </span>different from the <span>production</span> data. If a data scientist had promised to deliver 99% accuracy before the model is deployed, then senior managers are very likely to be disappointed when the application runs in production! When creating a new model, we split data into train and test splits, so the main purpose of the test dataset is to estimate model accuracy. But if the data in the test dataset is different to what the model will be see in production, then the evaluation metrics on the test <span>dataset cannot give a good guide to how the model will perform in production.</span></p>
<p>If the problem is that there is little or no actual labeled dataset to begin with, then one of the first steps to consider before any model training is to investigate if more data can be acquired. Acquiring data may involve setting up a mini production environment, partnering with a client or using a combination of semi-supervised and manual labelling. In the use case we just saw, I would consider it more important to set up the process to extract the digitized images before looking at any machine learning. Once this is set up, I would look to build up some training data—it still may not be enough to build a model, but it could be used but a proper test set to create evaluation metrics that would reflect real-life performance. This may appear obvious, as over optimistic expectations based on flawed evaluation metrics are probably one of the top three problems in data science projects.</p>
<p>One example of a very large scale project that managed this problem very well is this use case in <span>Airbnb: </span><a href="https://medium.com/airbnb-engineering/categorizing-listing-photos-at-airbnb-f9483f3ab7e3">https://medium.com/airbnb-engineering/categorizing-listing-photos-at-airbnb-f9483f3ab7e3</a>. They had a huge number of photos of house interiors, but these were not labeled with the room type. They took their existing labeled data and also performed quality assurance to check how accurate the labels were. It is often said in data science that creating machine learning models may only be 20% of the actual work involved—acquiring an accurate large labeled dataset that is representative of what the model will see in production is often the hardest task in a deep learning project.</p>
<p>Once you have a dataset in place, you need to split your data into train and test splits before modeling. If you have experience in traditional machine learning, you may start with a 70/30 split, that is, 70% for training the model and 30% for evaluating the model. However, this rule is less valid in the world of large datasets and training deep learning models. Again, the only reason to split data into train and test sets is to have a holdout set to estimate the model's performance. Therefore, you only need enough records in this dataset so that the accuracy estimate you get is reliable and has the precision you require. If you have a large dataset to begin with, then a smaller percentage might be adequate for the test dataset. Let me explain this with an example, where you want to improve on an existing machine learning model:</p>
<ul>
<li>A prior <span>machine learning model has 99.0% accuracy</span></li>
<li>There is a labeled dataset with 1,000,000 records</li>
</ul>
<p><span>If a new machine learning model is to be trained, then it should get at least 99.1% accuracy for you to be confident that it is an improvement on the existing model.</span> How many records do you need when evaluating the existing model? You only need enough records so that you are fairly sure that the accuracy on the new model is accurate to 0.1%. Therefore 50,000 records in the test set, which is 5% of the dataset, would be sufficient to evaluate your model. If the accuracy on these 50,000 records was 99.1%, that would be 49,550 records. This represents 50 more correctly classified records than the benchmark model, which would strongly suggest that the second model is a better model—<span>it </span>would be unlikely that the difference would be simply down to chance.</p>
<p>You may get resistance to the suggestion you use only 5% of data for model evaluation. However, the idea of splitting data into 70/30 splits goes back to the days of small datasets, such as the iris dataset with 150 records. We previously saw the following graph in <a href="28315a07-2bf0-45c8-8e6f-0e4f01616ca3.xhtml">Chapter 4</a>,<em> Training Deep Prediction Models</em><span>,</span> which showed how accuracy on machine learning algorithms tends to stagnate as the data size increases. Therefore, there was less of an incentive to maximize the amount of data that was available for training. Deep learning models can take advantage of more data, so if we can use less data for the test set, we should get a better model overall:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-593 image-border" src="assets/272cf949-b665-4241-8b57-34c1419ee8c9.png" style="width:29.50em;height:23.50em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 6.2: How model accuracy increases by dataset size for deep learning models versus other machine learning models</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data partition between training, test, and validation sets</h1>
                </header>
            
            <article>
                
<p><span>T</span><span>he previous section highlighted the importance of acquiring some data at an early stage in the project. But i</span>f you do not have enough data to train a deep learning model, it is possible to train on other data and apply it to your data. For example, you can use a model trained on ImageNet data for image classification tasks. In this scenario, you need to <span>use the real data that has been collected wisely. This section discusses some good practices on that subject.</span></p>
<div class="packt_infobox">If you have ever wondered why big companies such as Google, Apple, Facebook, Amazon, and so on have such a head start in AI, this is the reason why. While they have some of the best AI people in the world working for them, their chief advantage is that they have access to tons of <em>labeled</em> data that they can use to build their machine learning models.</div>
<p><span>In the previous section, we said that the sole purpose of a test set is to evaluate the model. But if that data is not from the same distribution as the data the model will see in prediction tasks, then the evaluation will be misleading.</span> One of the most important project priorities should be to acquire labeled data that is as similar to real-life data as soon as possible. Once you have that data, you need to be clever on how you use this valuable asset. The best use of this data, in order of priority, would be as follows:</p>
<ul>
<li>Can I use some of this data to create more training data? This could be through augmentation, or implementing an early prototype that users can interact with.</li>
<li>If you are building several models (which you should be), use some of the data in the validation set to tune the model.</li>
<li>Use the data in the test set to evaluate the model.</li>
<li>Use the data in the train set.</li>
</ul>
<p>Some of these suggestions may be contentious—especially when suggesting that you should use the data for the validation set before the test set. <span>Remember that the sole purpose of a test set is that it should only be used once to evaluate the model, so you only get one shot at using this data. </span>If I have only a small amount of realistic data, then I prefer to use it to tune the model and have a less precise evaluation metric than having a poorly performing model with a very precise <span>evaluation metric.</span></p>
<p>This approach is risky, and ideally you want your validation dataset and your test dataset to be from the same distribution and be representative of the data that the model will see in production. Unfortunately, when you are at <span>the early stages in machine learning projects with limited real-life data, then you have to make decisions on how best to use this data, and in this case it is better to use the limited data in the validation dataset rather than the test dataset.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Standardization</h1>
                </header>
            
            <article>
                
<p>Another important step in data preparation is standardizing data. In the previous chapter, for the MNIST data, all pixel values were divided by 255 so that the input data was between 0.0 and 1.0. In our case, we applied <span>min-</span>max <span>normalization, which transforms the data linearly using the following function:</span></p>
<p><em>xnew = (x-min(x))/(max(x)-min(x))</em></p>
<p>Since we already know that <em>min(x) = 0</em> and <em>max(x)=255</em>, this reduces to the following:</p>
<p><em>xnew = x / 255.0</em></p>
<p>The other most popular form of <span>standardization</span> scales the feature so that the mean is 0 and the standard deviation from the mean is 1. This is also known as <strong>z-scores</strong>, and the formula for it is as follows:</p>
<p><em>xnew = (x - mean(x)) / std.dev(x)</em></p>
<p>There are three reasons why we need to perform <span>standardization:</span></p>
<ul>
<li>It is especially important to <span>normalize our input features if the features are in different scales. A common example often cited in machine learning is predicting house prices from the number of bedrooms and the square foot. The number of bedrooms ranges from 1 to 10, while the square feet can range from 500 sq feet to 20,000 sq feet. Deep learning models expect features to be in the same range.</span></li>
<li><span>Even if all of our features are already in the same range, it is still advisable to </span>normalize the input features. Recall from <a href="6e6dd858-9f00-454a-8434-a95c59e85b25.xhtml">Chapter 3</a>, <em>Deep Learning Fundamentals</em>, that we looked at initializing the weights before model training. Any benefit from initializing weights will be cancelled if our features are not normalized. We also spoke about the problem of exploding and vanishing gradients. When features are on different scales this is more likely.</li>
<li>Even if we avoid both of the preceding problem, if we do not apply normalization, the model will take longer to train.</li>
</ul>
<p>For the churn model in <a href="28315a07-2bf0-45c8-8e6f-0e4f01616ca3.xhtml">Chapter 4</a>, <em>Training Deep Prediction Models</em>, all of the columns were monetary spent, so are already on the same scale. When we applied the log to each of these variables, it will have shrunk them down to values between -4.6 to 11, so there was no need to scale them to values between 0 and 1. When correctly applied, standardization has no negative consequences and so should be one of first steps applied to data preparation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data leakage</h1>
                </header>
            
            <article>
                
<p><strong>Data leakage</strong> is where a feature used to train the model has values that could not exist if the model was used in production. It occurs most frequently in time series data. For example, in our churn use case in <a href="28315a07-2bf0-45c8-8e6f-0e4f01616ca3.xhtml">Chapter 4</a>, <em>Training Deep Prediction Models</em><span>, </span>there were a number of categorical variables in the data that indicated customer segmentation. A data modeler may assume that these are good predictor variables, but it is not known how <span>and </span><span>when </span><span>these variables were set. They could be based on customer' spend, which means that if they are used in the prediction algorithm, there is a circular reference—an external process calculates the segment based on the spend and then this variable is used to predict spend!</span></p>
<p>When extracting data to build a model, you should be wary of categorical attributes and question when these variables could have been created and modified. Unfortunately, most database systems are poor at tracking the data lineage, so if in doubt you may consider omitting the variable from your model.</p>
<p>Another example of data leakage in image classification tasks is when attribute information within the image is used in the model. For example, if we build a model where the filenames were included as attributes, these names may hint at the class name. When the model is used in production, theses hints will not exist, so this is <span>also seen as data leakage.</span></p>
<p>We will see an example of data leakage in practice in the the <em>Use case—interpretability</em> section later in this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data augmentation</h1>
                </header>
            
            <article>
                
<p class="mce-root">One approach to increasing the accuracy in a model regardless of the amount of data you have is to create artificial examples based on existing data. This is called <span><strong>data augmentation</strong></span>. Data augmentation can also be used at test time to improve prediction accuracy.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using data augmentation to increase the training data</h1>
                </header>
            
            <article>
                
<p>We are going to apply data augmentation to the <kbd>MNIST</kbd> dataset that we used in previous chapters. <span>The code for this section is in</span> <kbd>Chapter6/explore.Rmd</kbd> <span>if you want to follow along. </span>In <a href="1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml">Chapter 5</a>, <em>Image Classification Using Convolutional Neural Networks, </em>we plotted some examples from the MNIST data, so we won't repeat the code again. It is included in the code file, and you can also refer back to the image in <a href="1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml">Chapter 5</a>, <em>Image Classification Using Convolutional Neural Networks</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-594 image-border" src="assets/aaa6ddab-8aa7-4016-a047-fbe2740489df.png" style="width:105.92em;height:75.92em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 6.3: The first 9 images in the MNIST dataset</div>
<p>We described data augmentation as <span>creating new data from an existing dataset. This means creating a new instance that is sufficiently different from the original instance but not so much that it no longer represents the data label. For image data, this might mean performing the following functions on the images:</span></p>
<ul>
<li><strong>Zooming</strong>: By zooming into the center of the image, your model may be better able to handle images at different scales.</li>
<li><strong>Shifting</strong>: Moving the image up, down, left, or right can make the deep learning model more aware of examples of images taken off-center.</li>
<li><strong>Rotation</strong>: By rotating images, the model will be able to recognize data that is off-center.</li>
<li><strong>Flipping</strong>: For many objects, flipping the images 90 degrees is valid. For example, a picture of a car from the left side can be flipped to show a similar image of the car from the right side. A deep model can take advantage of this new perspective.</li>
<li><strong>Adding noise</strong>: Sometimes, deliberately adding noise to images can force the deep learning model to find deeper meaning.</li>
<li><strong>Modifying color</strong>: By adding filters to the image, you can simulate different lighting conditions. For example, you can change an image taken in bright light so that it appears to be taken in poor lighting conditions.</li>
</ul>
<div class="packt_tip"><span>The goal of this task is to increase accuracy on the test dataset. However, t</span>he important rule of data augmentation is that the new data should attempt to simulate the data your model will use in production rather than trying to increase model accuracy on existing data. I cannot stress that enough. Getting 99% accuracy on a hold-out set means nothing if a model fails to work in a production environment because the data used to train and evaluate the model was not representative of real-life data. In our case, we can see that the MNIST images are grayscale and neatly centered, and so on. In a production use case, images are off-center and with different backgrounds and foregrounds (for example, with a brown background and blue writing), and so will not be classified correctly. You can attempt to pre-process the images so that you can format them to a similar manner (28 x 28 grayscale image with black background and data centered with a 2 x 2 margin), but a better solution is to train the model on typical data it will encounter in production. </div>
<p>If we look at the previous image, we can see that most of these data augmentation tasks are not applicable to the MNIST data. All of the images appear to be at the same zoom level already, so creating artificial examples at increased zoom will not help. Similarly, shifting is unlikely to work, since the images are already centered. Flipping image<span>s is definitely not valid, since most digits are not valid when flipped, example <em>7</em>. There is no evidence of existing random noise in our data, so this will not work either.</span></p>
<p>One technique that we can try is to rotate the images. We will <span>create two new artificial images for each existing image, the first artificial image will be rotated 15 degrees left and the second artificial image will be rotated 15 degrees right. Here are some of the artificial images after we have rotated the original images 15 degrees left:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-595 image-border" src="assets/e9a813a3-c0c5-4e64-a1ec-dce8d29c14be.png" style="width:106.58em;height:76.33em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 6.4: MNIST data rotated 15 degrees left</div>
<p>If we look at the the preceding screenshot, one strange anomaly exists. We have 10 classes, and using this approach may increase overall accuracy, but one class will not get as much uplift. The zero digit is the odd one out because rotating a zero still looks like a zero—we may still get an increase in accuracy for this class, but probably not as much as for the other classes. The function to rotate  image data is in <kbd>Chapter6/img_ftns.R</kbd>. It uses the <span><kbd>rotateImage</kbd> function from the <kbd>OpenImageR</kbd> package:</span></p>
<pre>rotateInstance &lt;-function (df,degrees)<br/>{<br/>  mat &lt;- as.matrix(df)<br/>  mat2 &lt;- rotateImage(mat, degrees, threads = 1)<br/>  df &lt;- data.frame(mat2)<br/>  return (df)<br/>}</pre>
<p>There is actually two types of data augmentation that we can apply to our dataset. The first type creates new training data from existing examples. But we can also use a technique called <strong>test time augmentation</strong> (<strong>TTA</strong>), which can be used during model evaluation. It makes copies of each test row and then uses these copies and the originals to vote for the category. We will see an example of this later.</p>
<p>The code to create datasets for the data augmentation is in <kbd>Chapter6/augment.R</kbd>. Note that this takes a long time to run, maybe 6-10 hours depending on your machine. It also needs approx. 300 MB of free space on the drive to create the new datasets. The code is not difficult; it loads in the data, and splits it into train and test sets. For the train data, it creates two new instances: one rotated 15 degrees left and one rotated 15 degrees right. It is important that the data used to evaluate the model performance is not included in the data augmentation process, that is, split the data into a train dataset first and only apply data augmentation to the train split.</p>
<p>When the data augmentation is complete, there will be a new file in the data folder called <kbd>train_augment.csv</kbd>. This file should have 113,400 rows. Our original dataset for <kbd>MNIST</kbd> had 42,000 rows; we took 10% of that for test purposes (that is, to validate our model) and were left with 37,800 rows. We then made two copies of these rows, meaning that we now have 3 rows for each previous row. This means that we have <em>37,800 x 3 = </em><span><em>113,400</em> rows in our training data file. <kbd>augment.R</kbd> also outputs the test data (4,200 rows) as <kbd>test0.csv</kbd> and an augmented test set (<kbd>test_augment.csv</kbd>), which we will cover later.</span></p>
<p>The code to run the neural network is in <kbd>Chapter6/mnist.Rmd</kbd>. The first part which uses the augmented data for training is almost identical to the code in <a href="1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml">Chapter 5</a>, <em>Image Classification Using Convolutional Neural Networks</em>. The only change is that it loads the data files created in <kbd>augment.R</kbd> (<span><kbd>train_augment.csv</kbd> and <kbd>test0.csv</kbd>),</span> so we we will not repeat all of the code for the model here again. Here is the confusion matrix and the final accuracy on the test dataset:</p>
<pre>## pred.label<br/>## test.y   0   1   2   3   4   5   6   7   8   9<br/>##      0 412   0   0   1   0   0   3   0   0   0<br/>##      1   0 447   1   2   0   0   0   5   0   0<br/>##      2   0   0 437   1   2   0   0   1   0   0<br/>##      3   0   0   3 432   0   0   0   1   1   0<br/>##      4   0   0   0   0 396   1   0   0   0   3<br/>##      5   1   0   0   1   0 378   1   0   0   1<br/>##      6   1   1   0   0   0   0 434   0   1   0<br/>##      7   0   1   2   0   1   0   0 398   0   1<br/>##      8   0   0   2   1   0   0   0   1 419   0<br/>##      9   0   0   0   0   5   0   0   1   1 399<br/>accuracy2 &lt;- sum(res$test.y == res$pred.label) / nrow(res)<br/>The accuracy of our model with augmented train data is 0.9885714.</pre>
<p>This compares to an accuracy of <kbd>0.9821429</kbd> from our model in <a href="1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml">Chapter 5</a>, <em>Image Classification Using Convolutional Neural Networks</em>, so this is a significant improvement. We have reduced our error rate by over 30% <em>(0.9885714-0.9835714</em><span><em>) / (1.0-0.9835714)</em>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Test time augmentation</h1>
                </header>
            
            <article>
                
<p>We can also use data augmentation during test time.<span> In the <kbd>augment.R</kbd> file, it created a file with the original test set of 4,200 rows (<kbd>data/test0.csv</kbd>), which was used that to evaluate the model. The <kbd>augment.R</kbd> file also created a file called <kbd>test_augment.csv</kbd>, which has the original 4,200 rows and 2 copies for each image. The copies are similar to what we did to augment the training data, that is, a row with data rotated 15 degrees left and a row with data rotated 15 degrees right. The three rows are outputted sequentially and we will use these 3 rows to <em>vote</em> for the winner. We need to take 3 records at a time from <kbd>test_augment.csv</kbd> and calculate the prediction value as the average of these three values. Here is the code that performs test time augmentation:</span></p>
<pre>test_data &lt;- read.csv("../data/test_augment.csv", header=TRUE)<br/>test.y &lt;- test_data[,1]<br/>test &lt;- data.matrix(test_data)<br/>test &lt;- test[,-1]<br/>test &lt;- t(test/255)<br/>test.array &lt;- test<br/>dim(test.array) &lt;- c(28, 28, 1, ncol(test))<br/><br/>preds3 &lt;- predict(model2, test.array)<br/>dfPreds3 &lt;- as.data.frame(t(preds3))<br/># res is a data frame with our predictions after train data augmentation,<br/># i.e. 4200 rows<br/>res$pred.label2 &lt;- 0<br/>for (i in 1:nrow(res))<br/>{<br/>   sum_r &lt;- dfPreds3[((i-1)*3)+1,] +<br/>            dfPreds3[((i-1)*3)+2,] + dfPreds3[(i*3),] <br/>   res[i,"pred.label2"] &lt;- max.col(sum_r)-1<br/>}<br/>accuracy3 &lt;- sum(res$test.y == res$pred.label2) / nrow(res)<br/>The accuracy of our CNN model with augmented train data and Test Time Augmentation (TTA) is 0.9895238.</pre>
<p>Doing this, we get predictions for 12,600 rows (<em>4,200 x 3</em>). The for loop runs through 4,200 times and takes 3 records at a time, calculating the average accuracy. The increase in accuracy over the accuracy using augmented training data is small, from <kbd>0.9885714</kbd> to <kbd>0.9895238</kbd>, which is approx. 0.1% (4 rows). We can look at the effect of TTA in the following code:</p>
<pre>tta_incorrect &lt;- nrow(res[res$test.y != res$pred.label2 &amp; res$test.y == res$pred.label,])<br/>tta &lt;- res[res$test.y == res$pred.label2 &amp; res$test.y != res$pred.label,c("pred.label","pred.label2")]<br/><br/>Number of rows where Test Time Augmentation (TTA) changed the prediction to the correct value 9 (nrow(tta)).<br/>Number of rows where Test Time Augmentation (TTA) changed the prediction to the incorrect value 5 (tta_incorrect).<br/><br/>tta<br/>##     pred.label pred.label2<br/>## 39           9           4<br/>## 268          9           4<br/>## 409          9           4<br/>## 506          8           6<br/>## 1079         2           3<br/>## 1146         7           2<br/>## 3163         4           9<br/>## 3526         4           2<br/>## 3965         2           8</pre>
<p>This table shows the 9 rows where the test time augmentation was correct and the previous model was wrong. We can see three cases where the previous model (<kbd>pred.model</kbd>) predicted <kbd>9</kbd> and the <span>test time augmentation</span><span> model </span>correctly predicted <kbd>4</kbd>. Although <span>test time augmentation </span>did not significantly increase our accuracy in this case, it can make a difference in other computer vision tasks.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using data augmentation in deep learning libraries</h1>
                </header>
            
            <article>
                
<p>We implemented data augmentation using R packages and it took a long time to generate our augmented data. It was useful for demonstration purposes, but MXNet and Keras support data augmentation functions. In MXNet, there are a range of functions in <kbd>mx.image.*</kbd> to do this (<a href="https://mxnet.incubator.apache.org/tutorials/python/data_augmentation.html">https://mxnet.incubator.apache.org/tutorials/python/data_augmentation.html</a>). In Keras, this is in <kbd>keras.preprocessing.*</kbd> (<a href="https://keras.io/preprocessing/image/">https://keras.io/preprocessing/image/</a>), which applies these automatically to your models. <span>In </span><a href="94299ae0-c3fc-4f1d-97a8-5e8b85b260e9.xhtml">Chapter 11</a>, <em>The Next Level in Deep Learning</em><span>, we show how to apply d</span><span>ata augmentation using Keras.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tuning hyperparameters</h1>
                </header>
            
            <article>
                
<p class="mce-root">All machine learning algorithms have hyper-parameters or settings that can change how they operate. <span>These hyper-parameters can improve the accuracy of a model or reduce the training time. </span>We have seen some of these hyper-parameters in previous chapters, particularly <a href="6e6dd858-9f00-454a-8434-a95c59e85b25.xhtml">Chapter 3</a>, <em>Deep Learning Fundamentals, </em>where we looked at the hyper-parameters that can be set in the <kbd>mx.model.FeedForward.create</kbd> function. The techniques in this section can help us find better values for the hyper-parameters.</p>
<p class="mce-root">Selecting hyper-parameters is not a magic bullet; if the raw data quality is poor or if there is not enough data to support training, then tuning <span>hyper-parameters will only get you so far</span>. In these cases, either acquiring additional variables/features that can be used as predictors and/or additional cases may be required.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Grid search</h1>
                </header>
            
            <article>
                
<p class="mce-root">For more information on tuning hyper-parameters, see Bengio, Y. (2012), particularly Section 3, <em>Hyperparameters</em>, which discusses the selection and characteristics of various hyper-parameters. Aside from manual trial and error, two other approaches for improving hyper-parameters are grid searches and random searches. In a grid search, several values for hyper-parameters are specified and all possible combinations are tried. This is perhaps easiest to see. In R, we can use the <kbd>expand.grid()</kbd> function to create all possible combinations of variables:</p>
<pre>expand.grid(<br/> layers=c(1,4),<br/> lr=c(0.01,0.1,0.5,1.0),<br/> l1=c(0.1,0.5))<br/>   layers    lr   l1<br/>1       1  0.01  0.1<br/>2       4  0.01  0.1<br/>3       1  0.10  0.1<br/>4       4  0.10  0.1<br/>5       1  0.50  0.1<br/>6       4  0.50  0.1<br/>7       1  1.00  0.1<br/>8       4  1.00  0.1<br/>9       1  0.01  0.5<br/>10      4  0.01  0.5<br/>11      1  0.10  0.5<br/>12      4  0.10  0.5<br/>13      1  0.50  0.5<br/>14      4  0.50  0.5<br/>15      1  1.00  0.5<br/>16      4  1.00  0.5</pre>
<p>Grid searching is effective when there are only a few values for a few hyper-parameters. However, when there are many values for some or many <span>hyper-</span>parameters, it quickly becomes unfeasible. For example, even with only two values for each of eight <span>hyper-</span>parameters, there are <em>2<sup>8</sup> = 256</em> combinations, which quickly becomes computationally impracticable. Also, if the interactions between <span>hyper-</span>parameters and model performance are small, then using grid search is an inefficient approach.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Random search</h1>
                </header>
            
            <article>
                
<p>An alternative approach to <span>hyper-parameter selection </span>is searching through random sampling. Rather than pre­-specifying all of the values to try and create all possible combinations, one can randomly sample values for the parameters, fit a model, store the results, and repeat. To get a very large sample size, this too would be computationally demanding, but you can specify just how many different models you are willing to run. Therefore this approach gives you a spread over the <span>combination of hyper-parameters.</span></p>
<p class="mce-root">For random sampling, all that need to be specified are values to randomly sample, or distributions to randomly draw from. Typically, some limits would also be set. For example, although a model could theoretically have any integer number of layers, some reasonable number (such as 1 to 10) is used rather than sampling integers from 1 to a billion.</p>
<p class="mce-root">To perform random sampling, we will write a function that takes a seed and then randomly samples a number of hyper-parameters, stores the sampled parameters, runs the model, and returns the results. Even though we are doing a random search to try and find better values, we are not sampling from every possible hyper-parameter. Many remain fixed at values we specify or their defaults.</p>
<p class="mce-root">For some <span>hyper-</span>parameters, specifying how to randomly sample values can take a bit of work. For example, when using dropout for regularization, it is common to have a relatively smaller amount of dropout for early hidden layers (0%-20%) and a higher amount for later hidden layers (50-80%). Choosing the right distributions allows us to encode this prior information into our random search. The following code plots the density of two beta distributions, and the results are shown in <em>Figure 6.5</em>:</p>
<pre>par(mfrow = c(2, 1))<br/>plot(<br/>  seq(0, .5, by = .001),<br/>  dbeta(seq(0, .5, by = .001), 1, 12),<br/>  type = "l", xlab = "x", ylab = "Density",<br/>  main = "Density of a beta(1, 12)")<br/><br/>plot(<br/>  seq(0, 1, by = .001)/2,<br/>  dbeta(seq(0, 1, by = .001), 1.5, 1),<br/>  type = "l", xlab = "x", ylab = "Density",<br/>  main = "Density of a beta(1.5, 1) / 2")</pre>
<p><span>By sampling from these distributions, we can ensure that our search focuses on small proportions of dropout for the early hidden layers, and in the <strong>0</strong> to <strong>0.50</strong> range for the hidden neurons with a tendency to over­sample from values </span>closer to <strong>0.50</strong>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-596 image-border" src="assets/dccb2eb7-acbb-40e0-885d-a59a081ad55e.png" style="width:29.42em;height:35.50em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 6.5: Using the beta distribution to select hyperparameters</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Use case—using LIME for interpretability</h1>
                </header>
            
            <article>
                
<p><span>Deep learning models are known to be difficult to interpret. Some approaches to model interpretability, including LIME, allow us to gain some insights into how the model came to its conclusions. Before we demonstrate LIME, I</span> will show how different data distributions and / or data leakage can cause problems when building deep learning models. We will reuse the deep learning churn model from <a href="28315a07-2bf0-45c8-8e6f-0e4f01616ca3.xhtml">Chapter 4</a>, <em>Training Deep Prediction Models</em>, but we are going to make one change to the data. We are going to introduce a bad variable that is highly correlated to the <em>y</em> value. We will only include this variable in the data used to train and evaluate the model. A separate test set from the original data will be kept to represent the data the model will see in production, this will not have the bad variable in it. The creation of this bad variable could simulate two possible scenarios we spoke about earlier:</p>
<ul>
<li><strong>Different data distributions</strong>: The bad variable does exist in the data that the model sees in production, but it has a different distribution which means the model does not perform as expected.</li>
<li><span><strong>Data leakage</strong>: Our bad variable is used to train and evaluate the model, but when the model is used in production, this variable is not available, so we assign it a zero value, which also means the model does not perform as expected.</span></li>
</ul>
<p>The code for this example is in <kbd>Chapter6/binary_predict_lime.R</kbd>. We will not cover the deep learning model in depth again, so go back to <a href="28315a07-2bf0-45c8-8e6f-0e4f01616ca3.xhtml">Chapter 4</a>,<span> </span><em>Training Deep Prediction Models</em>, if you need a refresher on how it works. We are going to make two changes to the model code:</p>
<ul>
<li>We will split the data into three parts: a train, validate, and test set. The train split is used to train the model, the validate set is used to evaluate the model when it is trained, and the test set represents the data that the model sees in production.</li>
<li>We will create the <kbd>bad_var</kbd> variable, and include it in the train and <span>validation </span>set, but not in the <span>test</span> set.</li>
</ul>
<p>Here is the code to split the data and create the <kbd>bad_var</kbd> variable:</p>
<pre># add feature (bad_var) that is highly correlated to the variable to be predicted<br/>dfData$bad_var &lt;- 0<br/>dfData[dfData$Y_categ==1,]$bad_var &lt;- 1<br/>dfData[sample(nrow(dfData), 0.02*nrow(dfData)),]$bad_var &lt;- 0<br/>dfData[sample(nrow(dfData), 0.02*nrow(dfData)),]$bad_var &lt;- 1<br/>table(dfData$Y_categ,dfData$bad_var)<br/>       0    1<br/>  0 1529   33<br/>  1   46 2325<br/>cor(dfData$Y_categ,dfData$bad_var)<br/>[1] 0.9581345<br/><br/>nobs &lt;- nrow(dfData)<br/>train &lt;- sample(nobs, 0.8*nobs)<br/>validate &lt;- sample(setdiff(seq_len(nobs), train), 0.1*nobs)<br/>test &lt;- setdiff(setdiff(seq_len(nobs), train),validate)<br/>predictorCols &lt;- colnames(dfData)[!(colnames(dfData) %in% c("CUST_CODE","Y_numeric","Y_categ"))]<br/><br/># remove columns with zero variance in train-set<br/>predictorCols &lt;- predictorCols[apply(dfData[train, predictorCols], 2, var, na.rm=TRUE) != 0]<br/><br/># for our test data, set the bad_var to zero<br/># our test dataset is not from the same distribution<br/># as the data used to train and evaluate the model<br/>dfData[test,]$bad_var &lt;- 0<br/><br/># look at all our predictor variables and <br/># see how they correlate with the y variable<br/>corr &lt;- as.data.frame(cor(dfData[,c(predictorCols,"Y_categ")]))<br/>corr &lt;- corr[order(-corr$Y_categ),]<br/>old.par &lt;- par(mar=c(7,4,3,1))<br/><br/>barplot(corr[2:11,]$Y_categ,names.arg=row.names(corr)[2:11],<br/>        main="Feature Correlation to target variable",cex.names=0.8,las=2)<br/>par(old.par)</pre>
<p>Our new variable is highly correlated with our <kbd>y</kbd> variable at <kbd>0.958</kbd>. We also created a bar plot of the most highly correlated features to the <kbd>y</kbd> variable, and we can see that correlation between this new variable and the <kbd>y</kbd><span> variable </span>is much higher than <span>correlation between </span>the other variables and the <kbd>y</kbd><span> variable</span>. If a feature is very highly correlated to the <kbd>y</kbd> variable, then this is usually a sign that something is wrong in the data preparation. It also indicates that a machine learning solution is not required because a simple mathematical formula will be able to predict the outcome variable. For a real project, this variable should not be included in the model. Here is the graph with the <span>features that are </span>most highly correlated with the <kbd>y</kbd><span> variable</span>, <span>the correlation of the <kbd>bad_var</kbd> variable is over <kbd>0.9</kbd>:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-597 image-border" src="assets/c51ab854-c930-482d-948e-83aa0e701d6f.png" style="width:41.75em;height:28.75em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 6.6: The top 10 correlations from <span>feature</span> to target variable</div>
<p>Before we go ahead and build the model, notice how we set this new feature to zero for the test set. Our test set in this example actually represents the data that the model will see when it is production, so we set it to zero to represent either a different data distribution or a data leakage problem. Here is the code that shows how the model performs on the validation set and on the test set:</p>
<pre>#### Verifying the model using LIME<br/><br/># compare performance on validation and test set <br/>print(sprintf(" Deep Learning Model accuracy on validate (expected in production) = %1.2f%%",acc_v))<br/><strong>[1] " Deep Learning Model accuracy on validate (expected in production) = 90.08%"</strong><br/>print(sprintf(" Deep Learning Model accuracy in (actual in production) = %1.2f%%",acc_t))<br/><strong>[1] " Deep Learning Model accuracy in (actual in production) = 66.50%"</strong></pre>
<p>The validation set here represents the data used to evaluate the model when it is being built, while the test set represents the future production data. The accuracy on the validation set is over 90%, but the accuracy on the test set is less than 70%. This shows how different data distributions and/or data leakage problems can cause over-estimations of model accuracy.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model interpretability with LIME</h1>
                </header>
            
            <article>
                
<p class="mce-root"><strong>LIME </strong>stands for <strong>Local Interpretable Model-Agnostic Explanations</strong>. LIME can explain the predictions of any machine learning classifier, not just deep learning models. It works by making small changes to the input for each instance and trying to map the local decision boundary for that instance. By doing so, it can see which variable has the most influence for that instance. It is explained in the following paper: Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin<em>. Why should I trust you?: Explaining the predictions of any classifier. Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. ACM, 2016</em>.</p>
<p><span>Let's look at using LIME to analyze the model from the previous section. We have to set up some boilerplate code to interface the MXNet and LIME structures, and then we can create LIME objects based on our training data:</span></p>
<pre># apply LIME to MXNet deep learning model<br/>model_type.MXFeedForwardModel &lt;- function(x, ...) {return("classification")}<br/>predict_model.MXFeedForwardModel &lt;- function(m, newdata, ...)<br/>{<br/>  pred &lt;- predict(m, as.matrix(newdata),array.layout="rowmajor")<br/>  pred &lt;- as.data.frame(t(pred))<br/>  colnames(pred) &lt;- c("No","Yes")<br/>  return(pred)<br/>}<br/>explain &lt;- lime(dfData[train, predictorCols], model, bin_continuous = FALSE)</pre>
<p>We then can pass in the first 10 records in the test set and create a plot to show feature importance:</p>
<pre>val_first_10 &lt;- validate[1:10]<br/><br/>explaination &lt;- lime::explain(dfData[val_first_10, predictorCols],explainer=explain,<br/>                              n_labels=1,n_features=3)<br/>plot_features(explaination) + labs(title="Churn Model - variable explanation")</pre>
<p>This will produce the following plot, which shows the features that were most influential in the model predictions:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-598 image-border" src="assets/d6bbe7d7-bf00-4481-ba69-1aa00dfaf64d.png" style="width:96.17em;height:64.08em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 6.7: Feature importance using LIME</span></div>
<p>Note how, in each case, the <kbd>bad_var</kbd> variable is the most important variable and its scale is much larger than the other features. This matches what we saw in <em>Figure 6.6</em>. The following graph shows the heatmap visualization for feature combinations for the 10 test cases:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-599 image-border" src="assets/765662ae-1728-4f81-8d55-1f1cb1634153.png" style="width:43.92em;height:29.92em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 6.8:</span><span> Feature heatmap using LIME</span></div>
<p>This example shows how to <span>apply LIME to </span>an existing deep learning model trained with MXNet to visualize which features were the most important for some of the predictions using the model<span>. We can see in Figures 6.7 and 6.8 that a single feature was almost completely responsible for predicting the <kbd>y</kbd> variable, which is an indication that there is an issue with different data distributions and/or data leakage problems. In practice, such a variable should be excluded from the model.</span></p>
<p>As a comparison, if we train a model without this field, and plot the feature importance again, we see that one feature does not dominate:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-600 image-border" src="assets/4a3b160d-5582-4943-81b7-ceafba51f828.png" style="width:51.92em;height:34.42em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 6.9: Feature importance using LIME (without the <kbd>bad_var</kbd> feature)</span></div>
<p>There is not one feature that is a number 1 feature, the explanation fit is 0.05 compared to 0.18 in <em>Figure 6.7,</em> and the significance bars for the three variables are on a similar scale. The following graph shows the feature heatmap using LIME:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-601 image-border" src="assets/b25a6e33-f61c-4d77-95cb-78c0b82f0ad1.png" style="width:43.67em;height:29.17em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 6.10:</span><span> Feature heatmap using LIME (without the <kbd>bad_var</kbd> feature)</span></div>
<p>Again, this plot shows us that more than one feature is being used. We can see that the scale of legend for <span>the feature weights </span>in the preceding graph is from 0.01 - 0.02. In <em>Figure 6.8</em>, the <span>scale of legend for </span><span>the feature weights was -0.2 - 0.2, indicating that some features (just one, actually) are dominating the model.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="mce-root">This chapter covered topics that are critical to success in deep learning projects. These included the different types of evaluation metric that can be used to evaluate the model. We looked at some issues that can come up in data preparation, including if you only have a small amount of data to train on and how to create different splits in the data, that is, how to create proper train, test, and validation datasets. We looked at two important issues that can cause the model to perform poorly in production, different data distributions, and data leakage. We saw how data augmentation can be used to improve an existing model by creating artificial data and looked at <span>tuning </span>hyperparameters in order to improve the performance of a deep learning model. We closed the chapter by examining a use case where we simulated a problem with different data distributions/data leakage and used LIME to interpret an existing deep learning model.</p>
<p>Some of the concepts in this chapter may seem somewhat theoretical; however they are absolutely critical to the success of machine learning projects! Many books cover this material toward the end, but it is included in this book at a relatively early stage to signify its importance.</p>
<p>In the next chapter, we are going to look at using deep learning for <strong>Natural Language Processing</strong> (<strong>NLP</strong>), or text data. Using deep learning for text data is <span>more efficient, </span>simpler, and often outperforms traditional NLP approaches.</p>


            </article>

            
        </section>
    </body></html>