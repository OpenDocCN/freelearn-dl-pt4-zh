["```py\nlibrary(tensorflow) \ndatasets <- tf$contrib$learn$datasets \nmnist <- datasets$mnist$read_data_sets(\"MNIST-data\", one_hot = TRUE)  \n\n```", "```py\ntrainX <- mnist$train$images \ntrainY <- mnist$train$labels \n\n```", "```py\nPCA_model <- prcomp(trainX, retx=TRUE) \n\n```", "```py\nRBM_model <- rbm(trainX, retx=TRUE, max_epoch=500,num_hidden =900) \n\n```", "```py\nPCA_pred_train <- predict(PCA_model) \nRBM_pred_train <- predict(RBM_model,type='probs')\n\n```", "```py\nPCA_pred_train <- as.data.frame(PCA_pred_train)\n class=\"MsoSubtleEmphasis\">RBM_pred_train <- as.data.frame(as.matrix(RBM_pred_train))\n\n```", "```py\n    trainY_num<- as.numeric(stringi::stri_sub(colnames(as.data.frame(trainY))[max.col(as.data.frame(trainY),ties.method=\"first\")],2))\n\n```", "```py\nggplot(PCA_pred_train, aes(PC1, PC2))+\n  geom_point(aes(colour = trainY))+ \n  theme_bw()+labs()+ \n  theme(plot.title = element_text(hjust = 0.5)) \n\n```", "```py\nggplot(RBM_pred_train, aes(Hidden_2, Hidden_3))+\n  geom_point(aes(colour = trainY))+ \n  theme_bw()+labs()+ \n  theme(plot.title = element_text(hjust = 0.5)) \n\n```", "```py\nvar_explain <- as.data.frame(PCA_model$sdev^2/sum(PCA_model$sdev^2)) \nvar_explain <- cbind(c(1:784),var_explain,cumsum(var_explain[,1])) \ncolnames(var_explain) <- c(\"PcompNo.\",\"Ind_Variance\",\"Cum_Variance\") \nplot(var_explain$PcompNo.,var_explain$Cum_Variance, xlim = c(0,100),type='b',pch=16,xlab = \"# of Principal Components\",ylab = \"Cumulative Variance\",main = 'PCA - Explained variance') \n\n```", "```py\nplot(RBM_model,xlab = \"# of epoch iterations\",ylab = \"Reconstruction error\",main = 'RBM - Reconstruction Error') \n\n```", "```py\n# Reset the graph \ntf$reset_default_graph() \n# Starting session as interactive session \nsess <- tf$InteractiveSession() \n\n```", "```py\nnum_input<-784L \nnum_hidden<-900L\n\n```", "```py\nW <- tf$placeholder(tf$float32, shape = shape(num_input, num_hidden)) \n\n```", "```py\nvb <- tf$placeholder(tf$float32, shape = shape(num_input)) \nhb <- tf$placeholder(tf$float32, shape = shape(num_hidden)) \n\n```", "```py\nX = tf$placeholder(tf$float32, shape=shape(NULL, num_input)) \nprob_h0= tf$nn$sigmoid(tf$matmul(X, W) + hb) \nh0 = tf$nn$relu(tf$sign(prob_h0 - tf$random_uniform(tf$shape(prob_h0))))\n\n```", "```py\nsess$run(tf$global_variables_initializer()) \ns1 <- tf$constant(value = c(0.1,0.4,0.7,0.9)) \ncat(sess$run(s1)) \ns2=sess$run(tf$random_uniform(tf$shape(s1))) \ncat(s2) \ncat(sess$run(s1-s2)) \ncat(sess$run(tf$sign(s1 - s2))) \ncat(sess$run(tf$nn$relu(tf$sign(s1 - s2)))) \n\n```", "```py\nprob_v1 = tf$nn$sigmoid(tf$matmul(h0, tf$transpose(W)) + vb)\nv1 = tf$nn$relu(tf$sign(prob_v1 - tf$random_uniform(tf$shape(prob_v1))))\nh1 = tf$nn$sigmoid(tf$matmul(v1, W) + hb) \n\n```", "```py\nerr = tf$reduce_mean(tf$square(X - v1)) \n\n```", "```py\nw_pos_grad = tf$matmul(tf$transpose(X), h0) \n\n```", "```py\nw_neg_grad = tf$matmul(tf$transpose(v1), h1)\n\n```", "```py\nCD = (w_pos_grad - w_neg_grad) / tf$to_float(tf$shape(X)[0])\n\n```", "```py\nupdate_w = W + alpha * CD \n\n```", "```py\nupdate_vb = vb + alpha * tf$reduce_mean(X - v1) \nupdate_hb = hb + alpha * tf$reduce_mean(h0 - h1) \n\n```", "```py\ncur_w = tf$Variable(tf$zeros(shape = shape(num_input, num_hidden), dtype=tf$float32)) \ncur_vb = tf$Variable(tf$zeros(shape = shape(num_input), dtype=tf$float32)) \ncur_hb = tf$Variable(tf$zeros(shape = shape(num_hidden), dtype=tf$float32)) \nprv_w = tf$Variable(tf$random_normal(shape=shape(num_input, num_hidden), stddev=0.01, dtype=tf$float32)) \nprv_vb = tf$Variable(tf$zeros(shape = shape(num_input), dtype=tf$float32)) \nprv_hb = tf$Variable(tf$zeros(shape = shape(num_hidden), dtype=tf$float32)) \n\n```", "```py\nsess$run(tf$global_variables_initializer()) \n\n```", "```py\noutput <- sess$run(list(update_w, update_vb, update_hb), feed_dict = dict(X=trainX, \n                                                                          W = prv_w$eval(), \n                                                                          vb = prv_vb$eval(), \n                                                                          hb = prv_hb$eval())) \nprv_w <- output[[1]]\nprv_vb <-output[[2]] \nprv_hb <-output[[3]]\n\n```", "```py\nsess$run(err, feed_dict=dict(X= trainX, W= prv_w, vb= prv_vb, hb= prv_hb)) \n\n```", "```py\nepochs=15 \nerrors <- list() \nweights <- list() \nu=1 \nfor(ep in 1:epochs){ \n  for(i in seq(0,(dim(trainX)[1]-100),100)){ \n    batchX <- trainX[(i+1):(i+100),] \n    output <- sess$run(list(update_w, update_vb, update_hb), feed_dict = dict(X=batchX, \n                                                                              W = prv_w, \n                                                                              vb = prv_vb, \n                                                                              hb = prv_hb)) \n    prv_w <- output[[1]]  \n    prv_vb <- output[[2]] \n    prv_hb <-  output[[3]] \n    if(i%%10000 == 0){ \n      errors[[u]] <- sess$run(err, feed_dict=dict(X= trainX, W= prv_w, vb= prv_vb, hb= prv_hb)) \n      weights[[u]] <- output[[1]] \n      u <- u+1 \n     cat(i , \" : \") \n    } \n  } \n  cat(\"epoch :\", ep, \" : reconstruction error : \", errors[length(errors)][[1]],\"\\n\") \n} \n\n```", "```py\nerror_vec <- unlist(errors)\nplot(error_vec,xlab=\"# of batches\",ylab=\"mean squared reconstruction error\",main=\"RBM-Reconstruction MSE plot\")\n\n```", "```py\nuw = t(weights[[length(weights)]])   # Extract the most recent weight matrix \nnumXpatches = 20    # Number of images in X-axis (user input) \nnumYpatches=20      # Number of images in Y-axis (user input) \npixels <- list() \nop <- par(no.readonly = TRUE) \npar(mfrow = c(numXpatches,numYpatches), mar = c(0.2, 0.2, 0.2, 0.2), oma = c(3, 3, 3, 3)) \nfor (i in 1:(numXpatches*numYpatches)) { \n  denom <- sqrt(sum(uw[i, ]^2)) \n  pixels[[i]] <- matrix(uw[i, ]/denom, nrow = numYpatches, ncol = numXpatches) \n  image(pixels[[i]], axes = F, col = gray((0:32)/32)) \n} \npar(op) \n\n```", "```py\nsample_image <- trainX[1:4,]\n\n```", "```py\nmw=melt(sample_image) \nmw$X3=floor((mw$X2-1)/28)+1 \nmw$X2=(mw$X2-1)%%28 + 1; \nmw$X3=29-mw$X3 \nggplot(data=mw)+geom_tile(aes(X2,X3,fill=value))+facet_wrap(~X1,nrow=2)+ \n  scale_fill_continuous(low='black',high='white')+coord_fixed(ratio=1)+ \n  labs(x=NULL,y=NULL,)+ \n  theme(legend.position=\"none\")+ \n  theme(plot.title = element_text(hjust = 0.5)) \n\n```", "```py\nhh0 = tf$nn$sigmoid(tf$matmul(X, W) + hb) \nvv1 = tf$nn$sigmoid(tf$matmul(hh0, tf$transpose(W)) + vb) \nfeed = sess$run(hh0, feed_dict=dict( X= sample_image, W= prv_w, hb= prv_hb)) \nrec = sess$run(vv1, feed_dict=dict( hh0= feed, W= prv_w, vb= prv_vb)) \n\n```", "```py\nmw=melt(rec) \nmw$X3=floor((mw$X2-1)/28)+1 \nmw$X2=(mw$X2-1)%%28 + 1 \nmw$X3=29-mw$X3 \nggplot(data=mw)+geom_tile(aes(X2,X3,fill=value))+facet_wrap(~X1,nrow=2)+ \n  scale_fill_continuous(low='black',high='white')+coord_fixed(ratio=1)+ \n  labs(x=NULL,y=NULL,)+ \n  theme(legend.position=\"none\")+ \n  theme(plot.title = element_text(hjust = 0.5)) \n\n```", "```py\ntxt <- readLines(\"movies.dat\", encoding = \"latin1\") \ntxt_split <- lapply(strsplit(txt, \"::\"), function(x) as.data.frame(t(x), stringsAsFactors=FALSE)) \nmovies_df <- do.call(rbind, txt_split) \nnames(movies_df) <- c(\"MovieID\", \"Title\", \"Genres\") \nmovies_df$MovieID <- as.numeric(movies_df$MovieID) \n\n```", "```py\nmovies_df$id_order <- 1:nrow(movies_df) \n\n```", "```py\nratings_df <- read.table(\"ratings.dat\", sep=\":\",header=FALSE,stringsAsFactors = F) \nratings_df <- ratings_df[,c(1,3,5,7)] \ncolnames(ratings_df) <- c(\"UserID\",\"MovieID\",\"Rating\",\"Timestamp\")\n\n```", "```py\nmerged_df <- merge(movies_df, ratings_df, by=\"MovieID\",all=FALSE) \n\n```", "```py\nmerged_df[,c(\"Timestamp\",\"Title\",\"Genres\")] <- NULL \n\n```", "```py\nmerged_df$rating_per <- merged_df$Rating/5 \n\n```", "```py\nnum_of_users <- 1000 \nnum_of_movies <- length(unique(movies_df$MovieID)) \ntrX <- matrix(0,nrow=num_of_users,ncol=num_of_movies) \nfor(i in 1:num_of_users){ \n  merged_df_user <- merged_df[merged_df$UserID %in% i,] \n  trX[i,merged_df_user$id_order] <- merged_df_user$rating_per \n} \n\n```", "```py\nsummary(trX[1,]); summary(trX[2,]); summary(trX[3,]) \n\n```", "```py\nnum_hidden = 20 \nnum_input = nrow(movies_df)\n\n```", "```py\nsess$run(tf$global_variables_initializer()) \noutput <- sess$run(list(update_w, update_vb, update_hb), feed_dict = dict(v0=trX, \n                                                                          W = prv_w$eval(), \n                                                                          vb = prv_vb$eval(), \n                                                                          hb = prv_hb$eval())) \nprv_w <- output[[1]]  \nprv_vb <- output[[2]] \nprv_hb <-  output[[3]] \nsess$run(err_sum, feed_dict=dict(v0=trX, W= prv_w, vb= prv_vb, hb= prv_hb))\n\n```", "```py\nepochs= 500 \nerrors <- list() \nweights <- list() \n\nfor(ep in 1:epochs){ \n  for(i in seq(0,(dim(trX)[1]-100),100)){ \n    batchX <- trX[(i+1):(i+100),] \n    output <- sess$run(list(update_w, update_vb, update_hb), feed_dict = dict(v0=batchX, \n                                                                              W = prv_w, \n                                                                              vb = prv_vb, \n                                                                              hb = prv_hb)) \n    prv_w <- output[[1]]  \n    prv_vb <- output[[2]] \n    prv_hb <-  output[[3]] \n    if(i%%1000 == 0){ \n      errors <- c(errors,sess$run(err_sum, feed_dict=dict(v0=batchX, W= prv_w, vb= prv_vb, hb= prv_hb))) \n      weights <- c(weights,output[[1]]) \n      cat(i , \" : \") \n    } \n  } \n  cat(\"epoch :\", ep, \" : reconstruction error : \", errors[length(errors)][[1]],\"\\n\") \n} \n\n```", "```py\nerror_vec <- unlist(errors) \nplot(error_vec,xlab=\"# of batches\",ylab=\"mean squared reconstruction error\",main=\"RBM-Reconstruction MSE plot\") \n\n```", "```py\ninputUser = as.matrix(t(trX[75,]))\nnames(inputUser) <- movies_df$id_order\n\n```", "```py\ninputUser <- inputUser[inputUser>0]\n\n```", "```py\ntop_rated_movies <- movies_df[as.numeric(names(inputUser)[order(inputUser,decreasing = TRUE)]),]$Title\ntop_rated_genres <- movies_df[as.numeric(names(inputUser)[order(inputUser,decreasing = TRUE)]),]$Genres\ntop_rated_genres <- as.data.frame(top_rated_genres,stringsAsFactors=F)\ntop_rated_genres$count <- 1\ntop_rated_genres <- aggregate(count~top_rated_genres,FUN=sum,data=top_rated_genres)\ntop_rated_genres <- top_rated_genres[with(top_rated_genres, order(-count)), ]\ntop_rated_genres$top_rated_genres <- factor(top_rated_genres$top_rated_genres, levels = top_rated_genres$top_rated_genres)\nggplot(top_rated_genres[top_rated_genres$count>1,],aes(x=top_rated_genres,y=count))+\ngeom_bar(stat=\"identity\")+\ntheme_bw()+\ntheme(axis.text.x = element_text(angle = 90, hjust = 1))+\nlabs(x=\"Genres\",y=\"count\",)+\ntheme(plot.title = element_text(hjust = 0.5))\n\n```", "```py\nhh0 = tf$nn$sigmoid(tf$matmul(v0, W) + hb)\nvv1 = tf$nn$sigmoid(tf$matmul(hh0, tf$transpose(W)) + vb)\nfeed = sess$run(hh0, feed_dict=dict( v0= inputUser, W= prv_w, hb= prv_hb))\nrec = sess$run(vv1, feed_dict=dict( hh0= feed, W= prv_w, vb= prv_vb))\nnames(rec) <- movies_df$id_order\n\n```", "```py\ntop_recom_genres <- movies_df[as.numeric(names(rec)[order(rec,decreasing = TRUE)]),]$Genres\ntop_recom_genres <- as.data.frame(top_recom_genres,stringsAsFactors=F)\ntop_recom_genres$count <- 1\ntop_recom_genres <- aggregate(count~top_recom_genres,FUN=sum,data=top_recom_genres)\ntop_recom_genres <- top_recom_genres[with(top_recom_genres, order(-count)), ]\ntop_recom_genres$top_recom_genres <- factor(top_recom_genres$top_recom_genres, levels = top_recom_genres$top_recom_genres)\nggplot(top_recom_genres[top_recom_genres$count>20,],aes(x=top_recom_genres,y=count))+\ngeom_bar(stat=\"identity\")+\ntheme_bw()+\ntheme(axis.text.x = element_text(angle = 90, hjust = 1))+\nlabs(x=\"Genres\",y=\"count\",)+\ntheme(plot.title = element_text(hjust = 0.5))\n\n```", "```py\ntop_recom_movies <- movies_df[as.numeric(names(rec)[order(rec,decreasing = TRUE)]),]$Title[1:10]\n\n```", "```py\nrequire(tensorflow)\n\n```", "```py\nRBM_hidden_sizes = c(900, 500 , 300 )\n\n```", "```py\nRBM <- function(input_data, num_input, num_output, epochs = 5, alpha = 0.1, batchsize=100){\n# Placeholder variables\nvb <- tf$placeholder(tf$float32, shape = shape(num_input))\nhb <- tf$placeholder(tf$float32, shape = shape(num_output))\nW <- tf$placeholder(tf$float32, shape = shape(num_input, num_output))\n# Phase 1 : Forward Phase\nX = tf$placeholder(tf$float32, shape=shape(NULL, num_input))\nprob_h0= tf$nn$sigmoid(tf$matmul(X, W) + hb) #probabilities of the hidden units\nh0 = tf$nn$relu(tf$sign(prob_h0 - tf$random_uniform(tf$shape(prob_h0)))) #sample_h_given_X\n# Phase 2 : Backward Phase\nprob_v1 = tf$nn$sigmoid(tf$matmul(h0, tf$transpose(W)) + vb)\nv1 = tf$nn$relu(tf$sign(prob_v1 - tf$random_uniform(tf$shape(prob_v1))))\nh1 = tf$nn$sigmoid(tf$matmul(v1, W) + hb)\n# calculate gradients\nw_pos_grad = tf$matmul(tf$transpose(X), h0)\nw_neg_grad = tf$matmul(tf$transpose(v1), h1)\nCD = (w_pos_grad - w_neg_grad) / tf$to_float(tf$shape(X)[0])\nupdate_w = W + alpha * CD\nupdate_vb = vb + alpha * tf$reduce_mean(X - v1)\nupdate_hb = hb + alpha * tf$reduce_mean(h0 - h1)\n# Objective function\nerr = tf$reduce_mean(tf$square(X - v1))\n# Initialize variables\ncur_w = tf$Variable(tf$zeros(shape = shape(num_input, num_output), dtype=tf$float32))\ncur_vb = tf$Variable(tf$zeros(shape = shape(num_input), dtype=tf$float32))\ncur_hb = tf$Variable(tf$zeros(shape = shape(num_output), dtype=tf$float32))\nprv_w = tf$Variable(tf$random_normal(shape=shape(num_input, num_output), stddev=0.01, dtype=tf$float32))\nprv_vb = tf$Variable(tf$zeros(shape = shape(num_input), dtype=tf$float32))\nprv_hb = tf$Variable(tf$zeros(shape = shape(num_output), dtype=tf$float32))\n# Start tensorflow session\nsess$run(tf$global_variables_initializer())\noutput <- sess$run(list(update_w, update_vb, update_hb), feed_dict = dict(X=input_data,\nW = prv_w$eval(),\nvb = prv_vb$eval(),\nhb = prv_hb$eval()))\nprv_w <- output[[1]]\nprv_vb <- output[[2]]\nprv_hb <- output[[3]]\nsess$run(err, feed_dict=dict(X= input_data, W= prv_w, vb= prv_vb, hb= prv_hb))\nerrors <- weights <- list()\nu=1\nfor(ep in 1:epochs){\nfor(i in seq(0,(dim(input_data)[1]-batchsize),batchsize)){\nbatchX <- input_data[(i+1):(i+batchsize),]\noutput <- sess$run(list(update_w, update_vb, update_hb), feed_dict = dict(X=batchX,\nW = prv_w,\nvb = prv_vb,\nhb = prv_hb))\nprv_w <- output[[1]]\nprv_vb <- output[[2]]\nprv_hb <- output[[3]]\nif(i%%10000 == 0){\nerrors[[u]] <- sess$run(err, feed_dict=dict(X= batchX, W= prv_w, vb= prv_vb, hb= prv_hb))\nweights[[u]] <- output[[1]]\nu=u+1\ncat(i , \" : \")\n}\n}\ncat(\"epoch :\", ep, \" : reconstruction error : \", errors[length(errors)][[1]],\"\\n\")\n}\nw <- prv_w\nvb <- prv_vb\nhb <- prv_hb\n# Get the output\ninput_X = tf$constant(input_data)\nph_w = tf$constant(w)\nph_hb = tf$constant(hb)\nout = tf$nn$sigmoid(tf$matmul(input_X, ph_w) + ph_hb)\nsess$run(tf$global_variables_initializer())\nreturn(list(output_data = sess$run(out),\nerror_list=errors,\nweight_list=weights,\nweight_final=w,\nbias_final=hb))\n}\n\n```", "```py\ninpX = trainX\nRBM_output <- list()\nfor(i in 1:length(RBM_hidden_sizes)){\nsize <- RBM_hidden_sizes[i]\n# Train the RBM\nRBM_output[[i]] <- RBM(input_data= inpX,\nnum_input= ncol(trainX),\nnum_output=size,\nepochs = 5,\nalpha = 0.1,\nbatchsize=100)\n# Update the input data\ninpX <- RBM_output[[i]]$output_data\n# Update the input_size\nnum_input = size\ncat(\"completed size :\", size,\"\\n\")\n}\n\n```", "```py\nerror_df <- data.frame(\"error\"=c(unlist(RBM_output[[1]]$error_list),unlist(RBM_output[[2]]$error_list),unlist(RBM_output[[3]]$error_list)),\n\"batches\"=c(rep(seq(1:length(unlist(RBM_output[[1]]$error_list))),times=3)),\n\"hidden_layer\"=c(rep(c(1,2,3),each=length(unlist(RBM_output[[1]]$error_list)))),\nstringsAsFactors = FALSE)\n\n```", "```py\nplot(error ~ batches,\nxlab = \"# of batches\",\nylab = \"Reconstruction Error\",\npch = c(1, 7, 16)[hidden_layer],\nmain = \"Stacked RBM-Reconstruction MSE plot\",\ndata = error_df)\nlegend('topright',\nc(\"H1_900\",\"H2_500\",\"H3_300\"),\npch = c(1, 7, 16))\n\n```", "```py\nNN_train <- function(Xdata,Ydata,Xtestdata,Ytestdata,input_size,\nlearning_rate=0.1,momentum = 0.1,epochs=10,\nbatchsize=100,rbm_list,dbn_sizes){\nlibrary(stringi)\n*## insert all the codes mentioned in next 11 points* }\n\n```", "```py\nweight_list <- list()\nbias_list <- list()\n# Initialize variables\nfor(size in c(dbn_sizes,ncol(Ydata))){\n#Initialize weights through a random uniform distribution\nweight_list <- c(weight_list,tf$random_normal(shape=shape(input_size, size), stddev=0.01, dtype=tf$float32))\n#Initialize bias as zeroes\nbias_list <- c(bias_list, tf$zeros(shape = shape(size), dtype=tf$float32))\ninput_size = size\n}\n\n```", "```py\n#Check if expected dbn_sizes are correct\nif(length(dbn_sizes)!=length(rbm_list)){\nstop(\"number of hidden dbn_sizes not equal to number of rbm outputs generated\")\n# check if expected sized are correct\nfor(i in 1:length(dbn_sizes)){\nif(dbn_sizes[i] != dbn_sizes[i])\nstop(\"Number of hidden dbn_sizes do not match\")\n}\n}\n\n```", "```py\nfor(i in 1:length(dbn_sizes)){\nweight_list[[i]] <- rbm_list[[i]]$weight_final\nbias_list[[i]] <- rbm_list[[i]]$bias_final\n}\n\n```", "```py\ninput <- tf$placeholder(tf$float32, shape = shape(NULL,ncol(Xdata)))\noutput <- tf$placeholder(tf$float32, shape = shape(NULL,ncol(Ydata)))\n\n```", "```py\ninput_sub <- list()\nweight <- list()\nbias <- list()\nfor(i in 1:(length(dbn_sizes)+1)){\nweight[[i]] <- tf$cast(tf$Variable(weight_list[[i]]),tf$float32)\nbias[[i]] <- tf$cast(tf$Variable(bias_list[[i]]),tf$float32)\n}\ninput_sub[[1]] <- tf$nn$sigmoid(tf$matmul(input, weight[[1]]) + bias[[1]])\nfor(i in 2:(length(dbn_sizes)+1)){\ninput_sub[[i]] <- tf$nn$sigmoid(tf$matmul(input_sub[[i-1]], weight[[i]]) + bias[[i]])\n}\n\n```", "```py\ncost = tf$reduce_mean(tf$square(input_sub[[length(input_sub)]] - output))\n\n```", "```py\ntrain_op <- tf$train$MomentumOptimizer(learning_rate, momentum)$minimize(cost)\n\n```", "```py\npredict_op = tf$argmax(input_sub[[length(input_sub)]],axis=tf$cast(1.0,tf$int32))\n\n```", "```py\ntrain_accuracy <- c()\ntest_accuracy <- c()\nfor(ep in 1:epochs){\nfor(i in seq(0,(dim(Xdata)[1]-batchsize),batchsize)){\nbatchX <- Xdata[(i+1):(i+batchsize),]\nbatchY <- Ydata[(i+1):(i+batchsize),]\n#Run the training operation on the input data\nsess$run(train_op,feed_dict=dict(input = batchX,\noutput = batchY))\n}\nfor(j in 1:(length(dbn_sizes)+1)){\n# Retrieve weights and biases\nweight_list[[j]] <- sess$run(weight[[j]])\nbias_list[[j]] <- sess$ run(bias[[j]])\n}\ntrain_result <- sess$run(predict_op, feed_dict = dict(input=Xdata, output=Ydata))+1\ntrain_actual <- as.numeric(stringi::stri_sub(colnames(as.data.frame(Ydata))[max.col(as.data.frame(Ydata),ties.method=\"first\")],2))\ntest_result <- sess$run(predict_op, feed_dict = dict(input=Xtestdata, output=Ytestdata))+1\ntest_actual <- as.numeric(stringi::stri_sub(colnames(as.data.frame(Ytestdata))[max.col(as.data.frame(Ytestdata),ties.method=\"first\")],2))\ntrain_accuracy <- c(train_accuracy,mean(train_actual==train_result))\ntest_accuracy <- c(test_accuracy,mean(test_actual==test_result))\ncat(\"epoch:\", ep, \" Train Accuracy: \",train_accuracy[ep],\" Test Accuracy : \",test_accuracy[ep],\"\\n\")\n}\n\n```", "```py\nreturn(list(train_accuracy=train_accuracy,\ntest_accuracy=test_accuracy,\nweight_list=weight_list,\nbias_list=bias_list))\n\n```", "```py\nNN_results <- NN_train(Xdata=trainX,\nYdata=trainY,\nXtestdata=testX,\nYtestdata=testY,\ninput_size=ncol(trainX),\nrbm_list=RBM_output,\ndbn_sizes = RBM_hidden_sizes)\n\n```", "```py\naccuracy_df <- data.frame(\"accuracy\"=c(NN_results$train_accuracy,NN_results$test_accuracy),\n\"epochs\"=c(rep(1:10,times=2)),\n\"datatype\"=c(rep(c(1,2),each=10)),\nstringsAsFactors = FALSE)\nplot(accuracy ~ epochs,\nxlab = \"# of epochs\",\nylab = \"Accuracy in %\",\npch = c(16, 1)[datatype],\nmain = \"Neural Network - Accuracy in %\",\ndata = accuracy_df)\nlegend('bottomright',\nc(\"train\",\"test\"),\npch = c( 16, 1))\n\n```", "```py\nlearning_rate = 0.005\nmomentum = 0.005\nminbatch_size = 25\nhidden_layers = c(400,100)\nbiases = list(-1,-1)\n\n```", "```py\narcsigm <- function(x){\nreturn(atanh((2*x)-1)*2)\n}\n\n```", "```py\nsigm <- function(x){\nreturn(tanh((x/2)+1)/2)\n}\n\n```", "```py\nbinarize <- function(x){\n# truncated rnorm\ntrnrom <- function(n, mean, sd, minval = -Inf, maxval = Inf){\nqnorm(runif(n, pnorm(minval, mean, sd), pnorm(maxval, mean, sd)), mean, sd)\n}\nreturn((x > matrix( trnrom(n=nrow(x)*ncol(x),mean=0,sd=1,minval=0,maxval=1), nrow(x), ncol(x)))*1)\n}\n\n```", "```py\nre_construct <- function(x){\nx = x - min(x) + 1e-9\nx = x / (max(x) + 1e-9)\nreturn(x*255)\n}\n\n```", "```py\ngibbs <- function(X,l,initials){\nif(l>1){\nbu <- (X[l-1][[1]] - matrix(rep(initials$param_O[[l-1]],minbatch_size),minbatch_size,byrow=TRUE)) %*%\ninitials$param_W[l-1][[1]]\n} else {\nbu <- 0\n}\nif((l+1) < length(X)){\ntd <- (X[l+1][[1]] - matrix(rep(initials$param_O[[l+1]],minbatch_size),minbatch_size,byrow=TRUE))%*%\nt(initials$param_W[l][[1]])\n} else {\ntd <- 0\n}\nX[[l]] <- binarize(sigm(bu+td+matrix(rep(initials$param_B[[l]],minbatch_size),minbatch_size,byrow=TRUE)))\nreturn(X[[l]])\n}\n\n```", "```py\nreparamBias <- function(X,l,initials){\nif(l>1){\nbu <- colMeans((X[[l-1]] - matrix(rep(initials$param_O[[l-1]],minbatch_size),minbatch_size,byrow=TRUE))%*%\ninitials$param_W[[l-1]])\n} else {\nbu <- 0\n}\nif((l+1) < length(X)){\ntd <- colMeans((X[[l+1]] - matrix(rep(initials$param_O[[l+1]],minbatch_size),minbatch_size,byrow=TRUE))%*%\nt(initials$param_W[[l]]))\n} else {\ntd <- 0\n}\ninitials$param_B[[l]] <- (1-momentum)*initials$param_B[[l]] + momentum*(initials$param_B[[l]] + bu + td)\nreturn(initials$param_B[[l]])\n}\n\n```", "```py\nreparamO <- function(X,l,initials){\ninitials$param_O[[l]] <- colMeans((1-momentum)*matrix(rep(initials$param_O[[l]],minbatch_size),minbatch_size,byrow=TRUE) + momentum*(X[[l]]))\nreturn(initials$param_O[[l]])\n}\n\n```", "```py\nDRBM_initialize <- function(layers,bias_list){\n# Initialize model parameters and particles\nparam_W <- list()\nfor(i in 1:(length(layers)-1)){\nparam_W[[i]] <- matrix(0L, nrow=layers[i], ncol=layers[i+1])\n}\nparam_B <- list()\nfor(i in 1:length(layers)){\nparam_B[[i]] <- matrix(0L, nrow=layers[i], ncol=1) + bias_list[[i]]\n}\nparam_O <- list()\nfor(i in 1:length(param_B)){\nparam_O[[i]] <- sigm(param_B[[i]])\n}\nparam_X <- list()\nfor(i in 1:length(layers)){\nparam_X[[i]] <- matrix(0L, nrow=minbatch_size, ncol=layers[i]) + matrix(rep(param_O[[i]],minbatch_size),minbatch_size,byrow=TRUE)\n}\nreturn(list(param_W=param_W,param_B=param_B,param_O=param_O,param_X=param_X))\n}\n\n```", "```py\nX <- trainX/255\n\n```", "```py\nlayers <- c(784,hidden_layers)\nbias_list <- list(arcsigm(pmax(colMeans(X),0.001)),biases[[1]],biases[[2]])\ninitials <-DRBM_initialize(layers,bias_list)\n\n```", "```py\nbatchX <- X[sample(nrow(X))[1:minbatch_size],]\n\n```", "```py\nfor(iter in 1:1000){\n\n# Perform some learnings\nfor(j in 1:100){\n# Initialize a data particle\ndat <- list()\ndat[[1]] <- binarize(batchX)\nfor(l in 2:length(initials$param_X)){\ndat[[l]] <- initials$param_X[l][[1]]*0 + matrix(rep(initials$param_O[l][[1]],minbatch_size),minbatch_size,byrow=TRUE)\n}\n\n# Alternate gibbs sampler on data and free particles\nfor(l in rep(c(seq(2,length(initials$param_X),2), seq(3,length(initials$param_X),2)),5)){\ndat[[l]] <- gibbs(dat,l,initials)\n}\nfor(l in rep(c(seq(2,length(initials$param_X),2), seq(1,length(initials$param_X),2)),1)){\ninitials$param_X[[l]] <- gibbs(initials$param_X,l,initials)\n}\n\n# Parameter update\nfor(i in 1:length(initials$param_W)){\ninitials$param_W[[i]] <- initials$param_W[[i]] + (learning_rate*((t(dat[[i]] - matrix(rep(initials$param_O[i][[1]],minbatch_size),minbatch_size,byrow=TRUE)) %*%\n(dat[[i+1]] - matrix(rep(initials$param_O[i+1][[1]],minbatch_size),minbatch_size,byrow=TRUE))) -\n(t(initials$param_X[[i]] - matrix(rep(initials$param_O[i][[1]],minbatch_size),minbatch_size,byrow=TRUE)) %*%\n(initials$param_X[[i+1]] - matrix(rep(initials$param_O[i+1][[1]],minbatch_size),minbatch_size,byrow=TRUE))))/nrow(batchX))\n}\n\nfor(i in 1:length(initials$param_B)){\ninitials$param_B[[i]] <- colMeans(matrix(rep(initials$param_B[[i]],minbatch_size),minbatch_size,byrow=TRUE) + (learning_rate*(dat[[i]] - initials$param_X[[i]])))\n}\n\n# Reparameterization\nfor(l in 1:length(initials$param_B)){\ninitials$param_B[[l]] <- reparamBias(dat,l,initials)\n}\nfor(l in 1:length(initials$param_O)){\ninitials$param_O[[l]] <- reparamO(dat,l,initials)\n}\n}\n\n# Generate necessary outputs\ncat(\"Iteration:\",iter,\" \",\"Mean of W of VL-HL1:\",mean(initials$param_W[[1]]),\" \",\"Mean of W of HL1-HL2:\",mean(initials$param_W[[2]]) ,\"\\n\")\ncat(\"Iteration:\",iter,\" \",\"SDev of W of VL-HL1:\",sd(initials$param_W[[1]]),\" \",\"SDev of W of HL1-HL2:\",sd(initials$param_W[[2]]) ,\"\\n\")\n\n# Plot weight matrices\nW=diag(nrow(initials$param_W[[1]]))\nfor(l in 1:length(initials$param_W)){\nW = W %*% initials$param_W[[l]]\nm = dim(W)[2] * 0.05\nw1_arr <- matrix(0,28*m,28*m)\ni=1\nfor(k in 1:m){\nfor(j in 1:28){\nvec <- c(W[(28*j-28+1):(28*j),(k*m-m+1):(k*m)])\nw1_arr[i,] <- vec\ni=i+1\n}\n}\nw1_arr = re_construct(w1_arr)\nw1_arr <- floor(w1_arr)\nimage(w1_arr,axes = TRUE, col = grey(seq(0, 1, length = 256)))\n}\n}\n\n```"]