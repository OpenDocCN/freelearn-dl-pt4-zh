<html><head></head><body>
        <section id="82E8E1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Application of Deep Learning in Text Mining</h1>
                
            
            <article>
                
<p class="calibre2">The current chapter we will cover the following topics:</p>
<ul class="calibre12">
<li class="calibre13">Performing preprocessing of textual data and extraction of sentiments</li>
<li class="calibre13">Analyzing documents using tf-idf</li>
<li class="calibre13">Performing sentiment prediction using LSTM network</li>
<li class="calibre13">Application using text2vec examples</li>
</ul>


            </article>

            
        </section>
    

        <section id="83CP01-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Performing preprocessing of textual data and extraction of sentiments</h1>
                
            
            <article>
                
<p class="calibre2">In this section, we will use Jane Austen's bestselling novel Pride and Prejudice, published in 1813, for our textual data preprocessing analysis. In R, we will use the <kbd class="calibre10">tidytext</kbd> package by Hadley Wickham to perform tokenization, stop word removal, sentiment extraction using predefined sentiment lexicons, <strong class="calibre1">term frequency - inverse document frequency</strong> (<strong class="calibre1">tf-idf</strong>) matrix creation, and to understand pairwise correlations among <em class="calibre9">n</em>-grams.</p>
<p class="calibre2">In this section, instead of storing text as a string or a corpus or a <strong class="calibre1">document term matrix</strong> (<strong class="calibre1">DTM</strong>), we process them into a tabular format of one token per row.</p>


            </article>

            
        </section>
    

        <section>

                            <header id="84B9I2-a0a93989f17f4d6cb68b8cfd331bc5ab">
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<p class="calibre2">Here is how we go about preprocessing:</p>
<ol class="calibre15">
<li value="1" class="calibre13">Load the required packages:</li>
</ol>
<pre class="calibre23">
load_packages=c("janeaustenr","tidytext","dplyr","stringr","ggplot2","wordcloud","reshape2","igraph","ggraph","widyr","tidyr") 
lapply(load_packages, require, character.only = TRUE) 
</pre>
<ol start="2" class="calibre15">
<li value="2" class="calibre13">Load the <kbd class="calibre10">Pride and Prejudice</kbd> dataset. The <kbd class="calibre10">line_num</kbd> attribute is analogous to the line number printed in the book:</li>
</ol>
<pre class="calibre23">
Pride_Prejudice &lt;- data.frame("text" = prideprejudice, 
                              "book" = "Pride and Prejudice", 
                              "line_num" = 1:length(prideprejudice), 
                              stringsAsFactors=F) 
</pre>
<ol start="3" class="calibre15">
<li value="3" class="calibre13">Now, perform tokenization to restructure the one-string-per-row format to a one-token-per-row format. Here, the token can refer to a single word, a group of characters, co-occurring words (<em class="calibre9">n</em>-grams), sentences, paragraphs, and so on. Currently, we will tokenize sentence into singular words:</li>
</ol>
<pre class="calibre23">
Pride_Prejudice &lt;- Pride_Prejudice %&gt;% unnest_tokens(word,text) 
</pre>
<ol start="4" class="calibre15">
<li value="4" class="calibre13">Then, remove the commonly occurring words such as <em class="calibre9">the</em>, <em class="calibre9">and</em>, <em class="calibre9">for</em>, and so on using the <kbd class="calibre10">stop words</kbd> removal corpus:</li>
</ol>
<pre class="calibre23">
data(stop_words) 
Pride_Prejudice &lt;- Pride_Prejudice %&gt;% anti_join(stop_words,<br class="title-page-tagline"/>by="word") 
</pre>
<ol start="5" class="calibre15">
<li value="5" class="calibre13">Extract the most common textual words used:</li>
</ol>
<pre class="calibre23">
most.common &lt;- Pride_Prejudice %&gt;% dplyr::count(word, sort = TRUE)
</pre>
<ol start="6" class="calibre15">
<li value="6" class="calibre13">Visualize the top 10 common occurring words, as shown in the following figure:</li>
</ol>
<div class="cdpaligncenter"><img class="image-border99" src="../images/00139.gif"/></div>
<div class="packt_figref">Top 10 common words</div>
<pre class="calibre23">
most.common$word  &lt;- factor(most.common$word , levels = most.common$word) 
ggplot(data=most.common[1:10,], aes(x=word, y=n, fill=word)) + 
  geom_bar(colour="black", stat="identity")+ 
  xlab("Common Words") + ylab("N Count")+ 
  ggtitle("Top 10 common words")+ 
  guides(fill=FALSE)+ 
  theme(plot.title = element_text(hjust = 0.5))+ 
  theme(text = element_text(size = 10))+ 
  theme(panel.background = element_blank(), panel.grid.major = element_blank(),panel.grid.minor = element_blank()) 
</pre>
<ol start="7" class="calibre15">
<li value="7" class="calibre13">Then, extract sentiments at a higher level (that is positive or negative) using the <kbd class="calibre10">bing</kbd> lexicon.</li>
</ol>
<pre class="calibre23">
Pride_Prejudice_POS_NEG_sentiment &lt;- Pride_Prejudice %&gt;% inner_join(get_sentiments("bing"), by="word") %&gt;% dplyr::count(book, index = line_num %/% 150, sentiment) %&gt;% spread(sentiment, n, fill = 0) %&gt;% mutate(net_sentiment = positive - negative)
</pre>
<ol start="8" class="calibre15">
<li value="8" class="calibre13">Visualize the sentiments across small sections (150 words) of text, as shown in the following figure:</li>
</ol>
<div class="cdpaligncenter"><img class="image-border100" src="../images/00088.gif"/></div>
<div class="packt_figref">Distribution of the number of positive and negative words across sentences of 150 words each</div>
<pre class="calibre23">
ggplot(Pride_Prejudice_POS_NEG_sentiment, aes(index, net_sentiment))+ 
  geom_col(show.legend = FALSE) + 
  geom_line(aes(y=mean(net_sentiment)),color="blue")+ 
  xlab("Section (150 words each)") + ylab("Values")+ 
  ggtitle("Net Sentiment (POS - NEG) of Pride and Prejudice")+ 
  theme(plot.title = element_text(hjust = 0.5))+ 
  theme(text = element_text(size = 10))+ 
  theme(panel.background = element_blank(), panel.grid.major = element_blank(),panel.grid.minor = element_blank())
</pre>
<ol start="9" class="calibre15">
<li value="9" class="calibre13">Now extract sentiments at a granular level (namely positive, negative, anger, disgust, surprise, trust, and so on.) using the <kbd class="calibre10">nrc</kbd> lexicon:</li>
</ol>
<pre class="calibre23">
Pride_Prejudice_GRAN_sentiment &lt;- Pride_Prejudice %&gt;% inner_join(get_sentiments("nrc"), by="word") %&gt;% dplyr::count(book, index = line_num %/% 150, sentiment) %&gt;% spread(sentiment, n, fill = 0)
</pre>
<ol start="10" class="calibre15">
<li value="10" class="calibre13">Visualize the variation across different sentiments defined, as shown in the following figure:</li>
</ol>
<div class="packt_figref"><img class="image-border101" src="../images/00121.gif"/></div>
<div class="packt_figref">Variation across different types of sentiments</div>
<pre class="calibre23">
ggplot(stack(Pride_Prejudice_GRAN_sentiment[,3:12]), aes(x = ind, y = values)) + 
  geom_boxplot()+ 
  xlab("Sentiment types") + ylab("Sections (150 words) of text")+ 
  ggtitle("Variation across different sentiments")+ 
  theme(plot.title = element_text(hjust = 0.5))+ 
  theme(text = element_text(size = 15))+ 
  theme(panel.background = element_blank(), panel.grid.major = element_blank(),panel.grid.minor = element_blank())
</pre>
<ol start="11" class="calibre15">
<li value="11" class="calibre13">Extract the most occurring positive and negative words based on the <kbd class="calibre10">bing</kbd> lexicon, and visualize them as shown in the following figure:</li>
</ol>
<div class="packt_figref"><img class="image-border102" src="../images/00015.gif"/></div>
<div class="packt_figref">Top 10 positive and negative words in the novel Pride and Prejudice</div>
<pre class="calibre23">
POS_NEG_word_counts &lt;- Pride_Prejudice %&gt;% inner_join(get_sentiments("bing"), by="word") %&gt;% dplyr::count(word, sentiment, sort = TRUE) %&gt;% ungroup() POS_NEG_word_counts %&gt;% group_by(sentiment) %&gt;% top_n(10) %&gt;% ungroup() %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(word, n, fill = sentiment)) + geom_col(show.legend = FALSE) + facet_wrap(~sentiment, scales = "free_y") + ggtitle("Top 10 positive and negative words")+ coord_flip() + theme(plot.title = element_text(hjust = 0.5))+ theme(text = element_text(size = 15))+ labs(y = NULL, x = NULL)+ theme(panel.background = element_blank(),panel.border = element_rect(linetype = "dashed", fill = NA))
</pre>
<ol start="12" class="calibre15">
<li value="12" class="calibre13">Generate a sentiment word cloud as shown in the following figure:</li>
</ol>
<div class="cdpaligncenter"><img class="image-border103" src="../images/00127.jpeg"/></div>
<div class="packt_figref">Word cloud of positive and negative words</div>
<pre class="calibre23">
Prejudice %&gt;% <br class="title-page-tagline"/>inner_join(get_sentiments("bing"), by = "word") %&gt;% dplyr::count(word, sentiment, sort = TRUE) %&gt;% acast(word ~ sentiment, value.var = "n", fill = 0) %&gt;% comparison.cloud(colors = c("red", "green"), max.words = 100,title.size=2, use.r.layout=TRUE, random.order=TRUE, scale=c(6,0.5)
</pre>
<ol start="13" class="calibre15">
<li value="13" class="calibre13">Now analyze sentiments across the chapters of the book:</li>
</ol>
<ol class="calibre15">
<li value="1" class="calibre16">
<ol class="calibre17">
<li value="1" class="calibre13">Extract <span>the chapters, and perform tokenization:</span></li>
</ol>
</li>
</ol>
<pre class="calibre54">
austen_books_df &lt;- as.data.frame(austen_books(),stringsAsFactors=F) austen_books_df$book &lt;- as.character(austen_books_df$book) Pride_Prejudice_chapters &lt;- austen_books_df %&gt;% group_by(book) %&gt;% filter(book == "Pride &amp; Prejudice") %&gt;% mutate(chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", ignore_case = TRUE)))) %&gt;% ungroup() %&gt;% unnest_tokens(word, text)
</pre>
<ol class="calibre15">
<li value="1" class="calibre16">
<ol start="2" class="calibre17">
<li value="2" class="calibre13">Extract the set <kbd class="calibre10">positive</kbd> and <kbd class="calibre10">negative</kbd> words from the <kbd class="calibre10">bing</kbd> lexicon:</li>
</ol>
</li>
</ol>
<pre class="calibre54">
bingNEG &lt;- get_sentiments("bing") %&gt;%  
  filter(sentiment == "negative")  
bingPOS &lt;- get_sentiments("bing") %&gt;%  
  filter(sentiment == "positive") 
</pre>
<ol class="calibre15">
<li value="1" class="calibre16">
<ol start="3" class="calibre17">
<li value="3" class="calibre13">Get the count of words for each chapter:</li>
</ol>
</li>
</ol>
<pre class="calibre54">
  wordcounts &lt;- Pride_Prejudice_chapters %&gt;% 
  group_by(book, chapter) %&gt;% 
  dplyr::summarize(words = n()) 
</pre>
<ol class="calibre15">
<li value="1" class="calibre16">
<ol start="4" class="calibre17">
<li value="4" class="calibre13">Extract the ratio of positive and negative words:</li>
</ol>
</li>
</ol>
<pre class="calibre54">
POS_NEG_chapter_distribution &lt;- merge ( Pride_Prejudice_chapters %&gt;% 
semi_join(bingNEG, by="word") %&gt;% 
group_by(book, chapter) %&gt;% 
dplyr::summarize(neg_words = n()) %&gt;% 
left_join(wordcounts, by = c("book", "chapter")) %&gt;% 
mutate(neg_ratio = round(neg_words*100/words,2)) %&gt;% 
filter(chapter != 0) %&gt;% 
ungroup(), 
Pride_Prejudice_chapters %&gt;% 
semi_join(bingPOS, by="word") %&gt;% 
group_by(book, chapter) %&gt;%            dplyr::summarize(pos_words = n()) %&gt;% 
left_join(wordcounts, by = c("book", "chapter")) %&gt;% 
mutate(pos_ratio = round(pos_words*100/words,2)) %&gt;% 
filter(chapter != 0) %&gt;% 
ungroup() ) 
</pre>
<ol class="calibre15">
<li value="1" class="calibre16">
<ol start="5" class="calibre17">
<li value="5" class="calibre13">Generate a sentiment flag for each chapter based on the proportion of positive and negative words:</li>
</ol>
</li>
</ol>
<pre class="calibre54">
POS_NEG_chapter_distribution$sentiment_flag &lt;- ifelse(POS_NEG_chapter_distribution$neg_ratio &gt; POS_NEG_chapter_distribution$pos_ratio,"NEG","POS") 
table(POS_NEG_chapter_distribution$sentiment_flag)  
</pre>


            </article>

            
        </section>
    

        <section id="859Q41-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How it works...</h1>
                
            
            <article>
                
<p class="calibre2">As mentioned earlier, we have used Jane Austen's famous novel <em class="calibre9">Pride and Prejudice</em> in this section, detailing the steps involved in tidying the data, and extracting sentiments using (publicly) available lexicons.</p>
<p class="calibre2">Steps 1 and 2 show the loading of the required <kbd class="calibre10">cran</kbd> packages and the required text. Steps 3 and 4 perform unigram tokenization and stop word removal. Steps 5 and 6 extract and visualize the top 10 most occurring words across all the 62 chapters. Steps 7 to 12 demonstrate high and granular-level sentiments using two widely used lexicons <kbd class="calibre10">bing</kbd> and <kbd class="calibre10">nrc</kbd>.</p>
<div class="packt_infobox">Both the lexicons contains a list of widely used English words that are tagged to sentiments. In <kbd class="calibre34">bing</kbd>, each word is tagged to one of the high level binary sentiments (positive or negative), and in <kbd class="calibre34">nrc</kbd>, each word is tagged to one of the granular-level multiple sentiments (positive, negative, anger, anticipation, joy, fear, disgust, trust, sadness, and surprise).</div>
<p class="calibre2">Each 150-word-long sentence is tagged to a sentiment, and the same has been shown in the figure showing the <em class="calibre9">Distribution of number of positive and negative words across sentences of 150 words each</em>. In step 13, chapter-wise sentiment tagging is performed using maximum occurrence of positive or negative words from the <kbd class="calibre10">bing</kbd> lexicon. Out of 62 chapters, 52 have more occurrences of positive lexicons, and 10 have more occurrences of negative lexicons.</p>


            </article>

            
        </section>
    

        <section id="868AM1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Analyzing documents using tf-idf</h1>
                
            
            <article>
                
<p class="calibre2">In this section, we will learn how to analyze documents quantitatively. A simple way is to look at the distribution of unigram words across the document and their frequency of occurrence, also termed as <strong class="calibre1">term frequency</strong> (<strong class="calibre1">tf</strong>). The words with higher frequency of occurrence generally tend to dominate the document.</p>
<p class="calibre2">However, one would disagree in case of generally occurring words such as the, is, of, and so on. Hence, these are removed by stop word dictionaries. Apart from these stop words, there might be some specific words that are more frequent with less relevance. Such kinds of words are penalized using their <strong class="calibre1">inverse document frequency</strong> (<strong class="calibre1">idf</strong>) values. Here, the words with higher frequency of occurrence are penalized.</p>
<div class="packt_infobox">The statistic tf-idf combines these two quantities (by multiplication) and provides a measure of importance or relevance of each word for a given document across multiple documents (or a corpus).</div>
<p class="calibre2">In this section, we will generate a tf-idf matrix across chapters of the book <em class="calibre9">Pride and Prejudice</em>.</p>


            </article>

            
        </section>
    

        <section id="876R81-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<p class="calibre2">Here is how we go about analyzing documents using tf-idf:</p>
<ol class="calibre15">
<li value="1" class="calibre13">Extract the text of all 62 chapters in the book <kbd class="calibre10">Pride and Prejudice</kbd>. Then, return chapter-wise occurrence of each word. The total words in the book are approx 1.22M.</li>
</ol>
<pre class="calibre23">
Pride_Prejudice_chapters &lt;- austen_books_df %&gt;% 
group_by(book) %&gt;% 
filter(book == "Pride &amp; Prejudice") %&gt;% 
mutate(linenumber = row_number(), 
chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",<br class="title-page-tagline"/>                                        ignore_case = TRUE)))) %&gt;% 
ungroup() %&gt;% 
unnest_tokens(word, text) %&gt;% 
count(book, chapter, word, sort = TRUE) %&gt;% 
ungroup() 
</pre>
<ol start="2" class="calibre15">
<li value="2" class="calibre13">Calculate the rank of words such that the most frequently occurring words have lower ranks. Also, visualize the term frequency by rank, as shown in the following figure:</li>
</ol>
<div class="cdpaligncenter"><img class="image-border104" src="../images/00016.jpeg"/></div>
<div class="packt_figref">This figure shows lower ranks for words with higher term-frequency (ratio) value</div>
<pre class="calibre23">
freq_vs_rank &lt;- Pride_Prejudice_chapters %&gt;%  
mutate(rank = row_number(),  
       term_frequency = n/totalwords) 
freq_vs_rank %&gt;%  
  ggplot(aes(rank, term_frequency)) +  
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +  
  scale_x_log10() + 
  scale_y_log10()
</pre>
<ol start="3" class="calibre15">
<li value="3" class="calibre13">Calculate the <kbd class="calibre10">tf-idf</kbd> value for each word using the <kbd class="calibre10">bind_tf-idf</kbd> function:</li>
</ol>
<pre class="calibre23">
Pride_Prejudice_chapters &lt;- Pride_Prejudice_chapters %&gt;% <br class="title-page-tagline"/>bind_tf_idf(word, chapter, n)
</pre>
<ol start="4" class="calibre15">
<li value="4" class="calibre13">Extract and visualize the top 15 words with higher values of tf-idf, as shown in the following figure:</li>
</ol>
<div class="cdpaligncenter"><img class="image-border105" src="../images/00018.jpeg"/></div>
<div class="packt_figref">tf-idf values of top 15 words</div>
<pre class="calibre23">
<br class="title-page-tagline"/>Pride_Prejudice_chapters %&gt;% 
  select(-totalwords) %&gt;% 
  arrange(desc(tf_idf)) 
 
Pride_Prejudice_chapters %&gt;% 
  arrange(desc(tf_idf)) %&gt;% 
  mutate(word = factor(word, levels = rev(unique(word)))) %&gt;%  
  group_by(book) %&gt;%  
  top_n(15) %&gt;%  
  ungroup %&gt;% 
  ggplot(aes(word, tf_idf, fill = book)) + 
  geom_col(show.legend = FALSE) + 
  labs(x = NULL, y = "tf-idf") + 
  facet_wrap(~book, ncol = 2, scales = "free") + 
  coord_flip() 
</pre>


            </article>

            
        </section>
    

        <section id="885BQ1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How it works...</h1>
                
            
            <article>
                
<p class="calibre2">As mentioned earlier, one can observe that the tf-idf scores of very common words such as <em class="calibre9">the</em> are close to zero and those of fewer occurrence words such as the proper noun <em class="calibre9">Austen</em> is close to one.</p>


            </article>

            
        </section>
    

        <section id="893SC1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Performing sentiment prediction using LSTM network</h1>
                
            
            <article>
                
<p class="calibre2">In this section, we will use LSTM networks to perform sentiment analysis. Along with the word itself, the LSTM network also accounts for the sequence using recurrent connections, which makes it more accurate than a traditional feed-forward neural network.</p>
<p class="calibre2">Here, we shall use the <kbd class="calibre10">movie reviews</kbd> dataset <kbd class="calibre10">text2vec</kbd> from the <kbd class="calibre10">cran</kbd> package. This dataset consists of 5,000 IMDb movie reviews, where each review is tagged with a binary sentiment flag (positive or negative).</p>


            </article>

            
        </section>
    

        <section id="8A2CU1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<p class="calibre2">Here is how you can proceed with sentiment prediction using LSTM:</p>
<ol class="calibre15">
<li value="1" class="calibre13">Load the required packages and movie reviews dataset:</li>
</ol>
<pre class="calibre23">
load_packages=c("text2vec","tidytext","tensorflow") 
lapply(load_packages, require, character.only = TRUE) 
data("movie_review") 
</pre>
<ol start="2" class="calibre15">
<li value="2" class="calibre13">Extract the movie reviews and labels as a dataframe and matrix respectively. In movie reviews, add an additional attribute <kbd class="calibre10">"Sno"</kbd> denoting the review number. In the labels matrix, add an additional attribute related to <kbd class="calibre10">negative flag</kbd>.</li>
</ol>
<pre class="calibre23">
reviews &lt;- data.frame("Sno" = 1:nrow(movie_review), 
                         "text"=movie_review$review, 
                         stringsAsFactors=F) 
 
labels &lt;- as.matrix(data.frame("Positive_flag" = movie_review$sentiment,"negative_flag" = (1<br class="title-page-tagline"/>                    movie_review$sentiment)))
</pre>
<ol start="3" class="calibre15">
<li value="3" class="calibre13">Extract all the unique words across the reviews, and get their count of occurrences (<em class="calibre9">n</em>). Also, tag each word with a unique integer (<kbd class="calibre10">orderNo</kbd>). Thus, each word is encoded using a unique integer, which shall be later used in the LSTM network.</li>
</ol>
<pre class="calibre23">
reviews_sortedWords &lt;- reviews %&gt;% unnest_tokens(word,text) %&gt;% dplyr::count(word, sort = TRUE) 
reviews_sortedWords$orderNo &lt;- 1:nrow(reviews_sortedWords) 
reviews_sortedWords &lt;- as.data.frame(reviews_sortedWords) 
</pre>
<ol start="4" class="calibre15">
<li value="4" class="calibre13">Now, assign the tagged words back to the reviews based on their occurrences:</li>
</ol>
<pre class="calibre23">
reviews_words &lt;- reviews %&gt;% unnest_tokens(word,text) 
reviews_words &lt;- plyr::join(reviews_words,reviews_sortedWords,by="word") 
</pre>
<ol start="5" class="calibre15">
<li value="5" class="calibre13">Using the outcome of step 4, create a list of reviews with each review transformed into a set of encoded numbers representing those words:</li>
</ol>
<pre class="calibre23">
reviews_words_sno &lt;- list() 
for(i in 1:length(reviews$text))<br class="title-page-tagline"/>{ 
  reviews_words_sno[[i]] &lt;- c(subset(reviews_words,Sno==i,orderNo)) 
} 
</pre>
<ol start="6" class="calibre15">
<li value="6" class="calibre13">In order to facilitate equal-length sequences to the LSTM network, let's restrict the review length to 150 words. In other words, reviews longer than 150 words will be truncated to the first 150, whereas shorter reviews will be made 150 words long by prefixing with the required number of zeroes. Thus, we now add in a new word <strong class="calibre1">0</strong>.</li>
</ol>
<pre class="calibre23">
reviews_words_sno &lt;- lapply(reviews_words_sno,function(x) <br class="title-page-tagline"/>{ 
  x &lt;- x$orderNo 
  if(length(x)&gt;150)<br class="title-page-tagline"/>  { 
    return (x[1:150]) 
  } <br class="title-page-tagline"/>  else <br class="title-page-tagline"/>  { 
  return(c(rep(0,150-length(x)),x)) 
  } 
})
</pre>
<ol start="7" class="calibre15">
<li value="7" class="calibre13">Now split the 5,000 reviews into training and testing reviews using a 70:30 split ratio. Also, bind the list of train and test reviews row wise into a matrix format, with rows representing reviews and columns representing the position of a word:</li>
</ol>
<pre class="calibre23">
train_samples &lt;- caret::createDataPartition(c(1:length(labels[1,1])),p = 0.7)$Resample1 
 
train_reviews &lt;- reviews_words_sno[train_samples] 
test_reviews &lt;- reviews_words_sno[-train_samples] 
 
train_reviews &lt;- do.call(rbind,train_reviews) 
test_reviews &lt;- do.call(rbind,test_reviews)  
</pre>
<ol start="8" class="calibre15">
<li value="8" class="calibre13">Similarly, also split the labels into train and test accordingly:</li>
</ol>
<pre class="calibre23">
train_labels &lt;- as.matrix(labels[train_samples,]) 
test_labels &lt;- as.matrix(labels[-train_samples,]) 
</pre>
<ol start="9" class="calibre15">
<li value="9" class="calibre13">Reset the graph, and start an interactive TensorFlow session:</li>
</ol>
<pre class="calibre23">
tf$reset_default_graph() 
sess&lt;-tf$InteractiveSession() 
</pre>
<ol start="10" class="calibre15">
<li value="10" class="calibre13">Define model parameters such as size of input pixels (<kbd class="calibre10">n_input</kbd>), step size (<kbd class="calibre10">step_size</kbd>), number of hidden layers (<kbd class="calibre10">n.hidden</kbd>), and number of outcome classes (<kbd class="calibre10">n.classes</kbd>):</li>
</ol>
<pre class="calibre23">
n_input&lt;-15 
step_size&lt;-10 
n.hidden&lt;-2 
n.class&lt;-2 
</pre>
<ol start="11" class="calibre15">
<li value="11" class="calibre13">Define training parameters such as learning rate (<kbd class="calibre10">lr</kbd>), number of inputs per batch run (<kbd class="calibre10">batch</kbd>), and number of iterations (<kbd class="calibre10">iteration</kbd>):</li>
</ol>
<pre class="calibre23">
lr&lt;-0.01 
batch&lt;-200 
iteration = 500
</pre>
<ol start="12" class="calibre15">
<li value="12" class="calibre13">Based on the RNN and LSTM functions defined in <a href="part0248.html#7CGBG1-a0a93989f17f4d6cb68b8cfd331bc5ab" target="_blank" class="calibre4">Chapter 6</a>, <em class="calibre9">Recurrent Neural Networks,</em> from the section <em class="calibre9">Run the optimization post initializing a session using global variables initializer</em>.</li>
</ol>
<pre class="calibre23">
sess$run(tf$global_variables_initializer()) 
train_error &lt;- c() 
for(i in 1:iteration){ 
  spls &lt;- sample(1:dim(train_reviews)[1],batch) 
  sample_data&lt;-train_reviews[spls,] 
  sample_y&lt;-train_labels[spls,] 
   
  # Reshape sample into 15 sequence with each of 10 elements 
  sample_data=tf$reshape(sample_data, shape(batch, step_size, n_input)) 
  out&lt;-optimizer$run(feed_dict = dict(x=sample_data$eval(session = sess), y=sample_y)) 
   
  if (i %% 1 == 0){ 
    cat("iteration - ", i, "Training Loss - ",  cost$eval(feed_dict = dict(x=sample_data$eval(), y=sample_y)), "\n") 
  } 
  train_error &lt;-  c(train_error,cost$eval(feed_dict = dict(x=sample_data$eval(), y=sample_y))) 
} 
</pre>
<ol start="13" class="calibre15">
<li value="13" class="calibre13">Plot the reduction in training errors across iterations as shown in the following figure:</li>
</ol>
<div class="cdpaligncenter"><img class="image-border106" src="../images/00118.gif"/></div>
<div class="packt_figref">Distribution of sentiment prediction error of training dataset</div>
<pre class="calibre23">
plot(train_error, main="Training sentiment prediction error", xlab="Iterations", ylab = "Train Error")
</pre>
<ol start="14" class="calibre15">
<li value="14" class="calibre13">Get the error of test data:</li>
</ol>
<pre class="calibre23">
test_data=tf$reshape(test_reviews, shape(-1, step_size, n_input)) <br class="title-page-tagline"/>cost$eval(feed_dict=dict(x= test_data$eval(), y=test_labels))
</pre>


            </article>

            
        </section>
    

        <section id="8B0TG1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How it works...</h1>
                
            
            <article>
                
<p class="calibre2">In steps 1 to 8, the movie reviews dataset is loaded, processed, and transformed into a set of train and test matrices, which can be directly used to train an LSTM network. Steps 9 to 14 are used to run LSTM using TensorFlow, as described in <a href="part0248.html#7CGBG1-a0a93989f17f4d6cb68b8cfd331bc5ab" target="_blank" class="calibre4">Chapter 6</a>, <em class="calibre9">Recurrent Neural Networks</em>. The figure <em class="calibre9">Distribution of sentiment prediction error of training dataset</em> shows the decline in training errors across 500 iterations.</p>


            </article>

            
        </section>
    

        <section id="8BVE21-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Application using text2vec examples</h1>
                
            
            <article>
                
<p class="calibre2">In this section, we will analyze the performance of logistic regression on various examples of <kbd class="calibre10">text2vec</kbd>.</p>


            </article>

            
        </section>
    

        <section id="8CTUK1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<p class="calibre2">Here is how we apply <kbd class="calibre10">text2vec</kbd>:</p>
<ol class="calibre15">
<li value="1" class="calibre13">Load the required packages and dataset:</li>
</ol>
<pre class="calibre23">
library(text2vec) 
library(glmnet) 
data("movie_review") 
</pre>
<ol start="2" class="calibre15">
<li value="2" class="calibre13">Function to perform Lasso logistic regression, and return the train and test <kbd class="calibre10">AUC</kbd> values:</li>
</ol>
<pre class="calibre23">
logistic_model &lt;- function(Xtrain,Ytrain,Xtest,Ytest)<br class="title-page-tagline"/>{ 
  classifier &lt;- cv.glmnet(x=Xtrain, y=Ytrain, 
  family="binomial", alpha=1, type.measure = "auc", 
  nfolds = 5, maxit = 1000) 
  plot(classifier) 
  vocab_test_pred &lt;- predict(classifier, Xtest, type = "response") 
  return(cat("Train AUC : ", round(max(classifier$cvm), 4), 
  "Test AUC : ",glmnet:::auc(Ytest, vocab_test_pred),"\n")) 
} 
</pre>
<ol start="3" class="calibre15">
<li value="3" class="calibre13">Split the movies review data into train and test in an 80:20 ratio:</li>
</ol>
<pre class="calibre23">
train_samples &lt;- caret::createDataPartition(c(1:length(labels[1,1])),p = 0.8)$Resample1 
train_movie &lt;- movie_review[train_samples,] 
test_movie &lt;- movie_review[-train_samples,] 
</pre>
<ol start="4" class="calibre15">
<li value="4" class="calibre13">Generate a DTM of all vocabulary words (without any stop word removal), and asses its performance using Lasso logistic regression:</li>
</ol>
<pre class="calibre23">
train_tokens &lt;- train_movie$review %&gt;% tolower %&gt;% word_tokenizer 
test_tokens &lt;- test_movie$review %&gt;% tolower %&gt;% word_tokenizer 
 
vocab_train &lt;- create_vocabulary(itoken(train_tokens,ids=train$id,progressbar = FALSE)) 
 
# Create train and test DTMs 
vocab_train_dtm &lt;- create_dtm(it = itoken(train_tokens,ids=train$id,progressbar = FALSE), 
                              vectorizer = vocab_vectorizer(vocab_train)) 
vocab_test_dtm &lt;- create_dtm(it = itoken(test_tokens,ids=test$id,progressbar = FALSE), 
                              vectorizer = vocab_vectorizer(vocab_train)) 
 
dim(vocab_train_dtm) 
dim(vocab_test_dtm) 
 
# Run LASSO (L1 norm) Logistic Regression 
logistic_model(Xtrain = vocab_train_dtm, 
               Ytrain = train_movie$sentiment, 
               Xtest = vocab_test_dtm, 
               Ytest = test_movie$sentiment) 
</pre>
<ol start="5" class="calibre15">
<li value="5" class="calibre13">Perform pruning using a list of stop words, and then assess the performance using Lasso logistic regression:</li>
</ol>
<pre class="calibre23">
data("stop_words") 
vocab_train_prune &lt;- create_vocabulary(itoken(train_tokens,ids=train$id,progressbar = FALSE), 
                                       stopwords = stop_words$word) 
 
vocab_train_prune &lt;- prune_vocabulary(vocab_train_prune,term_count_min = 15, 
                                      doc_proportion_min = 0.0005, 
                                      doc_proportion_max = 0.5) 
 
vocab_train_prune_dtm &lt;- create_dtm(it = itoken(train_tokens,ids=train$id,progressbar = FALSE), 
                              vectorizer = vocab_vectorizer(vocab_train_prune)) 
vocab_test_prune_dtm &lt;- create_dtm(it = itoken(test_tokens,ids=test$id,progressbar = FALSE), 
                             vectorizer = vocab_vectorizer(vocab_train_prune)) 
 
logistic_model(Xtrain = vocab_train_prune_dtm, 
               Ytrain = train_movie$sentiment, 
               Xtest = vocab_test_prune_dtm, 
               Ytest = test_movie$sentiment) 
</pre>
<ol start="6" class="calibre15">
<li value="6" class="calibre13">Generate a DTM using <em class="calibre9">n</em>-grams (uni and bigram words), and then assess the performance using Lasso logistic regression:</li>
</ol>
<pre class="calibre23">
vocab_train_ngrams &lt;- create_vocabulary(itoken(train_tokens,ids=train$id,progressbar = FALSE), 
                                        ngram = c(1L, 2L)) 
 
vocab_train_ngrams &lt;- prune_vocabulary(vocab_train_ngrams,term_count_min = 10, 
                                       doc_proportion_min = 0.0005, 
                                       doc_proportion_max = 0.5) 
 
vocab_train_ngrams_dtm &lt;- create_dtm(it = itoken(train_tokens,ids=train$id,progressbar = FALSE), 
                                    vectorizer = vocab_vectorizer(vocab_train_ngrams)) 
vocab_test_ngrams_dtm &lt;- create_dtm(it = itoken(test_tokens,ids=test$id,progressbar = FALSE), 
                                   vectorizer = vocab_vectorizer(vocab_train_ngrams)) 
 
 
logistic_model(Xtrain = vocab_train_ngrams_dtm, 
               Ytrain = train_movie$sentiment, 
               Xtest = vocab_test_ngrams_dtm, 
               Ytest = test_movie$sentiment) 
</pre>
<ol start="7" class="calibre15">
<li value="7" class="calibre13">Perform feature hashing, and then asses the performance using Lasso logistic regression:</li>
</ol>
<pre class="calibre23">
vocab_train_hashing_dtm &lt;- create_dtm(it = itoken(train_tokens,ids=train$id,progressbar = FALSE), 
                                      vectorizer = hash_vectorizer(hash_size = 2^14, ngram = c(1L, 2L))) 
vocab_test_hashing_dtm &lt;- create_dtm(it = itoken(test_tokens,ids=test$id,progressbar = FALSE), 
                                    vectorizer = hash_vectorizer(hash_size = 2^14, ngram = c(1L, 2L))) 
 
logistic_model(Xtrain = vocab_train_hashing_dtm, 
               Ytrain = train_movie$sentiment, 
               Xtest = vocab_test_hashing_dtm, 
               Ytest = test_movie$sentiment) 
</pre>
<ol start="8" class="calibre15">
<li value="8" class="calibre13">Using tf-idf transformation on full vocabulary DTM, assess the performance using Lasso logistic regression:</li>
</ol>
<pre class="calibre23">
vocab_train_tfidf &lt;- fit_transform(vocab_train_dtm, TfIdf$new()) 
vocab_test_tfidf &lt;- fit_transform(vocab_test_dtm, TfIdf$new()) 

logistic_model(Xtrain = vocab_train_tfidf, 
               Ytrain = train_movie$sentiment, 
               Xtest = vocab_test_tfidf, 
               Ytest = test_movie$sentiment)  
</pre>


            </article>

            
        </section>
    

        <section id="8DSF61-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How it works...</h1>
                
            
            <article>
                
<p class="calibre2">Steps 1 to 3 loads necessary packages, datasets, and functions required to assess different examples of <kbd class="calibre10">text2vec</kbd>. Logistic regression is implemented using the <kbd class="calibre10">glmnet</kbd> package with L1 penalty (Lasso regularization). In step 4, a DTM is created using all the vocabulary words present in the train movie reviews, and the test <kbd class="calibre10">auc</kbd> value is 0.918. In step 5, the train and test DTMs are pruned using stop words and frequency of occurrence.</p>
<p class="calibre2">The test <kbd class="calibre10">auc</kbd> value is observed as 0.916, not much decrease compared to using all the vocabulary words. In step 6, along with single words (or uni-grams), bi-grams are also added to the vocabulary. The test <kbd class="calibre10">auc</kbd> value increases to 0.928. Feature hashing is then performed in step 7, and the test <kbd class="calibre10">auc</kbd> value is 0.895. Though the <kbd class="calibre10">auc</kbd> value reduced, hashing is meant to improve run-time performance of larger datasets. Feature hashing is widely popularized by Yahoo. Finally, in step 8, we perform tf-idf transformation, which returns a test <kbd class="calibre10">auc</kbd> value of 0.907.</p>


            </article>

            
        </section>
    </body></html>