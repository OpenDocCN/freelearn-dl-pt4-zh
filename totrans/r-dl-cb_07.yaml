- en: Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The current chapter will introduce Reinforcement Learning. We will cover the
    following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a Markov Decision Process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing model-based learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing model-free learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Reinforcement Learning** (**RL**) is an area in machine learning that is
    inspired by psychology, such as how agents (software programs) can take actions
    in order to maximize cumulative rewards.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The RL is reward-based learning where the reward comes at the end or is distributed
    during the learning. For example, in chess, the reward will be assigned to winning
    or losing the game whereas in games such as tennis, every point won is a reward.
    Some of the commercial examples of RL are DeepMind from Google uses RL to master
    parkour. Similarly, Tesla is developing AI-driven technology using RL. An example
    of reinforcement architecture is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00111.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Interaction of an agent with environment in Reinforcement Learning
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic notations for RL are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**T(s, a, s'')**: Represents the transition model for reaching state *s''*
    when action *a* is taken at state *s*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00120.gif): Represents a policy which defines what action to take at
    every possible state ![](img/00114.gif)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '**R(s)**: Denotes the reward received by agent at state *s*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The current chapter will look at how to set up reinforcement models using R.
    The next sub-section will introduce `MDPtoolbox` from R.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a Markov Decision Process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Markov Decision Process** (**MDP**) forms the basis of setting up RL,
    where the outcome of a decision is semi-controlled; that is, it is partly random
    and partly controlled (by the decision-maker). An MDP is defined using a set of
    possible states (**S**), a set of possible actions (**A**), a real-values reward
    function (**R**), and a set of transition probabilities from one state to another
    state for a given action (**T**). In addition, the effects of an action performed
    on one state depends only on that state and not on its previous states.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, let us define an agent travelling across a 4 x 4 grid, as
    shown in following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00025.gif)'
  prefs: []
  type: TYPE_IMG
- en: A sample 4 x 4 grid of 16 states
  prefs: []
  type: TYPE_NORMAL
- en: 'This grid has 16 states (*S1*, *S2*....*S16*). In each state, the agent can
    perform four actions (*up*, *right*, *down*, *left*). However, the agent will
    be restricted to some actions based on the following constraints:'
  prefs: []
  type: TYPE_NORMAL
- en: The states across the edges shall be restricted to actions which point only
    toward states in the grid. For example, an agent in *S1* is restricted to the
    *right* or *down* action.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some state transitions have barriers, marked in red. For example, the agent
    cannot go *down* from *S2* to *S3*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each state is also assigned to a reward. The objective of the agent is to reach
    the destination with minimum moves, thereby achieving the maximum reward. Except
    state *S15* with a reward value of 100, all the remaining states have a reward
    value of *-1*.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we will use the `MDPtoolbox` package in R.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section will show you how to set up RL models using `MDPtoolbox` in R:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install and load the required package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the transition probabilities for action. Here, each row denotes `from
    state` and each column denotes `to state`. As we have 16 states, the transition
    probability matrix of each action shall be a 16 x 16 matrix, with each row adding
    upto 1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a list of transition probability matrices:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a reward matrix of dimensions: 16 (number of states) x 4 (number of
    actions):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Test whether the defined `TPMs` and `Rewards` satisfy a well-defined MDP. If
    it returns an empty string, then the MDP is valid:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Performing model-based learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the name suggests, the learning is augmented using a predefined model. Here,
    the model is represented in the form of transition probabilities and the key objective
    is to determine the optimal policy and value functions using these predefined
    model attributes (that is, `TPMs`). The policy is defined as a learning mechanism
    of an agent, traversing across multiple states. In other words, identifying the
    best action of an agent in a given state, to traverse to a next state, is termed
    a policy.
  prefs: []
  type: TYPE_NORMAL
- en: The objective of the policy is to maximize the cumulative reward of transitioning
    from the start state to the destination state, defined as follows, where *P(s)*
    is the cumulative policy *P* from a start state *s*, and *R* is the reward of
    transitioning from state *st* to state *s[t+1]* by performing an action at.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00028.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'The value function is of two types: the state-value function and the state-action
    value function. In the state-value function, for a given policy, it is defined
    as an expected reward to be in a particular state (including start state), whereas
    in the state-action value function, for a given policy, it is defined as an expected
    reward to be in a particular state (including the start state) and undertake a
    particular action.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, a policy is said to be optimal provided it returns the maximum expected
    cumulative reward, and its corresponding states are termed optimal state-value
    functions or its corresponding states and actions are termed optimal state-action
    value functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In model-based learning, the following iterative steps are performed in order
    to obtain an optimum policy, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00044.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Iterative steps to find an optimum policy
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we shall evaluate the policy using the state-value function.
    In each iteration, the policies are dynamically evaluated using the Bellman equation,
    as follows, where *V[i]* denotes the value at iteration *i*, *P* denotes an arbitrary
    policy of a given state *s* and action *a*, *T* denotes the transition probability
    from state *s* to state *s'* due to an action *a*, *R* denotes the reward at state
    *s'* while traversing from the state *s* post an action *a*, and ![](img/00057.gif)
    denotes a discount factor in the range of (0,1). The discount factor ensures higher
    importance to starting learning steps than later.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00031.gif)'
  prefs: []
  type: TYPE_IMG
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section shows you how to set up model-based RL:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the policy iteration using the state-action value function with the discount
    factor *Î¥ = 0.9*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the best (optimum) policy P* as shown in the following figure. The arrows
    marked in green show the direction of traversing *S1* to *S15*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00034.gif)'
  prefs: []
  type: TYPE_IMG
- en: Optimum policy using model-based iteration with an optimum path from *S1* to
    *S15*
  prefs: []
  type: TYPE_NORMAL
- en: 'Get the optimum value function V* for each state and plot them as shown in
    the following figure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00150.gif)'
  prefs: []
  type: TYPE_IMG
- en: Value functions of the optimal policy
  prefs: []
  type: TYPE_NORMAL
- en: Performing model-free learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unlike model-based learning, where dynamics of transitions are explicitly provided
    (as transition probabilities from one state to another state), in model-free learning,
    the transitions are supposed to be deduced and learned directly from the interaction
    between states (using actions) rather explicitly provided. Widely used frameworks
    of mode-free learning are **Monte Carlo** methods and the **Q-learning** technique.
    The former is simple to implement but convergence takes time, whereas the latter
    is complex to implement but is efficient in convergence due to off-policy learning.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will implement the Q-learning algorithm in R. The simultaneous
    exploration of the surrounding environment and exploitation of existing knowledge
    is termed off-policy convergence. For example, an agent in a particular state
    first explores all the possible actions of transitioning into next states and
    observes the corresponding rewards, and then exploits current knowledge to update
    the existing state-action value using the action generating the maximum possible
    reward.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Q learning returns a 2D Q-table of the size of the number of states x the
    number of actions. The values in the Q-table are updated based on the following
    formula, where *Q* denotes the value of state *s* and action *a*, *r''* denotes
    the reward of the next state for a selected action *a*, *Î¥* denotes the discount
    factor, and *Î±* denotes the learning rate:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00071.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'The framework for Q-learning is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00078.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Framework of Q-learning
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The section provide steps for how to set up Q-learning:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define 16 states:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Define four actions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the `transitionStateAction` function, which can simulate the transitions
    from one state *s* to another state *s''* using an action *a*. The function takes
    in the current state *s* and selected action *a*, and it returns the next state
    *s''* and corresponding reward *r''*. In case of constrained action, the next
    state returned is the current state *s* and the existing reward *r*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a function to perform Q-learning using `n` iterations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Set learning parameters such as `epsilon` and `learning_rate`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the Q-table after 500k iterations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Get the best (optimum) policy P*, as shown in the following figure. The arrows
    marked in green shows the direction of traversing *S1* to *S15:*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00082.gif)'
  prefs: []
  type: TYPE_IMG
- en: Optimum policy using model-free iteration with an optimum path from *S1* to
    *S15*
  prefs: []
  type: TYPE_NORMAL
