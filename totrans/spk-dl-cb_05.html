<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Predicting Fire Department Calls with Spark ML</h1>
                </header>
            
            <article>
                
<p>In this chapter, the following recipes will be covered:</p>
<ul>
<li>Downloading the San Francisco fire department calls dataset</li>
<li>Identifying the target variable of the logistic regression model</li>
<li>Preparing feature variables for the logistic regression model</li>
<li>Applying the logistic regression model</li>
<li>Evaluating the accuracy of the logistic regression model</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p><span>Classification models are a popular way to predict a defined categorical outcome. We use outputs from classification models all the time. Anytime we go to see a movie in a theatre, we are interested to know whether the film is considered correct? One of the most popular classification models in the data science community is a logistic regression. The logistic regression model produces a response that is activated by a sigmoid function. The sigmoid function uses the inputs from the model and produces an output that is between 0 and 1. That output is usually in a form of a probability score. Many deep learning models are also used for classification purposes. It is common to find logistic regression models performed in conjunction with deep learning models to help establish a baseline in which deep learning models are measured against. The sigmoid activation function is one of many activation functions that are also used in deep neural networks within deep learning to produce a probability output. We will utilize the built-in machine learning libraries within Spark to build a logistic regression model that will predict whether an incoming call to the San Francisco Fire department is actually related to a fire, rather than another incident.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Downloading the San Francisco fire department calls dataset</h1>
                </header>
            
            <article>
                
<p>The City of San Francisco does a great job of collecting fire department calls for services across their area. As it states on their website, e<span>ach record includes the call number, incident number, address, unit identifier, call type, and disposition. The official website containing San Francisco fire department call data can be found at the following link:</span></p>
<p><a href="https://data.sfgov.org/Public-Safety/Fire-Department-Calls-for-Service/nuek-vuh3">https://data.sfgov.org/Public-Safety/Fire-Department-Calls-for-Service/nuek-vuh3</a></p>
<p>There is some general information regarding the dataset with regards to the number of columns and rows, seen in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1506 image-border" src="assets/fc1781db-55cb-4a5e-b3a3-b822a672bbf1.png" style="width:42.08em;height:37.17em;"/></div>
<p>This current dataset, updated on 3/26/2018, has roughly <span class="packt_screen">4.61 M</span> rows and <span class="packt_screen">34</span> columns. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>The dataset is available in a <kbd>.csv</kbd> file and can be downloaded locally on to your machine, where it can then be imported into Spark.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>This section will walk through the steps to download and import the <kbd>.csv</kbd> file to our Jupyter notebook.</p>
<ol>
<li>Download the dataset from the website by selecting <span class="packt_screen">Export</span> and then <span class="packt_screen">CSV</span>, as seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1465 image-border" src="assets/b1f89e2e-1edf-4936-bb0e-090ddf5bdb0d.png" style="width:35.08em;height:26.25em;"/></div>
<ol start="2">
<li>If not already the case, name the downloaded dataset <kbd>Fire_Department_Calls_for_Service.csv</kbd></li>
</ol>
<ol start="3">
<li>Save the dataset to any local directory, although ideally it should be saved to the same folder that contains the Spark notebook that will be used in this chapter, as seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1466 image-border" src="assets/f4654852-1dd6-4f0f-8b99-9d28a57a58cc.png" style="width:37.67em;height:23.50em;"/></div>
<ol start="4">
<li>Once the dataset has been saved to the same directory as the notebook, execute the following <kbd>pyspark</kbd> script to import the dataset into Spark and create a dataframe called <kbd>df</kbd>:</li>
</ol>
<pre style="padding-left: 60px">from pyspark.sql import SparkSession<br/>spark = SparkSession.builder \<br/>                    .master("local") \<br/>                    .appName("Predicting Fire Dept Calls") \<br/>                    .config("spark.executor.memory", "6gb") \<br/>                    .getOrCreate()<br/><br/>df = spark.read.format('com.databricks.spark.csv')\<br/>                    .options(header='true', inferschema='true')\<br/>                    .load('Fire_Department_Calls_for_Service.csv')<br/>df.show(2)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>The dataset is saved to the same directory that houses the Jupyter notebook for ease of import into the Spark session.</p>
<ol>
<li>A local <kbd>pyspark</kbd> session is initialized by importing <kbd>SparkSession</kbd> from <kbd>pyspark.sql</kbd>.</li>
<li>A dataframe, <kbd>df</kbd>, is created by reading in the CSV file with the options <kbd>header = 'true'</kbd> and <kbd>inferschema = 'true'</kbd>.</li>
<li>Finally, it is always ideal to run a script to show the data that has been imported into Spark through the dataframe to confirm that the data has made its way through. The outcome of the script, showing the first two rows of the dataset from the San Francisco fire department calls, can be seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1467 image-border" src="assets/bf42a5ae-52b0-4108-9588-afb2deacfb74.png" style="width:143.42em;height:81.83em;"/></div>
<div class="packt_infobox">Please note that as we read the file into spark, we are using <kbd>.load()</kbd> to pull the <kbd>.csv</kbd> file into the Jupyter notebook. This is fine for our purposes as we are using a local cluster, but would not work if we were leveraging a cluster from Hadoop. </div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>The dataset is accompanied by a data dictionary that defines the headers for each of the 34 columns. This data dictionary can be accessed from the same website through the following link:</p>
<p><a href="https://data.sfgov.org/api/views/nuek-vuh3/files/ddb7f3a9-0160-4f07-bb1e-2af744909294?download=true&amp;filename=FIR-0002_DataDictionary_fire-calls-for-service.xlsx">https://data.sfgov.org/api/views/nuek-vuh3/files/ddb7f3a9-0160-4f07-bb1e-2af744909294?download=true&amp;filename=FIR-0002_DataDictionary_fire-calls-for-service.xlsx</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>The San Francisco government website allows for online visualization of the data, which can be used to do some quick data profiling. The visualization application can be accessed on the website by selecting the <span class="packt_screen">Visualize</span> dropdown, as seen in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1468 image-border" src="assets/5c564a36-8237-41fc-901c-1d15b50f97c3.png" style="width:162.50em;height:54.75em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Identifying the target variable of the logistic regression model</h1>
                </header>
            
            <article>
                
<p>A logistic regression model operates as a classification algorithm aiming to predict a binary outcome. In this section, we will specify the best column within the dataset to predict whether an incoming call to the operator is related to fire or non-fire incidents.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>We will visualize many of the data points in this section, which will require the following:</p>
<ol>
<li>Ensuring that <kbd>matplotlib</kbd> is installed by executing <kbd>pip install matplotlib</kbd> at the command line.</li>
<li>Running <kbd>import matplotlib.pyplot as plt</kbd> as well as ensuring graphs are viewed within cells by running <kbd>%matplotlib inline</kbd>.</li>
</ol>
<p>Additionally, there will be some manipulation of functions within <kbd>pyspark.sql</kbd> that requires <kbd>importing functions as F</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>This section will walk through visualizing the data from the San Francisco Fire Department.</p>
<ol>
<li>Execute the following script to get a cursory identification of the unique values in the <kbd>Call Type Group</kbd> column:</li>
</ol>
<pre style="padding-left: 60px">df.select('Call Type Group').distinct().show()</pre>
<ol start="2">
<li>There are five main categories:
<ol>
<li><kbd>Alarm</kbd>.</li>
<li><kbd>Potentially Life-threatening</kbd>.</li>
<li><kbd>Non Life-threatening</kbd>.</li>
<li><kbd>Fire</kbd>.</li>
<li><kbd>null</kbd>.</li>
</ol>
</li>
<li>Unfortunately, one of those categories is <kbd>null</kbd> values. It would be useful to get a row count of each unique value to identify how many null values there are in the dataset. Execute the following script to generate a row count of each unique value for the column <kbd>Call Type Group</kbd>:</li>
</ol>
<pre style="padding-left: 60px">df.groupBy('Call Type Group').count().show()</pre>
<ol start="4">
<li>Unfortunately, there are over 2.8 M rows of data that do not have a <kbd>Call Type Group</kbd> associated with them. That is over 60 percent of the available rows of 4.6 M. Execute the following script to view the imbalance of null values in a bar chart:</li>
</ol>
<pre style="padding-left: 60px">df2 = df.groupBy('Call Type Group').count()<br/>graphDF = df2.toPandas()<br/>graphDF = graphDF.sort_values('count', ascending=False)<br/><br/>import matplotlib.pyplot as plt<br/>%matplotlib inline<br/><br/>graphDF.plot(x='Call Type Group', y = 'count', kind='bar')<br/>plt.title('Call Type Group by Count')<br/>plt.show()</pre>
<ol start="5">
<li>Another indicator may need to be chosen to determine a target variable. Instead, we can profile <kbd>Call Type</kbd> to identify calls associated with fire versus all other calls. Execute the following script to profile <kbd>Call Type</kbd>:</li>
</ol>
<pre style="padding-left: 60px">df.groupBy('Call Type').count().orderBy('count', ascending=False).show(100)</pre>
<ol start="6">
<li>There do not appear to be any <kbd>null</kbd> values, as there were with <kbd>Call Type Group</kbd>. There are 32 unique categories for <kbd>Call Type</kbd>; therefore, it will be used as the target variable for fire incidents. Execute the following script to tag the columns containing <kbd>Fire</kbd> in<kbd>Call Type</kbd>:</li>
</ol>
<pre style="padding-left: 60px">from pyspark.sql import functions as F<br/>fireIndicator = df.select(df["Call Type"],F.when(df["Call Type"].like("%Fire%"),1)\<br/>                          .otherwise(0).alias('Fire Indicator'))<br/>fireIndicator.show()</pre>
<ol start="7">
<li>Execute the following script to retrieve the distinct counts of <kbd>Fire Indicator</kbd>:</li>
</ol>
<pre style="padding-left: 60px">fireIndicator.groupBy('Fire Indicator').count().show()</pre>
<ol start="8">
<li>Execute the following script to add the <kbd>Fire Indicator</kbd> column to the original dataframe, <kbd>df</kbd>:</li>
</ol>
<pre style="padding-left: 60px">df = df.withColumn("fireIndicator",\ <br/>F.when(df["Call Type"].like("%Fire%"),1).otherwise(0))</pre>
<ol start="9">
<li>
<p>Finally, add the <kbd>fireIndicator</kbd><span> </span>column has to the dataframe,<span> </span><kbd>df</kbd>, and confirm by executing the following script:</p>
</li>
</ol>
<pre style="padding-left: 60px">df.printSchema()</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>One of the key steps to building a successful logistic regression model is establishing a binary target variable that will be used as the prediction outcome. This section walks through the logic behind selecting our target variable:</p>
<ol>
<li>Data profiling of potential target columns is performed by identifying the unique column values of <kbd>Call Type Group</kbd>. We can view the unique values of the <kbd>Call Type Group</kbd> column, as seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1469 image-border" src="assets/d3ade658-22c0-4fa4-9a1d-ebae6ad1ecdc.png" style="width:34.00em;height:13.75em;"/></div>
<ol start="2">
<li>The goal is to identify whether there are any missing values within the <kbd>Call Type Group</kbd> column and what can be done with those missing values. Sometimes, the missing values in the columns can just be dropped, and other times they are manipulated to populate values.</li>
</ol>
<ol start="3">
<li>The following screenshot shows how many null values are present:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1470 image-border" src="assets/bca41dd4-acd7-4c95-971a-543d5a0dcbed.png" style="width:23.92em;height:10.00em;"/></div>
<ol start="4">
<li>Additionally, we can also plot how many <kbd>null</kbd> values are present to get a better visual sense of the abundance of values, as seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1507 image-border" src="assets/8834a412-6660-40fe-92be-abf4bcf63aa6.png" style="width:30.08em;height:30.67em;"/></div>
<ol start="5">
<li>Since there are over 2.8 M rows that are missing from <kbd>Call Type Group</kbd>, as seen in the <kbd>df.groupBy</kbd> script as well as the bar chart, it doesn't make sense to drop all of those values, as that is over 60 percent of the total row count from the dataset. Therefore, another column will need to be chosen as the target indicator.</li>
<li>While profiling the <kbd>Call Type</kbd> column, we find that there aren't any <span class="packt_screen">null</span> rows in the 32 unique possible values. This makes <kbd>Call Type</kbd> a better candidate for the target variable for the logistic regression model. The following is a screenshot of the <kbd>Call Type</kbd> column profiled:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/1977a0ab-f4a3-4174-9ea1-ff28ba2df5dc.png"/></div>
<ol start="7">
<li>Since logistic regression works best when there is a binary outcome, a new column is created using the <kbd>withColumn()</kbd> operator in the <kbd>df</kbd> dataframe to capture an indicator (0 or 1) as to whether a call is affiliated with a fire-related incident or a non-fire-related incident. The new column is called <kbd>fireIndicator</kbd> and can be seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1472 image-border" src="assets/580f9ef9-2299-4aae-8164-c174ba2c31db.png" style="width:127.58em;height:77.33em;"/></div>
<ol start="8">
<li>We can identify how prevalent fire calls are compared to the rest of the calls by doing a <kbd>groupBy().count()</kbd>, as seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1473 image-border" src="assets/d0b58340-59f9-418d-8d1b-cc62349a62e5.png" style="width:58.17em;height:13.92em;"/></div>
<ol start="9">
<li>It is best practice to confirm that the new column has been attached to the existing dataframe by executing the <kbd>printSchema()</kbd> script of the newly modified dataframe. The output of the new schema can be seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b34b4286-dfcf-4827-a2a3-4ce95aeea7ff.png" style="width:45.33em;height:41.58em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p><span>There were a couple of column manipulations done with the <kbd>pyspark.sql</kbd> module in this section. The </span><kbd>withColumn()</kbd><span> operator returns a new dataframe or modifies an existing dataframe by adding a new column or modifies an existing column of the same name. This is not to be confused with the <kbd>withColumnRenamed()</kbd> operator, which also returns a new dataframe, but by modifying the name of an existing column to a new column. Finally, we needed to perform some logical operations to convert values associated with</span> <kbd>Fire</kbd> <span>to 0 and without</span> <kbd>Fire</kbd> <span>to 1. This required using the <kbd>pyspark.sql.functions</kbd> module and incorporating the <kbd>where</kbd> function as an equivalent to a case statement used in SQL. The function created a case statement equation using the following syntax: </span></p>
<pre>CASE WHEN Call Type LIKE %Fire% THEN 1 ELSE 0 END</pre>
<p>The outcome of the new dataset for both columns, <kbd>Call Type</kbd> and <kbd>fireIndicator</kbd>, appear as the following:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7f49b4cf-6efd-4c04-ab4f-df0623e76d72.png" style="width:36.92em;height:30.08em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>In order to learn more about the <kbd>pyspark.sql</kbd> module available within Spark, visit the following website:</p>
<p><a href="http://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html">http://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing feature variables for the logistic regression model</h1>
                </header>
            
            <article>
                
<p>In the previous section, we identified our target variable that will be used as our predictor for fire calls in our logistic regression model. This section will focus on identifying all of the features that will best help the model identify what the target should be. This is known as <strong>feature selection</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>This section will require importing <kbd>StringIndexer</kbd> from <kbd>pyspark.ml.feature</kbd>. In order to ensure proper feature selection, we will need to map string columns to columns of indices. This will help generate distinct numeric values for categorical variables that will provide ease of computation for the machine learning model to ingest the independent variables used to predict the target outcome.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>This section will walk through the steps to prepare the feature variables for our model.</p>
<ol>
<li>Execute the following script to update the dataframe, <kbd>df</kbd>, by only selecting the fields that are independent of any fire indicators:</li>
</ol>
<pre style="padding-left: 60px">df = df.select('fireIndicator', <br/>    'Zipcode of Incident',<br/>    'Battalion',<br/>    'Station Area',<br/>    'Box', <br/>    'Number of Alarms',<br/>    'Unit sequence in call dispatch',<br/>    'Neighborhooods - Analysis Boundaries',<br/>    'Fire Prevention District',<br/>    'Supervisor District')<br/>df.show(5)</pre>
<ol start="2">
<li>The next step is to identify any null values within the dataframe and remove them if they exist. Execute the following script to identify the row count with any null values:</li>
</ol>
<pre style="padding-left: 60px">print('Total Rows')<br/>df.count()<br/>print('Rows without Null values')<br/>df.dropna().count()<br/>print('Row with Null Values')<br/>df.count()-df.dropna().count()</pre>
<ol start="3">
<li>There are <span class="packt_screen">16,551</span> rows with missing values. Execute the following script to update the dataframe to remove all rows with null values:</li>
</ol>
<pre style="padding-left: 60px">df = df.dropna()</pre>
<ol start="4">
<li>Execute the following script to retrieve the updated target count of <kbd>fireIndicator</kbd>:</li>
</ol>
<pre style="padding-left: 60px">df.groupBy('fireIndicator').count().orderBy('count', ascending = False).show()</pre>
<ol start="5">
<li>Import the <kbd>StringIndexer</kbd> <span>class from</span> <kbd>pyspark.ml.feature</kbd><span> to assign numeric values to each categorical variable for the features, as seen in the following script:</span></li>
</ol>
<pre style="padding-left: 60px">from pyspark.ml.feature import StringIndexer</pre>
<ol start="6">
<li>Create a Python list for all the feature variables that will be used in the model using the following script:</li>
</ol>
<pre style="padding-left: 60px">column_names = df.columns[1:]</pre>
<ol start="7">
<li>Execute the following script to specify the output column format, <kbd>outputcol</kbd>, that will be <kbd>stringIndexed</kbd> from the list of features from the input column, <kbd>inputcol</kbd>:</li>
</ol>
<pre style="padding-left: 60px">categoricalColumns = column_names<br/>indexers = []<br/>for categoricalCol in categoricalColumns:<br/>    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol+"_Index")<br/>    indexers += [stringIndexer]</pre>
<ol start="8">
<li>Execute the following script to create a <kbd>model</kbd> that will be used to <span><kbd>fit</kbd> the input columns and produce the newly defined output columns to the existing dataframe, <kbd>df</kbd>:</span></li>
</ol>
<pre style="padding-left: 60px">models = []<br/>for model in indexers:<br/>    indexer_model = model.fit(df)<br/>    models+=[indexer_model]<br/> <br/>for i in models:<br/>    df = i.transform(df)</pre>
<ol start="9">
<li>Execute the following script to define a final selection of the features in the dataframe, <kbd>df</kbd>, that will be used for the model:</li>
</ol>
<pre style="padding-left: 60px">df = df.select(<br/>          'fireIndicator',<br/>          'Zipcode of Incident_Index',<br/>          'Battalion_Index',<br/>          'Station Area_Index',<br/>          'Box_Index',<br/>          'Number of Alarms_Index',<br/>          'Unit sequence in call dispatch_Index',<br/>          'Neighborhooods - Analysis Boundaries_Index',<br/>          'Fire Prevention District_Index',<br/>          'Supervisor District_Index')</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p><span>This section will explain the logic behind the steps in preparing the feature variables for our model.</span></p>
<ol>
<li>Only the indicators in the dataframe that are truly independent of an indication of fire are selected to contribute to the logistic regression model that will predict the outcome. The reason this is performed is to remove any potential bias in the dataset that may already reveal the outcome of the prediction. This minimizes human interaction with the final outcome. The output of the updated dataframe can be seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/9f0de7d8-271d-4563-960e-425ba8cbba19.png" style="width:64.17em;height:32.67em;"/></div>
<div class="packt_infobox">Please note that the column <kbd>Neighborhooods - Analysis of Boundaries</kbd> is originally misspelled from the data we extract. We will continue to use the misspelling for the rest of the chapter for continuity purposes. However, the column name can be renamed using the <kbd>withColumnRenamed()</kbd> function in Spark.</div>
<ol start="2">
<li>The final selection of columns are chosen as the following:
<ul>
<li><kbd>fireIndicator</kbd></li>
<li><kbd>Zipcode of Incident</kbd></li>
<li><kbd>Battalion</kbd></li>
<li><kbd>Station Area</kbd></li>
<li><kbd>Box</kbd></li>
<li><kbd>Number of Alarms</kbd></li>
<li><kbd>Unit sequence in call dispatch</kbd></li>
<li><kbd>Neighborhooods - Analysis Boundaries</kbd></li>
<li><kbd>Fire Prevention District</kbd></li>
<li><kbd>Supervisor District</kbd></li>
</ul>
</li>
<li>These columns are selected to avoid data leakage in our modeling. Data leakage is common in modeling and can lead to invalid predictive models because it can incorporate features that are directly a result of the outcome we are trying to predict. Ideally, we wish to incorporate features that are truly independent of the outcome. There are several columns that appeared to be leaky and, hence, are removed from our dataframe and model.</li>
<li>All rows with missing or null values are identified and removed in order to get the very best performance out of the model without overstating or understating key features. An inventory of the rows with missing values can be calculated and shown to be <span class="packt_screen">16,551,</span> as seen in the following script:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/9df0dab7-e80a-4d5c-84fc-67811558ce32.png" style="width:27.58em;height:22.08em;"/></div>
<ol start="5">
<li>We can get a look at the frequency of calls that are fire-related versus those that are not, as seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/32117958-6e01-4ba3-9c54-bf04129c5181.png" style="width:58.50em;height:10.50em;"/></div>
<ol start="6">
<li><kbd>StringIndexer</kbd> is imported to help convert several of the categorical or string features into numerical values for ease of computation within the logistic regression model. The input of the features needs to be in a vector or array format, which is ideal for numeric values. A list of all the features that will be used in the model can be seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/cb62409e-94dc-4357-a5ac-e7127ff20596.png" style="width:34.58em;height:16.25em;"/></div>
<ol start="7">
<li><span>An indexer is built for each of the categorical variables specifying the input (</span><kbd>inputCol</kbd><span>) and output (</span><kbd>outputCol</kbd><span>) columns that will be used in the model. </span><span>Each column in the dataframe is adjusted or transformed to rebuild a new output with the updated indexing, ranging from 0 to the maximum value of the unique count of that specific column. The new column is appended with <kbd>_Index</kbd> at the end. While the updated column is created, the original column is still available in the dataframe, as seen in the following screenshot:</span></li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1474 image-border" src="assets/df67a60c-19da-4691-9903-e81d215ad749.png" style="width:42.25em;height:39.08em;"/></div>
<ol start="8">
<li>We can look at one of the newly created columns and compare it with the original to see how the strings have been converted to numeric categories. The following screenshot shows how <kbd>Neighborhooods - Analysis Boundaries</kbd><span> </span><span>compares with</span> <kbd><span>Neighborhooods - Analysis Boundaries_Index</span></kbd><span>:</span></li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/5b9432fe-0906-4632-808a-fd882682c1c5.png"/></div>
<ol start="9">
<li>The dataframe is then trimmed down to incorporate only the numerical values and remove the original categorical variables that were transformed. The non-numerical values no longer serve a purpose from a modeling perspective and are dropped from the dataframe.</li>
<li>The new columns are printed out to confirm that each value type of the dataframe is either double precision or integer, as seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1475 image-border" src="assets/61837070-2d38-48e9-92a3-e09a8984a90d.png" style="width:41.67em;height:24.75em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>A final look at the newly modified dataframe will reveal only numerical values, as seen in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/9c63a287-61af-4a3f-a311-d051054791e1.png" style="width:39.83em;height:16.42em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>In order to learn more about <kbd>StringIndexer</kbd>, visit the following website: <a href="https://spark.apache.org/docs/2.2.0/ml-features.html#stringindexer">https://spark.apache.org/docs/2.2.0/ml-features.html#stringindexer</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Applying the logistic regression model</h1>
                </header>
            
            <article>
                
<p>The stage is now set to apply the model to the dataframe.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>This section will focus on applying a very common classification model called <strong>logistic regression</strong>, which will involve importing some of the following from Spark:</p>
<pre>from pyspark.ml.feature import VectorAssembler<br/>from pyspark.ml.evaluation import BinaryClassificationEvaluator<br/>from pyspark.ml.classification import LogisticRegression</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p><span>This section will walk through the steps of applying our model and evaluating the results.</span></p>
<ol>
<li>Execute the following script to lump all of the feature variables in the dataframe in a list called <kbd>features</kbd>:</li>
</ol>
<pre style="padding-left: 60px">features = df.columns[1:]</pre>
<ol start="2">
<li>Execute the following to import <kbd>VectorAssembler</kbd> and configure the fields that will be assigned to the feature vector by assigning the <kbd>inputCols</kbd> and <kbd>outputCol</kbd>:</li>
</ol>
<pre style="padding-left: 60px">from pyspark.ml.feature import VectorAssembler<br/>feature_vectors = VectorAssembler(<br/>    inputCols = features,<br/>    outputCol = "features")</pre>
<ol start="3">
<li>Execute the following script to apply <kbd>VectorAssembler</kbd> to the dataframe with the <kbd>transform</kbd> function:</li>
</ol>
<pre style="padding-left: 60px">df = feature_vectors.transform(df)</pre>
<ol start="4">
<li>Modify the dataframe to remove all of the columns except for <kbd>fireIndicator</kbd> and <kbd>features</kbd>, as seen in the following script:</li>
</ol>
<pre style="padding-left: 60px">df = df.drop( 'Zipcode of Incident_Index',<br/>              'Battalion_Index',<br/>              'Station Area_Index',<br/>              'Box_Index',<br/>              'Number of Alarms_Index',<br/>              'Unit sequence in call dispatch_Index',<br/>              'Neighborhooods - Analysis Boundaries_Index',<br/>              'Fire Prevention District_Index',<br/>              'Supervisor District_Index')</pre>
<ol start="5">
<li>Modify the dataframe to rename <kbd>fireIndicator</kbd> to <kbd>label</kbd>, as seen in the following script:</li>
</ol>
<pre style="padding-left: 60px">df = df.withColumnRenamed('fireIndicator', 'label')</pre>
<ol start="6">
<li>Split the entire dataframe, <kbd>df</kbd>, into training and test sets in a 75:25 ratio, with a random seed set as <kbd>12345</kbd>, as seen in the following script:</li>
</ol>
<pre style="padding-left: 60px">(trainDF, testDF) = df.randomSplit([0.75, 0.25], seed = 12345)</pre>
<ol start="7">
<li>Import the <kbd>LogisticRegression</kbd> library from <kbd>pyspark.ml.classification</kbd> and configure to incorporate the <kbd>label</kbd> and <kbd>features</kbd> from the dataframe, and then fit on the training dataset, <kbd>trainDF</kbd>, as seen in the following script:</li>
</ol>
<pre style="padding-left: 60px">from pyspark.ml.classification import LogisticRegression<br/>logreg = LogisticRegression(labelCol="label", featuresCol="features", maxIter=10)<br/>LogisticRegressionModel = logreg.fit(trainDF)</pre>
<ol start="8">
<li>Transform the test dataframe, <kbd>testDF</kbd>, to apply the logistic regression model. The new dataframe with the scores from the prediction is called <kbd>df_predicted</kbd>, as seen in the following script:</li>
</ol>
<pre style="padding-left: 60px">df_predicted = LogisticRegressionModel.transform(testDF)</pre>
<ol start="4"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p><span>This section explains the logic behind the steps in applying our model and evaluating the results.</span></p>
<ol>
<li>Classification models work best when all of the features are combined in a single vector for training purposes. Therefore, we begin the vectorization process by collecting all of the features into a single list called <kbd>features</kbd>. Since our label is the first column of the dataframe, we exclude it and pull in every column after as a feature column or feature variable.</li>
<li>The vectorization process continues by converting all of the variables from the <kbd>features</kbd> list into a single vector output to a column called <kbd>features</kbd>. This process requires importing <kbd>VectorAssembler</kbd> from <kbd>pyspark.ml.feature</kbd>.</li>
<li>Applying <kbd>VectorAssembler</kbd> transforms the dataframe by creating a newly added column called <kbd>features</kbd>, as seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/dfe3ef28-1df5-4b4a-b749-2b9483542ed9.png" style="width:36.42em;height:25.33em;"/></div>
<ol start="4">
<li>At this point, the only columns that are necessary for us to use in the model are the label column, <kbd>fireIndicator</kbd>, and the <kbd>features</kbd> column. All of the other columns can be dropped from the dataframe as they will no longer be needed for modeling purposes.</li>
<li>Additionally, to help with the logistic regression model, we will change the column called <kbd>fireIndicator</kbd> to <kbd>label</kbd>. The output of the <kbd>df.show()</kbd> script can be seen in the following screenshot with the newly renamed columns:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/313f52f6-30e8-4e7a-a968-ff5152d5c2a9.png" style="width:40.33em;height:32.83em;"/></div>
<ol start="6">
<li>In order to minimize overfitting the model, the dataframe will be split into a testing and training dataset to fit the model on the training dataset, <kbd>trainDF</kbd>, and test it on the testing dataset, <kbd>testDF</kbd>. A random seed of <kbd>12345</kbd> is set to keep the randomness consistent each time the cell is executed. We can identify the row counts for the data split, as seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/2d20098a-229b-4a3b-9e7e-d3d6d4efa513.png" style="width:41.67em;height:9.00em;"/></div>
<ol start="7">
<li>A logistic regression model, <kbd>LogisticRegression</kbd>, is then imported from <kbd>pyspark.ml.classification</kbd> and configured to input the appropriate column names from the dataframe associated with the features and the label. Additionally, the logistic regression model is assigned to a variable called <kbd>logreg</kbd> that is then fit to train our data set, <kbd>trainDF</kbd>.</li>
<li>A new dataframe, <kbd>predicted_df</kbd>, is created based on the transformation of the test dataframe, <kbd>testDF</kbd>, once the logistic regression model is scored on it. The model creates three additional columns for <kbd>predicted_df</kbd>, based on the scoring. The three additional columns are <kbd>rawPrediction</kbd>, <kbd>probability</kbd>, and <kbd>prediction</kbd>, as seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1476 image-border" src="assets/889efec5-2794-45f2-af1d-f0fe21429c79.png" style="width:57.08em;height:14.92em;"/></div>
<ol start="9">
<li>Finally, the new columns in <kbd>df_predicted</kbd> can be profiled, as seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/8181ca5d-64f1-45b0-9b82-d487ad69dc1d.png" style="width:56.67em;height:12.08em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>One important thing to keep in mind because it may initially come off as being counter-intuitive is that our <span class="packt_screen">probability</span> threshold is set at 50 percent in our dataframe. Any call with a <span class="packt_screen">probability</span> of 0.500 and above is given a <span class="packt_screen">prediction</span> of <span class="packt_screen">0.0</span> and any call with a <span class="packt_screen">probability</span> of less than 0.500 is given a <span class="packt_screen">prediction</span> of <span class="packt_screen">1.0</span>. This was set during the pipeline development process and as long as we are aware of what the threshold is along with how the prediction is allocated, we are in good shape.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>To learn more about <kbd>VectorAssembler</kbd>, visit the following website:</p>
<p><a href="https://spark.apache.org/docs/latest/ml-features.html#vectorassembler">https://spark.apache.org/docs/latest/ml-features.html#vectorassembler</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating the accuracy of the logistic regression model</h1>
                </header>
            
            <article>
                
<p><span>We are now ready to evaluate the performance of predicting whether a call was correctly classified as a fire incident.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>We will perform the model analysis which will require importing the following:</p>
<ul>
<li><kbd>from sklearn import metrics</kbd></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>This section walks through the steps to evaluate the model performance.</p>
<ol start="1">
<li>Create a confusion matrix using the<span> </span><kbd>.crosstab()</kbd><span> </span>function, as seen in the following script:</li>
</ol>
<pre style="padding-left: 60px">df_predicted.crosstab('label', 'prediction').show()</pre>
<ol start="2">
<li>
<p>Import <kbd>metrics</kbd><span> </span>from<span> </span><kbd>sklearn</kbd><span> </span>to help measure accuracy using the following script:</p>
</li>
</ol>
<pre style="padding-left: 60px">from sklearn import metrics</pre>
<ol start="3">
<li>Create two variables for the<span> </span><kbd>actual</kbd><span> </span>and<span> </span><kbd>predicted</kbd><span> </span>columns from the dataframe that will be used to measure accuracy, using the following script:</li>
</ol>
<pre style="padding-left: 60px">actual = df_predicted.select('label').toPandas()<br/>predicted = df_predicted.select('prediction').toPandas()</pre>
<ol start="4">
<li>Compute the accuracy prediction score using the following script:</li>
</ol>
<pre style="padding-left: 60px">metrics.accuracy_score(actual, predicted)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>This section explains how the model performance is evaluated.</p>
<ol start="1">
<li>In order to compute the accuracy of our model, it is important to be able to identify how accurate our predictions were. Often, this is best visualized using a confusion matrix cross table that shows correct and incorrect prediction scores. We create a confusion matrix using the<span> </span><kbd>crosstab()</kbd><span> </span>function off the<span> </span><kbd>df_predicted</kbd><span> </span>dataframe that shows us we have 964,980 true negative predictions for labels that are 0 and we have 48,034 true positive predictions for labels that are 1, as seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/3b89bb6d-d661-4cec-92ba-d02a0849e8e4.png" style="width:42.92em;height:9.67em;"/></div>
<ol start="2">
<li>We know from earlier in this section that there are a total of 1,145,589 rows from the<span> </span><kbd>testDF</kbd><span> </span>dataframe; therefore, we can calculate the accuracy of the model using the following formula:<span> </span><em>(TP + TN) / Total</em>. The accuracy would then be 88.4 percent.</li>
<li>It is important to note that not all false scores are created equal. For example, it is more detrimental to classify a call as not relating to fire and ultimately have it be related to fire than vice-versa from a fire safety perspective. This is referred to as a false negative. There is a metric that accounts for a <strong>false negative</strong> (<strong>FN</strong>), known as <strong>recall</strong>.</li>
<li>While we can work out the accuracy manually, as seen in the last step, it is ideal to have the accuracy automatically calculated. This can be easily performed by importing <kbd>sklearn.metrics</kbd>, which is a module that is commonly used for scoring and model evaluation.</li>
<li><kbd>sklearn.metrics</kbd> takes in two parameters, the actual results that we have for labels and the predicted values we derived from the logistic regression model. Therefore, two variables are created,<span> </span><kbd>actual</kbd> and<span> </span><kbd>predicted</kbd>, and an accuracy score is calculated using the<span> </span><kbd>accuracy_score()</kbd><span> </span>function, as seen in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1477 image-border" src="assets/3370b494-009d-495f-80ee-685f447efd81.png" style="width:48.42em;height:12.58em;"/></div>
<ol start="6">
<li>The accuracy score is the same as we calculated manually, 88.4 percent.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>We now know that our model was able to accurately predict whether a call coming in is related to fire or not at a rate of 88.4 percent. At first, this may sound like a strong prediction; however, it's always important to compare this to a baseline score where every call was predicted as a non-fire call. The predicted dataframe,<span> </span><kbd>df_predicted</kbd>, had the following breakdown of labels<span> </span><kbd>1</kbd><span> </span>and<span> </span><kbd>0</kbd>, as seen in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6db3def2-822e-4727-bcf2-3ed64295f3a9.png" style="width:42.67em;height:10.25em;"/></div>
<p>We can run some statistics on that same dataframe to get the mean of label occurrences of value <kbd>1</kbd><span> </span>using the <kbd>df_predicted.describe('label').show()</kbd><span> </span><span>script</span><span>. The output of that script can be seen in the following screenshot:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1478 image-border" src="assets/66f86b60-b8b9-4a6e-a447-a40fbde76ba4.png" style="width:81.33em;height:26.67em;"/></div>
<p>A base model has a prediction value of<span> </span><kbd>1</kbd><span> </span>at a rate of 14.94 percent, or in other words, it has a prediction rate of <em>100 - 14.94</em> percent, which is 85.06 percent for a value of<span> </span><span class="packt_screen">0</span>. Therefore, since 85.06 percent is less than the model prediction rate of 88.4 percent, this model provides an improvement over a blind guess as to whether a call is fire-related or not.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>To learn more about accuracy vs. precision, visit the following website:</p>
<p><a href="https://www.mathsisfun.com/accuracy-precision.html">https://www.mathsisfun.com/accuracy-precision.html</a></p>


            </article>

            
        </section>
    </body></html>