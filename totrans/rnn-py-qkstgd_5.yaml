- en: Building Your Personal Assistant
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will focus our full attention on the practical side of
    recurrent neural networks when building a conversational chatbot. Using your most
    recent knowledge on sequence models, you will create an end-to-end model that
    aims to yield meaningful results. You will make use of a high-level TensorFlow-based
    library, called TensorLayer. This library makes it easier to create simple prototypes
    of complicated systems such as that of a chatbot. The main topics that will be
    covered are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**What are we building?**:This is a more detailed introduction to the exact
    problem and its solution'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Preparing the data**: As always, any deep learning model requires this step,
    so it is crucial to mention it here'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Creating the chatbot network**: You will learn how to use TensorLayer to
    build the graph for the sequence-to-sequence model used for the chatbot'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training the chatbot**: This step combines the data and the network graph
    in order to find the best possible combination of weights and biases'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Building a conversation**: This last step uses the already trained model,
    together with sample sentences, to produce a meaningful conversation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are we building?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The focus of this chapter is to walk you through building a simple conversational
    chatbot that is able to give answers to a set of different questions. Recently,
    chatbots have become more and more popular, and we can see them in numerous practical
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some areas where you can see the use of this software include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Communication between clients and businesses**, where the chatbot assists
    users in finding what they need, or provides support if something does not work
    properly. For example, Facebook offers a really handy way of implementing a chatbot
    for your business'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The personal assistant behind voice control systems such as Amazon Alexa,
    Apple Siri, and more**: You have a full end-to-end human-like conversation where
    you can set reminders, order products, and more'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our simple example will present a slightly augmented version of the TensorLayer
    chatbot code example ([https://github.com/tensorlayer/seq2seq-chatbot](https://github.com/tensorlayer/seq2seq-chatbot)).
    We will be using a dataset formed of pre-collected tweets and will utilize the
    sequence-to-sequence model. Recall from previous chapters that this kind of model
    uses two recurrent neural networks, where the first one is an encoder and the
    second one a decoder. Later, we will give more detail on how this architecture
    is used for building the chatbot.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will focus on how our data (tweets, in this case) is transformed
    to fit the model's requirements. We will first see how, using the files in the `data/`
    folder from the GitHub repo for this task, the model can help us extract the needed
    tweets. Then, we will look at how, with the help of a simple set of functions,
    we can split and transform the data to achieve the needed results.
  prefs: []
  type: TYPE_NORMAL
- en: 'An important file to examine is `data.py`, inside the `data/twitter` folder.
    It transforms plain text into a numeric format so it is easy for us to train the
    network. We won''t go deep into the implementation, since you can examine it by
    yourself. After running the code, we produce three important files:'
  prefs: []
  type: TYPE_NORMAL
- en: '`idx_q.npy`: This is an array of arrays containing index representation of
    all the words in different sentences forming the chatbot questions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`idx_a.npy`: This is an array of arrays containing index representation of
    all the words in different sentences forming the chatbot answers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metadata.pkl`: This contains both the *index to word* (`idx2w`) and *word
    to index* (`w2idx`) dictionaries used for this dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let's focus on the actual usage of this data. You can review it in the
    first 20 lines of `ch5_task.py` from the GitHub repository for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import several Python libraries that will be used throughout the
    whole program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a breakdown of these libraries, accompanied with descriptions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`time`: This is used for keeping track of how long our operations take. You
    will see its usage in the following section, where we train the network'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tensorflow`: This is used only for a handful of operations (initializing variables,
    optimizing the network using adam optimizer, and initializing the TensorFlow session: `tf.Session()`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tensorlayer`: As you already know, TensorLayer ([https://tensorlayer.readthedocs.io/en/stable/](https://tensorlayer.readthedocs.io/en/stable/))
    is a deep learning library on top of TensorFlow. It offers a wide range of methods
    and classes that make it easy for any developer to simply build solutions for
    complicated tasks. This library will help us construct and train our sequence-to-sequence
    model easily'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`shuffle`: We use this to shuffle all arrays, which represent different sentences,
    inside `trainX` and `trainY`. You will see how we obtain `trainX` and `trainY`
    in the following section'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`EmbeddingInputlayer`: A TensorLayer class that represents the input layer
    of a sequence-to-sequence model. As you know, every `Seq2Seq` model has two input
    layers, the encoder and the decoder'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Seq2Seq`: A TensorLayer class that builds a sequence-to-sequence model similar
    to the one in the following diagram:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/3c97d0f9-8a48-4ceb-951a-953bc79f5f8e.png)'
  prefs: []
  type: TYPE_IMG
- en: '`DenseLayer`: A TensorLayer representation of a fully connected (dense) layer.
    There are different types of layers that perform different transformations and
    are used in specific scenarios. For example, we have already used a recurrent
    layer, which is used for time series data. There is also a convolutional layer
    used for images, and so on. You can learn more about them in this video ([https://www.youtube.com/watch?v=FK77zZxaBoI](https://www.youtube.com/watch?v=FK77zZxaBoI))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`retrieve_seq_length_op2`: A TensorLayer function used for calculating the
    sequence length, excluding any paddings of zeros. We will use this function for
    both the encoding and decoding sequences'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After importing the libraries, we need to access the data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: First, we load the `metadata`, `idx_q`, and `idx_a` from the `data/twitter/`
    folder found in the GitHub repository. Second, we use the `split_dataset` method
    to separate the encoder (`idx_q`) and decoder (`idx_a`) data into training (70%),
    testing (15%), and validation (15%).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we convert `trainX, trainY, testX, testY, validX, validY` into Python
    lists, and then remove the padding (zero elements) from the end of every list
    using a TensorLayer function, `tlayer.prepro.remove_pad_sequences()`.
  prefs: []
  type: TYPE_NORMAL
- en: Combining the preceding operations leads to well-defined training, testing,
    and validation data. You will see how we make use of them during training and
    prediction later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the chatbot network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section is one of the most important, so you need to make sure you understand
    it quite well in order to grasp the full concept of our application. We will be
    introducing the network graph that will be used for training and prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'But first, let''s define the hyperparameters of the model. These are predefined
    constants that play a significant role in determining how well the model performs.
    As you will learn in the next chapter, our main task is to tweak the hyperparameters''
    values until we''re satisfied with the model''s prediction. In this case, an initial
    set of hyperparameters is selected. Of course, for better performance, one needs
    to do some optimization on them. This chapter won''t focus on this part but I
    highly recommend doing it using techniques from the last chapter of this book
    ([Chapter 6](85ffa70b-f86d-4432-9c53-9f3e2ab0e007.xhtml), *Improving Your RNN
    Performance*). The current hyperparameter selection is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a brief explanation of these hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`batch_size`: This determines how many elements each batch should have. Normally,
    training is done on batches where data is separated into subarrays, each with
    the size of `batch_size`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`embedding_dimension`: This determines the size of the word embedding vector.
    A single word from the input is encoded into a vector with the size of `embedding_dimension`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`learning_rate`: Its value determines how fast a network learns. It is typically
    a really small value (`0.001, 0.0001`). If the loss function does not decrease
    during training, it is good practice to reduce the learning rate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`number_epochs`: This determines the number of training iterations (epochs).
    In the beginning of each iteration, we shuffle the data and, since an epoch is too
    big to feed to the computer at once, we divide it into several smaller batches.
    Then we train the network using these batches. After every iteration, we shuffle
    again and run the second epoch. This operation is done for the number of epochs
    we have set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After determining the set of hyperparameters, the time comes for additional
    values that help us in building our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s examine each line, one by one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We use `xseq_len` and `yseq_len` to store the length of the encoder's and decoder's
    input sequence. Then, we make sure both values are equal, otherwise, the program
    will break.
  prefs: []
  type: TYPE_NORMAL
- en: '`n_step = int(xseq_len/batch_size)`: with this, we store the number of steps
    that our training is about to perform. This value is only used when printing the
    state of training and we will see its usage later in the chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We use `w2idx` and `idx2w` to store the word dictionary in both formats (the
    word as the dictionary key, and the ID as the dictionary key). These dictionaries
    are used when predicting the chatbot responses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We make `start_id = xvocab_size` and `end_id = xvocab_size + 1` to assure uniqueness
    of these two indices. They are used for indicating the start and end of a single
    sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we extend these dictionaries to include starting and ending elements.
    An example set of our data is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`encode_seqs` (the input encoder sentence): `[''how'', ''are'', ''you'', ''<PAD_ID>'']`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decode_seqs` (the input decoder sentence): `[''<START_ID>'', ''I'', ''am'',
    ''fine'', ''<PAD_ID>'']`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`target_seqs` (the predicted decoder sentence): `[''I'', ''am'', ''fine'',
    ''<END_ID>'', ''<PAD_ID>'']`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`target_mask` (a mask applied at each sequence): `[1, 1, 1, 1, 0]`. This is
    an array the same size as `target_seqs`, but has `0` at the places where padding
    is applied, and `1` everywhere else. You can learn more about masking in recurrent
    neural networks by reading this great Quora answer ([https://www.quora.com/What-is-masking-in-a-recurrent-neural-network-RNN](https://www.quora.com/What-is-masking-in-a-recurrent-neural-network-RNN))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The next step is to define our model structure. We start by introducing the
    model''s placeholders:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, this is the same set of variables shown previously. Each one
    has a `batch_size` dimension and `tf.int64` type. Then, we calculate the model
    output as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The purpose of the preceding line is to find the network's output using the
    input encoder and decoder sequences. We will define and explain the `model` method
    in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we define the loss function and optimizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the loss function is a cross entropy with applied mask to make
    sure each input sequence has the same length. The `logits` (predicted outputs)
    are taken from the preceding model output and are accessed using `net_out.outputs`.
    The `target_seqs` are the expected results for every input.
  prefs: []
  type: TYPE_NORMAL
- en: The model's optimizer is `AdamOptimizer` and is defined using the built-in function
    from TensorFlow, `tf.train.AdamOptimizer`. As usual, we pass the `learning_rate`
    to decide the rate of the `loss` function minimization.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last step is defining and explaining the `model` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'TensorLayer makes it as simple as possible to build the sequence-to-sequence
    model. It uses four main components:'
  prefs: []
  type: TYPE_NORMAL
- en: '`net_encode`: An encoder network using the `EmbeddingInputlayer` class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`net_decode`: A decoder network using the `EmbeddingInputlayer` class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`net_rnn`: A sequence-to-sequence model that combines the two aforementioned
    networks. It is implemented using the `Seq2Seq` class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`net_out`: The final, fully connected (dense) layer producing the end result.
    This layer is built on top of the sequence-to-sequence network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`net_encode` and `net_decode` are similarly initialized using `EmbeddingInputlayer`
    ([https://tensorlayer.readthedocs.io/en/stable/modules/layers.html#tensorlayer.layers.EmbeddingInputlayer](https://tensorlayer.readthedocs.io/en/stable/modules/layers.html#tensorlayer.layers.EmbeddingInputlayer)).
    Three important parameters are used: `inputs`, `vocabulary_size`, and `embedding_size`.
    The `inputs` are `encode_seqs` or `decode_seqs`, which we defined in the preceding
    section. In both cases, `vocabulary_size` is equal to `xvocab_size`, and the `embedding_size`
    is equal to `embedding_dimension`. This embedding layer transforms the input vector
    into one of the `embedding_dimension` size.'
  prefs: []
  type: TYPE_NORMAL
- en: '`net_rnn` combines both the encoder and decoder layers into a full sequence-to-sequence
    model. The parameters are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cell_fn`: The RNN cell used throughout the whole network. In our case, this
    is `BasicLSTMCell`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_hidden`: The number of hidden units in each of the two layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initializer`: The distribution used for defining the parameters (weights,
    biases).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encode_sequence_length`: This specifies the length of the encoder input sequence.
    It uses the `retrieve_seq_length_op2` ([https://tensorlayer.readthedocs.io/en/stable/modules/layers.html#tensorlayer.layers.retrieve_seq_length_op2](https://tensorlayer.readthedocs.io/en/stable/modules/layers.html#tensorlayer.layers.retrieve_seq_length_op2))
    method on the `encode_seqs`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decode_sequence_length`: This specifies the length of the decoder input sequence.
    It uses the `retrive_seq_length_op2` method on the `decode_seqs`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initial_state_encode`: If `None`, the initial state of the encoder networks is
    zero state and can be set automatically by the placeholder or another RNN.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_layer`: The number of RNN layers stacked together in each of the two networks
    (encoder and decoder).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_seq_2d`: If the value is `True`, return `2D Tensor [n_example, 2 *
    n_hidden]`, for stacking DenseLayer after it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the end, we use a fully connected (dense) layer `net_out` to calculate the
    final output of the network. It uses the `Seq2Seq` network as a previous layer,
    the vocabulary size (`xvocab_size`) as the number of units, and `tf.identity`
    as the activation function. It is normally used for explicit transport of a tensor
    between devices (for example, from a GPU to a CPU). In our case, we use it to
    build dummy nodes that copy the values from the previous layer.
  prefs: []
  type: TYPE_NORMAL
- en: One last thing to point out is the use of the `reuse` parameter and the `vs.reuse_variables()`
    method call. During training, we are not reusing the model's parameters (weights
    and biases), so `reuse = False`, but when predicting the chatbot response, we
    make use of the pre-trained parameters, and so have `reuse = True.` The method
    call triggers a reuse for the next set of calculations.
  prefs: []
  type: TYPE_NORMAL
- en: 'And with this, we have finished defining the model. There are only two parts
    left from now: training and predicting.'
  prefs: []
  type: TYPE_NORMAL
- en: Training the chatbot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once we have defined the model graph, we want to train it using our input data.
    Then, we will have a well-tuned set of parameters that can be used for accurate
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we specify the TensorFlow''s Session object that encapsulates the environment
    in which Operation (summation, subtraction, and so on) objects are executed and
    Tensor (placeholders, variables, and so on) objects are evaluated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: A good explanation of the `config` parameter can be found at [https://stackoverflow.com/questions/44873273/what-do-the-options-in-configproto-like-allow-soft-placement-and-log-device-plac](https://stackoverflow.com/questions/44873273/what-do-the-options-in-configproto-like-allow-soft-placement-and-log-device-plac).
    In summary, once we specify `allow_soft_placement`, the operations will be executed
    on the CPU only if there is no GPU registered. If this value is false, we are
    not allowed to execute any operation on a GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Only after running the second line (`sess.run(tf.global_variables_initializer())`)
    will all variables actually hold their values. Initially, they only store a persistent
    Tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will train the network using the `train()` function, defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Let's explain what the preceding code does line by line.
  prefs: []
  type: TYPE_NORMAL
- en: The implementation has two nested loops, where the outer one decides how many
    times the training should go through the whole set of data. This is often done
    using epochs, and aims to strengthen the model accuracy. It is rarely the case
    that weights and biases have learned enough from a certain example when it is
    propagated just once. This is the reason why we should go over every example multiple
    times—in our case, this will be 1,000 times (the number of epochs).
  prefs: []
  type: TYPE_NORMAL
- en: After we enter an epoch iteration, we shuffle the data using the `shuffle` function
    in `sklearn`, which prepares it for entering the inner loop. Then, we use `tl.iterate.minibatches`
    to split the data into sub-arrays each with the `batch_size` size. Each iteration
    inside the inner loop trains the network using the current batch of data.
  prefs: []
  type: TYPE_NORMAL
- en: Before calculating the optimizer, we do some small modification on X (encoder
    batch data) and Y (decoder batch data). As you remember, the model has an encoder
    input (`encoder_seqs`), a decoder input (`decoder_seqs`), and a target output
    (`target_seqs`) incorporated into two RNNs.
  prefs: []
  type: TYPE_NORMAL
- en: The first recurrent neural network is the encoder, which accepts `encoder_seqs`
    as an input. In the preceding code block, this is marked with *X*. We only need
    to add padding to this sequence before applying it to the network. Padding is
    the operation of adding zeros to the end of a sequence in order for it to match
    a fixed length, determined by the longest sequence in the training set. This network
    produces a single vector which is then used in the second RNN.
  prefs: []
  type: TYPE_NORMAL
- en: The second recurrent neural network accepts the encoded vector from the first
    RNN and a decoder input sequence (`decoder_seqs`), and returns a predicted result.
    During training, we compare the predicted result to a target sequence (`target_seqs`),
    which happens to be the exact same sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s clarify the preceding statement. Say you have the sentence *Hello, how
    are you?* as an input, and its response, *I am fine.*, as the output. The first
    sentence goes into the encoder network. The second sentence is the expected output
    of the second decoder network. We need to compare this expected output with the
    actual output that our decoder produces. We get the first word **I** and try to
    predict the following word **am**, then we get **am** and try to predict **fine**,
    and so on. In the beginning, our prediction will be way off, but with time, the
    weights and biases should be adjusted to produce accurate results. The following
    diagram can accompany the explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6a698f1d-cc35-4910-9cdd-ec42863c98b2.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, we need to add a starting symbol to `decoder_seqs` and an ending
    symbol to `target_seqs`. This is what `_decode_seqs = tl.prepro.sequences_add_start_id(Y,
    start_id=start_id, remove_last=False)` and `_target_seqs = tl.prepro.sequences_add_end_id(Y,
    end_id=end_id)` do, where `start_id = xvocab_size` and `end_id = xvocab_size+1`.
    Finally, we add padding to both sequences, equalizing the lengths.
  prefs: []
  type: TYPE_NORMAL
- en: Just before the actual training, we extract `_target_mask` from `_target_seqs`.
    Recall from earlier that if `_target_seqs = ["I", "am", "fine", "<END_ID>", "<PAD_ID>"]`,
    then `_target_mask = [1, 1, 1, 1, 0]`.
  prefs: []
  type: TYPE_NORMAL
- en: In the end, we use the sequence arrays defined previously to train our network.
    It might take some time, so we have added a printing statement every 200 iterations.
    I would recommend leaving your computer running overnight for this training so
    you extract the maximum potential from your data.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to use our model in predicting an actual output. Let's see
    how well it can handle this task.
  prefs: []
  type: TYPE_NORMAL
- en: Building a conversation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This step is really similar to the training one. The first difference is that
    we don't make any evaluation of our predictions, but instead use the input to
    generate the results. The second difference is that we use the already trained
    set of variables to yield this result. You will see how it is done later in this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make things clearer, we first initialize a new sequence-to-sequence model.
    Its purpose is to use the already trained weights and biases and make predictions
    based on different sets of inputs. We only have an encoder and decoder sequence,
    where the encoder one is an input sentence and the decoder sequence is fed one
    word at a time. We define the new model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, it follows exactly the same pattern as the training architecture,
    with the difference that our sequence matrices are of shape `1`, instead of `batch_size`.
  prefs: []
  type: TYPE_NORMAL
- en: An important thing to note is that when calculating the network's results, we
    must **reuse** the same parameters used during training. This step is essential
    because it makes sure our prediction is a result of the recent training we have
    done.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we calculate the final output, `y`, using the softmax function. This
    is usually done at the final layer to make sure that our vector values sum up
    to 1, and is a necessary step during classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'After defining our new model, the time comes for the actual prediction. We
    follow this pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: Generate an initial sentence that will start the conversation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert the sentence into a list of word indices using the `word2idx` dictionary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decide how many replies back and forth we want the conversation to have (in
    our case, this would be five).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the final state of the encoder by feeding the `net_rnn` (as defined
    previously) with the initial sentence.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we iteratively predict the next word using the previously predicted
    word and the network. At the first time step, we use `start_id`, as defined previously,
    as the first word from the decoder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'These steps are executed in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'An interesting thing to note is how `# 2\. decode, feed start_id, get first
    word` and `# 3\. decode, feed state iteratively` perform exactly the same action,
    but step #2 is a special case, focused on predicting only the first word. Step
    #3 uses this first word to iteratively predict all the others.'
  prefs: []
  type: TYPE_NORMAL
- en: '`tl.nlp.sample_top(o[0], top_k=3)` might also be confusing to you. This line
    samples an index from the probability array o[0], where you consider only three
    candidates. The same functionality goes for `w_id = tl.nlp.sample_top(o[0], top_k
    = 2)`. You can learn more on the TensorLayer documentation ([https://tensorlayer.readthedocs.io/en/stable/modules/nlp.html#sampling-functions](https://tensorlayer.readthedocs.io/en/stable/modules/nlp.html#sampling-functions)).'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we print the formed sentence of 30 words (we cap the number of words
    per sentence). If you trained the network long enough, you should see some decent
    results. If they don't satisfy you, then extensive work is needed. You will learn
    more about this in the upcoming [Chapter 6](85ffa70b-f86d-4432-9c53-9f3e2ab0e007.xhtml),
    *Improving Your RNN Performance*.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter reveals a full implementation of a chatbot system that manages
    to construct a short conversation. The prototype shows, in detail, each stage
    of building the intelligent chatbot. This includes collecting data, training the
    network, and making predictions (generating conversation).
  prefs: []
  type: TYPE_NORMAL
- en: For the network's architecture, we use the powerful encoder-decoder sequence-to-sequence
    model that utilizes two recurrent neural networks, while connecting them using
    an encoder vector. For the actual implementation, we make use of a deep learning
    library built on top of TensorFlow, called TensorLayer. It simplifies most of
    the work by introducing simple one-line implementations of standard models such
    as *sequence-to sequence*. In addition, this library is useful for preprocessing
    your data before using it for training.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter shifts focus to, probably, the most important part of building
    a recurrent neural network (and any deep learning model), which is how to improve
    your performance and actually make your program return satisfying results. As
    you have already seen, building a neural network follows a similar pattern in
    most basic/medium examples. The hard part is to make sure the implementations
    are actually useful and produce meaningful results. This will be the focus of
    our next chapter—I hope you enjoy it.
  prefs: []
  type: TYPE_NORMAL
- en: External links
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorLayer chatbot code example: [https://github.com/tensorlayer/seq2seq-chatbot](https://github.com/tensorlayer/seq2seq-chatbot)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorLayer library: [https://tensorlayer.readthedocs.io/en/stable/](https://tensorlayer.readthedocs.io/en/stable/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Layers in neural network:[ https://www.youtube.com/watch?v=FK77zZxaBoI](https://www.youtube.com/watch?v=FK77zZxaBoI)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is masking in a recurrent neural network (RNN)?: [https://www.quora.com/What-is-masking-in-a-recurrent-neural-network-RNN](https://www.quora.com/What-is-masking-in-a-recurrent-neural-network-RNN)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorLayer's embeddingInputlayer class: [https://tensorlayer.readthedocs.io/en/stable/modules/layers.html#tensorlayer.layers.EmbeddingInputlayer](https://tensorlayer.readthedocs.io/en/stable/modules/layers.html#tensorlayer.layers.EmbeddingInputlayer)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorLayer's `retrieve_seq_length_op2` method: [https://tensorlayer.readthedocs.io/en/stable/modules/layers.html#tensorlayer.layers.retrieve_seq_length_op2](https://tensorlayer.readthedocs.io/en/stable/modules/layers.html#tensorlayer.layers.retrieve_seq_length_op2)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow session's config parameter: [https://stackoverflow.com/questions/44873273/what-do-the-options-in-configproto-like-allow-soft-placement-and-log-device-plac](https://stackoverflow.com/questions/44873273/what-do-the-options-in-configproto-like-allow-soft-placement-and-log-device-plac)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorLayer Sampling Functions: [https://tensorlayer.readthedocs.io/en/stable/modules/nlp.html#sampling-functions](https://tensorlayer.readthedocs.io/en/stable/modules/nlp.html#sampling-functions)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
