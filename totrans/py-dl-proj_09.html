<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Object Detection Using OpenCV and TensorFlow</h1>
                </header>
            
            <article>
                
<p class="mce-root">Welcome to the second chapter focusing on computer vision in <em>Python Deep Learning Projects</em> (a data science pun to kick us off<span>!</span>). Let's think about what we accomplished in <a href="acee9abb-ee8f-4b59-8e5e-44ed24ad05c2.xhtml" target="_blank">Chapter 8</a>, <em>Handwritten Digits Classification Using ConvNets</em>, where we were able to train an image classifier with a <strong>convolutional neural network</strong> (<strong>CNN</strong>) to accurately classify handwritten digits in an image. What was a key characteristic of the raw data, and what was our business objective? The data was less complicated than it could have been because each image only had one handwritten digit in it and our goal was to accurately assign a digital label to the image.</p>
<p>What would have happened if each image had multiple handwritten digits in it? What would have happened if we had a video of the digits? What if we want to identify where the digits are in the image? These questions represent challenges that real-world data embodies, and they drive our data science innovation to new models and capabilities.  </p>
<p>Let's expand our line of questions and imagination to the next (hypothetical) business use case for our Python deep learning project, where we're looking to build, train, and test an object detection and classification model to be used by an automobile manufacturer in their new line of self-driving cars. Autonomous vehicles need to have fundamental computer vision capabilities that you and I have organically by way of our physiology and experiential learning. We as humans can examine our field of vision and report whether or not a specific item is present and where in relation to other objects that item (if present) is located. So, if I were to ask you if you see a chicken, you'd likely say no, unless you live on a farm and are looking out your window. But if I ask you if you see a keyboard, you'd likely say yes, and could even say that the keyboard is different from other objects and is in front of the wall before you.  </p>
<p>This is no trivial task for a computer. As Deep Learning Engineers, you are going to learn the intuition and model architecture that empowers you to build a powerful object detection and classification engine that we can envision being tested for use in autonomous vehicles. The data inputs that we're going to be working with in this chapter will be much more informationally complex than what we've had in previous projects, and the outcomes when we get them right will be that much more impressive.</p>
<p>So, let's get started!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Object detection intuition</h1>
                </header>
            
            <article>
                
<p>When you need your application to find and name things in an image, you need to build a deep neural network for object detection. The visual field is very complex, and a camera for still images and video captures frames with many, many objects in them. Object detection is used in manufacturing for process automation in production lines; autonomous vehicles sensing pedestrians, other cars, the road, and signs, for example; and, of course, facial recognition. Computer vision solutions based on machine learning and deep learning require you, the Data Scientist, to build, train, and evaluate models that can differentiate one object from another and then accurately classify those detected objects.  </p>
<p>As you've seen in other projects we've worked on, CNNs are very powerful models for image data. We need to look at expansions on the basic architecture that has performed so well on a single (still) image with simple information to see what works best for complex images and video.</p>
<p>Progress recently has been made with these networks: Faster R-CNN, <strong>region-based fully convolutional network</strong> (<strong>R-FCN</strong>), MultiBox, <strong>solid-state drive</strong> (<strong>SSD</strong>), and <strong>you only look once</strong> (<strong>YOLO</strong>). We've seen the value of these models in common consumer applications such as Google Photos and Pinterest Visual Search. We are even seeing some of these that are lightweight and fast enough to perform well on mobile devices.</p>
<p><span>Recent progress in the field can be researched with the following list of references:</span></p>
<ul>
<li><em>PVANET: Deep but Lightweight Neural Networks for Real-time Object Detection</em>, arXiv:1608.08021</li>
<li><em>R-CNN: Rich feature hierarchies for accurate object detection and semantic segmentation</em>, CVPR, 2014.</li>
<li><em>SPP: Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</em>, ECCV, 2014.</li>
<li><em>Fast R-CNN</em>, arXiv:1504.08083.</li>
<li><em>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</em>, arXiv:1506.01497.</li>
<li><em>R-CNN minus R</em>, arXiv:1506.06981.</li>
<li><em>End-to-end people detection in crowded scenes</em>, arXiv:1506.04878.</li>
<li><em>YOLO – You Only Look Once: Unified, Real-Time Object Detection</em>, arXiv:1506.02640</li>
<li><em>Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks</em></li>
<li><em>Deep Residual Network: Deep Residual Learning for Image Recognition</em></li>
<li><em>R-FCN: Object Detection via Region-based Fully Convolutional Networks</em></li>
<li><em>SSD: Single Shot MultiBox Detector</em>, arXiv:1512.02325</li>
</ul>
<p class="mce-root">Also, following is the timeline of how the evolution of object detection has developed from 1999–2017:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/53eba6c6-940b-430f-a304-a6e636fbc0fd.png" style="width:63.33em;height:22.08em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 9.1: The timeline of the evolution of object detection from 1999 to 2017</div>
<div class="packt_infobox">The files for this chapter can be found at <a href="https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter09">https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter09</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Improvements in object detection models</h1>
                </header>
            
            <article>
                
<p>Object detection and classification has been the subject of study for quite some time. The models that have been used build on the great success of previous researchers. A brief summary of progress history starts by highlighting the computer vision model called <strong>Histogram of Oriented Gradients</strong> (<strong>HOG</strong>) features that was developed by Navneet Dalal and Bill Triggs in 2005.</p>
<p>HOG features were fast and performed well. Interest in deep learning and the great success of CNNs that were more accurate classifiers due to their deep networks. But the problem was that the CNNs of the time were too slow in comparison.</p>
<p>The solution was to take advantage of the CNNs, improved classification capabilities and improve their speed with a technique and employ a selective search paradigm in what became known as R-CNN. Reducing the number of bounding boxes did show improvements in speed, but not sufficiently for the expectations.</p>
<p>SPP-net was a proposed solution, wherein a CNN representation for the whole image was calculated and drove CNN-calculated representations for each sub-section generated by selective search. Selective search uses image features to generate all the possible locations for an object by looking at pixel intensity, color, image texture, and a measure of insideness. These identified objects are then fed into the CNN model for classification.</p>
<p>This, in turn, saw improvements in a model named Fast R-CNN that trained end-to-end, and thereby fixed the primary problems with SPP-net and R-CNN. Advancing this technology further with a model named Faster R-CNN, the technique of using small regional proposal CNNs in place of the selective search performed very well.</p>
<p>Here is a quick overview of the Faster R-CNN object detection pipeline:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/368acba3-00c2-434a-8654-fbc475c089d2.png"/></div>
<p><span>A quick benchmark comparison of the versions of R-CNN discussed previously shows the following:</span></p>
<div>
<table border="1" style="border-collapse: collapse;border-color: #000000">
<tbody>
<tr>
<td style="width: 13.3846%"/>
<td style="width: 14.6154%">R-CNN</td>
<td style="width: 14%">Fast R-CNN</td>
<td style="width: 10%">Faster R-CNN</td>
</tr>
<tr>
<td style="width: 13.3846%">Average response time</td>
<td style="width: 14.6154%"> ~50 sec</td>
<td style="width: 14%">~2 sec</td>
<td style="width: 10%">~0.2 sec</td>
</tr>
<tr>
<td style="width: 13.3846%">Speed boost</td>
<td style="width: 14.6154%">1x</td>
<td style="width: 14%">25x</td>
<td style="width: 10%">250x</td>
</tr>
</tbody>
</table>
<p> </p>
<p>The performance improvement is impressive, with Faster R-CNN being one of the most accurate and fastest object detection algorithms deployed in real-time use cases. Other recent powerful alternatives include YOLO models, which we will look into in detail later in this chapter.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Object detection using OpenCV</h1>
                </header>
            
            <article>
                
<p>Let's start our project with a basic or traditional implementation of <strong>Open Source Computer Vision</strong> (<strong>OpenCV</strong>). This library is primarily targeted at real-time applications that need computer vision capabilities.</p>
<div class="packt_tip">OpenCV has its API wrappers in various languages such as C, C++, Python, and so on, and the best way forward is to build a quick prototype using Python wrappers or any other language you are comfortable with, and once you are ready with your code, rewrite it in C/C++ for production.</div>
<p>In this chapter, we will be using the Python wrappers to create our initial object detection module.</p>
<p>So, let's do it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A handcrafted red object detector</h1>
                </header>
            
            <article>
                
<p>In this section, we will learn how to create a feature extractor that will be able to detect any red object from the provided image using various image processing techniques such as erosion, dilation, blurring, and so on.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing dependencies </h1>
                </header>
            
            <article>
                
<p>First, we need to install OpenCV, which we do with this simple <kbd>pip</kbd> command:</p>
<pre><strong>pip install opencv-python</strong></pre>
<p>Then we will import it along with other modules for visualizations and matrix operations:</p>
<pre class="mce-root">import cv2<br/>import matplotlib<br/>from matplotlib import colors<br/>from matplotlib import pyplot as plt<br/>import numpy as np<br/>from __future__ import division</pre>
<p>Also, let's define some helper functions that will help us to plot the images and the contours:</p>
<pre><strong># Defining some helper function</strong><br/>def show(image):<br/>    # Figure size in inches<br/>    plt.figure(figsize=(15, 15))<br/>    <br/>    # Show image, with nearest neighbour interpolation<br/>    plt.imshow(image, interpolation='nearest')<br/>    <br/>def show_hsv(hsv):<br/>    rgb = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)<br/>    show(rgb)<br/>    <br/>def show_mask(mask):<br/>    plt.figure(figsize=(10, 10))<br/>    plt.imshow(mask, cmap='gray')<br/>    <br/>def overlay_mask(mask, image):<br/>    rgb_mask = cv2.cvtColor(mask, cv2.COLOR_GRAY2RGB)<br/>    img = cv2.addWeighted(rgb_mask, 0.5, image, 0.5, 0)<br/>    show(img)<br/><br/><br/>def find_biggest_contour(image):<br/>    image = image.copy()<br/>    im2,contours, hierarchy = cv2.findContours(image, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)<br/><br/>    contour_sizes = [(cv2.contourArea(contour), contour) for contour in contours]<br/>    biggest_contour = max(contour_sizes, key=lambda x: x[0])[1]<br/> <br/>    mask = np.zeros(image.shape, np.uint8)<br/>    cv2.drawContours(mask, [biggest_contour], -1, 255, -1)<br/>    return biggest_contour, mask<br/><br/>def circle_countour(image, countour):<br/>    image_with_ellipse = image.copy()<br/>    ellipse = cv2.fitEllipse(countour)<br/><br/>    cv2.ellipse(image_with_ellipse, ellipse, (0,255,0), 2)<br/>    return image_with_ellipse<br/><br/></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring image data</h1>
                </header>
            
            <article>
                
<p>The first thing in any data science problem is to explore and understand the data. This helps us to make our objective clear. So, let's first load the image and examine the properties of that image, such as the color spectrum and the dimensions:</p>
<pre><strong># Loading image and display</strong><br/>image = cv2.imread('./ferrari.png')<br/>show(image)</pre>
<p>Following is the output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/098e0712-7ea7-4ac1-90da-ed992e3e6110.png"/></p>
<p>Since the order of the image stored in the memory is <strong>Blue Green Red</strong> (<strong>BGR</strong>), we need to convert it into <strong>Red Green Blue</strong> (<strong>RGB</strong>):</p>
<pre>image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)<br/>show(image)</pre>
<p>Following is the output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/ac13e819-06e9-48c4-bc4b-484e1a59adc1.png" style="width:43.75em;height:29.50em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 9.2: The raw input image in RGB color format.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Normalizing the image</h1>
                </header>
            
            <article>
                
<p>We will be scaling down the image dimensions, for which we will be using the <kbd>cv2.resize()</kbd> function:</p>
<pre>max_dimension = max(image.shape)<br/>scale = 700/max_dimension<br/>image = cv2.resize(image, None, fx=scale,fy=scale)</pre>
<p class="mce-root">Now we will perform a blur operation to make the pixels more normalized, for which we will be using the Gaussian kernel. Gaussian filters are very popular in the research field and are used for various operations, one of which is the blurring effect that reduces the noise and balances the image. The following code performs a blur operation:</p>
<pre>image_blur = cv2.GaussianBlur(image, (7, 7), 0)</pre>
<p>Then we will convert the RGB-based image into an HSV color spectrum, which will help us to extract other characteristics of the image using color intensity, brightness, and shades:</p>
<pre>image_blur_hsv = cv2.cvtColor(image_blur, cv2.COLOR_RGB2HSV)</pre>
<p>Following is the output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d00d3547-1d33-4c34-8ee6-1abfc475d0f4.png" style="width:38.25em;height:25.50em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure: 9.3: The raw input image in HSV color format.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing a mask</h1>
                </header>
            
            <article>
                
<p>We need to create a mask that can detect the specific color spectrum; let's say red in our case. Now we will create two masks that will be performing feature extraction using the color values and the brightness factors:</p>
<pre><strong># filter by color</strong><br/>min_red = np.array([0, 100, 80])<br/>max_red = np.array([10, 256, 256])<br/>mask1 = cv2.inRange(image_blur_hsv, min_red, max_red)<br/><br/><strong># filter by brightness</strong><br/>min_red = np.array([170, 100, 80])<br/>max_red = np.array([180, 256, 256])<br/>mask2 = cv2.inRange(image_blur_hsv, min_red, max_red)<br/><br/><strong># Concatenate both the mask for better feature extraction</strong><br/>mask = mask1 + mask2</pre>
<p>Following is how our mask looks:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1407 image-border" src="assets/3e454d53-c8fa-420c-bbd0-267d80f9645d.png" style="width:47.58em;height:26.75em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Post-processing of a mask</h1>
                </header>
            
            <article>
                
<p>Once we are able to create our mask successfully, we need to perform some morphological operations, which are basic image processing operations used f<span>or the analysis and processing of geometrical structures. </span></p>
<p>First, we will create a kernel that will perform various morphological operations over the input image:</p>
<pre>kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15, 15))</pre>
<div class="packt_tip"><span><strong>Closing</strong>: </span><em>Dilation followed by erosion</em><span> is helpful to close small pieces inside the foreground objects or small black points on the object.</span></div>
<div>
<p>Now let's perform the<span> </span>close<span> </span>operation over the mask:</p>
</div>
<pre>mask_closed = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)</pre>
<div class="mce-root packt_tip">The opening operation <em>erosion followed by dilation</em><span> is used to remove noise.</span></div>
<div>
<p class="mce-root">Then we perform the opening operation:</p>
</div>
<pre>mask_clean = cv2.morphologyEx(mask_closed, cv2.MORPH_OPEN, kernel)</pre>
<p>Following is the output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1409 image-border" src="assets/42972971-f2af-4212-a1e9-c8e181bf2560.png" style="width:162.50em;height:119.17em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 9.4: This figure illustrated the output of morphological close and open operation (left side) and we combine the both to get the final processed mask(right side).</div>
<p>In the preceding screenshot you can see (in the left part of the screenshot) how the morphological operation changes the structure of the mask and when combining both the operations (in the right side of the screenshot) you get a denoised cleaner structure.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Applying a mask</h1>
                </header>
            
            <article>
                
<p>It's time to use the mask that we created to extract the object from the image. First, we will find the biggest contour using the helper function, which is the largest region of our object that we need to extract. Then apply the mask to the image and draw a circle bounding box on the extracted object:</p>
<pre><strong># Extract biggest bounding box</strong><br/>big_contour, red_mask = find_biggest_contour(mask_clean)<br/><br/><strong># Apply mask</strong><br/>overlay = overlay_mask(red_mask, image)<br/><br/><strong># Draw bounding box</strong><br/>circled = circle_countour(overlay, big_contour)<br/><br/>show(circled)</pre>
<p>Following is the output:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/4e4e5617-5b0c-483e-97a5-244910c92c22.png" style="width:35.75em;height:23.75em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 9.5: This figure shows that we have detected the red region (car body) from the image and plotted an ellipes around it.</div>
<p>Voila! So, we successfully extracted the image and also drew the bounding box around the object using simple image processing techniques.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Object detection using deep learning</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this section, we will learn how to build a world-class object detection module without much use of traditional handcrafting techniques. Here, will be using the deep learning approach, which is powerful enough to extract features automatically from the raw image and then use those features for classification and detection purposes.</p>
<p>First, we will build an object detector using a pre-baked Python library that can use most of the state-of-the-art pre-trained models, and later on, we will learn how to implement a really fast and accurate object detector using YOLO architecture.  </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Quick implementation of object detection</h1>
                </header>
            
            <article>
                
<p>Object detection saw an increase in adoption as a result of the industry trend towards deep learning after 2012. Accurate and increasingly fast models such<span> as R-CNN, Fast-RCNN, Faster-RCNN, and RetinaNet, and fast yet highly accurate ones like SSD and YOLO are in production today. In this section, we will use fully-functional pre-baked feature extractors in a Python library that can be used in just a few lines of code. Also, we will touch base regarding the production-grade setup for the same. </span></p>
<p>So, let's do it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing all the dependencies</h1>
                </header>
            
            <article>
                
<p>This is the same drill that we performed in the previous chapters. First let's install all the dependencies. Here, we are using a Python module called ImageAI (<a href="https://github.com/OlafenwaMoses/ImageAI" target="_blank">https://github.com/OlafenwaMoses/ImageAI</a>), which is an effective way to start building your own object detection application from scratch in no time:</p>
<pre>pip install tensorflow<br/>pip install keras<br/>pip install numpy<br/>pip install scipy<br/>pip install opencv-python<br/>pip install pillow<br/>pip install matplotlib<br/>pip install h5py<br/><strong># Here we are installing ImageAI</strong><br/>pip3 install https://github.com/OlafenwaMoses/ImageAI/releases/download/2.0.2/imageai-2.0.2-py3-none-any.whl</pre>
<div class="packt_tip"><span>We will be using the Python 3.x environment to run this module.</span></div>
<p>For this implementation, we are going to use a pre-trained <span>ResNet model that is trained on the COCO dataset (<a href="http://cocodataset.org/#home" target="_blank">http://cocodataset.org/#home</a>) (a large-scale object detection, segmentation, and captioning dataset). You can also use other pre-trained models such as follows:</span></p>
<ul>
<li><kbd>DenseNet-BC-121-32.h5</kbd> (<a href="https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/DenseNet-BC-121-32.h5" target="_blank">https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/DenseNet-BC-121-32.h5</a>) (31.7 MB)</li>
<li><kbd>inception_v3_weights_tf_dim_ordering_tf_kernels.h5</kbd> (<a href="https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/inception_v3_weights_tf_dim_ordering_tf_kernels.h5" target="_blank">https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/inception_v3_weights_tf_dim_ordering_tf_kernels.h5</a>) (91.7 MB)</li>
<li><kbd>resnet50_coco_best_v2.0.1.h5</kbd> (<a href="https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/resnet50_coco_best_v2.0.1.h5" target="_blank">https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/resnet50_coco_best_v2.0.1.h5</a>) (146 MB)</li>
<li><kbd>resnet50_weights_tf_dim_ordering_tf_kernels.h5</kbd> (<a href="https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/resnet50_weights_tf_dim_ordering_tf_kernels.h5" target="_blank">https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/resnet50_weights_tf_dim_ordering_tf_kernels.h5</a>) (98.1 MB)</li>
<li><kbd>squeezenet_weights_tf_dim_ordering_tf_kernels.h5</kbd> (<a href="https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/squeezenet_weights_tf_dim_ordering_tf_kernels.h5" target="_blank">https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/squeezenet_weights_tf_dim_ordering_tf_kernels.h5</a>) (4.83 MB)</li>
<li><kbd>yolo-tiny.h5</kbd> (<a href="https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/yolo-tiny.h5" target="_blank">https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/yolo-tiny.h5</a>) (33.9 MB)</li>
<li><kbd>yolo.h5</kbd> (<a href="https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/yolo.h5" target="_blank">https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/yolo.h5</a>): 237 MB</li>
</ul>
<p>To get the dataset, use the following command:</p>
<pre><strong>wget https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/resnet50_coco_best_v2.0.1.h5</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementation</h1>
                </header>
            
            <article>
                
<p>Now that we have all the dependencies and pre-trained models ready, we will implement a state-of-the-art object detection model. W<span>e will import the </span>ImageAI's<strong><span> </span></strong><span><kbd>ObjectDetection</kbd> class using the following code:</span></p>
<pre>from imageai.Detection import ObjectDetection<br/>import os<br/><span>model_path </span><span class="pl-k">=</span><span> os.getcwd()</span></pre>
<p>Then we create the instance for the <kbd>ObjectDetection</kbd> object and set the model type as <kbd>RetinaNet()</kbd>. Next, we set the part of the ResNet model that we downloaded and call the <kbd>loadModel()</kbd> function:</p>
<pre>object_detector = ObjectDetection()<br/>object_detector.setModelTypeAsRetinaNet()<br/>object_detector.setModelPath( os.path.join(model_path , "resnet50_coco_best_v2.0.1.h5"))<br/>object_detector.loadModel()</pre>
<p>Once the model is loaded into the memory, we can feed a new image to the model, which can be of any popular image format, such as JPEG, PNG, and so on. Also, the function has no constraint on the size of the image, so, you can use any dimensional data and the model will handle it internally. We are using <kbd>detectObjectsFromImage()</kbd> to feed the input image. This method returns the image with some more information such as the bounding box coordinates of the detected object, the label of the detected object, and the confidence score.</p>
<p>Following are some images that are used as input into the model and to perform the object detection:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="assets/f81fd00b-6c4a-4762-bb4d-db472d850275.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 9.6: Since I was traveling to Asia (Malaysia/Langkawi) while writing this chapter, I decided to give it a shot and use some real images that </span><span>I captured on the go.</span></div>
<p class="mce-root">The following code is used for inputting images into the model:</p>
<pre>object_detections = object_detector.detectObjectsFromImage(input_image=os.path.join(model_path , "image.jpg"), output_image_path=os.path.join(model_path , "imagenew.jpg"))</pre>
<p>Further, we iterate over the <kbd>object_detection</kbd> object to read all the objects that the model predicted with the respective confidence score:</p>
<pre>for eachObject in object_detections:<br/>    print(eachObject["name"] , " : " , eachObject["percentage_probability"]) </pre>
<p>Following are how the results look:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="assets/ed6f0e48-301f-4510-87c2-034ed0882f5e.png" style="width:39.50em;height:12.92em;"/></p>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="assets/f0a2c2e0-4b28-4521-8feb-b59dc0bae9bd.png" style="width:39.50em;height:18.00em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1459 image-border" src="assets/875aae8e-22f7-4858-9fdd-2728bee7c7a5.png" style="width:39.83em;height:16.75em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 9.7: The results extracted from the object detection model with the bounding box around the detected object. Results contain the name of the object and the confidence score.</div>
<p class="mce-root">So, we can see that the pre-trained models performed well enough with very few lines of code.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deployment</h1>
                </header>
            
            <article>
                
<p>Now that we have all base code ready, let's deploy the <kbd>ObjectDetection</kbd> modules into production. In this section, we will write a RESTful service that will accept the image as an input and returns the detected object as a response.</p>
<p>We will define a <kbd>POST</kbd> function that accepts the image files with the PNG<span class="s1">,</span> JPG<span class="s1">,</span> JPEG<span class="s1">,</span> and GIF extensions. The uploaded image path is sent to the <kbd>ObjectDetection</kbd> module, which performs the detection and returns the following JSON results:</p>
<pre class="mce-root">from flask import Flask, request, jsonify, redirect<br/>import os , json<br/>from imageai.Detection import ObjectDetection<br/><br/>model_path = os.getcwd()<br/><br/>PRE_TRAINED_MODELS = ["resnet50_coco_best_v2.0.1.h5"]<br/><br/><br/># Creating ImageAI objects and loading models<br/><br/>object_detector = ObjectDetection()<br/>object_detector.setModelTypeAsRetinaNet()<br/>object_detector.setModelPath( os.path.join(model_path , PRE_TRAINED_MODELS[0]))<br/>object_detector.loadModel()<br/>object_detections = object_detector.detectObjectsFromImage(input_image='sample.jpg')<br/><br/># Define model paths and the allowed file extentions<br/>UPLOAD_FOLDER = model_path<br/>ALLOWED_EXTENSIONS = set(['png', 'jpg', 'jpeg', 'gif'])<br/><br/>app = Flask(__name__)<br/>app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER<br/><br/><br/>def allowed_file(filename):<br/>    return '.' in filename and \<br/>           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS<br/><br/><br/>@app.route('/predict', methods=['POST'])<br/>def upload_file():<br/>    if request.method == 'POST':<br/>        # check if the post request has the file part<br/>        if 'file' not in request.files:<br/>            print('No file part')<br/>            return redirect(request.url)<br/>        file = request.files['file']<br/>        # if user does not select file, browser also<br/>        # submit a empty part without filename<br/>        if file.filename == '':<br/>            print('No selected file')<br/>            return redirect(request.url)<br/>        if file and allowed_file(file.filename):<br/>            filename = file.filename<br/>            file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename) <br/>            file.save(file_path) <br/><br/>    try:<br/>        object_detections = object_detector.detectObjectsFromImage(input_image=file_path)<br/>    except Exception as ex:<br/>        return jsonify(str(ex))<br/>    resp = []<br/>    for eachObject in object_detections :<br/>        resp.append([eachObject["name"],<br/>                     round(eachObject["percentage_probability"],3)<br/>                     ]<br/>                    )<br/><br/><br/>    return json.dumps(dict(enumerate(resp)))<br/>    <br/>if __name__ == "__main__":<br/>    app.run(host='0.0.0.0', port=4445)</pre>
<p>Save the file as <kbd>object_detection_ImageAI.py</kbd> and execute the following command to run the web services:</p>
<pre><strong>python object_detection_ImageAI.py</strong></pre>
<p>Following is the output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/71725c7a-3fab-4516-8cd7-1dd49fe574d0.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 9.8: Output on the Terminal screen after successful execution of the web service.</div>
<p>In a separate Terminal, you can now try to call the API, as shown in the following command:</p>
<pre><strong>curl -X POST \</strong><br/><strong>  http://0.0.0.0:4445/predict \</strong><br/><strong>  -H 'content-type: multipart/form-data; boundary=----WebKitFormBoundary7MA4YWxkTrZu0gW' \</strong><br/><strong>  -F file=@/Users/rahulkumar/Downloads/IMG_1651.JPG</strong></pre>
<p>Following will be the response output:</p>
<pre><strong>{</strong><br/><strong> "0": ["person",54.687],</strong><br/><strong> "1": ["person",56.77],</strong><br/><strong> "2": ["person",55.837],</strong><br/><strong> "3": ["person",75.93],</strong><br/><strong> "4": ["person",72.956],</strong><br/><strong> "5": ["bird",81.139]</strong><br/><strong>}</strong></pre>
<p>So, this was awesome; with just a few hours' work, you are ready with a production-grade object detection module that is something close to state-of-the-art.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Object Detection In Real-Time Using YOLOv2</h1>
                </header>
            
            <article>
                
<p><span>A great advancement in object detection and classification was made possible with a process where You Only Look Once (YOLO) at an input image. In this single pass, the goal is to set the coordinates for the corners of the bounding box to be drawn around the detected object and to then classify the object with a regression model. This process is capable of avoiding false positives because it takes into account contextual information from the whole image, and not just a smaller section as in a regional proposal of earlier described methods. The <strong>convolutional neural network</strong> (<strong>CNN</strong>) as follows can pass over the image once, and therefore be fast enough to function in applications where real-time processing is a requirement.</span></p>
<p><span>YOLOv2 predicts an N number of bounding boxes and associates a confidence level for the classification of the object for each individual grid in an S-by-S grid that is established in the immediately preceding step. </span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/294ebbd4-3a6f-4fbf-ad74-66368c53fa24.png" style="width:40.08em;height:10.92em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 9.9: The overview of how YOLO works. The input image is divided into grids and then been sent into the detection process which results in lots of bounding boxes which is further been filtered by applying some thresholds.</div>
<p><span>The outcome of this process is to produce a total of S-by-S by N complement of boxes. For a great percentage of these boxes you’ll get confidence scores that are quite low, and by applying a lower threshold (30% in this case), you can eliminate a majority of inaccurately classified objects as shown in the figure.</span></p>
<p><span>We will be using a pre-trained YOLOv2 model in this section for object detection and classification.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing the dataset</h1>
                </header>
            
            <article>
                
<p>In this part, we will explore the data preparation using the existing the COCO dataset and a custom dataset. If you want to train the YOLO model with lots of classes, then you can follow the instructions provided in the pre-existing part, or else if you want to build your custom object detector, then follow the instructions provided in the custom build section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using the pre-existing COCO dataset</h1>
                </header>
            
            <article>
                
<p><span>For this implementation, we will be using the COCO dataset. This is a great resource dataset for training YOLOv2 to detect, segment, and caption images on a large scale. Download the dataset from <a href="http://cocodataset.org">http://cocodataset.org</a> and run the following command in the terminal:</span></p>
<ol>
<li class="mce-root">Get the training dataset:</li>
</ol>
<pre style="padding-left: 60px"><strong>wget http://images.cocodataset.org/zips/train2014.zip</strong></pre>
<ol start="2">
<li class="mce-root"><span>Get the </span>validation dataset:</li>
</ol>
<pre style="padding-left: 60px"><strong>wget http://images.cocodataset.org/zips/val2014.zip</strong></pre>
<ol start="3">
<li><span>Get the train and validation annotations:</span></li>
</ol>
<pre style="padding-left: 60px"><strong>wget http://images.cocodataset.org/annotations/annotations_trainval2014.zip</strong></pre>
<p><span>Now, let's convert the </span><span>annotations in the COCO format to VOC format:</span></p>
<ol>
<li>Install Baker:</li>
</ol>
<pre style="padding-left: 60px"><strong>pip install baker</strong></pre>
<ol start="2">
<li class="mce-root">Create the folders to store the images and annotations:</li>
</ol>
<pre style="padding-left: 60px"><strong>mkdir images annotations</strong></pre>
<ol start="3">
<li>Unzip <kbd>train2014.zip</kbd> and <kbd>val2014.zip</kbd> under the <kbd>images</kbd> folder:</li>
</ol>
<pre style="padding-left: 60px"><strong>unzip <span>train2014.zip -d ./images/</span><span><br/></span>unzip val2014.zip -d ./images/</strong></pre>
<ol start="4">
<li>Unzip <kbd>annotations_trainval2014.zip</kbd> into <kbd>annotations</kbd> folder:</li>
</ol>
<pre style="padding-left: 60px"><strong><span>unzip </span></strong><span><strong>annotations_trainval2014.zip -d ./annotations/</strong><br/></span></pre>
<ol start="5">
<li>Create a folder to store the converted data:</li>
</ol>
<pre style="padding-left: 60px"><strong>mkdir output</strong><br/><strong>mkdir output/train</strong><br/><strong>mkdir output/val</strong><br/><br/><strong>python <span>coco2voc.py create_annotations /TRAIN_DATA_PATH train /OUTPUT_FOLDER/train<br/></span>python <span>coco2voc.py create_annotations /TRAIN_DATA_PATH val /OUTPUT_FOLDER/val</span></strong></pre>
<p>This is how the folder structure will look after the final transformation:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7be97663-700b-4520-8000-42130ae0f9b6.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 9.10: The illustration of the COCO data extraction and formatting process</div>
<div class="packt_tip">
<div class="page">
<div class="section">
<div class="layoutArea">
<div class="column">
<p><span>This establishes a perfect correspondence between the image and the annotation. When the validation set is empty, we will use a ratio of eight to automatically split the training and validation sets.</span></p>
</div>
</div>
</div>
</div>
</div>
<p>The result is that we will have two folders, <kbd>./images</kbd> and <kbd>./annotation</kbd>, for the training purpose.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using the custom dataset</h1>
                </header>
            
            <article>
                
<p>Now, if you want to build an object detector for your specific use case, then you will need to scrape around 100–200 images from the web and annotate them. There are lots of annotation tools available online, such as LabelImg (<a href="https://github.com/tzutalin/labelImg" target="_blank">https://github.com/tzutalin/labelImg</a>) or<span> </span><strong>Fast Image Data Annotation Tool</strong> (<strong>FIAT</strong>) (<a href="https://github.com/christopher5106/FastAnnotationTool" target="_blank">https://github.com/christopher5106/FastAnnotationTool</a>)<span>.</span></p>
<p>For you to play around with the custom object detector, we have provided some sample images with respective annotations. Look into the repository folder called <kbd>Chapter0 9/yolo/new_class/</kbd>.</p>
<p>Each image has its respective annotations, as shown in the following picture:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/930a677f-c2de-4451-9dff-262b62d6ffbc.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 9.11: The relation between the image and the annotation which is shown here</div>
<p class="mce-root">Also, let's download the pre-trained weights <span>from <a href="https://pjreddie.com/darknet/yolo/" target="_blank">https://pjreddie.com/darknet/yolo/</a>,</span> which we will use to initialize our model, and which will train the custom object detector on top of these pretrained weights:</p>
<pre><strong>wget https://pjreddie.com/media/files/yolo.weights</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing all the dependencies</h1>
                </header>
            
            <article>
                
<p>We will be using the Keras APIs with a TensorFlow approach to create the YOLOv2 architecture. Let's import all the dependencies:</p>
<pre><strong>pip install keras tensorflow tqdm numpy cv2 imgaug</strong></pre>
<p>Following is the code for this:</p>
<pre>from keras.models import Sequential, Model<br/>from keras.layers import Reshape, Activation, Conv2D, Input, MaxPooling2D, BatchNormalization, Flatten, Dense, Lambda<br/>from keras.layers.advanced_activations import LeakyReLU<br/>from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard<br/>from keras.optimizers import SGD, Adam, RMSprop<br/>from keras.layers.merge import concatenate<br/>import matplotlib.pyplot as plt<br/>import keras.backend as K<br/>import tensorflow as tf<br/>import imgaug as ia<br/>from tqdm import tqdm<br/>from imgaug import augmenters as iaa<br/>import numpy as np<br/>import pickle<br/>import os, cv2<br/>from preprocessing import parse_annotation, BatchGenerator<br/>from utils import WeightReader, decode_netout, draw_boxes<br/><br/>#<strong>Setting GPU configs</strong><br/>os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"<br/>os.environ["CUDA_VISIBLE_DEVICES"] = ""</pre>
<div class="packt_tip"><span>It is always recommended to use GPUs to train any YOLO models.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring the YOLO model</h1>
                </header>
            
            <article>
                
<p>YOLO models are designed with the set of hyperparameter and some other configuration. This configuration defines<span> the type of model to construct, as well as other parameters of the model such as the input image size and the list of anchors. You have two options at the moment: tiny YOLO and full YOLO. The following code defines the type of model to construct:</span></p>
<pre><span class="n"><strong># List of object that YOLO model will learn to detect from COCO dataset<br/></strong><br/>#LABELS</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'person'</span><span class="p">,</span> <span class="s1">'bicycle'</span><span class="p">,</span> <span class="s1">'car'</span><span class="p">,</span> <span class="s1">'motorcycle'</span><span class="p">,</span> <span class="s1">'airplane'</span><span class="p">,</span> <span class="s1">'bus'</span><span class="p">,</span> <span class="s1">'train'</span><span class="p">,</span> <span class="s1">'truck'</span><span class="p">,</span> <span class="s1">'boat'</span><span class="p">,</span> <span class="s1">'traffic light'</span><span class="p">,</span> <span class="s1">'fire hydrant'</span><span class="p">,</span> <span class="s1">'stop sign'</span><span class="p">,</span> <span class="s1">'parking meter'</span><span class="p">,</span> <span class="s1">'bench'</span><span class="p">,</span> <span class="s1">'bird'</span><span class="p">,</span> <span class="s1">'cat'</span><span class="p">,</span> <span class="s1">'dog'</span><span class="p">,</span> <span class="s1">'horse'</span><span class="p">,</span> <span class="s1">'sheep'</span><span class="p">,</span> <span class="s1">'cow'</span><span class="p">,</span> <span class="s1">'elephant'</span><span class="p">,</span> <span class="s1">'bear'</span><span class="p">,</span> <span class="s1">'zebra'</span><span class="p">,</span> <span class="s1">'giraffe'</span><span class="p">,</span> <span class="s1">'backpack'</span><span class="p">,</span> <span class="s1">'umbrella'</span><span class="p">,</span> <span class="s1">'handbag'</span><span class="p">,</span> <span class="s1">'tie'</span><span class="p">,</span> <span class="s1">'suitcase'</span><span class="p">,</span> <span class="s1">'frisbee'</span><span class="p">,</span> <span class="s1">'skis'</span><span class="p">,</span> <span class="s1">'snowboard'</span><span class="p">,</span> <span class="s1">'sports ball'</span><span class="p">,</span> <span class="s1">'kite'</span><span class="p">,</span> <span class="s1">'baseball bat'</span><span class="p">,</span> <span class="s1">'baseball glove'</span><span class="p">,</span> <span class="s1">'skateboard'</span><span class="p">,</span> <span class="s1">'surfboard'</span><span class="p">,</span> <span class="s1">'tennis racket'</span><span class="p">,</span> <span class="s1">'bottle'</span><span class="p">,</span> <span class="s1">'wine glass'</span><span class="p">,</span> <span class="s1">'cup'</span><span class="p">,</span> <span class="s1">'fork'</span><span class="p">,</span> <span class="s1">'knife'</span><span class="p">,</span> <span class="s1">'spoon'</span><span class="p">,</span> <span class="s1">'bowl'</span><span class="p">,</span> <span class="s1">'banana'</span><span class="p">,</span> <span class="s1">'apple'</span><span class="p">,</span> <span class="s1">'sandwich'</span><span class="p">,</span> <span class="s1">'orange'</span><span class="p">,</span> <span class="s1">'broccoli'</span><span class="p">,</span> <span class="s1">'carrot'</span><span class="p">,</span> <span class="s1">'hot dog'</span><span class="p">,</span> <span class="s1">'pizza'</span><span class="p">,</span> <span class="s1">'donut'</span><span class="p">,</span> <span class="s1">'cake'</span><span class="p">,</span> <span class="s1">'chair'</span><span class="p">,</span> <span class="s1">'couch'</span><span class="p">,</span> <span class="s1">'potted plant'</span><span class="p">,</span> <span class="s1">'bed'</span><span class="p">,</span> <span class="s1">'dining table'</span><span class="p">,</span> <span class="s1">'toilet'</span><span class="p">,</span> <span class="s1">'tv'</span><span class="p">,</span> <span class="s1">'laptop'</span><span class="p">,</span> <span class="s1">'mouse'</span><span class="p">,</span> <span class="s1">'remote'</span><span class="p">,</span> <span class="s1">'keyboard'</span><span class="p">,</span> <span class="s1">'cell phone'</span><span class="p">,</span> <span class="s1">'microwave'</span><span class="p">,</span> <span class="s1">'oven'</span><span class="p">,</span> <span class="s1">'toaster'</span><span class="p">,</span> <span class="s1">'sink'</span><span class="p">,</span> <span class="s1">'refrigerator'</span><span class="p">,</span> <span class="s1">'book'</span><span class="p">,</span> <span class="s1">'clock'</span><span class="p">,</span> <span class="s1">'vase'</span><span class="p">,</span> <span class="s1">'scissors'</span><span class="p">,</span> <span class="s1">'teddy bear'</span><span class="p">,</span> <span class="s1">'hair drier'</span><span class="p">,</span> <span class="s1">'toothbrush'</span><span class="p">]</span>
<br/><strong># Label for the custom curated dataset.</strong><br/>LABEL = ['kangaroo']<br/><span class="n">IMAGE_H</span><span class="p">,</span> <span class="n">IMAGE_W</span> <span class="o">=</span> <span class="mi">416</span><span class="p">,</span> <span class="mi">416</span>
<span class="n">GRID_H</span><span class="p">,</span>  <span class="n">GRID_W</span>  <span class="o">=</span> <span class="mi">13</span> <span class="p">,</span> <span class="mi">13</span>
<span class="n">BOX</span>              <span class="o">=</span> <span class="mi">5</span>
<span class="n">CLASS</span>            <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">LABELS</span><span class="p">)</span>
<span class="n">CLASS_WEIGHTS</span>    <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">CLASS</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">'float32'</span><span class="p">)</span>
<span class="n">OBJ_THRESHOLD</span>    <span class="o">=</span> <span class="mf">0.3</span>
<span class="n">NMS_THRESHOLD</span>    <span class="o">=</span> <span class="mf">0.3</span>
<span class="n">ANCHORS</span>          <span class="o">=</span> <span class="p">[</span><span class="mf">0.57273</span><span class="p">,</span> <span class="mf">0.677385</span><span class="p">,</span> <span class="mf">1.87446</span><span class="p">,</span> <span class="mf">2.06253</span><span class="p">,</span> <span class="mf">3.33843</span><span class="p">,</span> <span class="mf">5.47434</span><span class="p">,</span> <span class="mf">7.88282</span><span class="p">,</span> <span class="mf">3.52778</span><span class="p">,</span> <span class="mf">9.77052</span><span class="p">,</span> <span class="mf">9.16828</span><span class="p">]</span>

<span class="n">NO_OBJECT_SCALE</span>  <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">OBJECT_SCALE</span>     <span class="o">=</span> <span class="mf">5.0</span>
<span class="n">COORD_SCALE</span>      <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">CLASS_SCALE</span>      <span class="o">=</span> <span class="mf">1.0</span>

<span class="n">BATCH_SIZE</span>       <span class="o">=</span> <span class="mi">16</span>
<span class="n">WARM_UP_BATCHES</span>  <span class="o">=</span> <span class="mi">0</span>
<span class="n">TRUE_BOX_BUFFER</span>  <span class="o">=</span> <span class="mi">50</span></pre>
<p>Configure the path of the pre-trained model and the images, as in the following code:</p>
<pre><span class="n">wt_path</span> <span class="o">=</span> <span class="s1">'yolo.weights'</span>                      
<span class="n">train_image_folder</span> <span class="o">=</span> <span class="s1">'/new_class/images/'</span>
<span class="n">train_annot_folder</span> <span class="o">=</span> <span class="s1">'</span><span class="s1">/new_class/anno</span><span class="s1">/'</span> <br/><span class="n">valid_image_folder</span> <span class="o">=</span> <span class="s1">'/</span><span class="s1">new_class/images</span><span class="s1">/'</span> <br/><span class="n">valid_annot_folder</span> <span class="o">=</span> <span class="s1">'</span><span class="s1">/new_class/anno</span><span class="s1">/'</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining the YOLO v2 model</h1>
                </header>
            
            <article>
                
<p>Now let's have a look at the model architecture of the YOLOv2 model:</p>
<pre><span class="c1"># the function to implement the organization layer (thanks to github.com/allanzelener/YAD2K)</span>
<span class="k">def</span> <span class="nf">space_to_depth_x2</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">space_to_depth</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">block_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)<br/></span><span class="n">input_image</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">IMAGE_H</span><span class="p">,</span> <span class="n">IMAGE_W</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">true_boxes</span>  <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">TRUE_BOX_BUFFER</span> <span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="c1"># Layer 1</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">'same'</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">'conv_1'</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)(</span><span class="n">input_image</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">'norm_1'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">LeakyReLU</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Layer 2</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">'same'</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">'conv_2'</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">'norm_2'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">LeakyReLU</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Layer 3</span>
<span class="c1"># Layer 4</span>
<span class="c1"># Layer 23</span>
# For the entire architecture, please refer to the yolo/Yolo_v2_train.ipynb notebook here: https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter09/yolo/Yolo_v2_train.ipynb</pre>
<p>The network architecture that we just created can be found here: <a href="https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter09/Network_architecture/network_architecture.png" target="_blank">https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter09/Network_architecture/network_architecture.png</a></p>
<p>Following is the output:</p>
<pre><strong>Total params: 50,983,561
Trainable params: 50,962,889
Non-trainable params: 20,672</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training the model</h1>
                </header>
            
            <article>
                
<p>Following are the steps to train the model:</p>
<ol>
<li class="p1">Load the weights that we downloaded and use them to initialize the model:</li>
</ol>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython2 packt_figref">
<pre style="padding-left: 60px"><span class="k">weight_reader = WeightReader(wt_path)<br/>weight_reader.reset()<br/>nb_conv = 23<br/>for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">nb_conv</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">conv_layer</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_layer</span><span class="p">(</span><span class="s1">'conv_'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
    
    <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">nb_conv</span><span class="p">:</span>
        <span class="n">norm_layer</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_layer</span><span class="p">(</span><span class="s1">'norm_'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
        
        <span class="n">size</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">norm_layer</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="n">beta</span>  <span class="o">=</span> <span class="n">weight_reader</span><span class="o">.</span><span class="n">read_bytes</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
        <span class="n">gamma</span> <span class="o">=</span> <span class="n">weight_reader</span><span class="o">.</span><span class="n">read_bytes</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
        <span class="n">mean</span>  <span class="o">=</span> <span class="n">weight_reader</span><span class="o">.</span><span class="n">read_bytes</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
        <span class="n">var</span>   <span class="o">=</span> <span class="n">weight_reader</span><span class="o">.</span><span class="n">read_bytes</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>

        <span class="n">weights</span> <span class="o">=</span> <span class="n">norm_layer</span><span class="o">.</span><span class="n">set_weights</span><span class="p">([</span><span class="n">gamma</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">var</span><span class="p">])</span>       
        
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">conv_layer</span><span class="o">.</span><span class="n">get_weights</span><span class="p">())</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">bias</span>   <span class="o">=</span> <span class="n">weight_reader</span><span class="o">.</span><span class="n">read_bytes</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">conv_layer</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
        <span class="n">kernel</span> <span class="o">=</span> <span class="n">weight_reader</span><span class="o">.</span><span class="n">read_bytes</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">conv_layer</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
        <span class="n">kernel</span> <span class="o">=</span> <span class="n">kernel</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">conv_layer</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)))</span>
        <span class="n">kernel</span> <span class="o">=</span> <span class="n">kernel</span><span class="o">.</span><span class="n">transpose</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">conv_layer</span><span class="o">.</span><span class="n">set_weights</span><span class="p">([</span><span class="n">kernel</span><span class="p">,</span> <span class="n">bias</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">kernel</span> <span class="o">=</span> <span class="n">weight_reader</span><span class="o">.</span><span class="n">read_bytes</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">conv_layer</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
        <span class="n">kernel</span> <span class="o">=</span> <span class="n">kernel</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">conv_layer</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)))</span>
        <span class="n">kernel</span> <span class="o">=</span> <span class="n">kernel</span><span class="o">.</span><span class="n">transpose</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">conv_layer</span><span class="o">.</span><span class="n">set_weights</span><span class="p">([</span><span class="n">kernel</span><span class="p">])</span></pre>
<ol start="2">
<li class="p1">Randomize the weights of the last layer:</li>
</ol>
<pre style="padding-left: 60px"><span class="n">layer</span>   <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">]</span> <span class="c1"># the last convolutional layer</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()</span>

<span class="n">new_kernel</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">GRID_H</span><span class="o">*</span><span class="n">GRID_W</span><span class="p">)</span>
<span class="n">new_bias</span>   <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">GRID_H</span><span class="o">*</span><span class="n">GRID_W</span><span class="p">)</span>

<span class="n">layer</span><span class="o">.</span><span class="n">set_weights</span><span class="p">([</span><span class="n">new_kernel</span><span class="p">,</span> <span class="n">new_bias</span><span class="p">])</span></pre>
<ol start="3">
<li>Generate the configurations as in the following code:</li>
</ol>
<pre style="padding-left: 60px">generator_config = {<br/>    'IMAGE_H' : IMAGE_H, <br/>    'IMAGE_W' : IMAGE_W,<br/>    'GRID_H' : GRID_H, <br/>    'GRID_W' : GRID_W,<br/>    'BOX' : BOX,<br/>    'LABELS' : LABELS,<br/>    'CLASS' : len(LABELS),<br/>    'ANCHORS' : ANCHORS,<br/>    'BATCH_SIZE' : BATCH_SIZE,<br/>    'TRUE_BOX_BUFFER' : 50,<br/>}</pre>
<ol start="4">
<li>Create a training and validation batch:</li>
</ol>
<pre style="padding-left: 60px"><span class="n"><strong># Training batch data</strong><br/>train_imgs</span><span class="p">,</span> <span class="n">seen_train_labels</span> <span class="o">=</span> <span class="n">parse_annotation</span><span class="p">(</span><span class="n">train_annot_folder</span><span class="p">,</span> <span class="n">train_image_folder</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">LABELS</span><span class="p">)<br/></span>train_batch<span> </span><span class="o">=</span><span> </span><span class="n">BatchGenerator</span><span class="p">(</span><span class="n">train_imgs</span><span class="p">,</span><span> </span><span class="n">generator_config</span><span class="p">,</span><span> </span><span class="n">norm</span><span class="o">=</span><span class="n">normalize</span><span class="p">)<br/><br/><strong># Validation batch data</strong><br/></span><span class="n">valid_imgs</span><span class="p">,</span> <span class="n">seen_valid_labels</span> <span class="o">=</span> <span class="n">parse_annotation</span><span class="p">(</span><span class="n">valid_annot_folder</span><span class="p">,</span> <span class="n">valid_image_folder</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">LABELS</span><span class="p">)<br/></span>valid_batch<span> </span><span class="o">=</span><span> </span><span class="n">BatchGenerator</span><span class="p">(</span><span class="n">valid_imgs</span><span class="p">,</span><span> </span><span class="n">generator_config</span><span class="p">,</span><span> </span><span class="n">norm</span><span class="o">=</span><span class="n">normalize</span><span class="p">,</span><span> </span><span class="n">jitter</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span></pre>
<ol start="5">
<li>Set early stop and checkpoint callbacks:</li>
</ol>
<pre style="padding-left: 60px">early_stop = EarlyStopping(monitor='val_loss', <br/>                           min_delta=0.001, <br/>                           patience=3, <br/>                           mode='min', <br/>                           verbose=1)<br/><br/>checkpoint = ModelCheckpoint('weights_coco.h5', <br/>                             monitor='val_loss', <br/>                             verbose=1, <br/>                             save_best_only=True, <br/>                             mode='min', <br/>                             period=1)</pre>
<ol start="6">
<li>Use the following code to train the model:</li>
</ol>
<pre style="padding-left: 60px">tb_counter = len([log for log in os.listdir(os.path.expanduser('~/logs/')) if 'coco_' in log]) + 1<br/>tensorboard = TensorBoard(log_dir=os.path.expanduser('~/logs/') + 'coco_' + '_' + str(tb_counter), <br/>                          histogram_freq=0, <br/>                          write_graph=True, <br/>                          write_images=False)<br/><br/>optimizer = Adam(lr=0.5e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)<br/>#optimizer = SGD(lr=1e-4, decay=0.0005, momentum=0.9)<br/>#optimizer = RMSprop(lr=1e-4, rho=0.9, epsilon=1e-08, decay=0.0)<br/><br/>model.compile(loss=custom_loss, optimizer=optimizer)<br/><br/>model.fit_generator(generator = train_batch, <br/>                    steps_per_epoch = len(train_batch), <br/>                    epochs = 100, <br/>                    verbose = 1,<br/>                    validation_data = valid_batch,<br/>                    validation_steps = len(valid_batch),<br/>                    callbacks = [early_stop, checkpoint, tensorboard], <br/>                    max_queue_size = 3)</pre>
<p>Following is the output:</p>
<pre><strong>Epoch 1/2
11/11 [==============================] - 315s 29s/step - loss: 3.6982 - val_loss: 1.5416

Epoch 00001: val_loss improved from inf to 1.54156, saving model to weights_coco.h5
Epoch 2/2
11/11 [==============================] - 307s 28s/step - loss: 1.4517 - val_loss: 1.0636

Epoch 00002: val_loss improved from 1.54156 to 1.06359, saving model to weights_coco.h5</strong></pre>
<p><span>Following is the TensorBoard plots output for just two epochs:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/3263e010-3d23-4ae6-8524-a956ff552158.png" style="width:31.25em;height:49.42em;"/></p>
<p class="CDPAlignCenter CDPAlign">Figure 9.12: The figure represents the loss plots for 2 epochs</p>
</div>
</div>
</div>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating the model</h1>
                </header>
            
            <article>
                
<p>Once the training is complete, let's perform the prediction by feeding an input image into the model:</p>
<ol>
<li>First we will load the model into the memory:</li>
</ol>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<pre style="padding-left: 60px"><span class="n">model</span><span class="o">.</span><span class="n">load_weights</span><span class="p">(</span><span class="s2">"weights_coco.h5"</span><span class="p">)</span></pre>
<ol start="2">
<li>Now set the test image path and read it:</li>
</ol>
<pre style="padding-left: 60px">input_image_path = "my_test_image.jpg"<br/>image = cv2.imread(input_image_path)<br/>dummy_array = np.zeros((1,1,1,1,TRUE_BOX_BUFFER,4))<br/>plt.figure(figsize=(10,10))</pre>
<ol start="3">
<li>Normalize the image:</li>
</ol>
<pre style="padding-left: 60px">input_image = cv2.resize(image, (416, 416))<br/>input_image = input_image / 255.<br/>input_image = input_image[:,:,::-1]<br/>input_image = np.expand_dims(input_image, 0)</pre>
<ol start="4">
<li>Make a prediction:</li>
</ol>
<pre style="padding-left: 60px">netout = model.predict([input_image, dummy_array])<br/><br/>boxes = decode_netout(netout[0], <br/>                      obj_threshold=OBJ_THRESHOLD,<br/>                      nms_threshold=NMS_THRESHOLD,<br/>                      anchors=ANCHORS, <br/>                      nb_class=CLASS)<br/><br/>image = draw_boxes(image, boxes, labels=LABELS)<br/><br/>plt.imshow(image[:,:,::-1]); plt.show()</pre>
<p class=" highlight hl-ipython2"><span>So, here are some of the results:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1460 image-border" src="assets/b84c805d-2900-4d56-94f2-46ec024be573.png" style="width:29.58em;height:26.67em;"/><img src="assets/1798f426-6f94-43c4-a20f-ec39fcf72364.png" style="width:29.50em;height:25.75em;"/><img class="alignnone size-full wp-image-1461 image-border" src="assets/9a416a09-dfae-4b5a-a7f6-8b5b2b55c087.png" style="width:29.50em;height:21.67em;"/><img class="alignnone size-full wp-image-1462 image-border" src="assets/cb951cf2-9574-4e15-b96c-2f5612422785.png" style="width:29.58em;height:21.92em;"/></div>
<p>Congratulations—you have developed a state-of-the-art object detector that is very fast and reliable.</p>
<p>We learned about building a world class object detection model using YOLO architecture and the results seems to be very promising. Now you can also deploy the same on other mobile devices or Raspberry Pi.</p>
</div>
</div>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Image segmentation</h1>
                </header>
            
            <article>
                
<p>Image segmentation is the process of categorizing what is in a picture at a pixel level. For example, if you were given a picture with a person in it, separating the person from the image is known as segmentation and is done using pixel-level information.</p>
<p>We will be using the COCO dataset for image segmentation.</p>
<p>Following is what you should do before executing any of the SegNet scripts:</p>
<pre><strong>cd SegNet</strong><br/><strong>wget http://images.cocodataset.org/zips/train2014.zip</strong><br/><strong>mkdir images</strong><br/><strong>unzip train2014.zip -d images</strong></pre>
<p>When executing SegNet scripts, make sure that your present working directory is <kbd>SegNet</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Importing all the dependencies</h1>
                </header>
            
            <article>
                
<p>Make sure to restart the session before proceeding forward. </p>
<p>We will be using <kbd>numpy</kbd>, <kbd>pandas</kbd>, <kbd>keras</kbd>, <kbd>pylab</kbd>, <kbd>skimage</kbd>, <kbd>matplotlib</kbd>, and <kbd>pycocotools</kbd>, as in the following code:</p>
<pre>from __future__ import absolute_import<br/>from __future__ import print_function<br/><br/>import pylab<br/>import numpy as np<br/>import pandas as pd<br/>import skimage.io as io<br/>import matplotlib.pyplot as plt<br/><br/>from pycocotools.coco import COCO<br/>pylab.rcParams['figure.figsize'] = (8.0, 10.0)<br/>import cv2<br/><br/>import keras.models as models, Sequential<br/>from keras.layers import Layer, Dense, Dropout, Activation, Flatten, Reshape, Permute<br/>from keras.layers import Conv2D, MaxPool2D, UpSampling2D, ZeroPadding2D<br/>from keras.layers import BatchNormalization<br/><br/>from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau<br/>from keras.optimizers import Adam<br/><br/>import keras<br/>keras.backend.set_image_dim_ordering('th')<br/><br/>from tqdm import tqdm<br/>import itertools<br/>%matplotlib inline</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring the data</h1>
                </header>
            
            <article>
                
<p>We will start off by defining the location of the annotation file we will be using for image segmentation, and then we will initialize the COCO API:</p>
<pre># set the location of the annotation file associated with the train images<br/>annFile='annotations/annotations/instances_train2014.json'<br/><br/><strong># initialize COCO api with</strong><br/>coco = COCO(annFile)</pre>
<p>Following should be the output:</p>
<pre><strong>loading annotations into memory...
Done (t=12.84s)
creating index...
index created!</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Images</h1>
                </header>
            
            <article>
                
<p>Since we are building a binary segmentation model, let us consider the images from the <kbd>images/train2014</kbd> folder that are only tagged with the person label so that we can segment the person out of the image. The COCO API provides us with easy-to-use methods, two of which are the <kbd>getCatIds</kbd> and <kbd>getImgIds</kbd>. The following snippet will help us extract the image IDs of all the images with the label <kbd>person</kbd> tagged to it:</p>
<pre><strong># extract the category ids using the label 'person'</strong><br/>catIds = coco.getCatIds(catNms=['person'])<br/><br/><strong># extract the image ids using the catIds</strong><br/>imgIds = coco.getImgIds(catIds=catIds )<br/><br/><strong># print number of images with the tag 'person'</strong><br/>print("Number of images with the tag 'person' :" ,len(imgIds))</pre>
<p>This should be the output:</p>
<pre><strong>Number of images with the tag 'person' : 45174</strong></pre>
<p>Now let us use the following snippet to plot an image:</p>
<pre><strong># extract the details of image with the image id</strong><br/>img = coco.loadImgs(imgIds[2])[0]<br/>print(img)<br/><br/><strong># load the image using the location of the file listed in the image variable</strong><br/>I = io.imread('images/train2014/'+img['file_name'])<br/><br/><strong># display the image</strong><br/>plt.imshow(I)</pre>
<p>Following should be the output:</p>
<pre><strong>{'height': 426, 'coco_url': 'http://images.cocodataset.org/train2014/COCO_train2014_000000524291.jpg', 'date_captured': '2013-11-18 09:59:07', 'file_name': 'COCO_train2014_000000524291.jpg', 'flickr_url': 'http://farm2.staticflickr.com/1045/934293170_d1b2cc58ff_z.jpg', 'width': 640, 'id': 524291, 'license': 3}</strong></pre>
<p>We get the following picture as an output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/121f058f-bee6-4231-9169-85637d6d25df.png"/></p>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign">Figure 9.13: The plot representation a sample image from the dataset.</div>
<p>In the previous code snippet, we feed in an image ID to the <kbd>loadImgs</kbd> method of COCO to extract the details of the image it corresponds to. If you look at the output of the <kbd>img</kbd> variable, one of the keys listed is the <kbd>file_name</kbd> key. This key holds the name of the image located in the <kbd>images/train2014/</kbd> folder. </p>
<p>Then we read the image using the <kbd>imread</kbd> method of the <kbd>io</kbd> module we have already imported and plot it using <kbd>matplotlib.pyplot</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Annotations</h1>
                </header>
            
            <article>
                
<p>Now let us load the annotation corresponding to the previous picture and plot the annotation on top of the picture. The <kbd>coco.getAnnIds()</kbd> function helps load the annotation info of an image using its ID. Then, with the help of the <kbd>coco.loadAnns()</kbd> function, we load the annotations and plot it using the <kbd>coco.showAnns()</kbd> function. It is important that you first plot the image and then perform the annotation operations as shown in the following code snippet:</p>
<pre><strong># display the image</strong><br/>plt.imshow(I)<br/><br/><strong># extract the annotation id</strong> <br/>annIds = coco.getAnnIds(imgIds=img['id'], catIds=catIds, iscrowd=None)<br/><br/><strong># load the annotation</strong><br/>anns = coco.loadAnns(annIds)<br/><br/><strong># plot the annotation on top of the image</strong><br/>coco.showAnns(anns)</pre>
<p>Following should be the output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1411 image-border" src="assets/2953e1ec-c34f-4069-a3f0-92297a412b5f.png" style="width:23.83em;height:16.17em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 9.14: Visualizing annotation on an image</div>
<p>To be able to obtain the annotation label array, use the <kbd>coco.annToMask()</kbd> function as shown in the following code snippet. This array will help us form the segmentation target:</p>
<pre><strong># build the mask for display with matplotlib</strong><br/>mask = coco.annToMask(anns[0])<br/><br/><strong># display the mask</strong><br/>plt.imshow(mask)</pre>
<p>Following should be the output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1412 image-border" src="assets/16120870-43e2-45da-8eea-aa4a4388ac25.png" style="width:25.50em;height:17.33em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 9.15: Visualizing just the annotation</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing the data</h1>
                </header>
            
            <article>
                
<p>Let us now define a <kbd>data_list()</kbd> <span>function </span>that will automate the process of loading an image and its segmentation array into memory and resize them to the shape of 360*480 using OpenCV. This function returns two lists containing images and segmentation array:</p>
<pre>def data_list(imgIds, count = 12127, ratio = 0.2):<br/>    <strong>"""Function to load image and its target into memory."""</strong> <br/>    img_lst = []<br/>    lab_lst = []<br/><br/>    for x in tqdm(imgIds[0:count]):<br/>        <strong># load image details</strong><br/>        img = coco.loadImgs(x)[0]<br/>        <br/>        <strong># read image</strong><br/>        I = io.imread('images/train2014/'+img['file_name'])<br/>        if len(I.shape)&lt;3:<br/>            continue<br/>        <br/>        <strong># load annotation information</strong><br/>        annIds = coco.getAnnIds(imgIds=img['id'], catIds=catIds, iscrowd=None)<br/>        <br/>        <strong># load annotation</strong><br/>        anns = coco.loadAnns(annIds)<br/>        <br/>        <strong># prepare mask</strong><br/>        mask = coco.annToMask(anns[0])<br/>        <br/>        <strong># This condition makes sure that we select images having only one person</strong> <br/>        if len(np.unique(mask)) == 2:<br/>            <br/>            <strong># Next condition selects images where ratio of area covered by the </strong><br/><strong>            # person to the entire image is greater than the ratio </strong>parameter<br/><strong>            # This is done to not have large class imbalance</strong><br/>            if (len(np.where(mask&gt;0)[0])/len(np.where(mask&gt;=0)[0])) &gt; ratio :<br/>                <br/>                <strong># If you check, generated mask will have 2 classes i.e 0 and 2 </strong><br/><strong>                # (0 - background/other, 1 - person).</strong><br/><strong>                # to avoid issues with cv2 during the resize operation</strong><br/><strong>                # set label 2 to 1, making label 1 as the person.</strong> <br/>                mask[mask==2] = 1<br/>                <br/>                <strong># resize image and mask to shape (480, 360)</strong><br/>                I= cv2.resize(I, (480,360))<br/>                mask = cv2.resize(mask, (480,360))<br/><br/>                <strong># append mask and image to their lists</strong><br/>                img_lst.append(I)<br/>                lab_lst.append(mask)<br/>    return (img_lst, lab_lst)<br/><br/><strong># get images and their labels</strong><br/>img_lst, lab_lst = data_list(imgIds)<br/><br/>print('Sum of images for training, validation and testing :', len(img_lst))<br/>print('Unique values in the labels array :', np.unique(lab_lst[0]))</pre>
<p>Following should be the output:</p>
<pre><strong>Sum of images for training, validation and testing : 1997
Unique values in the labels array : [0 1]</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Normalizing the image </h1>
                </header>
            
            <article>
                
<p>First, let's define the <kbd>make_normalize()</kbd> <span>function</span><span>, which</span> accepts an image and performs the histogram normalization operation on it. The return object is a normalized array:</p>
<pre>def make_normalize(img):<br/>    <strong>"""Function to histogram normalize images."""</strong><br/>    norm_img = np.zeros((img.shape[0], img.shape[1], 3),np.float32)<br/><br/>    b=img[:,:,0]<br/>    g=img[:,:,1]<br/>    r=img[:,:,2]<br/><br/>    norm_img[:,:,0]=cv2.equalizeHist(b)<br/>    norm_img[:,:,1]=cv2.equalizeHist(g)<br/>    norm_img[:,:,2]=cv2.equalizeHist(r)<br/><br/>    return norm_img<br/><br/>plt.figure(figsize = (14,5))<br/>plt.subplot(1,2,1)<br/>plt.imshow(img_lst[9])<br/>plt.title(' Original Image')<br/>plt.subplot(1,2,2)<br/>plt.imshow(make_normalize(img_lst[9]))<br/>plt.title(' Histogram Normalized Image')</pre>
<p>Following should be the output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/fbc0136c-88e6-40b6-9fae-5f3968f90ac7.png" style="width:43.58em;height:16.50em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 9.16: Before and After histogram normalization on an image</div>
<p>In the preceding screenshot, we see the original picture on the left, which is very visible, and on the right we see the normalized picture, which is not at all visible.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Encoding</h1>
                </header>
            
            <article>
                
<p>With the <kbd>make_normalize()</kbd> function defined, let's now define a <kbd>make_target</kbd> function. This function accepts the segmentation array of shape (360,480) and then returns a segmentation target of shape (<kbd>360</kbd>,<kbd>480</kbd>,<kbd>2</kbd>). In the target, channel <kbd>0</kbd> represents the background and will have <kbd>1</kbd> in locations that represent the background in the image and zero elsewhere. C<span>hannel <kbd>1</kbd> represents the person and will have <kbd>1</kbd> in locations that </span>represent <span>the person in the image and <kbd>0</kbd> </span>elsewhere<span>.</span><span> The following code implements the function:</span></p>
<pre>def make_target(labels):<br/>    """Function to one hot encode targets."""<br/>    x = np.zeros([360,480,2])<br/>    for i in range(360):<br/>        for j in range(480):<br/>            x[i,j,labels[i][j]]=1<br/>    return x<br/><br/>plt.figure(figsize = (14,5))<br/>plt.subplot(1,2,1)<br/>plt.imshow(make_target(lab_lst[0])[:,:,0])<br/>plt.title('Background')<br/>plt.subplot(1,2,2)<br/>plt.imshow(make_target(lab_lst[0])[:,:,1])<br/>plt.title('Person')</pre>
<p class="CDPAlignLeft CDPAlign">Following should be the output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0cc9dfd9-3ec8-42d1-a7b1-ac13c6fc0845.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 9.17: Visualizing the encoded target arrays</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model data </h1>
                </header>
            
            <article>
                
<p>We will now define a function called <kbd>model_data()</kbd> that accepts a list of images and a list of labels. This function will apply the <kbd>make_normalize()</kbd> <span>function </span>on each image for the purpose of normalizing, and it will apply the <kbd>make_encode()</kbd> <span>function</span> on each label/segmentation array to obtain the encoded array. </p>
<p>The return of this function is two lists, one containing the normalized images and the other containing the corresponding target arrays:</p>
<pre>def model_data(images, labels):<br/>    <strong>"""Function to perform normalize and encode operation on each image."""</strong><br/>    <strong># empty label and image list</strong><br/>    array_lst = []<br/>    label_lst=[]<br/>    <br/>    <strong># apply normalize function on each image and encoding function on each label</strong><br/>    for x,y in tqdm(zip(images, labels)):<br/>        array_lst.append(np.rollaxis(normalized(x), 2))<br/>        label_lst.append(make_target(y))<br/>        <br/>    return np.array(array_lst), np.array(label_lst)<br/><br/><strong># Get model data</strong><br/>train_data, train_lab = model_data(img_lst, lab_lst)<br/><br/>flat_image_shape = 360*480<br/><br/><strong># reshape target array</strong><br/>train_label = np.reshape(train_lab,(-1,flat_image_shape,2))<br/><br/><strong># test data</strong><br/>test_data = test_data[1900:]<br/><strong># validation data</strong><br/>val_data = train_data[1500:1900]<br/><strong># train data</strong><br/>train_data = train_data[:1500]<br/><br/><strong># test label</strong><br/>test_label = test_label[1900:]<br/><strong># validation label</strong><br/>val_label = train_label[1500:1900]<br/><strong># train label</strong><br/>train_label = train_label[:1500]</pre>
<p>In the preceding snippet, we have also split the data into train, test, and validation sets, with the train set containing <kbd>1500</kbd> data points, the validation set containing <kbd>400</kbd> data points, and the test set containing <kbd>97</kbd> data points.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining hyperparameters</h1>
                </header>
            
            <article>
                
<p>The following are some of the <span>defined</span> hyperparameters that we will be using throughout the code, and they are totally configurable:</p>
<div>
<pre><strong># define optimizer</strong><br/>optimizer = Adam(lr=0.002)<br/><br/><strong># input shape to the model</strong><br/>input_shape=(3, 360, 480)<br/><br/><strong># training batchsize</strong><br/>batch_size = 6<br/><br/><strong># number of training epochs</strong><br/>nb_epoch = 60</pre>
<div class="packt_infobox">To learn more about<span> </span><kbd>optimizers</kbd><span> </span>and their APIs in Keras, visit <a href="https://keras.io/optimizers/">https://keras.io/optimizers/</a>. Reduce <kbd>batch_size</kbd> if you get a resource exhaustion error with respect to the GPU.<a href="https://keras.io/optimizers/"/></div>
</div>
<div class="packt_tip">Experiment with different learning rates, <kbd>optimizers</kbd>, and <kbd>batch_size</kbd> to see how these factors affect the quality of your model, and if you get better results, show them to the deep learning community.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Define SegNet</h1>
                </header>
            
            <article>
                
<p>For the purpose of image segmentation, we will build a SegNet model, which is very similar to the autoencoder we built in <a href="acee9abb-ee8f-4b59-8e5e-44ed24ad05c2.xhtml" target="_blank">Chapter 8</a>: <em>Handwritten Digits Classification Using ConvNets</em>, as shown:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/695f6e37-8388-4a91-9d86-fffda1adfe5d.png" style="width:45.50em;height:35.58em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 9.18: SegNet architecture used in this chapter</div>
<p>The SegNet model we'll define will accept (<em>3,360, 480</em>) images as input with (<em>172800, 2</em>) segmentation arrays as the targets, and it will have the following characteristics in the encoder:</p>
<ul>
<li>The first layer is a Convolution 2D layer with 64 filters of size 3*3, <span>with <kbd>activation</kbd> as</span><span> </span><kbd>relu</kbd>, followed by batch normalization, followed by downsampling with MaxPooling2D of size 2*2.</li>
<li>The second layer is a Convolution 2D layer with 128 filters of size 3*3, <span>with <kbd>activation</kbd> as</span><span> </span><kbd>relu</kbd>, followed by batch normalization, followed by downsampling with MaxPooling2D of size 2*2.</li>
<li>The third layer is a Convolution 2D layer with 256 filters of size 3*3, <span>with <kbd>activation</kbd> as</span><span> </span><kbd>relu</kbd>, followed by batch normalization, followed by downsampling with MaxPooling2D of size 2*2.</li>
<li>The fourth layer is again a Convolution 2D layer with 512 filters of size 3*3, <span>with <kbd>activation</kbd> as</span><span> </span><kbd>relu</kbd>, followed by batch normalization.</li>
</ul>
<p>And the model will have the following characteristics in the decoder:</p>
<ul>
<li>The first layer is a Convolution 2D layer with 512 filters of size 3*3, <span>with <kbd>activation</kbd> as</span><span> </span><kbd>relu</kbd>, followed by batch normalization, followed by downsampling with UpSampling2D of size 2*2.</li>
<li>The second layer is a Convolution 2D layer with 256 filters of size 3*3, <span>with <kbd>activation</kbd> as</span><span> </span><kbd>relu</kbd>, followed by batch normalization, followed by downsampling with UpSampling2D of size 2*2.</li>
<li>The third layer is a Convolution 2D layer with 128 filters of size 3*3, <span>with <kbd>activation</kbd> as</span><span> </span><kbd>relu</kbd>, followed by batch normalization, followed by downsampling with UpSampling2D of size 2*2.</li>
<li>The fourth layer is a Convolution 2D layer with 64 filters of size 3*3 with <kbd>activation</kbd> as<span> </span><kbd>relu</kbd>,<span> followed by batch normalization.</span></li>
<li>The fifth layer is a Convolution 2D layer with 2 filters of size 1*1, followed by Reshape, Permute and a <kbd>softmax</kbd> as <kbd>activation</kbd> layer for predicting scores.</li>
</ul>
<p>The model is described with the following code:</p>
<pre>model = Sequential()<br/><strong># Encoder</strong><br/>model.add(Layer(input_shape=input_shape))<br/>model.add(ZeroPadding2D())<br/>model.add(Conv2D(filters=64, kernel_size=(3,3), padding='valid', activation='relu'))<br/>model.add(BatchNormalization())<br/>model.add(MaxPool2D(pool_size=(2,2)))<br/><br/>model.add(ZeroPadding2D())<br/>model.add(Conv2D(filters=128, kernel_size=(3,3), padding='valid', activation='relu'))<br/>model.add(BatchNormalization())<br/>model.add(MaxPool2D(pool_size=(2,2)))<br/><br/>model.add(ZeroPadding2D())<br/>model.add(Conv2D(filters=256, kernel_size=(3,3), padding='valid', activation='relu'))<br/>model.add(BatchNormalization())<br/>model.add(MaxPool2D(pool_size=(2,2)))<br/><br/>model.add(ZeroPadding2D())<br/>model.add(Conv2D(filters=512, kernel_size=(3,3), padding='valid', activation='relu'))<br/>model.add(BatchNormalization())<br/><br/><strong># Decoder<br/></strong># For the remaining part of this section of the code refer to the segnet.ipynb file in the SegNet folder. Here is the github link: https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter09<br/><br/></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Compiling the model</h1>
                </header>
            
            <article>
                
<p>With the model defined, compile the model with '<kbd>categorical_crossentropy</kbd>' as <kbd>loss</kbd> and <kbd>optimizer</kbd> as <kbd>Adam</kbd>, as defined by the <kbd>optimizer</kbd> variable in the hyperparameters section. We will also define <kbd>ReduceLROnPlateau</kbd> to reduce the learning rate as needed when training, as follows:</p>
<pre><strong># compile model</strong><br/>model.compile(loss="categorical_crossentropy", optimizer=Adam(lr=0.002), metrics=["accuracy"])<br/><br/><strong># use ReduceLROnPlateau to adjust the learning rate</strong><br/>reduceLROnPlat = ReduceLROnPlateau(monitor='val_acc', factor=0.75, patience=5,<br/>                      min_delta=0.005, mode='max', cooldown=3, verbose=1)<br/><br/>callbacks_list = [reduceLROnPlat]</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fitting the model</h1>
                </header>
            
            <article>
                
<p>With the model compiled, we will now fit the model on the data using the <kbd>fit</kbd> method of the model. Here, since we are training on a small set of data, it is important to set the parameter shuffle to <kbd>True</kbd> so that the images are shuffled after each epoch:</p>
<pre><strong># fit the model</strong><br/>history = model.fit(train_data, train_label, callbacks=callbacks_list,<br/>                    batch_size=batch_size, epochs=nb_epoch,<br/>                    verbose=1, shuffle = True, validation_data = (val_data, val_label))</pre>
<p>This should be the output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/28aa5d4e-2914-4b58-9c56-19f9ff8f4da2.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 9.19: Training output</div>
<p>The following shows the accuracy and loss plots:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/2ed63cdf-9267-4845-9e39-e472d5301333.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 9.20: Plot showing training progression</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Testing the model</h1>
                </header>
            
            <article>
                
<p>With the model trained, evaluate the model on test data, as in the following:</p>
<pre>loss,acc = model.evaluate(test_data, test_label)<br/>print('Loss :', loss)<br/>print('Accuracy :', acc)</pre>
<p>This should be the output:</p>
<pre><strong>97/97 [==============================] - 7s 71ms/step
Loss : 0.5390811630131043
Accuracy : 0.7633129960482883</strong></pre>
<p>We see that the SegNet model we built has a loss of 0.539 and accuracy of 76.33 on test images.</p>
<p>Let's plot the test images and their corresponding generated segmentations to understand model learning:</p>
<pre>for i in range(3):<br/>    plt.figure(figsize = (10,3))<br/>    plt.subplot(1,2,1)<br/>    plt.imshow(img_lst[1900+i])<br/>    plt.title('Input')<br/>    plt.subplot(1,2,2)<br/>    plt.imshow(model.predict_classes(test_data[i:(i+1)*1]).reshape(360,480))<br/>    plt.title('Segmentation')</pre>
<p>Following should be the output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/24987ab2-aacf-4f3d-8d4f-d94bc4787c61.png" style="width:39.08em;height:43.08em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 9.21: Segmentation generated on test images</div>
<p>From the preceding figure, we see that the model was able to segment the person from the image.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Conclusion</h1>
                </header>
            
            <article>
                
<p>The first part of the project was to build an object detection classifier using YOLO architecture in Keras. </p>
<p>The second part of the project was to build a binary image segmentation model on COCO images that contain just a person, aside from the background. The goal was to build a good enough model to segment out the person from the background in the image.</p>
<p>The model we build by training on 1500 images, each of shape 360*480*3, has an accuracy of 79% on train data, and 78% on validation and test data. The model is successfully able to segment the person in the image, but the borders of the segmentations are slightly off from where they should be. This is due to using a small training set. Considering the number of images used for training, the model did a good job of segmenting.</p>
<p>There are more images available in this dataset that can be used for training, and it might take over a day to train on all of them using a Nvidia Tesla K80 GPU, but doing so will give you really good segmentation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In the first part of this chapter, we learnt how to build a <span>RESTful service for object</span> detection using an existing classifier, and we also learned to build an <span>accurate object detector using the</span> <span>YOLO architecture</span> object detection classifier <span>using</span> Keras, while also implementing transfer learning. In the second part of the chapter, we understood what image segmentation is and built an image segmentation model on images from the COCO dataset. <span>We also tested the performance of the object detector and the image segmenter on test data, and determined that we succeeded in achieving the goal.</span></p>


            </article>

            
        </section>
    </body></html>