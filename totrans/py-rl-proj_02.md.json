["```py\nsudo pip install gym \n```", "```py\ngit clone https://github.com/openai/gym \ncd gym \npip install -e .\n\n```", "```py\nbrew install cmake boost boost-python sdl2 swig wget\n```", "```py\napt-get install -y python-numpy python-dev cmake zlib1g-dev libjpeg-dev xvfb libav-tools xorg-dev python-opengl libboost-all-dev libsdl2-dev swig\n```", "```py\npip install 'gym[all]'\n```", "```py\nimport gym \n```", "```py\nenvironment = gym.make('CartPole-v0') \n```", "```py\nenvironment.reset() \n```", "```py\nfor dummy in range(100):\n    environment.render() \n    environment.step(environment.action_space.sample())\n\n```", "```py\nenvironment = gym.make('SpaceInvaders-v0')\n```", "```py\nenvironment = gym.make('Copy-v0')\n```", "```py\nenvironment = gym.make('Humanoid-v2')\n```", "```py\nenvironment = gym.make('HandManipulateBlock-v0')\n```", "```py\nimport gym\nimport numpy as np\nimport random\nimport math\n```", "```py\nenvironment = gym.make('CartPole-v0')\n```", "```py\nno_buckets = (1, 1, 6, 3)\nno_actions = environment.action_space.n\n```", "```py\nstate_value_bounds = list(zip(environment.observation_space.low, environment.observation_space.high))\nstate_value_bounds[1] = [-0.5, 0.5]\nstate_value_bounds[3] = [-math.radians(50), math.radians(50)]\n```", "```py\naction_index = len(no_buckets)\n```", "```py\nq_value_table = np.zeros(no_buckets + (no_actions,))\n```", "```py\nmin_explore_rate = 0.01\nmin_learning_rate = 0.1\n```", "```py\nmax_episodes = 1000\nmax_time_steps = 250\nstreak_to_end = 120\nsolved_time = 199\ndiscount = 0.99\nno_streaks = 0\n\n```", "```py\ndef select_action(state_value, explore_rate):\n    if random.random() < explore_rate:\n        action = environment.action_space.sample()\n    else:\n        action = np.argmax(q_value_table[state_value])\n    return action\n```", "```py\ndef select_explore_rate(x):\n    return max(min_explore_rate, min(1, 1.0 - math.log10((x+1)/25)))\n```", "```py\n\ndef select_learning_rate(x):\n    return max(min_learning_rate, min(0.5, 1.0 - math.log10((x+1)/25)))\n```", "```py\ndef bucketize_state_value(state_value):\n    bucket_indexes = []\n    for i in range(len(state_value)):\n        if state_value[i] <= state_value_bounds[i][0]:\n           bucket_index = 0\n        elif state_value[i] >= state_value_bounds[i][1]:\n            bucket_index = no_buckets[i] - 1\n        else:\n            bound_width = state_value_bounds[i][1] - state_value_bounds[i][0]\n            offset = (no_buckets[i]-1)*state_value_bounds[i][0]/bound_width\n            scaling = (no_buckets[i]-1)/bound_width\n            bucket_index = int(round(scaling*state_value[i] - offset))\n        bucket_indexes.append(bucket_index)\n    return tuple(bucket_indexes)\n\n```", "```py\nfor episode_no in range(max_episodes):\n    explore_rate = select_explore_rate(episode_no)\n    learning_rate = select_learning_rate(episode_no)\n\n    observation = environment.reset()\n\n    start_state_value = bucketize_state_value(observation)\n    previous_state_value = start_state_value\n\n    for time_step in range(max_time_steps):\n        environment.render()\n        selected_action = select_action(previous_state_value, explore_rate)\n        observation, reward_gain, completed, _ = environment.step(selected_action)\n        state_value = bucketize_state_value(observation)\n        best_q_value = np.amax(q_value_table[state_value])\n        q_value_table[previous_state_value + (selected_action,)] += learning_rate * (\n                reward_gain + discount * (best_q_value) - q_value_table[previous_state_value + (selected_action,)])\n```", "```py\n\n        print('Episode number : %d' % episode_no)\n        print('Time step : %d' % time_step)\n        print('Selection action : %d' % selected_action)\n        print('Current state : %s' % str(state_value))\n        print('Reward obtained : %f' % reward_gain)\n        print('Best Q value : %f' % best_q_value)\n        print('Learning rate : %f' % learning_rate)\n        print('Explore rate : %f' % explore_rate)\n        print('Streak number : %d' % no_streaks)\n\n        if completed:\n            print('Episode %d finished after %f time steps' % (episode_no, time_step))\n            if time_step >= solved_time:\n                no_streaks += 1\n            else:\n                no_streaks = 0\n            break\n\n        previous_state_value = state_value\n\n    if no_streaks > streak_to_end:\n        break\n\n```"]