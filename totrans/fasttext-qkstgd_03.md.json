["```py\n$ mkdir -p data/yelp\n$ cd data/yelp\n$ mv ~/Downloads/yelp_review.csv.zip .\n$ unzip yelp_review.csv.zip\nArchive: yelp_review.csv.zip\n inflating: yelp_review.csv\n```", "```py\n$ head -n1 yelp_review.csv\n \"review_id\",\"user_id\",\"business_id\",\"stars\",\"date\",\"text\",\"useful\",\"funny\",\"cool\"\n```", "```py\nimport csv\nimport sys\nw = csv.writer(sys.stdout)\nfor row in csv.DictReader(sys.stdin):\n    w.writerow([row['stars'], row['text'].replace('\\n', '')])\n```", "```py\n$ cat data/yelp/yelp_review.csv | \\\n python parse_yelp_dataset.py \\\n > data/yelp/yelp_review.v1.csv\n```", "```py\n$ python -c \"import nltk; nltk.download('stopwords')\"\n```", "```py\nimport io\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport sys\ndef get_lines():\n    lines = sys.stdin.readlines()\n    for line in lines:\n        yield line\nstop_words = set(stopwords.words('english'))\nfor line in get_lines():\n    words = line.lower().split()\n    newwords = [w for w in words if w not in stop_words]\n    print(' '.join(newwords))\n```", "```py\n$ cat data/yelp/yelp_review.v1.csv \\\n    | tr '[:upper:]' '[:lower:]' \\\n    > data/yelp/yelp_review.v2.csv\n```", "```py\n$  cat data/yelp/yelp_review.v2.csv \\\n    | sed -e 's/^/__label__/g' \\\n    > data/yelp/yelp_review.v3.csv\n```", "```py\n$ cat data/yelp/yelp_review.v3.csv \\\n    | sed -e \"s/'/ ' /g\" \\\n -e 's/\"//g' -e 's/\\./ \\. /g' -e 's/<br \\/>/ /g' \\\n -e 's/,/ , /g' -e 's/(/ ( /g' -e 's/)/ ) /g' \\\n     -e 's/\\!/ \\! /g' \\\n -e 's/\\?/ \\? /g' -e 's/\\;/ /g' \\\n     -e 's/\\:/ /g' > data/yelp/yelp_review.v4.csv\n```", "```py\n$ head -n 2 data/yelp/yelp_review.v4.csv\n __label__5 , super simple place but amazing nonetheless . it ' s been around since the 30 ' s and they still serve the same thing they started with a bologna and salami sandwich with mustard . staff was very helpful and friendly .\n __label__5 , small unassuming place that changes their menu every so often . cool decor and vibe inside their 30 seat restaurant . call for a reservation . we had their beef tartar and pork belly to start and a salmon dish and lamb meal for mains . everything was incredible ! i could go on at length about how all the listed ingredients really make their dishes amazing but honestly you just need to go . a bit outside of downtown montreal but take the metro out and it ' s less than a 10 minute walk from the station .\n```", "```py\ncat data/yelp/yelp_review.v4.csv | sed 's/\\,//g' > data/yelp/yelp_review.v5.csv\ncat data/yelp/yelp_review.v5.csv | sed 's/\\.//g' > data/yelp/yelp_review.v6.csv\nmv data/yelp/yelp_review.v6.csv data/yelp/yelp_review.v5.csv\n```", "```py\n$ cat data/yelp/yelp_review.v5.csv | tr -s \" \" > data/yelp/yelp_review.v6.csv\n```", "```py\n$ cat data/yelp/yelp_review.v6.csv | shuf > data/yelp/yelp_review.v7.csv\n```", "```py\n$ perl -MList::Util -e 'print List::Util::shuffle <>' \\\n    data/yelp/yelp_review.v6.csv \\\n    > data/yelp/yelp_review.v8.csv\n```", "```py\n$ awk -v lines=$(wc -l < data/yelp/yelp_review.v9.csv) \\\n    -v fact=0.80 \\\n    'NR <= lines * fact {print > \"train.txt\"; next} {print > \"val.txt\"}' \\\n    data/yelp/yelp_review.v9.csv\n```", "```py\n$ fasttext supervised -input data/yelp/train.txt -output result/yelp/star_model\n```", "```py\nRead 563M words\n Number of words: 1459620\n Number of labels: 5\n Progress: 100.0% words/sec/thread: 3327124 lr: 0.000000 loss: 0.789366 ETA: 0h 0m\n```", "```py\n1459620 100\n . -0.080999 ... -0.029536\n the -0.022696 ... 0.084717\n```", "```py\n$ fasttext test result/yelp/star_model.bin data/yelp/val.txt\n Now    1052334\n P@1    0.685\n R@1    0.685\n Number of examples: 1052334\n```", "```py\n$ mv data/yelp/val.testlabel data/yelp/val.testsentences\n$ cut -f 1 -d ' ' data/yelp/val.txt > data/yelp/val.testlabel\n$ cut -f 2- -d ' ' data/yelp/val.txt > data/yelp/val.testsentences\n$ fasttext predict result/yelp/star_model.bin data/yelp/val.testsentences > pexp\n$ python fasttext_confusion_matrix.py data/yelp/val.testlabel pexp\n Accuracy: 0.716503505541\n [[124224 16161 3347 864 1639]\n [ 24537 39460 19435 2748 1283]\n [ 5514 15646 63775 32668 5424]\n [ 1445 1815 22583 139956 78795]\n [ 1707 548 3071 59103 386586]]\n```", "```py\nimport argparse\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\ndef parse_labels(path):\n    with open(path, 'r') as f:\n        return np.array(list(map(lambda x: x[9:], f.read().split())))\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description='Display confusion matrix.')\n    parser.add_argument('test', help='Path to test labels')\n    parser.add_argument('predict', help='Path to predictions')\n    args = parser.parse_args()\n    test_labels = parse_labels(args.test)\n    pred_labels = parse_labels(args.predict)\n    eq = test_labels == pred_labels\n    print(\"Accuracy: \" + str(eq.sum() / len(test_labels)))\n    print(confusion_matrix(test_labels, pred_labels))\n```", "```py\n$ fasttext supervised -input data/yelp/train.txt -output result/yelp/star_model -epoch 25\n Read 563M words\n Number of words: 1459620\n Number of labels: 5\n Progress: 100.0% words/sec/thread: 3451048 lr: 0.000000 loss: 0.761496 ETA: 0h 0m\n```", "```py\nP@1 0.686\nR@1 0.686\n```", "```py\n$ fasttext supervised -input data/yelp/train.txt -output result/yelp/star_model -lr 1.0\n Read 563M words\n Number of words: 1459620\n Number of labels: 5\n Progress: 100.0% words/sec/thread: 3381014 lr: 0.000000 loss: 0.847610 ETA: 0h 0m\n```", "```py\nP@1 0.686\nR@1 0.686\n```", "```py\n$ fasttext supervised -input data/yelp/train.txt -output result/yelp/star_model -wordNgrams 2\n Read 563M words\n Number of words: 1459620\n Number of labels: 5\n Progress: 100.0% words/sec/thread: 1141636 lr: 0.000000 loss: 0.687991 ETA: 0h 0m\n```", "```py\n$ fasttext supervised -input data/yelp/train.txt -output result/yelp/star_model -wordNgrams 3\n Read 563M words\n Number of words: 1459620\n Number of labels: 5\n Progress: 100.0% words/sec/thread: 620672 lr: 0.000000 loss: 0.633638 ETA: 0h 0m\n $ fasttext test result/yelp/star_model.bin data/yelp/val.txt\n Now1052334\n P@1   0.717\n R@1   0.717\n Number of examples: 1052334\n```", "```py\n$ fasttext supervised -input data/yelp/train.txt -output result/yelp/star_model -wordNgrams 2 -lr 1.0 -epoch 10\n```", "```py\n$ fasttext supervised -input data/yelp/train.txt -output result/yelp/star_model_withprevecs -pretrainedVectors wiki-news-300d-1M.vec -dim 300\n Read 563M words\n Number of words: 1459620\n Number of labels: 5\n Progress: 100.0% words/sec/thread: 1959282 lr: 0.000000 loss: 0.788021 ETA: 0h 0m\n```", "```py\ndim=(10 20)\nlr=(0.1 0.3)\nepochs=(5 10)\n```", "```py\nfinal=(0 0 0)\nperformance=0\n```", "```py\nfor z in ${dim[@]}\ndo\n    for y in ${lr[@]}\n    do\n        for x in ${epochs[@]}\n        do\n            # train with the current set of parameters\n            ...\n\n            # test the current model\n            ...\n\n            # see if current model is the best model and update the final variable.\n            ...\n\n        done\n    done\ndone\n```", "```py\n$ ./fasttext supervised -input train.txt -output model -dim \"$z\" -lr \"$y\" -epoch \"$x\"\n```", "```py\n$ ./fasttext test model.bin test.txt > performance.txt\n```", "```py\npresent_performance=$(cat performance.txt | awk 'NR==2 {print $2}') # get the precision values\nif (( $(echo \"$present_performance > $performance\" | bc -l) )); then\n    # if current performance is the best performance till date\n    final[0]=\"$z\"\n    final[1]=\"$y\"\n    final[2]=\"$x\"\n    echo \"Performance values changed to ${final[@]}\"\n    echo \"present accuracy:\"\n    cat performance.txt\nfi\n```", "```py\n$ DATADIR=data\n$ RESULTDIR=result\n$ ./fasttext supervised -input \"${DATADIR}/train.txt\" -output \"${RESULTDIR}/model\" -dim 10 -lr 0.1 -wordNgrams 2 -minCount 1 -bucket 10000000 -epoch 5 -thread 4\nRead 298M words\nNumber of words: 1454893\nNumber of labels: 5\nProgress: 100.0% words/sec/thread: 2992746 lr: 0.000000 loss: 0.634722 eta: 0h0m\n$ fastText-0.1.0 git:(master) ./fasttext quantize -output \"${RESULTDIR}/model\" -input \"${DATADIR}/train.txt\" -qnorm -retrain -epoch 1 -cutoff 100000\nProgress: 100.0% words/sec/thread: 2382426 lr: 0.000000 loss: 0.711356 eta: 0h0m h-14m\n```", "```py\n$ du -sh $RESULTDIR/model.bin\n466M result/yelp/model.bin\n$ du -sh $RESULTDIR/model.ftz\n1.6M result/yelp/model.ftz\n```", "```py\n$ ./fasttext test $RESULTDIR/model.bin $DATADIR/val.txt\nN 1052334\nP@1 0.699\nR@1 0.699\nNumber of examples: 1052334\n$ ./fasttext test $RESULTDIR/model.ftz $DATADIR/val.txt\nN 1052334\nP@1 0.7\nR@1 0.7\nNumber of examples: 1052334\n```", "```py\n$ fasttext dump result/yelp/star_model.bin args\n dim 100\n ws 5\n epoch 5\n minCount 1\n neg 5\n wordNgrams 2\n loss softmax\n model sup\n bucket 2000000\n minn 0\n maxn 0\n lrUpdateRate 100\n t 0.0001\n```", "```py\n$ fasttext dump result/yelp/star_model.bin dict > dumpdict\n$ ls -lhrt dumpdict\n -rw-rw-r-- 1 joydeep joydeep 27M Apr 2 11:05 dumpdict\n$ head dumpdict\n 1459625\n. 34285218 word\nthe 23539464 word\n, 16399539 word\nand 16299051 word\ni 14330427 word\na 12034982 word\nto 11508988\n' 8907643 word\nwas 8272495 word\n$ tail dumpdict\nm&m\\/chocolate 1 word\ndrops-surprisingly 1 word\ncrinkle-also 1 word\ncookie-humungo 1 word\ndishes\\/restaurants 1 word\n__label__5 1802332 label\n__label__4 978722 label\n__label__1 585128 label\n__label__3 492454 label\n__label__2 350698 label\n```", "```py\n$ fasttext dump result/yelp/star_model.bin input > dumpinput\n $ cat dumpinput | wc -l\n 3459621\n $ fasttext dump result/yelp/star_model.bin output > dumpoutput\n $ cat dumpoutput | wc -l\n 5\n```", "```py\n$ bash get-wikimedia.sh\n Saving data in data/wikimedia/20180402\n Choose a language (e.g. en, bh, fr, etc.): ja\n Chosen language: ja\n Continue to download (WARNING: This might be big and can take a long time!)(y/n)? y\n Starting download...\n --2018-04-02 19:32:40-- https://dumps.wikimedia.org/jawiki/latest/jawiki-latest-pages-articles.xml.bz2\n Resolving dumps.wikimedia.org (dumps.wikimedia.org)... 208.80.154.11, 2620:0:861:1:208:80:154:11\n Connecting to dumps.wikimedia.org (dumps.wikimedia.org)|208.80.154.11|:443... connected.\n HTTP request sent, awaiting response... 200 OK\n Length: 2656121742 (2.5G) [application/octet-stream]\n Saving to: ‘data/wikimedia/20180402/jawiki-latest-pages-articles.xml.bz2’\n\n jawiki-latest-pages-articles.xml.bz 0%[\n```", "```py\nbzip2 -d enwiki-20170820-pages-articles.xml.bz2\n```", "```py\nperl wikifil.pl data/enwik9 > data/fil9\n```", "```py\nfind data/jap_wiki/cleaned_jap_text.txt/ -type f -exec cat {} \\; | python japanese_parser.py -d /var/lib/mecab/dic/ipadic-utf8 > data/jap_wiki_parsed.txt\n```", "```py\n$ cat data/jap_wiki_parsed.txt | sed \"s/^.*https.*$//g\" > \n  data/jap_wiki_parsed1.txt\n $ cat data/jap_wiki_parsed1.txt | tr '[:upper:]' '[:lower:]' > data/jap_wiki_parsed2.txt\n $ cat data/jap_wiki_parsed2.txt | sed   \n  \"s/[abcdefghijklmnopqrstuvwxyz]//g\" > data/jap_wiki_parsed3.txt\n $ cat data/jap_wiki_parsed3.txt | tr -s \" \" >             \n   data/jap_wiki_parsed4.txt\n $ cat data/jap_wiki_parsed4.txt | awk 'NF' | awk '{$1=$1;print}' >  \n   data/jap_wiki_parsed5.txt\n```", "```py\n$ mkdir -p result/jap_wiki\n fasttext skipgram -input data/jap_wiki_parsed5.txt -output    \n  result/jap_wiki/jap\n```", "```py\n$ fasttext skipgram -input data/jap_wiki/fil_cleaned_jap_text3.txt - \n  output result/jap_wiki/jap\n```", "```py\n$ fasttext cbow -input data/jap_wiki/fil_cleaned_jap_text3.txt -output \n  result/jap_wiki/jap_cbow\n```", "```py\n$ fasttext nn result/jap_wiki/jap.bin\n Pre-computing word vectors... done.\n Query word? ![](img/00009.jpeg)\n ![](img/00010.jpeg) 0.949732\n ![](img/00011.jpeg) 0.945276\n ![](img/00012.jpeg) 0.943492\n```", "```py\n$ fasttext analogies result/jap_wiki/jap.bin\n Query triplet (A - B + C)? ![](img/00014.jpeg) (man woman king)\n ![](img/00015.jpeg) 0.856853\n ![](img/00016.jpeg) 0.855393\n ![](img/00017.jpeg) 0.852085\n```", "```py\n$ fasttext print-word-vectors wiki.it.300.bin < oov_words.txt\n```", "```py\n$ fasttext supervised -input train.txt -output model -epoch 25 - \n  wordNgrams 2 -dim 300 -loss hs -thread 7 -minCount 1 -lr 1.0 -verbose   \n  2 -pretrainedVectors wiki.ru.vec\n```"]