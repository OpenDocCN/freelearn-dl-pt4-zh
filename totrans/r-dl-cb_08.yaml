- en: Application of Deep Learning in Text Mining
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The current chapter we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Performing preprocessing of textual data and extraction of sentiments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing documents using tf-idf
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing sentiment prediction using LSTM network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application using text2vec examples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing preprocessing of textual data and extraction of sentiments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will use Jane Austen's bestselling novel Pride and Prejudice,
    published in 1813, for our textual data preprocessing analysis. In R, we will
    use the `tidytext` package by Hadley Wickham to perform tokenization, stop word
    removal, sentiment extraction using predefined sentiment lexicons, **term frequency
    - inverse document frequency** (**tf-idf**) matrix creation, and to understand
    pairwise correlations among *n*-grams.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, instead of storing text as a string or a corpus or a **document
    term matrix** (**DTM**), we process them into a tabular format of one token per
    row.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is how we go about preprocessing:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the required packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the `Pride and Prejudice` dataset. The `line_num` attribute is analogous
    to the line number printed in the book:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, perform tokenization to restructure the one-string-per-row format to a
    one-token-per-row format. Here, the token can refer to a single word, a group
    of characters, co-occurring words (*n*-grams), sentences, paragraphs, and so on.
    Currently, we will tokenize sentence into singular words:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, remove the commonly occurring words such as *the*, *and*, *for*, and
    so on using the `stop words` removal corpus:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the most common textual words used:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Visualize the top 10 common occurring words, as shown in the following figure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00139.gif)'
  prefs: []
  type: TYPE_IMG
- en: Top 10 common words
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Then, extract sentiments at a higher level (that is positive or negative) using
    the `bing` lexicon.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Visualize the sentiments across small sections (150 words) of text, as shown
    in the following figure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00088.gif)'
  prefs: []
  type: TYPE_IMG
- en: Distribution of the number of positive and negative words across sentences of
    150 words each
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now extract sentiments at a granular level (namely positive, negative, anger,
    disgust, surprise, trust, and so on.) using the `nrc` lexicon:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Visualize the variation across different sentiments defined, as shown in the
    following figure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00121.gif)'
  prefs: []
  type: TYPE_IMG
- en: Variation across different types of sentiments
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the most occurring positive and negative words based on the `bing`
    lexicon, and visualize them as shown in the following figure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00015.gif)'
  prefs: []
  type: TYPE_IMG
- en: Top 10 positive and negative words in the novel Pride and Prejudice
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Generate a sentiment word cloud as shown in the following figure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00127.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Word cloud of positive and negative words
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now analyze sentiments across the chapters of the book:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Extract the chapters, and perform tokenization:'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the set `positive` and `negative` words from the `bing` lexicon:'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the count of words for each chapter:'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the ratio of positive and negative words:'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Generate a sentiment flag for each chapter based on the proportion of positive
    and negative words:'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned earlier, we have used Jane Austen's famous novel *Pride and Prejudice*
    in this section, detailing the steps involved in tidying the data, and extracting
    sentiments using (publicly) available lexicons.
  prefs: []
  type: TYPE_NORMAL
- en: Steps 1 and 2 show the loading of the required `cran` packages and the required
    text. Steps 3 and 4 perform unigram tokenization and stop word removal. Steps
    5 and 6 extract and visualize the top 10 most occurring words across all the 62
    chapters. Steps 7 to 12 demonstrate high and granular-level sentiments using two
    widely used lexicons `bing` and `nrc`.
  prefs: []
  type: TYPE_NORMAL
- en: Both the lexicons contains a list of widely used English words that are tagged
    to sentiments. In `bing`, each word is tagged to one of the high level binary
    sentiments (positive or negative), and in `nrc`, each word is tagged to one of
    the granular-level multiple sentiments (positive, negative, anger, anticipation,
    joy, fear, disgust, trust, sadness, and surprise).
  prefs: []
  type: TYPE_NORMAL
- en: Each 150-word-long sentence is tagged to a sentiment, and the same has been
    shown in the figure showing the *Distribution of number of positive and negative
    words across sentences of 150 words each*. In step 13, chapter-wise sentiment
    tagging is performed using maximum occurrence of positive or negative words from
    the `bing` lexicon. Out of 62 chapters, 52 have more occurrences of positive lexicons,
    and 10 have more occurrences of negative lexicons.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing documents using tf-idf
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn how to analyze documents quantitatively. A simple
    way is to look at the distribution of unigram words across the document and their
    frequency of occurrence, also termed as **term frequency** (**tf**). The words
    with higher frequency of occurrence generally tend to dominate the document.
  prefs: []
  type: TYPE_NORMAL
- en: However, one would disagree in case of generally occurring words such as the,
    is, of, and so on. Hence, these are removed by stop word dictionaries. Apart from
    these stop words, there might be some specific words that are more frequent with
    less relevance. Such kinds of words are penalized using their **inverse document
    frequency** (**idf**) values. Here, the words with higher frequency of occurrence
    are penalized.
  prefs: []
  type: TYPE_NORMAL
- en: The statistic tf-idf combines these two quantities (by multiplication) and provides
    a measure of importance or relevance of each word for a given document across
    multiple documents (or a corpus).
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will generate a tf-idf matrix across chapters of the book
    *Pride and Prejudice*.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is how we go about analyzing documents using tf-idf:'
  prefs: []
  type: TYPE_NORMAL
- en: Extract the text of all 62 chapters in the book `Pride and Prejudice`. Then,
    return chapter-wise occurrence of each word. The total words in the book are approx
    1.22M.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Calculate the rank of words such that the most frequently occurring words have
    lower ranks. Also, visualize the term frequency by rank, as shown in the following
    figure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00016.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: This figure shows lower ranks for words with higher term-frequency (ratio) value
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Calculate the `tf-idf` value for each word using the `bind_tf-idf` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract and visualize the top 15 words with higher values of tf-idf, as shown
    in the following figure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00018.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: tf-idf values of top 15 words
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned earlier, one can observe that the tf-idf scores of very common
    words such as *the* are close to zero and those of fewer occurrence words such
    as the proper noun *Austen* is close to one.
  prefs: []
  type: TYPE_NORMAL
- en: Performing sentiment prediction using LSTM network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will use LSTM networks to perform sentiment analysis. Along
    with the word itself, the LSTM network also accounts for the sequence using recurrent
    connections, which makes it more accurate than a traditional feed-forward neural
    network.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we shall use the `movie reviews` dataset `text2vec` from the `cran` package.
    This dataset consists of 5,000 IMDb movie reviews, where each review is tagged
    with a binary sentiment flag (positive or negative).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is how you can proceed with sentiment prediction using LSTM:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the required packages and movie reviews dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Extract the movie reviews and labels as a dataframe and matrix respectively.
    In movie reviews, add an additional attribute `"Sno"` denoting the review number.
    In the labels matrix, add an additional attribute related to `negative flag`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Extract all the unique words across the reviews, and get their count of occurrences
    (*n*). Also, tag each word with a unique integer (`orderNo`). Thus, each word
    is encoded using a unique integer, which shall be later used in the LSTM network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, assign the tagged words back to the reviews based on their occurrences:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the outcome of step 4, create a list of reviews with each review transformed
    into a set of encoded numbers representing those words:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: In order to facilitate equal-length sequences to the LSTM network, let's restrict
    the review length to 150 words. In other words, reviews longer than 150 words
    will be truncated to the first 150, whereas shorter reviews will be made 150 words
    long by prefixing with the required number of zeroes. Thus, we now add in a new
    word **0**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now split the 5,000 reviews into training and testing reviews using a 70:30
    split ratio. Also, bind the list of train and test reviews row wise into a matrix
    format, with rows representing reviews and columns representing the position of
    a word:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, also split the labels into train and test accordingly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Reset the graph, and start an interactive TensorFlow session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Define model parameters such as size of input pixels (`n_input`), step size
    (`step_size`), number of hidden layers (`n.hidden`), and number of outcome classes
    (`n.classes`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Define training parameters such as learning rate (`lr`), number of inputs per
    batch run (`batch`), and number of iterations (`iteration`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Based on the RNN and LSTM functions defined in [Chapter 6](part0248.html#7CGBG1-a0a93989f17f4d6cb68b8cfd331bc5ab),
    *Recurrent Neural Networks,* from the section *Run the optimization post initializing
    a session using global variables initializer*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the reduction in training errors across iterations as shown in the following
    figure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00118.gif)'
  prefs: []
  type: TYPE_IMG
- en: Distribution of sentiment prediction error of training dataset
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the error of test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In steps 1 to 8, the movie reviews dataset is loaded, processed, and transformed
    into a set of train and test matrices, which can be directly used to train an
    LSTM network. Steps 9 to 14 are used to run LSTM using TensorFlow, as described
    in [Chapter 6](part0248.html#7CGBG1-a0a93989f17f4d6cb68b8cfd331bc5ab), *Recurrent
    Neural Networks*. The figure *Distribution of sentiment prediction error of training
    dataset* shows the decline in training errors across 500 iterations.
  prefs: []
  type: TYPE_NORMAL
- en: Application using text2vec examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will analyze the performance of logistic regression on various
    examples of `text2vec`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is how we apply `text2vec`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the required packages and dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Function to perform Lasso logistic regression, and return the train and test
    `AUC` values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Split the movies review data into train and test in an 80:20 ratio:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Generate a DTM of all vocabulary words (without any stop word removal), and
    asses its performance using Lasso logistic regression:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform pruning using a list of stop words, and then assess the performance
    using Lasso logistic regression:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Generate a DTM using *n*-grams (uni and bigram words), and then assess the
    performance using Lasso logistic regression:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform feature hashing, and then asses the performance using Lasso logistic
    regression:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Using tf-idf transformation on full vocabulary DTM, assess the performance
    using Lasso logistic regression:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Steps 1 to 3 loads necessary packages, datasets, and functions required to assess
    different examples of `text2vec`. Logistic regression is implemented using the
    `glmnet` package with L1 penalty (Lasso regularization). In step 4, a DTM is created
    using all the vocabulary words present in the train movie reviews, and the test
    `auc` value is 0.918\. In step 5, the train and test DTMs are pruned using stop
    words and frequency of occurrence.
  prefs: []
  type: TYPE_NORMAL
- en: The test `auc` value is observed as 0.916, not much decrease compared to using
    all the vocabulary words. In step 6, along with single words (or uni-grams), bi-grams
    are also added to the vocabulary. The test `auc` value increases to 0.928\. Feature
    hashing is then performed in step 7, and the test `auc` value is 0.895\. Though
    the `auc` value reduced, hashing is meant to improve run-time performance of larger
    datasets. Feature hashing is widely popularized by Yahoo. Finally, in step 8,
    we perform tf-idf transformation, which returns a test `auc` value of 0.907.
  prefs: []
  type: TYPE_NORMAL
