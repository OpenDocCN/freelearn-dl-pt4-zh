<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Natural Language Processing</h1>
                </header>
            
            <article>
                
<p>Automatic speech recognition has a lot of potential applications, such as audio transcription, dictation, audio search, and virtual assistants. I am sure that everyone has interacted with at least one of the virtual assistants by now, be it Apple's Siri, Amazon's Alexa, or Google's Assistant. At the core of all these speech recognition systems are a set of statistical models over the different words or sounds in a language. And since speech has a temporal structure, HMMs are the most natural framework to model it. </p>
<p>HMMs are virtually at the core of all speech recognition systems and the core concepts in modeling haven't changed much in a long time. But over time, a lot of sophisticated techniques have been developed to build better systems. In the following sections, we will try to cover the main concepts leading to the development of these systems.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Part-of-speech tagging</h1>
                </header>
            
            <article>
                
<p>The first problem that we will look into is known as <strong>part-of-speech tagging </strong><span>(</span><strong>POS tagging</strong><span>)</span>. According to Wikipedia, POS tagging, also known as <strong>grammatical tagging</strong> or <strong>word-category disambiguation</strong>, is the process of marking up a word in a text as corresponding to a particular part of speech based on both its definition and its context, that is, its relationship with adjacent and related words in a phrase, sentence, or paragraph. A simpler version of this, which is usually taught in schools, is classifying words as noun, verbs, adjectives, and so on. </p>
<p>POS tagging is not as easy as it sounds because the same word can take different parts of speech in different contexts. A simple example of this is the word <em>dogs</em>. The word <em>dogs</em> is usually considered a noun, but in the following sentence, it acts like a verb:</p>
<p class="CDPAlignCenter CDPAlign"><em>The sailor dogs the hatch</em>.</p>
<p class="CDPAlignLeft CDPAlign"><span>Correct grammatical tagging will reflect that <em>dogs</em> is used as a verb here, not as the more common plural noun. Grammatical context is one way to determine this; semantic analysis can also be used to infer that <em>sailor</em> and <em>hatch</em> implicate <em>dogs</em> as:</span></p>
<ul>
<li class="CDPAlignLeft CDPAlign"><span>In the nautical context</span></li>
<li class="CDPAlignLeft CDPAlign"><span>An action applied to the object <em>hatch</em></span></li>
</ul>
<p>In teaching English, generally only nine parts of speech are taught: noun, verb, article, adjective, preposition, pronoun, adverb, conjunction, and interjection. But we can divide words into more categories and subcategories for finer-grained tagging. For example, nouns can be sub-categorized into plural, possessive, and singular. Similarly, verbs can be sub-categorized on the basis of tense, aspect, and so on. In general, computer-based POS tagging systems are able to distinguish 50 to 150 separate parts of speech for English. Work on stochastic<span> </span>methods for tagging Koine Greek has used over 1,000 parts of speech and found that about as many words were ambiguous<span> </span>there as in English.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Code</h1>
                </header>
            
            <article>
                
<p><span>For the code example, we will use the <kbd>pomegranate</kbd></span><span> library to build an HMM for POS tagging. Pomegranate can be installed by running the following on the command line:</span></p>
<p class="mce-root"/>
<pre><strong>pip install pomegranate</strong></pre>
<p><span>In this example, we will not go into the details of the statistical POS tagger. The data we are using is a copy of the Brown corpus. The Brown corpus contains 500 samples of English-language text, totaling roughly 1,000,000 words, compiled from works published in the United States in 1961.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting data</h1>
                </header>
            
            <article>
                
<p>Let's start by defining some functions to read the data from the <kbd>corpus</kbd> files:</p>
<pre class="graf graf--pre graf-after--p"># Imports <br/>import random<br/>from itertools import chain<br/>from collections import Counter, defaultdict<br/><br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>from pomegranate import State, HiddenMarkovModel, DiscreteDistribution<br/><br/><br/>Sentence = namedtuple("Sentence", "words tags")<br/><br/><br/>def read_data(filename):<br/>    """<br/>    Function to read tagged sentence data.<br/><br/>    Parameters<br/>    ----------<br/>    filename: str<br/>        The path to the file from where to read the data.<br/><br/>    <br/>    """<br/>    with open(filename, 'r') as f:<br/>        sentence_lines = [l.split("\n") for l in f.read().split("\n\n")]<br/>    return OrderedDict(((s[0], Sentence(*zip(*[l.strip().split("\t") for l in s[1:]]))) <br/>                               for s in sentence_lines if s[0])<br/><br/>def read_tags(filename):<br/>    """<br/>    Function to read a list of word tag classes.<br/><br/>    Parameters<br/>    ----------<br/>    filename: str<br/>        The path to the file from where to read the tags.<br/>    """<br/>    with open(filename, 'r') as f:<br/>        tags = f.read().split("\n")<br/>    return frozenset(tags)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Let's now define a couple of classes, <kbd>Subset</kbd> and <kbd>Dataset</kbd>, to make it easier to handle the data:</p>
<pre class="graf graf--pre graf-after--p">class Subset(namedtuple("BaseSet", "sentences keys vocab X tagset Y N stream")):<br/>    """<br/>    Class to handle a subset of the whole data. This is required when we split the<br/>    data into training and test sets.<br/>    """<br/>    def __new__(cls, sentences, keys):<br/>        word_sequences = tuple([sentences[k].words for k in keys])<br/>        tag_sequences = tuple([sentences[k].tags for k in keys])<br/>        wordset = frozenset(chain(*word_sequences))<br/>        tagset = frozenset(chain(*tag_sequences))<br/>        N = sum(1 for _ in chain(*(sentences[k].words for k in keys)))<br/>        stream = tuple(zip(chain(*word_sequences), chain(*tag_sequences)))<br/>        return super().__new__(cls, {k: sentences[k] for k in keys}, keys, wordset,             <br/>                               word_sequences, tagset, tag_sequences, N, stream.__iter__)<br/><br/>    def __len__(self):<br/>        return len(self.sentences)<br/><br/>    def __iter__(self):<br/>        return iter(self.sentences.items())<br/><br/>class Dataset(namedtuple("_Dataset", "sentences keys vocab X tagset Y" + <br/>                                     "training_set testing_set N stream")):<br/>    """<br/>    Class to represent the data in structured form for easy processing.<br/>    """<br/>    def __new__(cls, tagfile, datafile, train_test_split=0.8, seed=112890):<br/>        tagset = read_tags(tagfile)<br/>        sentences = read_data(datafile)<br/>        keys = tuple(sentences.keys())<br/>        wordset = frozenset(chain(*[s.words for s in sentences.values()]))<br/>        word_sequences = tuple([sentences[k].words for k in keys])<br/>        tag_sequences = tuple([sentences[k].tags for k in keys])<br/>        N = sum(1 for _ in chain(*(s.words for s in sentences.values())))<br/> <br/>        # split data into train/test sets<br/>        _keys = list(keys)<br/>        <br/>        if seed is not None:<br/>            random.seed(seed)<br/>        <br/>        random.shuffle(_keys)<br/>        split = int(train_test_split * len(_keys))<br/>        training_data = Subset(sentences, _keys[:split])<br/>        testing_data = Subset(sentences, _keys[split:])<br/>        stream = tuple(zip(chain(*word_sequences), chain(*tag_sequences)))<br/>        return super().__new__(cls, dict(sentences), keys, wordset, word_sequences, tagset,<br/>                               tag_sequences, training_data, testing_data, N, stream.__iter__)<br/><br/>    def __len__(self):<br/>        return len(self.sentences)<br/><br/>    def __iter__(self):<br/>        return iter(self.sentences.items())</pre>
<p>Now, let's try to initialize the <kbd>Dataset</kbd> class and see how it works:</p>
<pre class="graf graf--pre graf-after--p">&gt;&gt;&gt; data = Dataset("tags-universal.txt", "brown-universal.txt", train_test_split=0.8)<br/><br/>&gt;&gt;&gt; print("There are {} sentences in the corpus.".format(len(data)))<br/>There are 57340 sentences in the corpus.<br/>&gt;&gt;&gt; print("There are {} sentences in the training set.".format(len(data.training_set)))<br/>There are 45872 sentences in the training set.<br/>&gt;&gt;&gt; print("There are {} sentences in the testing set.".format(len(data.testing_set)))<br/>There are 11468 sentences in the testing set.</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring the data</h1>
                </header>
            
            <article>
                
<p>Let's now explore the data to better understand how our classes store the information. We have randomly selected the <kbd>b100-38532</kbd> key:</p>
<pre class="graf graf--pre graf-after--p">&gt;&gt;&gt; key = 'b100-38532'<br/>&gt;&gt;&gt; print("Sentence: {}".format(key))<br/>Sentence: b100–38532<br/>&gt;&gt;&gt; print("words: {!s}".format(data.sentences[key].words))\<br/>words: ('Perhaps', 'it', 'was', 'right', ';', ';')<br/>&gt;&gt;&gt; print("tags: {!s}".format(data.sentences[key].tags))<br/>tags: ('ADV', 'PRON', 'VERB', 'ADJ', '.', '.')</pre>
<p>We can also check the unique elements in the <kbd>corpus</kbd>:</p>
<pre class="graf graf--pre graf-after--p">&gt;&gt;&gt; print("There are a total of {} samples of {} unique words in the corpus.".format(<br/>              data.N, len(data.vocab)))<br/>There are a total of 1161192 samples of 56057 unique words in the corpus.<br/><br/>&gt;&gt;&gt; print("There are {} samples of {} unique words in the training set.".format(<br/>              data.training_set.N, len(data.training_set.vocab)))<br/>There are 928458 samples of 50536 unique words in the training set.<br/><br/>&gt;&gt;&gt; print("There are {} samples of {} unique words in the testing set.".format(<br/>              data.testing_set.N, len(data.testing_set.vocab)))<br/>There are 232734 samples of 25112 unique words in the testing set.<br/>&gt;&gt;&gt; print("There are {} words in the test set that are missing in the training set.".format(<br/>              len(data.testing_set.vocab - data.training_set.vocab)))<br/>There are 5521 words in the test set that are missing in the training set.</pre>
<p>We can also use the <kbd>X</kbd> and <kbd>Y</kbd> attributes of the <kbd>Dataset</kbd> class to access the words and the corresponding tags:</p>
<pre class="graf graf--pre graf-after--p">&gt;&gt;&gt; for i in range(2):<br/>...     print("Sentence {}:".format(i + 1), data.X[i])<br/>...     print("Labels {}:".format(i + 1), data.Y[i], "\n")<br/>Sentence 1: ('Mr.', 'Podger', ‘had', 'thanked', 'him', 'gravely', ',', 'and', 'now', 'he', 'made', 'use', 'of', 'the', 'advice', '.')<br/>Labels 1: ('NOUN', 'NOUN', 'VERB', 'VERB', 'PRON', 'ADV', '.', 'CONJ', 'ADV', 'PRON', 'VERB', 'NOUN', 'ADP', 'DET', 'NOUN', '.')<br/><br/>Sentence 2: ('But', 'there', 'seemed', 'to', 'be', 'some', 'difference', 'of', 'opinion', 'as', 'to', 'how', 'far', 'the', 'board', 'should', 'go', ',', 'and', 'whose', 'advice', 'it', 'should', 'follow', '.')<br/>Labels 2: ('CONJ', 'PRT', 'VERB', 'PRT', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'ADP', 'ADP', 'ADV', 'ADV', 'DET', 'NOUN', 'VERB','VERB', '.', 'CONJ', 'DET', 'NOUN', 'PRON', 'VERB', 'VERB', '.')<br/><br/></pre>
<p>We can also use the <kbd>stream</kbd> method to iterate over pairs of a word and its tag:</p>
<pre class="graf graf--pre graf-after--p">&gt;&gt;&gt; for i, pair in enumerate(data.stream()):<br/>...     print(pair)<br/>...     if i &gt; 3:<br/>...         break<br/>('Podger', 'NOUN')<br/>('had', 'VERB')<br/>('thanked', 'VERB')<br/>('him', 'PRON')</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Finding the most frequent tag</h1>
                </header>
            
            <article>
                
<p>Now, just to compare the performance of our HMM model, let's build a <strong>most frequent class tagger</strong> (<strong>MFC</strong> <strong>Tagger</strong>). We start by defining a function to count the pairs of tags and words:</p>
<pre class="graf graf--pre graf-after--p">def pair_counts(tags, words):<br/>    d = defaultdict(lambda: defaultdict(int))<br/>    for tag, word in zip(tags, words):<br/>        d[tag][word] += 1<br/>        <br/>    return d<br/>tags = [tag for i, (word, tag) in enumerate(data.training_set.stream())]<br/>words = [word for i, (word, tag) in enumerate(data.training_set.stream())]</pre>
<p>Now, let's define the <kbd>MFCTagger</kbd> class:</p>
<pre class="graf graf--pre graf-after--p">FakeState = namedtuple('FakeState', 'name')<br/><br/>class MFCTagger:<br/>    missing = FakeState(name = '&lt;MISSING&gt;')<br/>    <br/>    def __init__(self, table):<br/>        self.table = defaultdict(lambda: MFCTagger.missing)<br/>        self.table.update({word: FakeState(name=tag) for word, tag in table.items()})<br/>        <br/>    def viterbi(self, seq):<br/>        """This method simplifies predictions by matching the Pomegranate viterbi() interface"""<br/>        return 0., list(enumerate(["&lt;start&gt;"] + [self.table[w] for w in seq] + ["&lt;end&gt;"]))<br/>    <br/>tags = [tag for i, (word, tag) in enumerate(data.training_set.stream())]<br/>words = [word for i, (word, tag) in enumerate(data.training_set.stream())]<br/><br/>word_counts = pair_counts(words, tags)<br/>mfc_table = dict((word, max(tags.keys(), key=lambda key: tags[key])) for word, tags in word_counts.items())<br/><br/>mfc_model = MFCTagger(mfc_table)</pre>
<p>Here are some helper functions to make predictions from the model:</p>
<pre class="graf graf--pre graf-after--p">def replace_unknown(sequence):<br/>    return [w if w in data.training_set.vocab else 'nan' for w in sequence]<br/><br/>def simplify_decoding(X, model):<br/>    _, state_path = model.viterbi(replace_unknown(X))<br/>    return [state[1].name for state in state_path[1:-1]]</pre>
<pre class="graf graf--pre graf-after--p">&gt;&gt;&gt; for key in data.testing_set.keys[:2]:<br/>...     print("Sentence Key: {}\n".format(key))<br/>...     print("Predicted labels:\n-----------------")<br/>...     print(simplify_decoding(data.sentences[key].words, mfc_model))<br/>...     print()<br/>...     print("Actual labels:\n--------------")<br/>...     print(data.sentences[key].tags)<br/>...     print("\n")</pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0768458e-588b-4b73-834e-d7c7a8e3c4a8.png"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating model accuracy</h1>
                </header>
            
            <article>
                
<p>To check how well our model performs, let's evaluate the accuracy of our model:</p>
<pre class="graf graf--pre graf-after--p">def accuracy(X, Y, model):    <br/>    correct = total_predictions = 0<br/>    for observations, actual_tags in zip(X, Y):<br/>        <br/>        # The model.viterbi call in simplify_decoding will return None if the HMM<br/>        # raises an error (for example, if a test sentence contains a word that<br/>        # is out of vocabulary for the training set). Any exception counts the<br/>        # full sentence as an error (which makes this a conservative estimate).<br/>        try:<br/>            most_likely_tags = simplify_decoding(observations, model)<br/>            correct += sum(p == t for p, t in zip(most_likely_tags, actual_tags))<br/>        except:<br/>            pass<br/>        total_predictions += len(observations)<br/>    return correct / total_predictions</pre>
<pre class="graf graf--pre graf-after--p">&gt;&gt;&gt; mfc_training_acc = accuracy(data.training_set.X, data.training_set.Y, mfc_model)<br/>&gt;&gt;&gt; print("Training accuracy mfc_model: {:.2f}%".format(100 * mfc_training_acc))<br/>Training accuracy mfc_model: 95.72%</pre>
<pre class="graf graf--pre graf-after--pre">&gt;&gt;&gt; mfc_testing_acc = accuracy(data.testing_set.X, data.testing_set.Y, mfc_model)<br/>&gt;&gt;&gt; print("Testing accuracy mfc_model: {:.2f}%".format(100 * mfc_testing_acc))<br/>Testing accuracy mfc_model: 93.01%</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">An HMM-based tagger</h1>
                </header>
            
            <article>
                
<p>Now, we will try to build a POS tagger using HMM and hopefully it will improve our prediction performance. We will first define some helper functions:</p>
<pre class="graf graf--pre graf-after--h3">def unigram_counts(sequences):<br/>    return Counter(sequences)<br/><br/>tags = [tag for i, (word, tag) in enumerate(data.training_set.stream())]<br/>tag_unigrams = unigram_counts(tags)</pre>
<pre class="graf graf--pre graf-after--p">def bigram_counts(sequences):<br/>    d = Counter(sequences)<br/>    return d<br/><br/>tags = [tag for i, (word, tag) in enumerate(data.stream())]<br/>o = [(tags[i],tags[i+1]) for i in range(0,len(tags)-2,2)]<br/>tag_bigrams = bigram_counts(o)</pre>
<pre class="graf graf--pre graf-after--p">def starting_counts(sequences):<br/>    d = Counter(sequences)<br/>    return d<br/><br/>tags = [tag for i, (word, tag) in enumerate(data.stream())]<br/>starts_tag = [i[0] for i in data.Y]<br/>tag_starts = starting_counts(starts_tag)</pre>
<pre class="graf graf--pre graf-after--p">def ending_counts(sequences):<br/>    d = Counter(sequences)<br/>    return d<br/><br/>end_tag = [i[len(i)-1] for i in data.Y]<br/>tag_ends = ending_counts(end_tag)</pre>
<p>Let's build the model now:</p>
<pre class="graf graf--pre graf-after--p">basic_model = HiddenMarkovModel(name="base-hmm-tagger")<br/><br/>tags = [tag for i, (word, tag) in enumerate(data.stream())]<br/>words = [word for i, (word, tag) in enumerate(data.stream())]<br/><br/>tags_count=unigram_counts(tags)<br/>tag_words_count=pair_counts(tags,words)<br/><br/>starting_tag_list=[i[0] for i in data.Y]<br/>ending_tag_list=[i[-1] for i in data.Y]<br/><br/>starting_tag_count=starting_counts(starting_tag_list)#the number of times a tag occurred at the start<br/>ending_tag_count=ending_counts(ending_tag_list)      #the number of times a tag occurred at the end<br/><br/>to_pass_states = []<br/>for tag, words_dict in tag_words_count.items():<br/>    total = float(sum(words_dict.values()))<br/>    distribution = {word: count/total for word, count in words_dict.items()}<br/>    tag_emissions = DiscreteDistribution(distribution)<br/>    tag_state = State(tag_emissions, name=tag)<br/>    to_pass_states.append(tag_state)<br/><br/>basic_model.add_states()<br/><br/>start_prob={} <br/><br/>for tag in tags:<br/>    start_prob[tag]=starting_tag_count[tag]/tags_count[tag]<br/><br/>for tag_state in to_pass_states :<br/>    basic_model.add_transition(basic_model.start,tag_state,start_prob[tag_state.name])<br/><br/>end_prob={}<br/><br/>for tag in tags:<br/>    end_prob[tag]=ending_tag_count[tag]/tags_count[tag]<br/>for tag_state in to_pass_states :<br/>    basic_model.add_transition(tag_state,basic_model.end,end_prob[tag_state.name])<br/><br/>transition_prob_pair={}<br/><br/>for key in tag_bigrams.keys():<br/>    transition_prob_pair[key]=tag_bigrams.get(key)/tags_count[key[0]]<br/>for tag_state in to_pass_states :<br/>    for next_tag_state in to_pass_states :<br/>        basic_model.add_transition(tag_state,next_tag_state,transition_prob_pair[(tag_state.name,next_tag_state.name)])<br/><br/>basic_model.bake()</pre>
<pre class="graf graf--pre graf-after--p">&gt;&gt;&gt; for key in data.testing_set.keys[:2]:<br/>...     print("Sentence Key: {}\n".format(key))<br/>...     print("Predicted labels:\n-----------------")<br/>...     print(simplify_decoding(data.sentences[key].words, basic_model))<br/>...     print()<br/>...     print("Actual labels:\n--------------")<br/>...     print(data.sentences[key].tags)<br/>...     print("\n")</pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/9a73582f-4681-4e84-9d86-3ebf5b0f6d93.png" style="width:57.75em;height:23.67em;"/></p>
<pre class="graf graf--pre graf-after--p">&gt;&gt;&gt; hmm_training_acc = accuracy(data.training_set.X, data.training_set.Y, basic_model)<br/>&gt;&gt;&gt; print("Training accuracy basic hmm model: {:.2f}%".format(100 * hmm_training_acc))<br/>Training accuracy basic hmm model: 97.49%<br/><br/>&gt;&gt;&gt; hmm_testing_acc = accuracy(data.testing_set.X, data.testing_set.Y, basic_model)<br/>&gt;&gt;&gt; print("Testing accuracy basic hmm model: {:.2f}%".format(100 * hmm_testing_acc))<br/>Testing accuracy basic hmm model: 96.09%</pre>
<p class="mce-root"/>
<p>We can see that the HMM-based model has been able to improve the accuracy of our model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Speech recognition</h1>
                </header>
            
            <article>
                
<p>In the 1950s, Bell Labs was the pioneer in speech recognition. The early designed systems were limited to a single speaker and had a very limited vocabulary. After around 70 years of work, the current speech-recognition systems are able to work with speech from multiple speakers and can recognize thousands of words in multiple languages. A detailed discussion of all the techniques used is beyond the scope of this book as enough work has been done on each technique to have a book on itself.</p>
<p>But the general workflow for a speech-recognition system is to first capture the audio by converting the physical sound into an electrical signal using a microphone. The electrical signal generated by the microphone is analog and needs to be converted to a digital form for storage and processing, for which analog-to-digital converters are used. Once we have the speech in digital form, we can apply algorithms on it to understand the speech.</p>
<p>As mentioned before, most of the state-of-the-art speech-recognition systems still use the concept of <strong>Hidden Markov Models</strong> (<strong>HMM</strong>) as their core. This is based on the assumption that a speech signal is a stationary process in a short time period of a few milliseconds. Hence, the first step for the speech-recognition system is to split the signal into small fragments of around 10 milliseconds. Then the power spectrum of each fragment is mapped to a vector of real numbers, known as <strong>cepstral coefficients</strong>. The dimension of this vector is usually small, although more accurate systems usually work with more than 32 dimensions. The final output of the HMM model is a sequence of these vectors. Once we have these vectors, these groups of vectors are matched to one or more phonemes, which are a fundamental unit of speech. But for effectively matching these groups of vectors to phonemes, we need to train our system since there is a huge variation in the sound of phonemes between different speakers as well as different utterances from the same speaker. Once we have the sequence of phonemes, our system tries to guess the most likely word that could have possibly produced that sequence of phonemes.</p>
<p>As we can imagine, this whole detection process can be computationally quite expensive. For dealing with this complexity issue, modern speech-recognition systems use neural networks for feature-transformation and dimensionality-reduction before using the HMM for recognition. Another commonly used technique to reduce computation is to use voice activity detectors, which can detect the regions in the signal that contain speech. Using this information, we can design the recognizer to only spend computation on the parts of the signal that contain speech.</p>
<p>Fortunately, Python has a very developed ecosystem to work with speech recognition. In the next section, we will look into the different Python packages available for working with speech recognition.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Python packages for speech recognition</h1>
                </header>
            
            <article>
                
<p>The Python package hosting service, PyPI, has a lot of speech-recognition systems listed. Some of the most commonly used ones are as follows:</p>
<ul>
<li>SpeechRecognition (<a href="https://pypi.org/project/SpeechRecognition/" target="_blank">https://pypi.org/project/SpeechRecognition/</a>)</li>
<li>apiai (<a href="https://pypi.org/project/apiai/" target="_blank">https://pypi.org/project/apiai/</a>)</li>
<li>assemblyai (<a href="https://pypi.org/project/assemblyai/" target="_blank">https://pypi.org/project/assemblyai/</a>)</li>
<li>pocketsphinx<span> (<a href="https://pypi.org/project/pocketsphinx/" target="_blank">https://pypi.org/project/pocketsphinx/</a>)</span></li>
<li>google-cloud-speech (<a href="https://pypi.org/project/google-cloud-speech/" target="_blank">https://pypi.org/project/google-cloud-speech/</a>)</li>
<li>watson-developer-cloud (<a href="https://pypi.org/project/watson-developer-cloud/" target="_blank">https://pypi.org/project/watson-developer-cloud/</a>)</li>
</ul>
<p>Some of these Python packages (such as <kbd>apiai</kbd>) offer more than just speech recognition and have implementations of natural language processing algorithms, using which the user can identify the speaker's intent from speech. The other packages focus only on speech recognition, which can be used to convert audio to text.</p>
<p>In this chapter, we will use the <kbd>SpeechRecognition</kbd> package. We have chosen <kbd>SpeechRecognition</kbd> for two reasons:</p>
<ul>
<li>It has a very simple-to-use API to directly access and process audio signals. For other packages, we usually need to write small scripts for them to be able to access files.</li>
<li>It is a wrapper over several popular speech APIs and therefore is extremely flexible, and multiple services can be used without making much change to our code.</li>
</ul>
<p>So, to start using <kbd>SpeechRecognition</kbd>, we need to install the package. Since it's hosted on PyPI, it can be installed directly using <kbd>pip</kbd>:</p>
<pre><strong>pip install SpeechRecognition</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Basics of SpeechRecognition</h1>
                </header>
            
            <article>
                
<p class="mce-root CDPAlignLeft CDPAlign">The most important class in the package is the <kbd>Recognizer</kbd> class as it handles most of the recognition tasks. We can specify different settings and functionality for recognizing speech from an audio source while initializing the class. </p>
<p>The <kbd>Recognizer</kbd> class can be initialized very easily without passing any argument:</p>
<div class="highlight python">
<pre><span>&gt;&gt;&gt; import speech_recognition as sr<br/></span><span class="gp">&gt;&gt;&gt; </span><span class="n">r</span> <span class="o">=</span> <span class="n">sr</span><span class="o">.</span><span class="n">Recognizer</span><span class="p">()</span></pre>
<p>Each instance of the <kbd>Recognizer</kbd> class has seven different possible methods that can be used to convert speech to text. Each of these methods uses a specific speech-recognition service. The seven methods are the following:</p>
<ul>
<li><kbd>recognize_bing</kbd>: Uses Microsoft's Bing Speech.</li>
<li><kbd>recognize_google</kbd>: Uses Google's Web Speech API.</li>
<li><kbd>recognize_google_cloud</kbd>: Uses Google's Cloud Speech. Using this method would need <kbd>google-cloud-speech</kbd> to be installed, which can be easily installed through <kbd>pip</kbd><em> </em>by running <kbd><span>pip install google-cloud-speech</span></kbd>.</li>
<li><kbd>recognize_houndify</kbd>: Uses SoundHound's Houndify.</li>
<li><kbd>recognize_ibm</kbd>: Uses IBM's speech to text.</li>
<li><kbd>recognize_sphinx</kbd>: Uses CMU's Sphinx. This method has a dependency on <kbd>PocketSphinx</kbd>, which can be installed by running <kbd>pip install pocketsphinx</kbd>.</li>
<li><kbd>recognize_wit</kbd>: Uses Wit.ai.</li>
</ul>
</div>
<p><span>One important thing to keep in mind while using these methods is that since most of these recognition services are offered by companies through web APIs, we need an internet connection to access these services. Also, some of these services only allow usage after registering with them online. Out of these seven, only <kbd>recognize_sphinx</kbd> works offline.</span></p>
<p>Out of all these web APIs, only Google's Web Speech API works without any registration or API key. Therefore, to keep things simple, we will use that in the rest of this chapter. </p>
<div class="packt_tip">The recognize methods throw <kbd>RequestError</kbd> if the server is unavailable, there is no internet connection, or the API quota limits are met.</div>
<p>The next thing that we would need in order to do any recognition is some audio data. <kbd>SpeechRecognition</kbd> provides direct functionality to either work with an audio file or use audio from an attached microphone. In the following sections, we will look into both of these methods.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Speech recognition from audio files</h1>
                </header>
            
            <article>
                
<p>To start working with audio files, we first need to download one. For the following example, we will use the <kbd>harvard.wav</kbd> file, which can be downloaded from <a href="https://raw.githubusercontent.com/realpython/python-speech-recognition/master/audio_files/harvard.wav" target="_blank">https://raw.githubusercontent.com/realpython/python-speech-recognition/master/audio_files/harvard.wav</a>.</p>
<div class="packt_tip">Make sure to save the audio files in the same directory from where the Python interpreter is running. Otherwise, for the following code, the path to the files will need to be modified.</div>
<p>For working with audio files, <kbd>SpeechRecognition</kbd> has the <kbd>AudioFile</kbd> class, which can be used for reading and working with audio files. We can use the <kbd>record</kbd> method of <kbd>AudioFile</kbd> to process the contents of the audio file before it can be used with the <kbd>Recognizer</kbd> class:</p>
<pre><span class="gp">&gt;&gt;&gt; </span><span class="n">harvard</span> <span class="o">=</span> <span class="n">sr</span><span class="o">.</span><span class="n">AudioFile</span><span class="p">(</span><span class="s1">'harvard.wav'</span><span class="p">)<br/></span><span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">harvard</span> <span class="k">as</span> <span class="n">source</span><span class="p">:<br/></span><span class="gp">... </span>   <span class="n">audio</span> <span class="o">=</span> <span class="n">r</span><span class="o">.</span><span class="n">record</span><span class="p">(</span><span class="n">source</span><span class="p">)</span></pre>
<p>The context manager opens the audio and records its content into <kbd>audio</kbd>, which is an instance of <kbd>AudioFile</kbd>. We can check it by calling the <kbd>type</kbd> method on <kbd>audio</kbd>:</p>
<pre><span class="gp">&gt;&gt;&gt; </span><span class="nb">type</span><span class="p">(</span><span class="n">audio</span><span class="p">)<br/></span><span class="go">&lt;class 'speech_recognition.AudioData'&gt;</span></pre>
<p>Now, once we have an <kbd>AudioFile</kbd> instance, we can call any of the recognize methods with it as an argument. The recognize method would call the specific web API to translate the speech from the audio file and return the following text:</p>
<pre>&gt;&gt;&gt; r.recognize_google(audio)<br/>    'the stale smell of old beer lingers it takes heat<br/>    to bring out the odor a cold dip restores health and<br/>    zest a salt pickle taste fine with ham tacos al<br/>    Pastore are my favorite a zestful food is the hot<br/>    cross bun'</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>In this case, we transcribed the whole audio file, but what if we want to only translate a specific part of the audio file? This can be done by passing additional arguments to the <kbd>record</kbd> method:</p>
<pre><span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">harvard</span> <span class="k">as</span> <span class="n">source</span><span class="p">:<br/></span><span class="gp">... </span>    <span class="n">audio_part</span> <span class="o">=</span> <span class="n">r</span><span class="o">.</span><span class="n">record</span><span class="p">(</span><span class="n">source</span><span class="p">,</span> <span class="n">duration</span><span class="o">=</span><span class="mi">4</span><span class="p">)<br/></span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r</span><span class="o">.</span><span class="n">recognize_google</span><span class="p">(</span><span class="n">audio_part</span><span class="p">)<br/></span><span class="go">'the stale smell of old beer lingers'</span></pre>
<div class="packt_tip">The <kbd>record</kbd> method keeps a pointer in the audio file to point at the position until which recording has happened. So, if we do another record of four seconds on the same file, it will record from the four-second mark to the eight-second mark of the original audio file.</div>
<p>In the preceding example, we transcribed a part of the audio file but the starting point was the start of the file. What if we want to start at a different time point? It can be done by passing another argument, <kbd>offset</kbd>, to the <kbd>record</kbd> method:</p>
<pre><span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">harvard</span> <span class="k">as</span> <span class="n">source</span><span class="p">:<br/></span><span class="gp">... </span>    <span class="n">audio_offset</span> <span class="o">=</span> <span class="n">r</span><span class="o">.</span><span class="n">record</span><span class="p">(</span><span class="n">source</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">duration</span><span class="o">=</span><span class="mi">3</span><span class="p">)<br/></span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">recognizer</span><span class="o">.</span><span class="n">recognize_google</span><span class="p">(</span><span class="n">audio_offset</span><span class="p">)<br/></span><span class="go">'it takes heat to bring out the odor'</span></pre>
<p>If you listen to the <kbd>harvard.wav</kbd> file, you will realize that the recording is done in perfect conditions without any external noise, but that is usually not the case in real-life audio. Let's try to transcribe another audio signal, <kbd>jackhammer.wav</kbd>, which can be downloaded from <a href="https://raw.githubusercontent.com/realpython/python-speech-recognition/master/audio_files/jackhammer.wav" target="_blank">https://raw.githubusercontent.com/realpython/python-speech-recognition/master/audio_files/jackhammer.wav</a>. If you listen to the audio file, you can notice that it has a lot of background noise. Let's try to transcribe this file and see how the recognizer performs:</p>
<pre><span class="gp">&gt;&gt;&gt; </span><span class="n">jackhammer</span> <span class="o">=</span> <span class="n">sr</span><span class="o">.</span><span class="n">AudioFile</span><span class="p">(</span><span class="s1">'jackhammer.wav'</span><span class="p">)<br/></span><span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">jackhammer</span> <span class="k">as</span> <span class="n">source:<br/></span><span class="gp">... </span>    <span class="n">audio_jack</span> <span class="o">=</span> <span class="n">r</span><span class="o">.</span><span class="n">record</span><span class="p">(</span><span class="n">source</span><span class="p">)<br/></span><span class="gp"><br/>&gt;&gt;&gt; </span><span class="n">r</span><span class="o">.</span><span class="n">recognize_google</span><span class="p">(</span><span class="n">audio_jack</span><span class="p">)<br/></span><span class="go">'the snail smell of old gear vendors'</span></pre>
<p>As we can see, the transcription is way off. In such cases, we can use the <kbd>adjust_for_ambient_noise</kbd> method provided in the <kbd>Recognizer</kbd> class to calibrate our audio signal with the noise. </p>
<p class="mce-root"/>
<p class="mce-root"/>
<p><kbd>adjust_for_ambient_noise</kbd> by default uses the first one second of data to do the calibration, but we can change that by passing a <kbd>duration</kbd> argument to it:</p>
<pre><span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">jackhammer</span> <span class="k">as</span> <span class="n">source</span><span class="p">:<br/></span><span class="gp">... </span>    <span class="n">r</span><span class="o">.</span><span class="n">adjust_for_ambient_noise</span><span class="p">(</span><span class="n">source, duration=1</span><span class="p">)<br/></span><span class="gp">... </span>    <span class="n">audio</span> <span class="o">=</span> <span class="n">r</span><span class="o">.</span><span class="n">record</span><span class="p">(</span><span class="n">source</span><span class="p">)<br/></span><span class="gp"><br/>&gt;&gt;&gt; </span><span class="n">r</span><span class="o">.</span><span class="n">recognize_google</span><span class="p">(</span><span class="n">audio</span><span class="p">)<br/></span><span class="go">'still smell of old beer vendors'</span></pre>
<p>If we don't want to lose much information, we can reduce the value of the <kbd>duration</kbd> argument, but that can result in a poorer calibration. As we can see, the transcription is still not perfect, but it is much better than when we didn't use <kbd>adjust_for_ambient_noise</kbd>. We can actually get better results by trying to clean the noise from the audio using signal processing techniques, which are outside the scope of this book.</p>
<p>Another thing that we can do in such cases is to look at all the most likely transcriptions by the recognizer. It can be done by using the <kbd>show_all</kbd> argument while calling the <kbd>recognize</kbd> method:</p>
<pre><span class="gp">&gt;&gt;&gt; </span><span class="n">r</span><span class="o">.</span><span class="n">recognize_google</span><span class="p">(</span><span class="n">audio</span><span class="p">,</span> <span class="n">show_all</span><span class="o">=</span><span class="bp">True</span><span class="p">)<br/></span><span class="go">{'alternative': [<br/></span><span class="go">  {'transcript': 'the snail smell like old Beer Mongers'}, <br/></span><span class="go">  {'transcript': 'the still smell of old beer vendors'}, <br/></span><span class="go">  {'transcript': 'the snail smell like old beer vendors'},<br/></span><span class="go">  {'transcript': 'the stale smell of old beer vendors'}, <br/></span><span class="go">  {'transcript': 'the snail smell like old beermongers'}, <br/></span><span class="go">  {'transcript': 'destihl smell of old beer vendors'}, <br/></span><span class="go">  {'transcript': 'the still smell like old beer vendors'}, <br/></span><span class="go">  {'transcript': 'bastille smell of old beer vendors'}, <br/></span><span class="go">  {'transcript': 'the still smell like old beermongers'}, <br/></span><span class="go">  {'transcript': 'the still smell of old beer venders'}, <br/></span><span class="go">  {'transcript': 'the still smelling old beer vendors'}, <br/></span><span class="go">  {'transcript': 'musty smell of old beer vendors'}, <br/></span><span class="go">  {'transcript': 'the still smell of old beer vendor'}<br/></span><span class="go">], 'final': True}</span></pre>
<p>Using this, we can then choose the best transcription for our specific problem.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Speech recognition using the microphone</h1>
                </header>
            
            <article>
                
<p>In the previous section, we used the recognizer methods to transcribe speech from audio files. In this section, we will do a transcription using speech recorded from our microphone.</p>
<p>But before we get into that, we will need to install an additional package, called <kbd>PyAudio</kbd>. It is also available on PyPI and can be installed directly using <kbd>pip: pip install PyAudio</kbd>. </p>
<p>In the previous section, for working with audio files, we were using the <kbd>AudioFile</kbd> class, but for working with a microphone, we will need to use the <kbd>Microphone</kbd> class. Most of the recognition API still remains the same. Let's take a simple example to understand how it works:</p>
<pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">speech_recognition</span> <span class="kn">as</span> <span class="nn">sr<br/></span><span class="gp">&gt;&gt;&gt; </span><span class="n">r</span> <span class="o">=</span> <span class="n">sr</span><span class="o">.</span><span class="n">Recognizer</span><span class="p">()<br/></span><span class="gp"><br/>&gt;&gt;&gt; </span><span class="n">mic</span> <span class="o">=</span> <span class="n">sr</span><span class="o">.</span><span class="n">Microphone</span><span class="p">()<br/></span><span class="gp"><br/>&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">mic</span> <span class="k">as</span> <span class="n">source</span><span class="p">:<br/></span><span class="gp">... </span>    <span class="n">r</span><span class="o">.</span><span class="n">adjust_for_ambient_noise</span><span class="p">(</span><span class="n">source</span><span class="p">)<br/></span><span class="gp">...     </span><span class="n">audio</span> <span class="o">=</span> <span class="n">r</span><span class="o">.</span><span class="n">listen</span><span class="p">(</span><span class="n">source</span><span class="p">)</span></pre>
<p>Similarly to how we had initialized the <kbd>AudioFile</kbd> class in the previous section, we need to initialize the <kbd>Microphone</kbd> class this time. Also, instead of <kbd>record</kbd>, we need to call the <kbd>listen</kbd> method to record the audio. The Python interpreter would wait for a while to record audio when executing the previous code block. Try saying something into the microphone. Once the interpreter prompt returns, we can call the <kbd>recognize</kbd> method to transcribe the recorded audio:</p>
<pre><span class="gp">&gt;&gt;&gt; </span><span class="n">r</span><span class="o">.</span><span class="n">recognize_google</span><span class="p">(</span><span class="n">audio</span><span class="p">)<br/></span><span class="go">'hello world' # This would depend on what you said in the microphone</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we looked into two of the major applications of HMMs: POS tagging and speech recognition. We coded the POS tagger using a most-frequent tag algorithm and used the <kbd>pomegranate</kbd> package to build one based on HMM. We compared the performance using both these methods and saw that an HMM-based approach outperforms the most-frequent tag method. Then, we used the <kbd>SpeechRecognition</kbd> package to transcribe audio to text using Google's Web Speech API. We looked into using the package with both audio files and live audio from a microphone.</p>
<p>In the next chapter, we will explore more applications of HMMs, specifically in the field of image recognition.</p>


            </article>

            
        </section>
    </body></html>