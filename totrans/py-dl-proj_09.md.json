["```py\npip install opencv-python\n```", "```py\nimport cv2\nimport matplotlib\nfrom matplotlib import colors\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom __future__ import division\n```", "```py\n# Defining some helper function\ndef show(image):\n    # Figure size in inches\n    plt.figure(figsize=(15, 15))\n\n    # Show image, with nearest neighbour interpolation\n    plt.imshow(image, interpolation='nearest')\n\ndef show_hsv(hsv):\n    rgb = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n    show(rgb)\n\ndef show_mask(mask):\n    plt.figure(figsize=(10, 10))\n    plt.imshow(mask, cmap='gray')\n\ndef overlay_mask(mask, image):\n    rgb_mask = cv2.cvtColor(mask, cv2.COLOR_GRAY2RGB)\n    img = cv2.addWeighted(rgb_mask, 0.5, image, 0.5, 0)\n    show(img)\n\ndef find_biggest_contour(image):\n    image = image.copy()\n    im2,contours, hierarchy = cv2.findContours(image, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n\n    contour_sizes = [(cv2.contourArea(contour), contour) for contour in contours]\n    biggest_contour = max(contour_sizes, key=lambda x: x[0])[1]\n\n    mask = np.zeros(image.shape, np.uint8)\n    cv2.drawContours(mask, [biggest_contour], -1, 255, -1)\n    return biggest_contour, mask\n\ndef circle_countour(image, countour):\n    image_with_ellipse = image.copy()\n    ellipse = cv2.fitEllipse(countour)\n\n    cv2.ellipse(image_with_ellipse, ellipse, (0,255,0), 2)\n    return image_with_ellipse\n\n```", "```py\n# Loading image and display\nimage = cv2.imread('./ferrari.png')\nshow(image)\n```", "```py\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nshow(image)\n```", "```py\nmax_dimension = max(image.shape)\nscale = 700/max_dimension\nimage = cv2.resize(image, None, fx=scale,fy=scale)\n```", "```py\nimage_blur = cv2.GaussianBlur(image, (7, 7), 0)\n```", "```py\nimage_blur_hsv = cv2.cvtColor(image_blur, cv2.COLOR_RGB2HSV)\n```", "```py\n# filter by color\nmin_red = np.array([0, 100, 80])\nmax_red = np.array([10, 256, 256])\nmask1 = cv2.inRange(image_blur_hsv, min_red, max_red)\n\n# filter by brightness\nmin_red = np.array([170, 100, 80])\nmax_red = np.array([180, 256, 256])\nmask2 = cv2.inRange(image_blur_hsv, min_red, max_red)\n\n# Concatenate both the mask for better feature extraction\nmask = mask1 + mask2\n```", "```py\nkernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15, 15))\n```", "```py\nmask_closed = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n```", "```py\nmask_clean = cv2.morphologyEx(mask_closed, cv2.MORPH_OPEN, kernel)\n```", "```py\n# Extract biggest bounding box\nbig_contour, red_mask = find_biggest_contour(mask_clean)\n\n# Apply mask\noverlay = overlay_mask(red_mask, image)\n\n# Draw bounding box\ncircled = circle_countour(overlay, big_contour)\n\nshow(circled)\n```", "```py\npip install tensorflow\npip install keras\npip install numpy\npip install scipy\npip install opencv-python\npip install pillow\npip install matplotlib\npip install h5py\n# Here we are installing ImageAI\npip3 install https://github.com/OlafenwaMoses/ImageAI/releases/download/2.0.2/imageai-2.0.2-py3-none-any.whl\n```", "```py\nwget https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/resnet50_coco_best_v2.0.1.h5\n```", "```py\nfrom imageai.Detection import ObjectDetection\nimport os\nmodel_path = os.getcwd()\n```", "```py\nobject_detector = ObjectDetection()\nobject_detector.setModelTypeAsRetinaNet()\nobject_detector.setModelPath( os.path.join(model_path , \"resnet50_coco_best_v2.0.1.h5\"))\nobject_detector.loadModel()\n```", "```py\nobject_detections = object_detector.detectObjectsFromImage(input_image=os.path.join(model_path , \"image.jpg\"), output_image_path=os.path.join(model_path , \"imagenew.jpg\"))\n```", "```py\nfor eachObject in object_detections:\n    print(eachObject[\"name\"] , \" : \" , eachObject[\"percentage_probability\"]) \n```", "```py\nfrom flask import Flask, request, jsonify, redirect\nimport os , json\nfrom imageai.Detection import ObjectDetection\n\nmodel_path = os.getcwd()\n\nPRE_TRAINED_MODELS = [\"resnet50_coco_best_v2.0.1.h5\"]\n\n# Creating ImageAI objects and loading models\n\nobject_detector = ObjectDetection()\nobject_detector.setModelTypeAsRetinaNet()\nobject_detector.setModelPath( os.path.join(model_path , PRE_TRAINED_MODELS[0]))\nobject_detector.loadModel()\nobject_detections = object_detector.detectObjectsFromImage(input_image='sample.jpg')\n\n# Define model paths and the allowed file extentions\nUPLOAD_FOLDER = model_path\nALLOWED_EXTENSIONS = set(['png', 'jpg', 'jpeg', 'gif'])\n\napp = Flask(__name__)\napp.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\n\ndef allowed_file(filename):\n    return '.' in filename and \\\n           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\n@app.route('/predict', methods=['POST'])\ndef upload_file():\n    if request.method == 'POST':\n        # check if the post request has the file part\n        if 'file' not in request.files:\n            print('No file part')\n            return redirect(request.url)\n        file = request.files['file']\n        # if user does not select file, browser also\n        # submit a empty part without filename\n        if file.filename == '':\n            print('No selected file')\n            return redirect(request.url)\n        if file and allowed_file(file.filename):\n            filename = file.filename\n            file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename) \n            file.save(file_path) \n\n    try:\n        object_detections = object_detector.detectObjectsFromImage(input_image=file_path)\n    except Exception as ex:\n        return jsonify(str(ex))\n    resp = []\n    for eachObject in object_detections :\n        resp.append([eachObject[\"name\"],\n                     round(eachObject[\"percentage_probability\"],3)\n                     ]\n                    )\n\n    return json.dumps(dict(enumerate(resp)))\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=4445)\n```", "```py\npython object_detection_ImageAI.py\n```", "```py\ncurl -X POST \\\n http://0.0.0.0:4445/predict \\\n -H 'content-type: multipart/form-data; boundary=----WebKitFormBoundary7MA4YWxkTrZu0gW' \\\n -F file=@/Users/rahulkumar/Downloads/IMG_1651.JPG\n```", "```py\n{\n \"0\": [\"person\",54.687],\n \"1\": [\"person\",56.77],\n \"2\": [\"person\",55.837],\n \"3\": [\"person\",75.93],\n \"4\": [\"person\",72.956],\n \"5\": [\"bird\",81.139]\n}\n```", "```py\nwget http://images.cocodataset.org/zips/train2014.zip\n```", "```py\nwget http://images.cocodataset.org/zips/val2014.zip\n```", "```py\nwget http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n```", "```py\npip install baker\n```", "```py\nmkdir images annotations\n```", "```py\nunzip train2014.zip -d ./images/\nunzip val2014.zip -d ./images/\n```", "```py\nunzip annotations_trainval2014.zip -d ./annotations/\n\n```", "```py\nmkdir output\nmkdir output/train\nmkdir output/val\n\npython coco2voc.py create_annotations /TRAIN_DATA_PATH train /OUTPUT_FOLDER/train\npython coco2voc.py create_annotations /TRAIN_DATA_PATH val /OUTPUT_FOLDER/val\n```", "```py\nwget https://pjreddie.com/media/files/yolo.weights\n```", "```py\npip install keras tensorflow tqdm numpy cv2 imgaug\n```", "```py\nfrom keras.models import Sequential, Model\nfrom keras.layers import Reshape, Activation, Conv2D, Input, MaxPooling2D, BatchNormalization, Flatten, Dense, Lambda\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\nfrom keras.optimizers import SGD, Adam, RMSprop\nfrom keras.layers.merge import concatenate\nimport matplotlib.pyplot as plt\nimport keras.backend as K\nimport tensorflow as tf\nimport imgaug as ia\nfrom tqdm import tqdm\nfrom imgaug import augmenters as iaa\nimport numpy as np\nimport pickle\nimport os, cv2\nfrom preprocessing import parse_annotation, BatchGenerator\nfrom utils import WeightReader, decode_netout, draw_boxes\n\n#Setting GPU configs\nos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n```", "```py\n# List of object that YOLO model will learn to detect from COCO dataset \n#LABELS = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n\n# Label for the custom curated dataset.\nLABEL = ['kangaroo']\nIMAGE_H, IMAGE_W = 416, 416\nGRID_H,  GRID_W  = 13 , 13\nBOX              = 5\nCLASS            = len(LABELS)\nCLASS_WEIGHTS    = np.ones(CLASS, dtype='float32')\nOBJ_THRESHOLD    = 0.3\nNMS_THRESHOLD    = 0.3\nANCHORS          = [0.57273, 0.677385, 1.87446, 2.06253, 3.33843, 5.47434, 7.88282, 3.52778, 9.77052, 9.16828]\n\nNO_OBJECT_SCALE  = 1.0\nOBJECT_SCALE     = 5.0\nCOORD_SCALE      = 1.0\nCLASS_SCALE      = 1.0\n\nBATCH_SIZE       = 16\nWARM_UP_BATCHES  = 0\nTRUE_BOX_BUFFER  = 50\n```", "```py\nwt_path = 'yolo.weights'                      \ntrain_image_folder = '/new_class/images/'\ntrain_annot_folder = '/new_class/anno/' \nvalid_image_folder = '/new_class/images/' \nvalid_annot_folder = '/new_class/anno/'\n```", "```py\n# the function to implement the organization layer (thanks to github.com/allanzelener/YAD2K)\ndef space_to_depth_x2(x):\n    return tf.space_to_depth(x, block_size=2)\ninput_image = Input(shape=(IMAGE_H, IMAGE_W, 3))\ntrue_boxes  = Input(shape=(1, 1, 1, TRUE_BOX_BUFFER , 4))\n\n# Layer 1\nx = Conv2D(32, (3,3), strides=(1,1), padding='same', name='conv_1', use_bias=False)(input_image)\nx = BatchNormalization(name='norm_1')(x)\nx = LeakyReLU(alpha=0.1)(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\n\n# Layer 2\nx = Conv2D(64, (3,3), strides=(1,1), padding='same', name='conv_2', use_bias=False)(x)\nx = BatchNormalization(name='norm_2')(x)\nx = LeakyReLU(alpha=0.1)(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\n\n# Layer 3\n# Layer 4\n# Layer 23\n# For the entire architecture, please refer to the yolo/Yolo_v2_train.ipynb notebook here: https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter09/yolo/Yolo_v2_train.ipynb\n```", "```py\nTotal params: 50,983,561\nTrainable params: 50,962,889\nNon-trainable params: 20,672\n```", "```py\nweight_reader = WeightReader(wt_path)\nweight_reader.reset()\nnb_conv = 23\nfor i in range(1, nb_conv+1):\n    conv_layer = model.get_layer('conv_' + str(i))\n\n    if i < nb_conv:\n        norm_layer = model.get_layer('norm_' + str(i))\n\n        size = np.prod(norm_layer.get_weights()[0].shape)\n\n        beta  = weight_reader.read_bytes(size)\n        gamma = weight_reader.read_bytes(size)\n        mean  = weight_reader.read_bytes(size)\n        var   = weight_reader.read_bytes(size)\n\n        weights = norm_layer.set_weights([gamma, beta, mean, var])       \n\n    if len(conv_layer.get_weights()) > 1:\n        bias   = weight_reader.read_bytes(np.prod(conv_layer.get_weights()[1].shape))\n        kernel = weight_reader.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n        kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n        kernel = kernel.transpose([2,3,1,0])\n        conv_layer.set_weights([kernel, bias])\n    else:\n        kernel = weight_reader.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n        kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n        kernel = kernel.transpose([2,3,1,0])\n        conv_layer.set_weights([kernel])\n```", "```py\nlayer   = model.layers[-4] # the last convolutional layer\nweights = layer.get_weights()\n\nnew_kernel = np.random.normal(size=weights[0].shape)/(GRID_H*GRID_W)\nnew_bias   = np.random.normal(size=weights[1].shape)/(GRID_H*GRID_W)\n\nlayer.set_weights([new_kernel, new_bias])\n```", "```py\ngenerator_config = {\n    'IMAGE_H' : IMAGE_H, \n    'IMAGE_W' : IMAGE_W,\n    'GRID_H' : GRID_H, \n    'GRID_W' : GRID_W,\n    'BOX' : BOX,\n    'LABELS' : LABELS,\n    'CLASS' : len(LABELS),\n    'ANCHORS' : ANCHORS,\n    'BATCH_SIZE' : BATCH_SIZE,\n    'TRUE_BOX_BUFFER' : 50,\n}\n```", "```py\n# Training batch data\ntrain_imgs, seen_train_labels = parse_annotation(train_annot_folder, train_image_folder, labels=LABELS)\ntrain_batch = BatchGenerator(train_imgs, generator_config, norm=normalize)\n\n# Validation batch data\nvalid_imgs, seen_valid_labels = parse_annotation(valid_annot_folder, valid_image_folder, labels=LABELS)\nvalid_batch = BatchGenerator(valid_imgs, generator_config, norm=normalize, jitter=False)\n```", "```py\nearly_stop = EarlyStopping(monitor='val_loss', \n                           min_delta=0.001, \n                           patience=3, \n                           mode='min', \n                           verbose=1)\n\ncheckpoint = ModelCheckpoint('weights_coco.h5', \n                             monitor='val_loss', \n                             verbose=1, \n                             save_best_only=True, \n                             mode='min', \n                             period=1)\n```", "```py\ntb_counter = len([log for log in os.listdir(os.path.expanduser('~/logs/')) if 'coco_' in log]) + 1\ntensorboard = TensorBoard(log_dir=os.path.expanduser('~/logs/') + 'coco_' + '_' + str(tb_counter), \n                          histogram_freq=0, \n                          write_graph=True, \n                          write_images=False)\n\noptimizer = Adam(lr=0.5e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n#optimizer = SGD(lr=1e-4, decay=0.0005, momentum=0.9)\n#optimizer = RMSprop(lr=1e-4, rho=0.9, epsilon=1e-08, decay=0.0)\n\nmodel.compile(loss=custom_loss, optimizer=optimizer)\n\nmodel.fit_generator(generator = train_batch, \n                    steps_per_epoch = len(train_batch), \n                    epochs = 100, \n                    verbose = 1,\n                    validation_data = valid_batch,\n                    validation_steps = len(valid_batch),\n                    callbacks = [early_stop, checkpoint, tensorboard], \n                    max_queue_size = 3)\n```", "```py\nEpoch 1/2\n11/11 [==============================] - 315s 29s/step - loss: 3.6982 - val_loss: 1.5416\n\nEpoch 00001: val_loss improved from inf to 1.54156, saving model to weights_coco.h5\nEpoch 2/2\n11/11 [==============================] - 307s 28s/step - loss: 1.4517 - val_loss: 1.0636\n\nEpoch 00002: val_loss improved from 1.54156 to 1.06359, saving model to weights_coco.h5\n```", "```py\nmodel.load_weights(\"weights_coco.h5\")\n```", "```py\ninput_image_path = \"my_test_image.jpg\"\nimage = cv2.imread(input_image_path)\ndummy_array = np.zeros((1,1,1,1,TRUE_BOX_BUFFER,4))\nplt.figure(figsize=(10,10))\n```", "```py\ninput_image = cv2.resize(image, (416, 416))\ninput_image = input_image / 255.\ninput_image = input_image[:,:,::-1]\ninput_image = np.expand_dims(input_image, 0)\n```", "```py\nnetout = model.predict([input_image, dummy_array])\n\nboxes = decode_netout(netout[0], \n                      obj_threshold=OBJ_THRESHOLD,\n                      nms_threshold=NMS_THRESHOLD,\n                      anchors=ANCHORS, \n                      nb_class=CLASS)\n\nimage = draw_boxes(image, boxes, labels=LABELS)\n\nplt.imshow(image[:,:,::-1]); plt.show()\n```", "```py\ncd SegNet\nwget http://images.cocodataset.org/zips/train2014.zip\nmkdir images\nunzip train2014.zip -d images\n```", "```py\nfrom __future__ import absolute_import\nfrom __future__ import print_function\n\nimport pylab\nimport numpy as np\nimport pandas as pd\nimport skimage.io as io\nimport matplotlib.pyplot as plt\n\nfrom pycocotools.coco import COCO\npylab.rcParams['figure.figsize'] = (8.0, 10.0)\nimport cv2\n\nimport keras.models as models, Sequential\nfrom keras.layers import Layer, Dense, Dropout, Activation, Flatten, Reshape, Permute\nfrom keras.layers import Conv2D, MaxPool2D, UpSampling2D, ZeroPadding2D\nfrom keras.layers import BatchNormalization\n\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom keras.optimizers import Adam\n\nimport keras\nkeras.backend.set_image_dim_ordering('th')\n\nfrom tqdm import tqdm\nimport itertools\n%matplotlib inline\n```", "```py\n# set the location of the annotation file associated with the train images\nannFile='annotations/annotations/instances_train2014.json'\n\n# initialize COCO api with\ncoco = COCO(annFile)\n```", "```py\nloading annotations into memory...\nDone (t=12.84s)\ncreating index...\nindex created!\n```", "```py\n# extract the category ids using the label 'person'\ncatIds = coco.getCatIds(catNms=['person'])\n\n# extract the image ids using the catIds\nimgIds = coco.getImgIds(catIds=catIds )\n\n# print number of images with the tag 'person'\nprint(\"Number of images with the tag 'person' :\" ,len(imgIds))\n```", "```py\nNumber of images with the tag 'person' : 45174\n```", "```py\n# extract the details of image with the image id\nimg = coco.loadImgs(imgIds[2])[0]\nprint(img)\n\n# load the image using the location of the file listed in the image variable\nI = io.imread('images/train2014/'+img['file_name'])\n\n# display the image\nplt.imshow(I)\n```", "```py\n{'height': 426, 'coco_url': 'http://images.cocodataset.org/train2014/COCO_train2014_000000524291.jpg', 'date_captured': '2013-11-18 09:59:07', 'file_name': 'COCO_train2014_000000524291.jpg', 'flickr_url': 'http://farm2.staticflickr.com/1045/934293170_d1b2cc58ff_z.jpg', 'width': 640, 'id': 524291, 'license': 3}\n```", "```py\n# display the image\nplt.imshow(I)\n\n# extract the annotation id \nannIds = coco.getAnnIds(imgIds=img['id'], catIds=catIds, iscrowd=None)\n\n# load the annotation\nanns = coco.loadAnns(annIds)\n\n# plot the annotation on top of the image\ncoco.showAnns(anns)\n```", "```py\n# build the mask for display with matplotlib\nmask = coco.annToMask(anns[0])\n\n# display the mask\nplt.imshow(mask)\n```", "```py\ndef data_list(imgIds, count = 12127, ratio = 0.2):\n    \"\"\"Function to load image and its target into memory.\"\"\" \n    img_lst = []\n    lab_lst = []\n\n    for x in tqdm(imgIds[0:count]):\n        # load image details\n        img = coco.loadImgs(x)[0]\n\n        # read image\n        I = io.imread('images/train2014/'+img['file_name'])\n        if len(I.shape)<3:\n            continue\n\n        # load annotation information\n        annIds = coco.getAnnIds(imgIds=img['id'], catIds=catIds, iscrowd=None)\n\n        # load annotation\n        anns = coco.loadAnns(annIds)\n\n        # prepare mask\n        mask = coco.annToMask(anns[0])\n\n        # This condition makes sure that we select images having only one person \n        if len(np.unique(mask)) == 2:\n\n            # Next condition selects images where ratio of area covered by the \n # person to the entire image is greater than the ratio parameter\n # This is done to not have large class imbalance\n            if (len(np.where(mask>0)[0])/len(np.where(mask>=0)[0])) > ratio :\n\n                # If you check, generated mask will have 2 classes i.e 0 and 2 \n # (0 - background/other, 1 - person).\n # to avoid issues with cv2 during the resize operation\n # set label 2 to 1, making label 1 as the person. \n                mask[mask==2] = 1\n\n                # resize image and mask to shape (480, 360)\n                I= cv2.resize(I, (480,360))\n                mask = cv2.resize(mask, (480,360))\n\n                # append mask and image to their lists\n                img_lst.append(I)\n                lab_lst.append(mask)\n    return (img_lst, lab_lst)\n\n# get images and their labels\nimg_lst, lab_lst = data_list(imgIds)\n\nprint('Sum of images for training, validation and testing :', len(img_lst))\nprint('Unique values in the labels array :', np.unique(lab_lst[0]))\n```", "```py\nSum of images for training, validation and testing : 1997\nUnique values in the labels array : [0 1]\n```", "```py\ndef make_normalize(img):\n    \"\"\"Function to histogram normalize images.\"\"\"\n    norm_img = np.zeros((img.shape[0], img.shape[1], 3),np.float32)\n\n    b=img[:,:,0]\n    g=img[:,:,1]\n    r=img[:,:,2]\n\n    norm_img[:,:,0]=cv2.equalizeHist(b)\n    norm_img[:,:,1]=cv2.equalizeHist(g)\n    norm_img[:,:,2]=cv2.equalizeHist(r)\n\n    return norm_img\n\nplt.figure(figsize = (14,5))\nplt.subplot(1,2,1)\nplt.imshow(img_lst[9])\nplt.title(' Original Image')\nplt.subplot(1,2,2)\nplt.imshow(make_normalize(img_lst[9]))\nplt.title(' Histogram Normalized Image')\n```", "```py\ndef make_target(labels):\n    \"\"\"Function to one hot encode targets.\"\"\"\n    x = np.zeros([360,480,2])\n    for i in range(360):\n        for j in range(480):\n            x[i,j,labels[i][j]]=1\n    return x\n\nplt.figure(figsize = (14,5))\nplt.subplot(1,2,1)\nplt.imshow(make_target(lab_lst[0])[:,:,0])\nplt.title('Background')\nplt.subplot(1,2,2)\nplt.imshow(make_target(lab_lst[0])[:,:,1])\nplt.title('Person')\n```", "```py\ndef model_data(images, labels):\n    \"\"\"Function to perform normalize and encode operation on each image.\"\"\"\n    # empty label and image list\n    array_lst = []\n    label_lst=[]\n\n    # apply normalize function on each image and encoding function on each label\n    for x,y in tqdm(zip(images, labels)):\n        array_lst.append(np.rollaxis(normalized(x), 2))\n        label_lst.append(make_target(y))\n\n    return np.array(array_lst), np.array(label_lst)\n\n# Get model data\ntrain_data, train_lab = model_data(img_lst, lab_lst)\n\nflat_image_shape = 360*480\n\n# reshape target array\ntrain_label = np.reshape(train_lab,(-1,flat_image_shape,2))\n\n# test data\ntest_data = test_data[1900:]\n# validation data\nval_data = train_data[1500:1900]\n# train data\ntrain_data = train_data[:1500]\n\n# test label\ntest_label = test_label[1900:]\n# validation label\nval_label = train_label[1500:1900]\n# train label\ntrain_label = train_label[:1500]\n```", "```py\n# define optimizer\noptimizer = Adam(lr=0.002)\n\n# input shape to the model\ninput_shape=(3, 360, 480)\n\n# training batchsize\nbatch_size = 6\n\n# number of training epochs\nnb_epoch = 60\n```", "```py\nmodel = Sequential()\n# Encoder\nmodel.add(Layer(input_shape=input_shape))\nmodel.add(ZeroPadding2D())\nmodel.add(Conv2D(filters=64, kernel_size=(3,3), padding='valid', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2,2)))\n\nmodel.add(ZeroPadding2D())\nmodel.add(Conv2D(filters=128, kernel_size=(3,3), padding='valid', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2,2)))\n\nmodel.add(ZeroPadding2D())\nmodel.add(Conv2D(filters=256, kernel_size=(3,3), padding='valid', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2,2)))\n\nmodel.add(ZeroPadding2D())\nmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding='valid', activation='relu'))\nmodel.add(BatchNormalization())\n\n# Decoder # For the remaining part of this section of the code refer to the segnet.ipynb file in the SegNet folder. Here is the github link: https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter09\n\n```", "```py\n# compile model\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.002), metrics=[\"accuracy\"])\n\n# use ReduceLROnPlateau to adjust the learning rate\nreduceLROnPlat = ReduceLROnPlateau(monitor='val_acc', factor=0.75, patience=5,\n                      min_delta=0.005, mode='max', cooldown=3, verbose=1)\n\ncallbacks_list = [reduceLROnPlat]\n```", "```py\n# fit the model\nhistory = model.fit(train_data, train_label, callbacks=callbacks_list,\n                    batch_size=batch_size, epochs=nb_epoch,\n                    verbose=1, shuffle = True, validation_data = (val_data, val_label))\n```", "```py\nloss,acc = model.evaluate(test_data, test_label)\nprint('Loss :', loss)\nprint('Accuracy :', acc)\n```", "```py\n97/97 [==============================] - 7s 71ms/step\nLoss : 0.5390811630131043\nAccuracy : 0.7633129960482883\n```", "```py\nfor i in range(3):\n    plt.figure(figsize = (10,3))\n    plt.subplot(1,2,1)\n    plt.imshow(img_lst[1900+i])\n    plt.title('Input')\n    plt.subplot(1,2,2)\n    plt.imshow(model.predict_classes(test_data[i:(i+1)*1]).reshape(360,480))\n    plt.title('Segmentation')\n```"]