["```py\nspark = SparkSession.builder \\\n         .master(\"local\") \\\n         .appName(\"RecommendationEngine\") \\\n         .config(\"spark.executor.memory\", \"6gb\") \\\n         .getOrCreate()\n```", "```py\nimport os\nos.listdir('ml-latest-small/')\n```", "```py\nmovies = spark.read.format('com.databricks.spark.csv')\\\n            .options(header='true', inferschema='true')\\\n            .load('ml-latest-small/movies.csv')\ntags = spark.read.format('com.databricks.spark.csv')\\\n            .options(header='true', inferschema='true')\\\n            .load('ml-latest-small/tags.csv')\nlinks = spark.read.format('com.databricks.spark.csv')\\\n            .options(header='true', inferschema='true')\\\n            .load('ml-latest-small/links.csv')\nratings = spark.read.format('com.databricks.spark.csv')\\\n            .options(header='true', inferschema='true')\\\n            .load('ml-latest-small/ratings.csv')\n```", "```py\nprint('The number of rows in movies dataset is {}'.format(movies.toPandas().shape[0]))\nprint('The number of rows in ratings dataset is {}'.format(ratings.toPandas().shape[0]))\nprint('The number of rows in tags dataset is {}'.format(tags.toPandas().shape[0]))\nprint('The number of rows in links dataset is {}'.format(links.toPandas().shape[0]))\n```", "```py\nfor i in ratings.columns:\n     ratings = ratings.withColumnRenamed(i, i+'_1') \n```", "```py\ntemp1 = ratings.join(movies, ratings.movieId_1 == movies.movieId, how = 'inner')\n```", "```py\ntemp2 = temp1.join(links, temp1.movieId_1 == links.movieId, how = 'inner')\n```", "```py\nmainDF = temp2.join(tags, (temp2.userId_1 == tags.userId) & (temp2.movieId_1 == tags.movieId), how = 'left')\n```", "```py\nmainDF = mainDF.select('userId_1',\n                       'movieId_1',\n                       'rating_1',\n                       'title', \n                       'genres', \n                       'imdbId',\n                       'tmdbId', \n                       'timestamp_1').distinct()\n```", "```py\nmovies.createOrReplaceTempView('movies_')\nlinks.createOrReplaceTempView('links_')\nratings.createOrReplaceTempView('ratings_')\n```", "```py\nmainDF_SQL = \\\nsqlContext.sql(\n\"\"\"\n    select\n    r.userId_1\n    ,r.movieId_1\n    ,r.rating_1\n    ,m.title\n    ,m.genres\n    ,l.imdbId\n    ,l.tmdbId\n    ,r.timestamp_1\n    from ratings_ r\n\n    inner join movies_ m on \n    r.movieId_1 = m.movieId\n    inner join links_ l on \n    r.movieId_1 = l.movieId\n\"\"\"\n)\n```", "```py\nmainDF.describe('rating_1').show\n```", "```py\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nmainDF.select('rating_1').toPandas().hist(figsize=(16, 6), grid=True)\nplt.title('Histogram of Ratings')\nplt.show()\n```", "```py\nmainDF.groupBy(['rating_1']).agg({'rating_1':'count'})\\\n .withColumnRenamed('count(rating_1)', 'Row Count').orderBy([\"Row Count\"],ascending=False)\\\n .show()\n```", "```py\nuserId_frequency = mainDF.groupBy(['userId_1']).agg({'rating_1':'count'})\\\n         .withColumnRenamed('count(rating_1)', '# of Reviews').orderBy([\"# of             Reviews\"],ascending=False)\n```", "```py\nuserId_frequency.select('# of Reviews').toPandas().hist(figsize=(16, 6), grid=True)\nplt.title('Histogram of User Ratings')\nplt.show()\n```", "```py\nmainDF = mainDF.withColumnRenamed('userId_1', 'userid')\nmainDF = mainDF.withColumnRenamed('movieId_1', 'movieid')\nmainDF = mainDF.withColumnRenamed('rating_1', 'rating')\nmainDF = mainDF.withColumnRenamed('timestamp_1', 'timestamp')\nmainDF = mainDF.withColumnRenamed('imdbId', 'imdbid')\nmainDF = mainDF.withColumnRenamed('tmdbId', 'tmdbid')\n```", "```py\nimport pyspark.sql.functions as F\nmainDF = mainDF.withColumn(\"rating\", F.round(mainDF[\"rating\"], 0))\n```", "```py\nfrom pyspark.ml.feature import StringIndexer\nstring_indexer = StringIndexer(inputCol=\"genres\", outputCol=\"genreCount\")\nmainDF = string_indexer.fit(mainDF).transform(mainDF)\n```", "```py\nmainDF = mainDF.select('rating', 'userid', 'movieid', 'imdbid', 'tmdbid', 'timestamp', 'genreCount')\n```", "```py\ntrainDF, testDF = mainDF.randomSplit([0.8, 0.2], seed=1234)\n```", "```py\nimport numpy as np\n\nxtrain_array = np.array(trainDF.select('userid','movieid', 'genreCount').collect())\nxtest_array = np.array(testDF.select('userid','movieid', 'genreCount').collect())\n\nytrain_array = np.array(trainDF.select('rating').collect())\nytest_array = np.array(testDF.select('rating').collect()\n```", "```py\nimport keras.utils as u\nytrain_OHE = u.to_categorical(ytrain_array)\nytest_OHE = u.to_categorical(ytest_array)\n```", "```py\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\n```", "```py\nmodel = Sequential()\nmodel.add(Dense(32, activation='relu', input_dim=xtrain_array.shape[1]))\nmodel.add(Dense(10, activation='relu'))\nmodel.add(Dense(ytrain_OHE.shape[1], activation='softmax'))\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n```", "```py\naccuracy_history = model.fit(xtrain_array, ytrain_OHE, epochs=20, batch_size=32)\n```", "```py\nplt.plot(accuracy_history.history['acc'])\nplt.title('Accuracy vs. Epoch')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.show()\n\nplt.plot(accuracy_history.history['loss'])\nplt.title('Loss vs. Epoch')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.show()\n```", "```py\nscore = model.evaluate(xtest_array, ytest_OHE, batch_size=128)\naccuracy_rate = score[1]*100\nprint('accuracy is {}%'.format(round(accuracy_rate,2)))\n```"]