- en: Introduction to the Markov Process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will develop the basic concepts that we need to understand
    **Hidden Markov Models** (**HMM**). We will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Random processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Markov processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Markov chains or discrete-time Markov processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuous-time Markov chains
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we always do in statistics, let's start with a simple example of rolling
    a dice. If we consider rolling a fair dice, the outcome of the dice can be anything
    from 1 to 6, and is random. To represent such situations (the outcome of rolling
    the dice in this case), in mathematics we use the concept of random variables.
    We come across a lot of such variables in our everyday lives. Another example
    could be ordering food at a restaurant. In this case, the outcome could be any
    food item on the menu. In general terms, a random variable is a variable whose
    possible values are outcomes of a random phenomenon. The possible states of the
    outcomes are also known as the **domain of the random variable**, and the outcome
    is based on the probability distribution defined over the domain of the random
    variable.
  prefs: []
  type: TYPE_NORMAL
- en: Coming back to rolling the dice, the domain of the random variable outcome*,
    O, *is given by *domain(O) = (1, 2, 3, 4, 5, 6),* and the probability distribution
    is given by a uniform distribution *P(o) = 1/6 ∀ ∈ domain(O)*. Similarly, in the
    case of the restaurant example, for the random variable *choosing a dish*, the
    domain would be every item on the menu, and the probability distribution would
    depend on your food preference. In both of the previous examples, the domain of
    the random variable has discrete variables; such random variables are known as
    **discrete random variables**. But it's also possible for the domain to be a continuous
    space. For example, consider the random variable representing the stock price
    of Googletomorrow. The domain of this random variable will be all positive real
    numbers with most of the probability mass distributed around ±5% of today's price.
    Such random variables are known as **continuous random variables**.
  prefs: []
  type: TYPE_NORMAL
- en: Random processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we discussed random variables that are able to mathematically
    represent the outcomes of a single random phenomenon. But what if we want to represent
    these random events over some period of time or the length of an experiment? For
    example, let's say we want to represent the stock prices for a whole day at intervals
    of every one hour, or we want to represent the height of a ball at intervals of every
    one second after being dropped from some height in a vacuum. For such situations,
    we would need a set of random variables, each of which will represent the outcome
    at the given instance of time. These sets of random variables that represent random
    variables over a period of time are also known as **random processes**.It is worth
    noting that the domains of all these random variables are the same. Therefore,
    we can also think of the process as just changing the states.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we have been talking about random variables at different instances of
    time, but it doesn't need to be time-based in every case. It could be just some
    other event. But since, in most cases, it is usually time, and it is much easier
    to talk about random processes in terms of time, we will use time to represent
    any such event. The same concepts will apply to creating a model if it varies
    over some other event instead of time.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's discuss the previous two examples in more detail. Starting with the
    example of dropping the ball from a height in a vacuum, if we know the exact value
    of gravity and the height from which the ball is being dropped, we will be able
    to determine the exact location of the ball at every interval of one second using
    Newton's laws of motion.
  prefs: []
  type: TYPE_NORMAL
- en: Such random processes, in which we can deterministically find the state of each
    random variable given the initial conditions (in this case, dropping the ball,
    zero initial velocity) and the parameters of the system (in this case, the value
    of gravity), are known as **deterministic random processes** (commonly called **deterministic
    processes**).
  prefs: []
  type: TYPE_NORMAL
- en: Now let's go to the second example; representing the stock price over time.
    In this case, even if we know the current price and the exact probability distribution
    of the price at the next one hour mark, we won't be able to deterministically
    compute the value. These random processes, in which we can't determine the state
    of a process, even if we are given the initial conditions and all the parameters
    of the system, are known as **stochastic random processes** (commonly called **processes**).
    A very good way of understanding or getting a feel for a stochastic process is
    to think of it as being the opposite of a deterministic process.
  prefs: []
  type: TYPE_NORMAL
- en: Markov processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A stochastic process is called a **Markov process **if the state of the random
    variable at the next instance of time depends only on the outcome of the random
    variable at the current time. In simplistic mathematical terms, for a stochastic
    process, *S = {R1, R[2], . . ., R[n]} = {R}[t=1, . . ., n]*, to be a Markov process,
    it must satisfy the following condition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/57bcdc2f-a875-446c-82c5-fa07de0db4d9.png)'
  prefs: []
  type: TYPE_IMG
- en: According to the previous condition, the probability distribution for any variable
    at any given instance in a Markov process is a conditional distribution, which
    is conditioned only on the random variable at the last time instance. This property
    of a system, such that the future states of the system depend only on the current
    state of the system, is also known as the **Markov property**. Systems satisfying
    the Markov property are also known as **memoryless systems** since they don't
    need to remember the previous states to compute the distribution of the next state,
    or, in other words, the next state depends only on the current state of the system.
  prefs: []
  type: TYPE_NORMAL
- en: A very common example used to explain the Markov process is a drunk man walking
    along a street. We consider that, since the man is drunk, he can either take a
    step backward, a step forward, or stay in his current position, which is given
    by some distribution of these, let's say *[0.4, 0.4, 0.2]*. Now, given the position
    of the man at any given instance in time, his position at the next instance depends
    only on his current position and the parameters of the system (his step size and
    the probability distribution of possible actions). Therefore, this is an example
    of a Markov process.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous example, let''s assume that the drunk man takes an action (steps
    forward/backward or stays in his position) at fixed intervals of time and his
    step size is always the same. With these considerations, the Markov process in
    our example has a discrete state space. Also, since the man takes steps after
    fixed intervals of time, we can think of it as a discrete time. But Markov processes
    don''t need to have discrete state space or discrete time intervals. Considering
    discrete and continuous time as well as discrete and continuous state space, we
    can categorize Markov processes into four main categories:'
  prefs: []
  type: TYPE_NORMAL
- en: Discrete time and discrete state space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discrete time and continuous state space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuous time and discrete state space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuous time and continuous state space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will discuss each of these categories of Markov process in more detail in
    the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Python and packages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before moving ahead, we need to set up Python and all the packages required
    to run the code examples. For all the code examples in this book, we will be using
    Python 3.4\. All the example code in the book is also available on GitHub at [https://github.com/PacktPublishing/HandsOnMarkovModelswithPython](https://github.com/PacktPublishing/HandsOnMarkovModelswithPython).
    We highly recommend using Miniconda to set up your environment for running the
    examples. Miniconda can be downloaded from [https://conda.io/miniconda.html](https://conda.io/miniconda.html).
  prefs: []
  type: TYPE_NORMAL
- en: Installation on Windows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Miniconda can be installed on a Windows system by just double-clicking on the
    downloaded `.exe` file and following the installation instructions. After installation,
    we will need to create a `conda` environment and install all the required packages
    in the environment. To create a new Python 3.4 environment with the name `hmm`,
    run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'After creating the environment, we will need to activate it and install the
    required packages in it. This can be done using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Installation on Linux
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'On Linux, after downloading the `Miniconda` file, we will need to give it execution
    permissions and then install it. This can be done using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'After executing the file, we can simply follow the installation instructions.
    Once installed, we will need to create a new environment and install the required
    packages. We can create a new Python 3.4 environment with the name `hmm` using
    the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the environment has been created, we will need to activate it and install
    the packages inside it using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Markov chains or discrete-time Markov processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A Markov chain is a type of Markov process in which the time is discrete. However,
    there is a lot of disagreement among researchers on what categories of Markov
    process should be called **Markov chain**. But, most commonly, it is used to refer
    to discrete-state-space Markov processes. Therefore, a Markov chain is a stochastic
    process over a discrete state space satisfying the Markov property. More formally,
    we can say that a discrete-time Markov chain is a sequence of random variables
    *X[1]*, *X[2]*, *X[3]*, ... that satisfy the Markov property, namely that the
    probability of moving from the current state to the next state depends only on
    the present state and not on any of the previous states. In terms of the probability
    distribution, we can say that, given that the system is at time instance *n*,
    the conditional distribution of the states at the next time instance, *n + 1*,
    is conditionally independent of the state of the system at time instances *{1,
    2, . . ., n-1}*, given the state of the random variable at time instance *n*.
    This can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/834a5c22-1466-4e66-bf09-24972d32a079.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Markov chains are often represented using directed graphs. The nodes in the
    directed graphs represent the different possible states of the random variables,
    and the edges represent the probability of the system going from one state to
    the other in the next time instance. Let''s take a simple example of predicting
    the weather to understand this representation better. We will consider that there
    are three possible states of the random variable *Weather={Sunny, Rainy, Snowy}*,
    and possible Markov chains for this can be represented as shown in *Figure 1.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/346d701d-7454-4060-9e93-059c68a637f2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.1: A simple Markov chain on the random variable, representing the random
    variable Weather={Sunny, Rainy, Snowy}and showing the probability of the random
    variable switching to other states in the next time instance'
  prefs: []
  type: TYPE_NORMAL
- en: One of the main points to understand in Markov chains is that we are modeling
    the outcomes of a sequence of random variables over time. This is sometimes confusing
    for people since the model is represented using a single graph, which doesn't
    mention anything about time. So, the name state transitions is not a particularly
    good name for this, since the state is not changing for any random variable; rather,
    we are trying to determine the state of the next random variable given the observed
    state of our current random variable. Coming back to our example, we can see that
    the nodes of the graph represent the different possible states of the random variable *Weather, *and
    the edges between them show the probability of the next random variable taking
    the different possible states, given the state of the current random variable.
    The self-loops show the probability of the model staying in its current state.
    In the previous Markov chain, let's say we know that the observed state of the
    current random variable is *Sunny, *then the probability that the random variable
    at the next time instance will also take the value *Sunny *is *0.8*. It could
    also take the value *Rainy *with a probability of *0.19*, or *Snowy* with a probability
    of *0.01*. One thing to note here is that the sum of all the probability values
    on all the outward edges from any state should equal 1, since it's an exhaustive
    event.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s try to code this simple Markov chain. We will start by defining
    a simple `MarkovChain` class, and we will keep on adding methods to this class
    as we go through this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can try out our example with this `MarkovChain` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In the previous code example, you might find your outputs to be different from
    what's shown here. This is because the Markov chain is probabilistic in nature
    and it picks on the next state based on a probability distribution, which can
    give different outputs on different runs.
  prefs: []
  type: TYPE_NORMAL
- en: So far in the discussion, we have considered that the probability space of the
    variables doesn't change over different instances of time. This is known as a **time-homogeneous
    Markov chain**, but it is also possible to have a **time-inhomogeneous Markov
    chain**, which also has a lot of applications but is outside the scope of this
    book.
  prefs: []
  type: TYPE_NORMAL
- en: Parameterization of Markov chains
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the code for the Markov chain in the previous section, we used a dictionary
    to parameterize the Markov chain that had the probability values of all the possible
    state transitions. Another way of representing state transitions is using a **transition
    matrix**. The transition matrix, as the name suggests, uses a tabular representation
    for the transition probabilities. The transition matrix for the example in *Figure
    1.1* is shown in the following table.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table shows the transition matrix for the Markov chain shown
    in *Figure 1.1*. The probability values represent the probability of the system
    going from the state in the row to the states mentioned in the columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **States** | **Sunny** | **Rainy** | **Snowy** |'
  prefs: []
  type: TYPE_TB
- en: '| **Sunny** | 0.8 | 0.19 | 0.01 |'
  prefs: []
  type: TYPE_TB
- en: '| **Rainy** | 0.2 | 0.7 | 0.1 |'
  prefs: []
  type: TYPE_TB
- en: '| **Snowy** | 0.1+ | 0.2 | 0.7 |'
  prefs: []
  type: TYPE_TB
- en: 'The transition matrix represents the same information as in the dictionary,
    but in a more compact way. For this reason, the transition matrix is the standard
    way of representing Markov chains. Let''s modify our `MarkovChain` class so that
    it can accept a transition matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this code should also give similar results to what we got in the previous
    section. Using a transition matrix might not seem like a good idea because it
    requires us to create extra variables to store the indices. But, in cases when
    we have hundreds of states, using a transition matrix is much more efficient than
    using the simple dictionary implementation. In the case of a transition matrix,
    we can simply use NumPy indexing to get the probability values in the `next_state`
    method, whereas we were looping over all the state names in the previous implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Properties of Markov chains
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will talk about the different properties of Markov chains,
    namely reducibility, periodicity, transience and recurrence, ergodicity, and steady-state
    analysis and limiting distributions. We will also try some simple examples of
    our `MarkovChain` class to show these properties.
  prefs: []
  type: TYPE_NORMAL
- en: Reducibility
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A Markov chain is said to be **irreducible** if we can reach any state of the
    given Markov chain from any other state. In terms of states, state *j* is said
    to be **accessible** from another state *i *if a system that started at state
    *i* has a non-zero probability of getting to the state *j*. In more formal terms,
    state *j* is said to be accessible from state *i* if an integer *n[ij] ≥ 0* exists
    such that the following condition is met:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/08868cdf-6f36-4bf9-9a0a-238a7b3ae4ac.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The *n[ij]* here is basically the number of steps it takes to go from state
    *i* to *j*, and it can be different for different pairs of values for *i* and
    *j*. Also, for a given state *i*, if all the values for *n[ij] = 0*, it means
    that all the states of the Markov chain are directly accessible from it. The accessibility
    relation is reflexive and transitive, but not necessary symmetric. We can take
    a simple example to understand this property:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2157c03e-4397-4110-a66a-e37822909cf6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.2: An example of an irreducible Markov chain'
  prefs: []
  type: TYPE_NORMAL
- en: In the previous example, it can be clearly seen that all of the states are accessible
    from all other states and hence are irreducible.
  prefs: []
  type: TYPE_NORMAL
- en: Note in the examples in *Figure 1.2* and *Figure 1.3* that we haven't represented
    edges if probability values are 0\. This helps to keep the model less complicated
    and easier to read.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we can see that state **D** is not accessible from
    **A**, **B**, or **C**. Also, state **C** is not accessible from either **A**
    or **B**. But all the states are accessible from state **D**, and states **A**
    and **B** are accessible from **C**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/90f569c3-3c03-4fcc-8f3d-cd039301b53d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.3: An example of a reducible Markov chain'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also add a couple of methods to our `MarkovChain` class to check which
    states in our chain are reachable and whether our chain is irreducible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s give our examples a try using the examples in *Figure 1.2* and *Figure
    1.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Periodicity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'State *i* is said to have period *k* if any possible path to return to state
    *i* would be a multiple of *k* steps. Formally, it is defined like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6dbaa677-d15d-4024-a027-174e6458d04f.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *gcd *means the **greatest common divisor** (**GCD**). Basically, *k* is
    the GCD of the length/number of steps of all possible paths from state *i* back
    to itself. If there are no possible paths from state *i *back to itself, then
    the period for it is not defined. We also need to note that *k* has nothing to
    do with the number of steps required to return to the starting state. For example,
    let's say that for any given state the number of steps required to return to it
    are *(4, 6, 8, 12, 16)*. In this case *k=2*, but the minimum number of steps required
    to return is *4,* and *2* doesn't even appear in the list of possible numbers
    of steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'For any given state in the Markov chain, if *k=1*, the state is said to be
    **aperiodic**. A Markov chain is called aperiodic if all of its states are aperiodic.
    One major thing to note is that, in the case of an irreducible Markov chain, a
    single aperiodic state is enough to imply that all the states are aperiodic. Let''s
    take a simple example and check the periodicity of different states:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e557d34b-4c7d-488b-aac4-79b0078332b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.4: Markov chain is also periodic
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous example, we can easily see that for state **A** the possible
    paths to return are **A** -> **B** -> **C** -> **A** or **A** -> **B** -> **C**
    -> **D** -> **E** -> **C** -> **A**. For these two paths, the path lengths are
    3 and 6, respectively, and hence state **A** has a period of 3\. Similarly, **B**,
    **C**, **D**, and **E** also each has a period of 3 in the Markov chain, and hence
    the Markov chain is also periodic:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/53bdcce6-38f9-4282-8046-827e2ae07a11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.5: Example of Markov Chain with aperiodic states.'
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we added a couple of extra edges, due to which the possible
    path lengths for **A** are now *3, 5, 7, ...;* and for **B** are *2, 3, 4, 5,
    ...* And, since the GCD of these path lengths is 1, states **A** and **B** are
    both now aperiodic. Similarly, we can compute the period of other nodes, each
    of which is also 1, and hence the Markov chain is also aperiodic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now add a couple of new methods to our `MarkovChain` class to compute
    the period of different states and check whether our model is aperiodic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now try out our methods on our examples. In this example, we will randomly
    assign probability values to different transitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Transience and recurrence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Given that we start at state *i*, it is called **transient **if there is a
    non-zero probability that we will never return to state *i*. To define this in
    more formal terms, let''s consider a random variable *T[i]* as the first return
    time to state *i*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/04d20803-cef5-45c5-9b34-dc573ecd193a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s now define another term, ![](img/4f34bad6-91a7-4962-84fc-d352a981f4fc.png),
    as the probability of the system returns to state *i* after *n* steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/270650b0-6e5f-44da-ae58-7dd809c8bc7c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we can define that any given state *i* is transient if the following condition
    is met:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2f7cf787-a6e8-4932-80c9-222ebe3bf2e5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding equation, we are basically checking whether the total sum
    of probabilities of returning to state *i* in step sizes less than ![](img/5e7bf897-9732-416f-9b50-5f97c4443743.png)
    is less than *1*. If the total sum is less than *1*, it would mean that the probability
    of *T[i]* to be ![](img/698f246f-4292-4db7-9ec1-0662ddf1b719.png) is greater than
    *0* which would mean that the state *i* is transient. The given state *i* is called **recurrent**
    if it is not transient:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e52def1e-c33f-4ac5-890c-54ae185acdab.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.6:'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding example, we can see that states **A** and **B** are transient
    because **A** doesn't have any incoming edge. **B** does have an incoming edge,
    but it's incoming from another transient state and therefore it is also transient.
    Hence, once the system leaves state **A** or **B**, it won't be able to come back.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is really simple to check whether a given state is transient or not. We
    can simply check whether there are any incoming edges from other states or not.
    If not, the state is transient. Let''s write a simple method to check this for
    our `MarkovChain` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can use this method in our example in *Figure 1.6* to check which nodes
    are transient:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In the following subsections, we will talk about the statistical properties
    of the random variable *T[i]*.
  prefs: []
  type: TYPE_NORMAL
- en: Mean recurrence time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first-return time for the initial state *i* is also known as the **hitting
    time**. It was represented using the random variable *T[i]* in the previous section.
    The **mean recurrence time **of state *i* is defined as its expected return time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ed32432b-7b90-47b3-b018-58e9ccf05b4b.png)'
  prefs: []
  type: TYPE_IMG
- en: If the mean recurrence time, *M[i]*, is finite, the state is called **positive
    recurrent**, otherwise it is called **null recurrent**.
  prefs: []
  type: TYPE_NORMAL
- en: Expected number of visits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As is evident from the name, the **expected number of visits **for any state
    *i* is the number of times the system is expected to be in that state. Also, a
    given state *i* is recurrent if and only if the expected number of visits to *i* is
    infinite:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fb6b31d3-c6c7-495d-a26b-ad26a66abaff.png)'
  prefs: []
  type: TYPE_IMG
- en: Absorbing states
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'State *i* is said to be an **absorbing state **if it is impossible for a system
    to leave that state once it reaches it. For a state to be an absorbing state,
    the probability of staying in the same state should be *1*, and all the other
    probabilities should be *0*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/09d3d7e8-7bed-43c4-aaed-c50658b6e009.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In a Markov chain, if all the states are absorbing, then we call it an absorbing
    Markov chain:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ed11565f-4c97-40c8-b946-587c3d705b3a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.7: An example showing an absorbing state C, since the probability
    of transitioning from state C to C is **1**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, we can add a very simple method to check for absorbing states in our
    `MarkovChain` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We can again check whether our state in the example is absorbing by creating
    a Markov chain and using the `is_absorbing` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Ergodicity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: State *i* is said to be ergodicif it is recurrent, has a period of *1*, and
    has a finite mean recurrence time. If all the states of a Markov chain are ergodic,
    then it's an ergodic Markov chain. In general terms, a Markov chain is ergodic
    if there is a number *N*, such that any state in the system can be reached from
    any other state in any number of steps greater than or equal to the number *N*.
    Therefore, in the case of a fully connected transition matrix, where all transitions
    have a non-zero probability, this condition is fulfilled with *N=1*.
  prefs: []
  type: TYPE_NORMAL
- en: Steady-state analysis and limiting distributions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In a Markov chain, vector *π* is called the **stationary distribution** if *∀
    j ∈ s* satisfies the following conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a953572f-46e4-4143-8949-bb5c54f86fd0.png)![](img/a08e7d8a-dc8a-4a1f-88f6-c1785c4aee42.png)![](img/abef43c3-94bd-45eb-86ef-e533349dc6b2.png)'
  prefs: []
  type: TYPE_IMG
- en: The stationary distribution is one of the most important properties of Markov
    chains, and we will talk about it in much more detail in later sections of this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous-time Markov chains
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Continuous-time Markov chains are quite similar to discrete-time Markov chains
    except for the fact that in the continuous case we explicitly model the transition
    time between the states using a positive-value random variable. Also, we consider
    the system at all possible values of time instead of just the transition times.
  prefs: []
  type: TYPE_NORMAL
- en: Exponential distributions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The random variable *x* is said to have an exponential distribution with a
    rate of distribution of *λ* if its probability density function is defined as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b3978ef3-e2da-4d7a-a261-2b515257ad70.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the rate of distribution *λ* needs to be greater than *0*. We can also
    compute the expectation of *X* as this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/75e9d1c4-2111-4f43-a9e3-10abfa201cd6.png)'
  prefs: []
  type: TYPE_IMG
- en: We see that the expectation of *X* is inversely proportional to the rate of
    learning. This means that an exponential distribution with a higher rate of learning
    would have a lower expectation. The exponential distribution is often used to
    model problems that involve modelling time until some event happens. A simple
    example could be modelling the time before an alarm clock goes off, or the time
    before a server comes to your table in a restaurant. And, as we know ![](img/dd100edc-df74-492f-8eb3-5ec358f9ebc2.png),
    the higher the learning rate, the sooner we would expect the event to happen,
    and hence the name *learning rate*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also compute the second moment and the variance of the exponential distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b1ae65c6-6e54-47db-87eb-707b6fa61733.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And, using the first moment and the second moment, we can compute the variance
    of the distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ef34e702-b79c-40a3-8895-4ae604c4887d.png)![](img/825fd73b-5528-4cc5-8038-7f6591e99edd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.x: Probability distribution of exponential distribution'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/01d538af-5079-4bf5-9a31-acceb2ec78a0.png) ![](img/5e2aefcc-b7b3-41d8-ba93-19e3fd346718.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we will move on to some of the properties of the exponential distribution
    that are relevant to our example:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Memoryless**:*Figure 1.x* shows a plot of an exponential distribution. In
    the diagram, we can clearly see that the graph after any given point (*a* in this
    case) is an exact copy of the original distribution. We can also say that an exponential
    distribution that is conditioned on (*X > a*) still stays exponential. If we think
    about this property in terms of our examples, it means that if we had an alarm
    clock, and at any time, *t*, we check that it still hasn''t gone off, we can still
    determine the distribution over the time ahead of *t*, which will be the same
    exponential distribution. This property of the exponential distribution is known
    as being **memoryless**,since at any given point in time, if you know the current
    state of the system (in this example, that the alarm hasn''t gone off), you can
    determine the probability distribution over time in the future. This property
    of exponential distributions is quite similar to Markov chains, as you may recall
    from previous sections.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Probability of minimum value**:Let''s say we have *n* independent exponential
    distributions over the random variables *X[0]*, . . ., *X[n]* with learning rates *λ[0],
    ..., λ[n]*, respectively. For these distributions, we can prove that the distribution
    of *min(X[0], . . ., X[n])* is also an exponential distribution with learning
    rate ![](img/61a44bbe-5d1f-438a-a39b-77f63415ceb4.png):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/d2d9ee00-7dc9-4139-bfc6-1033a6dc3bb4.png)'
  prefs: []
  type: TYPE_IMG
- en: We will use both of these properties of the exponential distribution in our
    example for the continuous time Markov chain in a later section.
  prefs: []
  type: TYPE_NORMAL
- en: Poisson process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Poisson process is a continuous process, and there can be multiple interpretations
    of it, which lead to different possible definitions. In this section, we will
    start with the formal definition and build up to a more simple, intuitive definition. A
    continuous-time stochastic process *N(t):t > 0* is a **Poisson process** with
    a rate *λ > 0* if the following conditions are met:'
  prefs: []
  type: TYPE_NORMAL
- en: '*N(0) = 0*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has **stationary** and **independent increments**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The distribution of *N(t)* is Poisson with mean *λt*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/ee5e44d4-e407-47e7-b423-c6b65a4838c4.png)'
  prefs: []
  type: TYPE_IMG
- en: First of all, we need to define what the stationary and independent increments
    are. For a continuous-time stochastic process, *X(t): ≥ 0*, an increment is defined
    as the difference in state of the system between two time instances; that is,
    given two time instances *s* and *t* with *s < t*, the increment from time *s* to
    time *t* is *X(t) - X(s)*. As the name suggests, a process is said to have a stationary
    increment if its distribution for the increment depends only on the time difference.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, a process is said to have a stationary increment if the distribution
    of *X(t[1]) - X(s[1])* is equal to *X(t[2]) - X(s[2])* if *t[1] > s[1],t[2] >
    s[2]* and *t[1] - s[1] = t[2] -s[2]*. A process is said to have an independent
    increment if any two increments in disjointed time intervals are independent;
    that is, if *t[1] > s[1] > t[2] > s[2]*, then the increments *X(t[2]) - X(s[2])* and *X(t1) -
    X(s1)* are independent.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's come back to defining the Poisson process. The Poisson process is
    essentially a counting process that counts the number of events that have occurred
    before time *t*. This count of the number of events before time *t* is given by
    *N(t),* and, similarly, the number of events occurring between time intervals
    *t* and *t + s* is given by *N(t + s) - N(t)*. The value *N(t + s) - N(t)* is
    Poisson-distributed with a mean *λ[s]*. We can see that the Poisson process has
    stationary increments in fixed time intervals, but as ![](img/8fc844b3-9885-4aea-8137-0b980d454a11.png),
    the value of *N(t)* will also approach infinity; that is, ![](img/a4c5c95a-61b3-4f89-a6f3-4b6eaaa929c3.png).
    Another thing worth noting is that, as the value of *λ* increases, the number
    of events happening will also increase, and that is why *λ* is also known as the
    **rate of the process**.
  prefs: []
  type: TYPE_NORMAL
- en: 'This brings us to our second simplified definition of the Poisson process.
    A continuous-time stochastic process *N(t): t ≥ 0* is called a Poisson process
    with the rate of learning *λ > 0* if the following conditions are met:'
  prefs: []
  type: TYPE_NORMAL
- en: '**N(0) = 0**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is a counting process; that is, *N(T)* gives the count of the number of events
    that have occurred before time *t*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The times between the events are distributed independently and identically,
    with an exponential distribution having a learning rate of *λ*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuous-time Markov chain example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, since we have a basic understanding of exponential distributions and the
    Poisson process, we can move on to the example to build up a continuous-time Markov
    chain. In this example, we will try to show how the properties of exponential
    distributions can be used to build up generic continuous-time Markov chains. Let's
    consider a hotel reception where *n* receptionists are working in parallel. Also
    consider that the guests arrive according to a Poisson process, with rate *λ*,
    and the service time for each guest is represented using an exponential random
    variable with learning rate *µ*. Also, if all the receptionists are busy when
    a new guest arrives, he/she will depart without getting any service. Now let's
    consider that a new guest arrives and finds all the receptionists are busy, and
    let's try to compute the expected number of busy receptionists in the next time
    interval.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start by assuming that *T[k]* represents the number of *k* busy receptionists
    in the next time instance. We can also use *T[k]* to represent the expected number
    of busy receptionists found by the next arriving guest if *k* receptionists are
    busy at the current time instance. These two representations of *T[k]* are equivalent
    because of the memoryless property of exponential distributions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Firstly, *T[0]* is clearly *0*, because if there are currently *0* busy receptionists,
    the next arrival will also find *0* busy receptionists for sure. Now, considering
    *T[1]*, if there are currently *i* busy receptionists, the next arriving guest
    will find *1* busy receptionist if the time to the next arrival is less than the
    remaining service time for the busy receptionist. From the memoryless property,
    we know that the next arrival time is exponentially distributed with a learning
    rate of *λ*, and the remaining service time is also exponentially distributed
    with a learning rate of *µ*. Therefore, the probability that the next guest will
    find one receptionist busy is ![](img/068b24db-2f99-4a87-b9cd-014c064a8012.png)
    and hence the following is true:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/442d615d-93a8-4b50-83df-cad87da8a688.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In general, we consider the situation that *k* receptionists are busy. We can
    obtain an expression for *T[k]* by conditioning on what happens first. When we
    have *k* receptionists busy, we can think of basically *k+1* independent exponential
    distributions: *k* exponentials with a learning rate of *µ* for the remaining
    service time for each receptionist, and *1* exponential distribution with a learning
    rate of *λ* for the next arriving guest. In our case, we want to condition on
    whether a service completion happens first or a new guest arrives first. The time
    for a service completion will be the minimum of the *k* exponentials. This first
    completion time is also exponentially distributed with a learning rate of *kµ*.
    Now, the probability of having a service completion before the next guest arrives
    is ![](img/7ce58380-fab8-4c4b-9d1f-f3f3fe154a99.png). Similarly, the probability
    of the next thing happening being a guest arrival is ![](img/175c5826-e3d2-475c-b4a7-3429afbc162f.png).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, based on this, we can say that if the next event is service completion,
    then the expected number of busy receptionists will be *T[k-1]*. Otherwise, if
    a guest arrives first, there will be *k* busy receptionists. Therefore we have
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/82b218d4-3aba-4497-a550-b40a0f84beb6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We need to just solve this recursion now. *T[2]* will be given by this equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/10c9c1e2-44bf-4b47-95db-9f7d20a07ea5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we continue this same pattern, we will get *T[3]* as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bd73ad20-4547-46a9-81f7-b7c796b8e79c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can see a pattern in the values of *T[1]* and *T[2]*, and therefore we can
    write a general term for it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bb49a251-7a25-4539-a1a2-cd330a334033.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s point out our observations on the previous example:'
  prefs: []
  type: TYPE_NORMAL
- en: At any given time instance, if there are *i* busy receptionists, for *i < n* there
    are *i + 1* independent exponential distributions, with *i* of them having rate *µ*,
    and *1* of them having rate *λ*. The time until the process makes a jump is exponential,
    and its rate is given by *iµ + λ*. If all the receptionists are busy, then only
    the *n* exponential distributions corresponding to the service time can trigger
    a jump, and the time until the process makes a jump is exponential with rate *nµ*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the process jumps from state *i*, for *i < n*, it jumps to state *i + 1* with
    probability ![](img/ed6d8ec5-8072-4d0c-a69a-ee917ca54e2a.png), and jumps to state
    *i - 1* with probability of ![](img/1db82956-143a-4aea-9a00-55b931fdcf1f.png).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the process makes a jump from state *i*, we can start up a whole new set
    of distributions corresponding to the state we jumped to. This is because, even
    though some of the old exponential distributions haven't triggered, it's equivalent
    to resetting or replacing those distributions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every time we jump to state *i*, regardless of when the time is, the distribution
    of how long we stay in state *i* and the probabilities of where we jump to next
    when we leave state *i* are the same. In other words, the process is time-homogeneous.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding description of a continuous-time stochastic process corresponds
    to a continuous-time Markov chain. In the next section, we will try to define
    it in a more formal way.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous-time Markov chain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we showed an example of a continuous-time Markov chain
    to give an indication of how it works. Let's now move on to formally define it.
    In a continuous-time Markov chain with a discrete state space *S,* for each state
    *i ∈ S* we have an associated set of *n[i]* independent exponential distributions
    with rates *q[i], j[1], ..., q[i],j[n[i]]*, where *j[1], ..., j[n[i]]* is the
    set of possible states the process may jump to when it leaves state *i*. And,
    when the process enters state *i*, the amount of time it spends in state *i* is
    exponentially distributed with rate *v[i] = q[i]j[1]+...+q[i]j[n[i]]*, and when
    it leaves state *i* it will go to state *j[l]* with probability ![](img/4f414e44-4ad4-415c-b90a-e4faab286f59.png)
    for *l = 1, ...,n[i]*.
  prefs: []
  type: TYPE_NORMAL
- en: We can also extend the Markov property from the discrete-time case to continuous
    time.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a continuous-time stochastic process *(X(t) : t ≥ 0)* with state space
    *S*, we say it has the Markov property if the following condition is met:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/32561f53-1637-4826-ad1c-8ce3f50d5a35.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *0 ≤ t[1] ≤ t2 ≤. . . .t[n-1 ]≤ s ≤ t* is any non-decreasing sequence
    of *n + 1* times, and *i[1, i]2, . . ., i[n-1], i, j∈ S* are any *n + 1* states
    in the state space, for any integer *n ≥ 1*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, we can extend time-homogeneity to the case of continuous-time Markov
    chains. We say that a continuous-time Markov chain is time homogenous if, for
    any *s ≤ t*, and any states *i*, *j ∈ S*, the following condition is met:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/11009b6b-202e-4776-af19-20bc7b346c53.png)'
  prefs: []
  type: TYPE_IMG
- en: As in the case of discrete-time Markov chains, a continuous-time Markov chain
    does not need to be time-homogeneous, but non-homogeneous Markov chains are out
    of scope for this book. For more details on non-homogeneous Markov chains, you
    can refer to Cheng-Chi Huang's thesis on the topic: [https://lib.dr.iastate.edu/cgi/viewcontent.cgi?article=8613&context=rtd](https://lib.dr.iastate.edu/cgi/viewcontent.cgi?article=8613&context=rtd).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s define the transition probability for a continuous-time Markov chain.
    Just as the rates *q[ij]* in a continuous-time Markov chain are the counterpart
    of the transition probabilities *p[ij]* in a discrete-time Markov chain, there
    is a counterpart to the n-step transition probabilities *p[ij](t)* for a time-homogeneous,
    continuous-time Markov chain, which is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4e6b5a80-1819-4567-a31b-770a6c9f1798.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we gave a detailed introduction to Markov chains. We talked
    about different types of Markov chains, mainly chains with a discrete state space,
    with either discrete time or continuous time. We also introduced the concepts
    of time-homogeneous and non-time-homogeneous Markov chains. We discussed the different
    properties of Markov chains in detail, and provided relevant examples and code.
  prefs: []
  type: TYPE_NORMAL
- en: Markov chains and their properties are the basic concepts on which HMMs are
    built. In the next chapter, we will discuss HMMs in much more detail.
  prefs: []
  type: TYPE_NORMAL
