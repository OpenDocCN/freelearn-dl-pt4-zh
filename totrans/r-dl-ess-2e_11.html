<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">The Next Level in Deep Learning</h1>
                </header>
            
            <article>
                
<p class="mce-root">We have almost come to the end of our journey in deep learning with R. This chapter is a bit of a mixed bag of topics. We will begin this chapter by revisiting an image classification task and building a complete image classification solution image files rather than tabular data. We will then move on to explaining transfer learning, where you can use an existing model on a new dataset. Next we discuss an important consideration in any machine learning project - how will your model be used in deployment, that is, production? We will show how to create a REST API that allows any programming language to call a deep learning model in R to predict on new data. We will then move on to briefly discussing two other deep learning topics: Generative Adversarial Networks and reinforcement learning. Finally, we will close this chapter and the book by providing a few other resources that you may be interested in.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Building a complete image classification solution</li>
<li>The ImageNet dataset</li>
<li>Transfer learning</li>
<li>Deploying TensorFlow models</li>
<li>Generative adversarial networks</li>
<li>Reinforcement learning</li>
<li>Additional deep learning resources</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Image classification models</h1>
                </header>
            
            <article>
                
<p>We covered image classification in Chapter 5,<em> Image Classification Using Convolutional Neural Networks</em>. In that chapter, we described convolutional and pooling layers that are essential for deep learning tasks involving images. We also built a number of models on a simple dataset, the MNIST dataset. Here, we are going to look at some advanced topics in image classification. First, we will build a complete image classification model using image files as input. We will look at callbacks, which are a great aid in building complex deep learning models. A call-back function will be used to persist (save) a model to file, which will be loaded back later. We then use this model in our next example, which is transfer learning. This is where you use some of the layers in a pre-trained model on new data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a complete image classification solution</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>We have built a few image classification models, but they used the MNIST dataset that was loaded from Keras or from CSV files. The data was always in tabular format. Obviously that is not how images are stored in most situations. This section looks at how to build an </span><span>image classification model using a collection of image files. </span><span>The first task is to acquire a set of image files. We are going to load the <kbd>CIFAR10</kbd> data that is included in Keras and save the data as image files. We will then use those files to build a deep learning model. After this exercise, you will know how to create a deep learning image classification task with your own image files.</span></p>
<p><span>The deep learning model in this chapter is not a complex model. The focus</span> is to show how the data pipeline for an image classification task is structured. We look at how to arrange the image files, how to use data augmentation and how callbacks can be used during training.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating the image data</h1>
                </header>
            
            <article>
                
<p>The first step is to create the image files. The code for this section is in the <kbd>Chapter11/gen_cifar10_data.R</kbd> folder. We will load the<span> </span><span><kbd>CIFAR10</kbd> data and save the image files in the data directory. The first step is to create the directory structure. There are 10 classes in the <kbd>CIFAR10</kbd> dataset: we will save 8 classes for building a model and we will use 2 classes in a later section (<em>Transfer learning</em>). The following code creates the following directories under <kbd>data</kbd>:</span></p>
<ul>
<li><kbd><span>cifar_10_images</span></kbd></li>
<li><kbd>cifar_10_images/data1</kbd></li>
<li><kbd>cifar_10_images/data2</kbd></li>
<li><kbd>cifar_10_images/data1/train</kbd></li>
<li><kbd>cifar_10_images/data1/valid</kbd></li>
<li><kbd>cifar_10_images/data2/train</kbd></li>
<li><kbd>cifar_10_images/data2/valid</kbd></li>
</ul>
<p><span>This is the structure that Keras expects image data to be stored in. If you use this structure, then the images can be used to train a model in Keras. In the first part of the code, we create these directories:</span></p>
<pre>library(keras)<br/>library(imager)<br/># this script loads the cifar_10 data from Keras<br/># and saves the data as individual images<br/><br/># create directories,<br/># we will save 8 classes in the data1 folder for model building<br/># and use 2 classes for transfer learning<br/>data_dir &lt;- "../data/cifar_10_images/"<br/>if (!dir.exists(data_dir))<br/>  dir.create(data_dir)<br/>if (!dir.exists(paste(data_dir,"data1/",sep="")))<br/>  dir.create(paste(data_dir,"data1/",sep=""))<br/>if (!dir.exists(paste(data_dir,"data2/",sep="")))<br/>  dir.create(paste(data_dir,"data2/",sep=""))<br/>train_dir1 &lt;- paste(data_dir,"data1/train/",sep="")<br/>valid_dir1 &lt;- paste(data_dir,"data1/valid/",sep="")<br/>train_dir2 &lt;- paste(data_dir,"data2/train/",sep="")<br/>valid_dir2 &lt;- paste(data_dir,"data2/valid/",sep="")<br/><br/>if (!dir.exists(train_dir1))<br/>  dir.create(train_dir1)<br/>if (!dir.exists(valid_dir1))<br/>  dir.create(valid_dir1)<br/>if (!dir.exists(train_dir2))<br/>  dir.create(train_dir2)<br/>if (!dir.exists(valid_dir2))<br/>  dir.create(valid_dir2)</pre>
<p><span>Under each of the train and valid </span><span>directories</span><span>, a separate </span><span>directory </span><span>is used for each category. We save the images for 8 classes under the </span><kbd>data1</kbd> folder, and <span>save the images for 2 classes under the </span><kbd>data2</kbd><span> folder:</span></p>
<pre># load CIFAR10 dataset<br/>c(c(x_train,y_train),c(x_test,y_test)) %&lt;-% dataset_cifar10()<br/># get the unique categories,<br/># note that unique does not mean ordered!<br/># save 8 classes in data1 folder<br/>categories &lt;- unique(y_train)<br/>for (i in categories[1:8])<br/>{<br/>  label_dir &lt;- paste(train_dir1,i,sep="")<br/>  if (!dir.exists(label_dir))<br/>    dir.create(label_dir)<br/>  label_dir &lt;- paste(valid_dir1,i,sep="")<br/>  if (!dir.exists(label_dir))<br/>    dir.create(label_dir)<br/>}<br/># save 2 classes in data2 folder<br/>for (i in categories[9:10])<br/>{<br/>  label_dir &lt;- paste(train_dir2,i,sep="")<br/>  if (!dir.exists(label_dir))<br/>    dir.create(label_dir)<br/>  label_dir &lt;- paste(valid_dir2,i,sep="")<br/>  if (!dir.exists(label_dir))<br/>    dir.create(label_dir)<br/>}</pre>
<p class="mce-root">Once we have created the directories, the next step is to save the images in the correct directories, which we will do in the following code:</p>
<pre># loop through train images and save in the correct folder<br/>for (i in 1:dim(x_train)[1])<br/>{<br/>  img &lt;- x_train[i,,,]<br/>  label &lt;- y_train[i,1]<br/>  if (label %in% categories[1:8])<br/>    image_array_save(img,paste(train_dir1,label,"/",i,".png",sep=""))<br/>  else<br/>    image_array_save(img,paste(train_dir2,label,"/",i,".png",sep=""))<br/>  if ((i %% 500)==0)<br/>    print(i)<br/>}<br/><br/># loop through test images and save in the correct folder<br/>for (i in 1:dim(x_test)[1])<br/>{<br/>  img &lt;- x_test[i,,,]<br/>  label &lt;- y_test[i,1]<br/>  if (label %in% categories[1:8])<br/>    image_array_save(img,paste(valid_dir1,label,"/",i,".png",sep=""))<br/>  else<br/>    image_array_save(img,paste(valid_dir2,label,"/",i,".png",sep=""))<br/>  if ((i %% 500)==0)<br/>    print(i)<br/>}</pre>
<p>Finally, as we have done previously, we will do a validation check to ensure that our images are correct. Let's load in 9 images from one category. We want to check that the images display correctly and that they are from the same class:</p>
<pre># plot some images to verify process<br/>image_dir &lt;- list.dirs(valid_dir1, full.names=FALSE, recursive=FALSE)[1]<br/>image_dir &lt;- paste(valid_dir1,image_dir,sep="")<br/>img_paths &lt;- paste(image_dir,list.files(image_dir),sep="/")<br/><br/>par(mfrow = c(3, 3))<br/>par(mar=c(2,2,2,2))<br/>for (i in 1:9)<br/>{<br/>  im &lt;- load.image(img_paths[i])<br/>  plot(im)<br/>}</pre>
<p>This produces the following plot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-745 image-border" src="assets/eca7b405-c020-4861-8ca1-966e7b3653b0.png" style="width:26.33em;height:26.50em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 11.1: Sample CIFAR10 images</span></div>
<p>This looks good! The images display correctly and we can see that these images all appear to be of the same class, which is cars. The images are out of focus, but that is because they are only thumbnail images of size 32 x 32.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building the deep learning model</h1>
                </header>
            
            <article>
                
<p>Once you have run the script from the preceding section, you should have 40,000 images for training in the <kbd>cifar_10_images/data1/train</kbd><span> </span>directory and 8<span>,000 images for validation in the </span><kbd>cifar_10_images/data1/valid</kbd><span> directory. </span>We will train a model with this data. The code for this section is in the <kbd>Chapter11/build_cifar10_model.R</kbd> folder. The first section creates the model definition, which should be familiar to you:</p>
<pre>library(keras)<br/><br/># train a model from a set of images<br/># note: you need to run gen_cifar10_data.R first to create the images!<br/>model &lt;- keras_model_sequential()<br/>model %&gt;%<br/>  layer_conv_2d(name="conv1", input_shape=c(32, 32, 3),<br/>    filter=32, kernel_size=c(3,3), padding="same"<br/>  ) %&gt;%<br/>  layer_activation("relu") %&gt;%<br/>  layer_conv_2d(name="conv2",filter=32, kernel_size=c(3,3),<br/>                padding="same") %&gt;%<br/>  layer_activation("relu") %&gt;%<br/>  layer_max_pooling_2d(pool_size=c(2,2)) %&gt;%<br/>  layer_dropout(0.25,name="drop1") %&gt;%<br/>  <br/>  layer_conv_2d(name="conv3",filter=64, kernel_size=c(3,3),<br/>                padding="same") %&gt;%<br/>  layer_activation("relu") %&gt;%<br/>  layer_conv_2d(name="conv4",filter=64, kernel_size=c(3,3),<br/>                padding="same") %&gt;%<br/>  layer_activation("relu") %&gt;%<br/>  layer_max_pooling_2d(pool_size=c(2,2)) %&gt;%<br/>  layer_dropout(0.25,name="drop2") %&gt;%<br/>  <br/>  layer_flatten() %&gt;%<br/>  layer_dense(256) %&gt;%<br/>  layer_activation("relu") %&gt;%<br/>  layer_dropout(0.5) %&gt;%<br/>  layer_dense(256) %&gt;%<br/>  layer_activation("relu") %&gt;%<br/>  layer_dropout(0.5) %&gt;%<br/>  <br/>  layer_dense(8) %&gt;%<br/>  layer_activation("softmax")<br/><br/>model %&gt;% compile(<br/>  loss="categorical_crossentropy",<br/>  optimizer="adam",<br/>  metrics="accuracy"<br/>)</pre>
<p>The model definition was adapted from the VGG16 architecture, which we will see later. I used a smaller number of blocks and fewer nodes. Note that the <span>final dense layer must have 8 nodes, because there are </span>only 8<span>, not 10</span> classes in the <kbd>data1</kbd> folder.</p>
<p>The next part sets up a data generator; the purpose of this is to load batches of image files into the model as it is being trained. We can also apply data augmentation to the <span>train dataset </span>in the data generator. We will select to create artificial data by randomly flipping the images horizontally, shifting images horizontally/vertically, and rotating the images by up to 15 degrees. We saw in<span> </span><a href="13e9a742-84df-48e5-bbfd-ade33dcdd01a.xhtml" target="_blank"/>Chapter 6, <em>Tuning and Optimizing Models,</em><span> </span>that data augmentation can significantly improve existing models:</p>
<pre># set up data generators to stream images to the train function<br/>data_dir &lt;- "../data/cifar_10_images/"<br/>train_dir &lt;- paste(data_dir,"data1/train/",sep="")<br/>valid_dir &lt;- paste(data_dir,"data1/valid/",sep="")<br/><br/># in CIFAR10<br/># there are 50000 images in training set<br/># and 10000 images in test set<br/># but we are only using 8/10 classes,<br/># so its 40000 train and 8000 validation<br/>num_train &lt;- 40000<br/>num_valid &lt;- 8000<br/>flow_batch_size &lt;- 50<br/># data augmentation<br/>train_gen &lt;- image_data_generator(<br/>  rotation_range=15,<br/>  width_shift_range=0.2,<br/>  height_shift_range=0.2,<br/>  horizontal_flip=TRUE,<br/>  rescale=1.0/255)<br/># get images from directory<br/>train_flow &lt;- flow_images_from_directory(<br/>  train_dir,<br/>  train_gen,<br/>  target_size=c(32,32),<br/>  batch_size=flow_batch_size,<br/>  class_mode="categorical"<br/>)<br/><br/># no augmentation on validation data<br/>valid_gen &lt;- image_data_generator(rescale=1.0/255)<br/>valid_flow &lt;- flow_images_from_directory(<br/>  valid_dir,<br/>  valid_gen,<br/>  target_size=c(32,32),<br/>  batch_size=flow_batch_size,<br/>  class_mode="categorical"<br/>)</pre>
<p>Once <span>the data generators </span>have been set up, we will also use two callback functions. Callback functions allow you to run custom code after a specific number of batches / epochs have executed. You can write your own callbacks or use some predefined callback functions. Previously, we used callbacks for logging metrics, but here the callbacks will implement model checkpointing and early stopping, which are are often used when building complex deep learning models.</p>
<p>Model checkpointing is used to save the model weights to disk. You can then load the model from disk into memory and use it for predicting new data, or you can continue training the model from the point it was saved to disk. You can save the weights after every epoch, this might be useful if you are using cloud resources and are worried about the machine terminating suddenly. Here, we use it to keep the best model we have seen so far in training. After every epoch, it checks the validation loss, and if it is lower than the<span> </span><span>validation loss in the </span>existing file, it saves the model.</p>
<p>Early stopping allows you to stop training a model when the performance no longer improves. Some people refer to it as<span> </span><span>a form of regularization because </span>early stopping can prevent a model from overfitting. While it can avoid overfitting, it works very differently to the <span>regularization</span><span> techniques, such as L1, L2, weight decay, and dropout, that we saw in <a href="6e6dd858-9f00-454a-8434-a95c59e85b25.xhtml">Chapter 3</a><em>, Deep Learning Fundamentals</em>. When using early stopping, you usually would allow the model to continue for a few epochs even if performance is no longer improving, the number of epochs allowed before stopping training is known as <em>patience</em> in Keras. Here we set it to 10, that is, if we have 10 epochs where the model has failed to improve, we stop training. </span>Here is the code to create the callbacks that we will use in our model:</p>
<pre># call-backs<br/>callbacks &lt;- list(<br/>  callback_early_stopping(monitor="val_acc",patience=10,mode="auto"),<br/>  callback_model_checkpoint(filepath="cifar_model.h5",mode="auto",<br/>                            monitor="val_loss",save_best_only=TRUE)<br/>)</pre>
<p>Here is the code to train the model:</p>
<pre># train the model using the data generators and call-backs defined above<br/>history &lt;- model %&gt;% fit_generator(<br/>  train_flow,<br/>  steps_per_epoch=as.integer(num_train/flow_batch_size),<br/>  epochs=100,<br/>  callbacks=callbacks,<br/>  validation_data=valid_flow,<br/>  validation_steps=as.integer(num_valid/flow_batch_size)<br/>)</pre>
<p>One thing to note here is that we have to manage the steps per epoch for the train and validation generators. When you set up a generator, you don't know how much data is actually there, so we need to set the number of steps for each epoch. This is simply the number of records divided by the batch size.</p>
<p>This model should take less than an hour to train on a GPU and significantly longer if training on a CPU. As the model is training, the best model is saved in <kbd>cifar_model.h5</kbd>. The best result on my machine was after epoch 64, when the validation accuracy was about 0.80. The model continued to train for another 10 epochs after this but failed to improve. Here is a plot of the training metrics:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-746 image-border" src="assets/649c0029-c744-46ad-a55b-c07d56855380.png" style="width:54.67em;height:30.08em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 11.2: Output metrics during model training</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using the saved deep learning model</h1>
                </header>
            
            <article>
                
<p>Now that we have built our deep learning model, we can restart R and reload the model from disk. The code for this section is in the <kbd>Chapter11/use_cifar10_model.R</kbd> folder. We will load the<span> model that was created in the previous section by using the following code:</span></p>
<pre>library(keras)<br/><br/># load model trained in build_cifar10_model.R<br/>model &lt;- load_model_hdf5("cifar_model.h5")</pre>
<p><span>We will use this model to generate a</span> prediction for an image file from the validation set. We will pick the first directory in the validation folder and then pick the 7th file from that folder. We load the image and apply the same preprocessing to it as we did when <span>preprocessing the images during training, which is to normalize the data by dividing the pixel values by 255.0. Here is the code that loads the image and generates the prediction:</span></p>
<pre>&gt; valid_dir &lt;-"../data/cifar_10_images/data1/valid/"<br/>&gt; first_dir &lt;- list.dirs(valid_dir, full.names=FALSE, recursive=FALSE)[1]<br/>&gt; valid_dir &lt;- paste(valid_dir,first_dir,sep="")<br/>&gt; img_path &lt;- paste(valid_dir,list.files(valid_dir)[7],sep="/")<br/><br/># load image and convert to shape we can use for prediction<br/>&gt; img &lt;- image_load(img_path, target_size = c(32,32))<br/>&gt; x &lt;- image_to_array(img)<br/>&gt; x &lt;- array_reshape(x, c(1, dim(x)))<br/>&gt; x &lt;- x / 255.0<br/>&gt; preds &lt;- model %&gt;% predict(x)<br/>&gt; preds &lt;- round(preds,3)<br/>&gt; preds<br/>      [,1] [,2] [,3] [,4] [,5] [,6] [,7]  [,8]<br/>[1,] 0.997    0    0    0    0    0    0 0.003</pre>
<p>The model predicts that the input is from <span>the first class</span> with 99.7% certainty. Since we chose the first directory in the validation set, the prediction is correct.</p>
<p>The final thing we will do with our model is to evaluate it on a directory of image files. We will also show how to generate predictions for an entire <span>directory of image files. This code loads the images from the directories using data generators, similar to how we trained the model. Here is the code that evaluates and predicts categories by using the model for the validation images we saved to disk:</span></p>
<pre>&gt; valid_dir &lt;-"../data/cifar_10_images/data1/valid/"<br/>&gt; flow_batch_size &lt;- 50<br/>&gt; num_valid &lt;- 8000<br/>&gt; <br/>&gt; valid_gen &lt;- image_data_generator(rescale=1.0/255)<br/>&gt; valid_flow &lt;- flow_images_from_directory(<br/>   valid_dir,<br/>   valid_gen,<br/>   target_size=c(32,32),<br/>   batch_size=flow_batch_size,<br/>   class_mode="categorical"<br/> )<br/>&gt; <br/>&gt; evaluate_generator(model,valid_flow,<br/>   steps=as.integer(num_valid/flow_batch_size))<br/>$`loss`<br/>[1] 0.5331386<br/><br/>$acc<br/>[1] 0.808625</pre>
<p><span>The validation accuracy is <kbd>80.86%</kbd>, which is similar to what we observed during model training, this confirms that the model was saved correctly to disk. Here is the code that</span><span> generates </span><span>predictions for all 8,000 validation images:</span></p>
<pre>&gt; preds &lt;- predict_generator(model,valid_flow,<br/>    steps=as.integer(num_valid/flow_batch_size))<br/>&gt; dim(preds)<br/>[1] 8000 8<br/> <br/>&gt; # view the predictions,<br/>&gt; preds &lt;- round(preds,3)<br/>&gt; head(preds)<br/>      [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]<br/>[1,] 0.000 0.000 0.000 0.000 0.000 0.000 0.999 0.001<br/>[2,] 0.000 0.007 0.001 0.002 0.990 0.000 0.000 0.000<br/>[3,] 0.000 0.855 0.069 0.032 0.021 0.017 0.002 0.002<br/>[4,] 0.134 0.001 0.000 0.000 0.000 0.000 0.001 0.864<br/>[5,] 0.011 0.064 0.057 0.226 0.051 0.515 0.004 0.073<br/>[6,] 0.108 0.277 0.135 0.066 0.094 0.091 0.052 0.179</pre>
<p>We can see that our prediction output has 8,000 rows and 8 columns, so for each <span>validation image, there is </span>a probability for each category. We can see that the sum for each row is 1.0 and that there is usually one class that has a significant probability. For example, the model is predicting that the first image is in class 7 with a probability of 99.9%.</p>
<p>We have now built a complete image classification solution using image files. This template can be reused for other tasks once the image data is stored in the same directory structure. If the new task had a different number of categories, then all you would need to change is the number of nodes in the last dense layer and possibly the softmax activation. However, if you did have a new image classification task that involved real-life images, then you probably would get better results by using an existing model and using transfer learning. Before I explain how to do that, I will provide some background on the ImageNet dataset, which is often used to train complex models which are then used in transfer learning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The ImageNet dataset</h1>
                </header>
            
            <article>
                
<p><span>From 2010, an annual image classification competition has been run called the <strong>ImageNet Large Scale Visual Recognition Challenge</strong> (<strong>ILSVRC</strong>). The image set consists of over 14 million images that have been labelled with over 1,000 categories. Without this dataset, there would not be the huge interest in deep learning that there is today. It has provided the stimulus for research in deep learning through the competition. The models and weights learnt on the Imagenet dataset have then been used in thousands of other deep learning models through transfer learning. The actual history of ImageNet is an interesting story. The following link (<a href="https://qz.com/1034972/the-data-that-changed-the-direction-of-ai-research-and-possibly-the-world/">https://qz.com/1034972/the-data-that-changed-the-direction-of-ai-research-and-possibly-the-world/</a>) explains how the project</span><span> originally got little attention, but that changed with a number of linked events:</span></p>
<ul>
<li>The ILSVRC became the benchmark for image classification for researchers.</li>
<li>NVIDIA had released libraries that allowed access to <strong>graphical processing units</strong> (<strong>GPUs</strong>). GPUs are designed to do massive parallel matrix operations, which is exactly what is needed to create deep neural networks.</li>
<li>Geoffrey Hinton, Ilya Sutskever, and Alex Krizhevsky from the University of Toronto created a deep convolutional neural network architecture called <strong>AlexNet</strong> that won the competition in 2012. Although this was not the first use of convolutional neural networks, their submission beat the next approach by a huge margin.</li>
<li>Researchers noticed that when they trained models using the ImageNet dataset, they could use them on other classification tasks. They almost always got much better performance from using the ImageNet model and then using transfer learning than just training a model from scratch on the original dataset.</li>
</ul>
<p>The advances in image classification can be tracked with some notable entries in the <span>ILSVRC</span> <span>competition:</span></p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td style="width: 61%">
<p><strong>Team</strong></p>
</td>
<td style="width: 11%">
<p><strong>Year</strong></p>
</td>
<td style="width: 12.8509%">
<p><strong>Error rate</strong></p>
</td>
</tr>
<tr>
<td style="width: 61%">
<p><span>2011 ILSVRC winner (not deep learning)</span></p>
</td>
<td style="width: 11%">
<p>2011</p>
</td>
<td style="width: 12.8509%">
<p>25.8%</p>
</td>
</tr>
<tr>
<td style="width: 61%">
<p><span>AlexNet (7 layers)</span></p>
</td>
<td style="width: 11%">
<p>2012</p>
</td>
<td style="width: 12.8509%">
<p>15.3%</p>
</td>
</tr>
<tr>
<td style="width: 61%">
<p><span>VGG Net (16 layers)</span></p>
</td>
<td style="width: 11%">
<p>2014</p>
</td>
<td style="width: 12.8509%">
<p>7.32%</p>
</td>
</tr>
<tr>
<td style="width: 61%">
<p><span>GoogLeNet / Inception (19 layers)</span></p>
</td>
<td style="width: 11%">
<p>2014</p>
</td>
<td style="width: 12.8509%">
<p>6.67%</p>
</td>
</tr>
<tr>
<td style="width: 61%">
<p><span>ResNet (152 layers)</span></p>
</td>
<td style="width: 11%">
<p>2015</p>
</td>
<td style="width: 12.8509%">
<p>3.57%</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p class="mce-root">VGGNet, Inception, and Resnet are all available in Keras. A complete list of available networks can be found at <a href="https://keras.rstudio.com/reference/index.html#section-applications">https://keras.rstudio.com/reference/index.html#section-applications</a>.</p>
<p class="mce-root">The models for these networks can be loaded in Keras and used to classify a new image into one of the 1,000 categories in ImageNet. We will look at this next. If you have a new classification task with a different set of images, then you can also use these networks and then use transfer learning, which we will look at later in this chapter. The number of categories can be different; you do not need to have 1,000 categories for your task.</p>
<p>Perhaps the simplest model to begin with is VGGNet, as it is not that different to what we saw in <a href="1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml">Chapter 5</a>, <em>Image Classification Using Convolutional Neural Networks</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Loading an existing model</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this section, we will load an existing model (VGGNet) in Keras and use it to classify a new image. The code for this section can be found in the <kbd>Chapter11/vgg_model.R</kbd>. We will begin by loading the model and looking at its architecture:</p>
<pre>&gt; library(keras)<br/>&gt; model &lt;- application_vgg16(weights = 'imagenet', include_top = TRUE)<br/><br/>&gt; summary(model)<br/>_________________________________________________________________________<br/>Layer (type)                     Output Shape                 Param # <br/>=========================================================================<br/>input_1 (InputLayer)             (None, 224, 224, 3)          0 <br/>_________________________________________________________________________<br/>block1_conv1 (Conv2D)            (None, 224, 224, 64)         1792 <br/>_________________________________________________________________________<br/>block1_conv2 (Conv2D)            (None, 224, 224, 64)         36928 <br/>_________________________________________________________________________<br/>block1_pool (MaxPooling2D)       (None, 112, 112, 64)         0 <br/>_________________________________________________________________________<br/>block2_conv1 (Conv2D)            (None, 112, 112, 128)        73856 <br/>_________________________________________________________________________<br/>block2_conv2 (Conv2D)            (None, 112, 112, 128)        147584 <br/>_________________________________________________________________________<br/>block2_pool (MaxPooling2D)       (None, 56, 56, 128)          0 <br/>_________________________________________________________________________<br/>block3_conv1 (Conv2D)            (None, 56, 56, 256)          295168 <br/>_________________________________________________________________________<br/>block3_conv2 (Conv2D)            (None, 56, 56, 256)          590080 <br/>_________________________________________________________________________<br/>block3_conv3 (Conv2D)            (None, 56, 56, 256)          590080 <br/>_________________________________________________________________________<br/>block3_pool (MaxPooling2D)       (None, 28, 28, 256)          0 <br/>_________________________________________________________________________<br/>block4_conv1 (Conv2D)            (None, 28, 28, 512)          1180160 <br/>_________________________________________________________________________<br/>block4_conv2 (Conv2D)            (None, 28, 28, 512)          2359808 <br/>_________________________________________________________________________<br/>block4_conv3 (Conv2D)            (None, 28, 28, 512)          2359808 <br/>_________________________________________________________________________<br/>block4_pool (MaxPooling2D)       (None, 14, 14, 512)          0 <br/>_________________________________________________________________________<br/>block5_conv1 (Conv2D)            (None, 14, 14, 512)          2359808 <br/>_________________________________________________________________________<br/>block5_conv2 (Conv2D)            (None, 14, 14, 512)          2359808 <br/>_________________________________________________________________________<br/>block5_conv3 (Conv2D)            (None, 14, 14, 512)          2359808 <br/>_________________________________________________________________________<br/>block5_pool (MaxPooling2D)       (None, 7, 7, 512)            0 <br/>_________________________________________________________________________<br/>flatten (Flatten)                (None, 25088)                0 <br/>_________________________________________________________________________<br/>fc1 (Dense)                      (None, 4096)                 102764544 <br/>_________________________________________________________________________<br/>fc2 (Dense)                      (None, 4096)                 16781312 <br/>_________________________________________________________________________<br/>predictions (Dense)              (None, 1000)                 4097000 <br/>=========================================================================<br/>Total params: 138,357,544<br/>Trainable params: 138,357,544<br/>Non-trainable params: 0<br/>_________________________________________________________________________</pre>
<p>This model looks complicated, but when you look at it in detail, there is nothing that we haven't seen before. There are two blocks with two convolutional layers, which are then followed by a max pooling layer. These are followed by three blocks with three <span>convolutional layers, which are then followed by a </span><span>max pooling layer. Finally, we have a flatten layer and three dense layers. The last dense layer has 1,000 nodes, which is the number of categories in the ImageNet dataset.</span></p>
<p>Let's use this model to make a prediction for a new image. This image is a bicycle, albeit an unusual one – it is a time trial bicycle:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-748 image-border" src="assets/6c765554-eb64-4046-9290-bd8fbd86f830.jpg" style="width:47.92em;height:28.58em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 11.3: Test image for classification</div>
<p><span>The following code block processes the image into a suitable format to use in the VGG model. It loads the image and resizes it to the dimensions of the images used to train the model <kbd>(224, 224)</kbd>. We then have to preprocess the image data before calling the <kbd>predict</kbd> function. Finally, there is a helper function in Keras called </span><kbd>imagenet_decode_predictions</kbd><span> that we can use to get the prediction categories and the probabilities:</span></p>
<pre>&gt; img_path &lt;- "image1.jpg"<br/>&gt; img &lt;- image_load(img_path, target_size = c(224,224))<br/>&gt; x &lt;- image_to_array(img)<br/>&gt; x &lt;- array_reshape(x, c(1, dim(x)))<br/>&gt; x &lt;- imagenet_preprocess_input(x)<br/><br/>&gt; preds &lt;- model %&gt;% predict(x)<br/>&gt; imagenet_decode_predictions(preds, top = 5)<br/>[[1]]<br/>  class_name       class_description      score<br/>1  n02835271   bicycle-built-for-two 0.31723219<br/>2  n03792782   mountain_bike         0.16578741<br/>3  n03891332   parking_meter         0.12548350<br/>4  n04485082   tripod                0.06399463<br/>5  n09193705   alp                   0.04852912</pre>
<p>The top prediction is <kbd>bicycle-built-for-two</kbd> at just over 30%, and the second best prediction is <kbd>mountain_bike</kbd> at 16.5%. ImageNet has a category for a tricycle and unicycle (and even a <kbd>Model_T</kbd> car!), but <span>does not seem to have a category for a bicycle, so this prediction is a not a bad result. However, <kbd>mountain_bike</kbd></span> i<span>s probably a more accurate category for this image as it definitely is not a bicycle for two people!</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Transfer learning</h1>
                </header>
            
            <article>
                
<p class="mce-root">One of the few disadvantages deep learning has over traditional machine learning is that it requires lots of data. <span>Transfer learning is one way to overcome this, by using the weights of a previously trained model (usually trained on ImageNet data) and then applying them to a new problem set.</span></p>
<p><span>The ImageNet dataset consists of 15 million images in 1,000 classes. </span>Since we can reuse parts of a model that has been trained on this amount of data, it may be possible to train the new model with just a few hundred images per category. This would depend on the images being somewhat related to the data used in the original model. For example, trying to use transfer learning from ImageNet models (which is trained on photographs) on data from other domains (for example, satellite or medical scans), would be more difficult and would require much more data. Some of the concerns we raised in <a href="13e9a742-84df-48e5-bbfd-ade33dcdd01a.xhtml">Chapter 6</a>, <em>Tuning and Optimizing Models</em>, about different data sources also applies. If the data is from a different type of data distribution, for example, mobile images, off-center photos, different lighting conditions, and so on, this will also matter. This is where creating more synthetic data through data augmentation can make a big difference.</p>
<p>We will now apply transfer learning using the model we built in the <em>Building the deep learning model</em> section. Recall that we only used 8/10 of the classes in building and evaluating this model. We will now build a new model using transfer learning that will differentiate between the 2 remaining classes. The code for this section can be found in the <kbd>Chapter11/cifar_txr.R</kbd> folder:</p>
<ol>
<li>We will use the model we built in the previous section and load it using the following code:</li>
</ol>
<pre style="padding-left: 60px">library(keras)<br/><br/># load model trained in build_cifar10_model.R<br/>model &lt;- load_model_hdf5("cifar_model.h5")</pre>
<ol start="2">
<li>Next, we will call <kbd>trainable_weights</kbd> on the model object to get the number of trainable layers. This will count all the non-activation layers in our model.</li>
</ol>
<pre style="padding-left: 60px">&gt; length(model$trainable_weights)<br/>[1] 14</pre>
<ol start="3">
<li>Next, we freeze the early layers in our model<span>. Freezing the layers in a model means that the weights will not be updated during back-propagation. We freeze the convolutional blocks, but do not freeze the dense layers at the end of the model. We use the names we set in the model definition to set the first and last layers to freeze. </span></li>
<li><span>We then call <kbd>trainable_weights</kbd> on the model once more to confirm that the number changed from the preceding value, <kbd>14</kbd>, to <kbd>6</kbd>. Here is the code for freezing the layers:</span></li>
</ol>
<pre style="padding-left: 60px">freeze_weights(model,from="conv1", to="drop2")<br/>length(model$trainable_weights)<br/>[1] 6</pre>
<ol start="5">
<li>Next, we will remove the last dense layer and last activation layer from our model by calling the <kbd>pop_layer</kbd> function twice in the following code. We need to do this because our new task has 2 classes and not 8:</li>
</ol>
<pre style="padding-left: 60px"># remove the softmax layer<br/>pop_layer(model)<br/>pop_layer(model)</pre>
<ol start="6">
<li>Now, we can add a new layer with <span>2 nodes (because we have 2 classes in the </span>new task) by using the following code:</li>
</ol>
<pre style="padding-left: 60px"># add a new layer that has the correct number of nodes for the new task<br/>model %&gt;%<br/>  layer_dense(name="new_dense",units=2, activation='softmax')<br/>summary(model)</pre>
<ol start="7">
<li>The following code block compiles the model again and sets up the generators to load the data. This is similar to what we saw when we built the model. One difference is that we do not use data augmentation here:</li>
</ol>
<pre style="padding-left: 60px"># compile the model again<br/>model %&gt;% compile(<br/>  loss = "binary_crossentropy",<br/>  optimizer="adam",<br/>  metrics=c('accuracy')<br/>)<br/><br/># set up data generators to stream images to the train function<br/>data_dir &lt;- "../data/cifar_10_images/"<br/>train_dir &lt;- paste(data_dir,"data2/train/",sep="")<br/>valid_dir &lt;- paste(data_dir,"data2/valid/",sep="")<br/><br/># in CIFAR10, # there are 50000 images in training set<br/># and 10000 images in test set<br/># but we are only using 2/10 classes,<br/># so its 10000 train and 2000 validation<br/>num_train &lt;- 10000<br/>num_valid &lt;- 2000<br/>flow_batch_size &lt;- 50<br/># no data augmentation<br/>train_gen &lt;- image_data_generator(rescale=1.0/255)<br/># get images from directory<br/>train_flow &lt;- flow_images_from_directory(<br/>  train_dir,<br/>  train_gen,<br/>  target_size=c(32,32),<br/>  batch_size=flow_batch_size,<br/>  class_mode="categorical"<br/>)<br/><br/># no augmentation on validation data<br/>valid_gen &lt;- image_data_generator(rescale=1.0/255)<br/>valid_flow &lt;- flow_images_from_directory(<br/>  valid_dir,<br/>  valid_gen,<br/>  target_size=c(32,32),<br/>  batch_size=flow_batch_size,<br/>  class_mode="categorical"<br/>)</pre>
<ol start="8">
<li>Finally, we can train the model by using the following code:</li>
</ol>
<pre style="padding-left: 60px">&gt; history &lt;- model %&gt;% fit_generator(<br/>+ train_flow,<br/>+ steps_per_epoch=as.integer(num_train/flow_batch_size),<br/>+ epochs=10,<br/>+ validation_data=valid_flow,<br/>+ validation_steps=as.integer(num_valid/flow_batch_size)<br/>+ )<br/>Found 10000 images belonging to 2 classes.<br/>Found 2000 images belonging to 2 classes.<br/>Epoch 1/10<br/>200/200 [==============================] - 5s 27ms/step - loss: 0.3115 - acc: 0.8811 - val_loss: 0.1529 - val_acc: 0.9425<br/>Epoch 2/10<br/>200/200 [==============================] - 4s 20ms/step - loss: 0.1971 - acc: 0.9293 - val_loss: 0.1316 - val_acc: 0.9550<br/>Epoch 3/10<br/>200/200 [==============================] - 4s 20ms/step - loss: 0.1637 - acc: 0.9382 - val_loss: 0.1248 - val_acc: 0.9540<br/>Epoch 4/10<br/>200/200 [==============================] - 4s 20ms/step - loss: 0.1367 - acc: 0.9497 - val_loss: 0.1200 - val_acc: 0.9575<br/>Epoch 5/10<br/>200/200 [==============================] - 4s 20ms/step - loss: 0.1227 - acc: 0.9543 - val_loss: 0.1148 - val_acc: 0.9605<br/>Epoch 6/10<br/>200/200 [==============================] - 4s 20ms/step - loss: 0.1161 - acc: 0.9559 - val_loss: 0.1110 - val_acc: 0.9625<br/>Epoch 7/10<br/>200/200 [==============================] - 4s 20ms/step - loss: 0.1022 - acc: 0.9622 - val_loss: 0.1118 - val_acc: 0.9620<br/>Epoch 8/10<br/>200/200 [==============================] - 4s 20ms/step - loss: 0.0996 - acc: 0.9655 - val_loss: 0.1068 - val_acc: 0.9645<br/>Epoch 9/10<br/>200/200 [==============================] - 4s 20ms/step - loss: 0.0861 - acc: 0.9687 - val_loss: 0.1095 - val_acc: 0.9655<br/>Epoch 10/10<br/>200/200 [==============================] - 4s 20ms/step - loss: 0.0849 - acc: 0.9696 - val_loss: 0.1189 - val_acc: 0.9620</pre>
<p>The best accuracy was on epoch 9 when we got <kbd>96.55%</kbd> accuracy. This is significantly better than what we got on the multi-classification model (approximately 81%), but binary classification tasks are much easier than <span>multi-classification tasks. We can also see that the model was very quick to train, because it only had to update the weights in the last few layers.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deploying TensorFlow models</h1>
                </header>
            
            <article>
                
<p class="mce-root">Historically, one of the perceived disadvantages of using R for data science projects was the difficulty in deploying machine learning models built in R. This often meant that companies used R mainly as a prototyping tool to build models which were then rewritten in another language, such as Java and .NET. It is also one of the main reasons cited for companies switching to Python for data science as Python has more <em>glue code</em>, which allows it to interface with other programming languages.</p>
<p>Thankfully, this is changing. One interesting new product from RStudio, called<span> </span><span>RStudio </span>Connect, allows companies to create a platform for sharing R-Shiny applications, reports in R Markdown, dashboards, and models. This allows companies to serve machine learning models using a REST interface.</p>
<p>The TensorFlow (and Keras) models we have created in this book can be deployed without any runtime dependency on either R or Python. One way of doing this is TensorFlow Serving, which is an open source software library for serving TensorFlow models. Another option is to use the Google CloudML interface that we saw in<span> </span><a href="2ea4d422-70f7-47af-a330-f0901f6f5fd3.xhtml">Chapter 10</a><em>, Running Deep Learning Models in the Cloud</em>. This allows you to create a publicly available REST API that can be called from your applications. TensorFlow models can also be deployed to iPhones and Android mobile phones.</p>
<p>There are two basic options for scoring models in production:</p>
<ul>
<li><strong>Batch mode</strong>: In batch mode, a set of data is scored offline and the prediction results are stored and used elsewhere</li>
<li><strong>Real-time mode</strong>: In <span>real-time mode, the data is scored immediately, usually a record at a time, and the results are immediately used.</span></li>
</ul>
<p><span>For a lot of applications, batch mode is more than adequate. You should carefully consider if you really need a </span><span>real-time prediction system as it is requires more resources and needs constant monitoring. </span>It is much more efficient to score records in a batch rather than individually. Another advantage of batch mode is that you know the demand on the application beforehand and can plan resources accordingly. With real-time systems, a spike in demand or a denial of service attack can cause problems with your prediction model.</p>
<p>We have already seen batch mode for a saved model in the <em>Using the saved deep learning model</em><span> </span>section in this chapter. So, let's look at how we can build a REST interface to get a prediction on new data from a deep learning model in real-time. This will use the <kbd>tfdeploy</kbd> package. The code for this section can be found in the <kbd>Chapter11/deploy_model.R</kbd>. We are going to build a simple model based on the MNIST dataset and then create a web interface where we can submit a new image for classification. Here is the first part of the code that builds the model and prints out the predictions for the first 5 rows in the test set:</p>
<pre>library(keras)<br/>#devtools::install_github("rstudio/tfdeploy")<br/>library(tfdeploy)<br/><br/># load data<br/>c(c(x_train, y_train), c(x_test, y_test)) %&lt;-% dataset_mnist()<br/><br/># reshape and rescale<br/>x_train &lt;- array_reshape(x_train, dim=c(nrow(x_train), 784)) / 255<br/>x_test &lt;- array_reshape(x_test, dim=c(nrow(x_test), 784)) / 255<br/><br/># one-hot encode response<br/>y_train &lt;- to_categorical(y_train, 10)<br/>y_test &lt;- to_categorical(y_test, 10)<br/><br/># define and compile model<br/>model &lt;- keras_model_sequential()<br/>model %&gt;%<br/>  layer_dense(units=256, activation='relu', input_shape=c(784),name="image") %&gt;%<br/>  layer_dense(units=128, activation='relu') %&gt;%<br/>  layer_dense(units=10, activation='softmax',name="prediction") %&gt;%<br/>  compile(<br/>    loss='categorical_crossentropy',<br/>    optimizer=optimizer_rmsprop(),<br/>    metrics=c('accuracy')<br/>  )<br/><br/># train model<br/>history &lt;- model %&gt;% fit(<br/>  x_train, y_train,<br/>  epochs=10, batch_size=128,<br/>  validation_split=0.2<br/>)<br/>preds &lt;- round(predict(model, x_test[1:5,]),0)<br/>head(preds)<br/>     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]<br/>[1,]    0    0    0    0    0    0    0    1    0     0<br/>[2,]    0    0    1    0    0    0    0    0    0     0<br/>[3,]    0    1    0    0    0    0    0    0    0     0<br/>[4,]    1    0    0    0    0    0    0    0    0     0<br/>[5,]    0    0    0    0    1    0    0    0    0     0</pre>
<p>There is nothing new about this code. Next, we will create a JSON file for one image file in the test set. <span>JSON stands for JavaScript Object Notation, and is the accepted standard for </span>serializing and sending data over a network connection. If HTML is the language for computers-to-human<span> </span><span>web </span>communication, <span>JSON is the language for computers-to-computers web communication. It is heavily used in microservice architecture, which is a framework for building a complex web ecosystem from lots of small web services.</span> <span>The data in the JSON file must have the same preprocessing applied as what was done during training</span> <span>–</span><span> since we normalized the training data, we must also normalize the test data. </span>The following code creates a JSON file with the values for the first instance in the testset and saves the file to <kbd>json_image.json</kbd>:</p>
<pre><br/># create a json file for an image from the test set<br/>json &lt;- "{\"instances\": [{\"image_input\": ["<br/>json &lt;- paste(json,paste(x_test[1,],collapse=","),sep="")<br/>json &lt;- paste(json,"]}]}",sep="")<br/>write.table(json,"json_image.json",row.names=FALSE,col.names=FALSE,quote=FALSE)</pre>
<p>Now that we have a JSON file, let's create a REST web interface for our model:</p>
<pre>export_savedmodel(model, "savedmodel")<br/>serve_savedmodel('savedmodel', browse=TRUE)</pre>
<p>Once you do this, a new web page should pop up that is similar to the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-749 image-border" src="assets/a936938e-a122-4b20-ba84-3ca27b978e20.png" style="width:65.67em;height:63.58em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 11.4: Swagger UI for the TensorFlow model REST web service</div>
<p>This is a Swagger UI page showing the RESTful web services for the TensorFlow model. This allows us to test our API. While we could try to use this interface, it is easier to use the JSON file we just created. Open up Command Prompt on your machine, browse to the <kbd>Chapter11</kbd> code directory, and run the following command:</p>
<pre>curl -X POST -H "Content-Type: application/json" -d @json_image.json http://localhost:8089/serving_default/predict</pre>
<p>You should get the following response:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-750 image-border" src="assets/67891d57-0caf-4506-956a-8f71e9589c86.png" style="width:100.33em;height:13.17em;"/></p>
<p>The REST web interface returns another JSON string with these results. We can see that the 8th entry in the list is 1.0 and that all the other numbers are extremely small. This matches the prediction for the first row that we saw in the code at the start of this section.</p>
<div class="packt_infobox">I imagine that half of the people reading this are very excited about this and the other half couldn't care less! The half that really like this can see how R can be used to serve model predictions that interface with web applications. This opens up huge possibilities for using R, where beforehand it was believed that you either had to use Python or you had to redevelop models in other languages. The half that couldn't care less probably never had to deal with these issues with R, but in time they will see how important this is as well!</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Other deep learning topics</h1>
                </header>
            
            <article>
                
<p>Two topics that get a lot of attention in deep learning are <strong><span>Generative Adversarial Networks </span>(GANs)</strong> and reinforcement learning. We only briefly introduce both topics, t<span>here is no code for this section for a couple of reasons. Firstly both topics are very advanced and trying to create a use-case that is non-trivial would require a few chapters for each topic. Secondly, reinforcement learning is not well supported in R, so creating an example would be difficult. Despite this, I include both of these topics in the book because I believe they are important emerging areas in deep learning that you should definitely be aware of.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generative adversarial networks</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>Generative Adversarial Networks have been called <em>the coolest thing since sliced bread</em> by Yann LeCunn, one of the most prominent people in deep learning. If he believes that, then we should all take notice!</span></p>
<p>Most of our models in this book have been discriminative models, that is, we try to differentiate one class from another. However in <a href="e0045e3c-8afd-4e59-be9f-29e652a9a8b1.xhtml">Chapter 9</a>, <em>Anomaly Detection and Recommendation Systems</em> we created a generative model in the anomaly detection use-case. This model could create new data, albeit a different representation of the input data. Creating complex generative models is a very hot research topic in deep learning. Many believe that <span>generative models </span>can solve many problems in deep learning, including one of the biggest, which is the lack of correctly labelled data. However before GANs, it was difficult to judge how good a generative model actually was. A group of researchers led by Ian Goodfellow proposed Generative Adversarial Networks (GANs) (Goodfellow, Ian, et al. <em>Generative adversarial nets.</em> Advances in neural information processing systems. 2014) that could be used to create realistic artificial data.</p>
<p>In GANs, two models are trained together, the first is a generative model G that creates new data. The second model is a discriminative model D that tries to predict if an example is from the real dataset, or has been created by the generative model G. The basic GAN idea is for the generative model to try to fool the discriminative model, while <span>the discriminative model must try to tell the differences from fake data and real data</span>. The generator keeps creating new data and refining its process until the discriminative model can no longer tell the difference between the generated data and the real training data.</p>
<p>In the paper, the process is compared to a team of counterfeiters creating fake currency (the generative model) and the police who are trying to detect the counterfeit currency (the discriminative model). Both models improve incrementally until it is impossible to differentiate between the counterfeit currency and the real currency.</p>
<p>GANs are notoriously hard to train. One paper that documented a working approach to training GANs on image data called their approach deep convolutional generative adversarial networks (Radford, Alec, Luke Metz, and Soumith Chintala. <em>Unsupervised representation learning with deep convolutional generative adversarial networks</em>. arXiv preprint arXiv:1511.06434 (2015)). In this paper, they recommended a number of guidelines to train stable <span>deep convolutional generative adversarial networks (</span>DCGANs):</p>
<ul>
<li>Replace any pooling layers with strided convolutions (discriminator) and fractional-strided convolutions (generator).</li>
<li>Use batchnorm for both models.</li>
<li>Remove fully connected hidden layers for deep architectures.</li>
<li>For the generator, use tanh activation in the output layer and ReLU elsewhere.</li>
<li><span>For the discriminator, u</span>se LeakyReLU activation for all layers.</li>
</ul>
<p>Training <span>DCGANs is an iterative process, t</span>he following steps are repeated:</p>
<ul>
<li>First the generator creates some new examples.</li>
<li>The discriminator is trained using real data and generated data.</li>
<li>After the discriminator has been trained, both models are trained together. The discriminator's weights are frozen, but its gradients are used in the generator model so that the generator can update it's weights.</li>
</ul>
<p>During this loop, it is vital that one model does not dominate the other model, they should both improve together. If the discriminator is too smart and is very confident that the instances from the generator are fakes, then there is no signal passed back to the generator and it can no longer improve. Similarly, if the generator finds a clever trick to fool the discriminator, it may generate images that are too similar, or of only one input category and the GAN again fails to improve. This shows the difficulty in training any GAN's, you have to find a set of parameters that works for the data and keeps two models synchronized. A good reference from one of the authors of the DCGAN paper on advice to make GANs work is <a href="https://github.com/soumith/ganhacks">https://github.com/soumith/ganhacks</a>.</p>
<p>GANs have many potential use-cases including being able to train with less data. They also could be used to <span>predict missing data, e.g. add definition to blurred images / videos. They in reinforcement learning, which we discuss in the next section.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reinforcement learning</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>Reinforcement learning has a deceptively </span>simple<span> definition: an agent interacts with its environment and changes its behaviors based on the the consequences of its actions. This is actually how humans and animals behave in the real world and is why many people believe that reinforcement learning is the key to achieving artificial general intelligence (AGI).</span></p>
<p class="mce-root"><span>Artificial general intelligence (AGI) will be achieved if and when computers can perform complex tasks as well as people. This also requires that computers be able to adapt their current knowledge to new problems, just as humans do. Experts disagree on whether AGI is even possible. </span>If we take the very first image from <a href="00c01383-1886-46d0-9435-29dfb3e08055.xhtml">Chapter 1</a><em>, Getting Started with Deep Learning</em>, we can see that the definition of artificial intelligence (<em>... performing functions that require intelligence when performed by people</em>) closely resembles the definition of <span>reinforcement learning:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-751 image-border" src="assets/7c6cc2bc-45b6-48ae-ac40-0278d97af008.png" style="width:31.00em;height:31.00em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 11.5: <span>The relationship between artificial intelligence, machine learning, and deep learning</span></div>
<p class="mce-root"><span>David Silver, one of the most prominent people in reinforcement learning and one of the main people involved in AlphaGo, coined the following formula:</span></p>
<p class="mce-root CDPAlignCenter CDPAlign"><em><span>Artificial intelligence = Reinforcement learning + Deep learning</span></em></p>
<p class="mce-root">One well-known example of reinforcement learning is an algorithm that can play a number of Atari 2600 video games better than most people by just using image pixels as <span>input to the algorithm</span>. The reinforcement learning algorithm learns by playing the game thousands, maybe millions of times, and learns what actions it needs to take to achieve rewards, which could be to collect points or to keep its avatar alive as long as possible. Perhaps the best known example in reinforcement learning is AlphaGo, which defeated one of the best players in the world in Go. AlphaGo is a hybrid artificial system that was composed of neural networks, reinforcement learning, and heuristic search algorithms. It is much harder to program a computer to win in a game of Go than other games, such as chess, because brute-force approaches are not feasible. An additional problem in Go is the difficulty in evaluating the current position.</p>
<p class="mce-root"><span>A formal definition of</span> reinforcement <span>learning is where an agent observes a state</span> <em>s<sub>t</sub></em> <span>at timestep</span> <em>t</em><span>. When in state</span> <em>s<sub>t</sub></em><span>, the agent interacts with its environment by taking action, which means that the agent transitions to the new state</span> <em>s<sub>t+1</sub></em><span>. The movement into a new state is linked with a reward, and the goal of the agent is to learn a policy that maximizes the expected rewards. The rewards could be cumulative and/or discounted; for example, near-time rewards are worth more than far-off returns. The value function is the prediction of the future reward. If the new state</span> <em>s<sub>t+1</sub></em> <span>is dependent only on the previous state</span> <em>s<sub>t</sub></em> <span>and the action</span> <em>a<sub>t</sub></em><span>, then it becomes a Markov process. However, one of the major problems in reinforcement learning is that rewards may be sparse and that there may a long delay between an action and achieving the reward. There is also the problem where an immediate reward might cause the agent to go down a path that could ultimately be destructive. For example, in a computer game, the agent could take an immediate step of trying to maximize a score, but this ultimately means that the character <em>dies</em> sooner. In a more real-life scenario, for example, a self-driving car, if the goal is to get to a location quickly, then the agent might decide to drive dangerously, putting passengers and other road users at risk.</span></p>
<p class="mce-root">The core elements in reinforcement learning include the following:</p>
<ul>
<li>Rewards are the gains that an agent can achieve in the near-term.</li>
<li>A value function is the expected reward an agent can expect to achieve from the current state. The value function looks at the long-term rewards / goals, so this may mean taking actions that do not maximize rewards in the short-term.</li>
<li>A policy guides the actions that an agent can take, it maps the states to possible actions from that state.</li>
<li>The model is encapsulation of the environment that the agent interacts with. As such it is an incomplete representation of the physical world, but as long as it can accurately simulate the next step given an action, and calculate the reward, then it is an adequate representation that can be used for reinforcement learning.</li>
</ul>
<p class="mce-root">Other important mechanisms in reinforcement learning include multi-label classification, memory, unsupervised learning, knowledge transfer (using knowledge learned from one problem to solve related problems), search (to select the next best action by looking at all possible permutations <em>x</em> moves ahead), multi-agent RL, and learning to learn. We will not go into detail on these tasks, some may already be familiar to you. However, this list does highlight the complexity involved in <span>reinforcement learning.</span></p>
<p class="mce-root"><span>Deep learning can be used as a component in reinforcement learning to work on subtasks, such as object detection, speech recognition, NLP, and so on. Deep learning can also be an integral part of reinforcement learning when it is used in the key components of reinforcement learning, which are the value function, policy, and the environmental model. This is called deep reinforcement learning (deep RL). For example, by using recurrent connections between hidden units, Hausknecht and Stone built a deep recurrent Q-network (DRQN) that could predict the speed of the ball in the computer game </span><strong>Pong</strong><span>. Another research area in linking deep learning with RL is for imitation learning. In imitation learning, an agent learns by observing an </span><em>expert</em><span>. It is especially useful where there are delayed rewards and evaluating the current position is hard. But imitation learning can be costly, so one approach is to use GANs to produce artificial data to be used in reinforcement learning.</span></p>
<p class="mce-root">Even though AlphaGo managed to beat the world champion in Go, it is nowhere near solving the problem of artificial general intelligence. DeepMind are a dedicated artificial intelligence company who combined experts in reinforcement learning, supervised learning and tree search functions and huge hardware resources to solve a single problem. <span>AlphaGo was trained on a dataset of 30 million game states and simulated millions of games. The version that beat one of the best players in the world in Go used almost 2,000 CPUs and 300 GPUs. Before it could beat the world champion, it was coached by the European champion, although the early version did beat him</span><span> first. However, </span>AlphaGo solves only one problem, it cannot even generalize to other board games. Therefore, it does not come anywhere near solving artificial general intelligence.</p>
<p><span>One of the more honest appraisals of AlphaGo is from Andrej Karpathy, who is a distinguished researcher in deep learning and currently is director of artificial intelligence at Tesla. He posted a blog called <strong>AlphaGo, in context</strong> (</span><a href="https://medium.com/@karpathy/alphago-in-context-c47718cb95a5">https://medium.com/@karpathy/alphago-in-context-c47718cb95a5</a><span>) after AlphaGo defeated the number one ranked player in 2017. Karpathy listed the following limitations of Go compared to other artificial intelligence tasks:</span></p>
<ul>
<li>The game is fully deterministic, that is, rules are fixed and fully known beforehand. In comparison, most real-world problems are not</li>
<li>The game is fully observable, <span>that is,</span> complete information is known to all parties, there are no hidden variables or states.</li>
<li>The games has a discrete action space, <span>that is,</span> there is a fixed number of allowable actions</li>
<li>A perfect simulator exists, <span>that is,</span> you can model millions of examples in a safe space. Real-life artificial intelligence does not have this.</li>
<li>The game is relatively short.</li>
<li>There are historical datasets from previous games</li>
</ul>
<p>If we consider self-driving cars as an artificial intelligence task, it probably does not match any of these properties.</p>
<p><span>One unusual quirk in AlphaGo games with the world champion is that it sometimes passed on moves that would have captured board space. As humans, when we play games, we sometimes crave immediate feedback and therefore make moves to achieve short-term rewards. AlphaGo was programmed to win the game, regardless of the margin, so was quite content to pass on making such moves during the games. It is interesting that some expert Go players believe that they can improve by studying the strategies of AlphaGo. We have come full circle – humans trying to imitate the actions of computers, which in turn are modeled on the actions of humans.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Additional deep learning resources</h1>
                </header>
            
            <article>
                
<p class="mce-root">This section consists of some recommendations if you wish to continue your development in deep learning. My first recommendation is that you ensure that you have run all the code examples in this book. You are getting less than 50% of the benefit if you just read this book without running the code. Go through the examples, change the code to try and beat the results I got, re-write the MXNet code to Keras code, and so on.</p>
<p>I strongly recommend <span><em>Deep Learning Specialization</em> on Coursera by Andrew Ng (<a href="https://www.coursera.org/specializations/deep-learning">https://www.coursera.org/specializations/deep-learning</a>). Unfortunately, it is in Python, but it still is a great resource. Also in Python are two excellent courses on fast.ai (<a href="http://www.fast.ai/">http://www.fast.ai/</a>) from Jeremy Howard. These two options take opposite approaches: the <em>Deep Learning</em> specialization on Coursera takes a bottom-up approach that goes from theory to practice, and the fast.ai courses show you practical examples from the start and only afterwards shows you the theory.</span></p>
<p>Another excellent resource is Kaggle (<a href="https://www.kaggle.com/competitions">https://www.kaggle.com/competitions</a>). Kaggle hosts competitions where data scientists compete to get the best score in machine learning competitions. Many of these tasks are computer vision tasks. I am not a huge fan of the competitions, because I think that they ignore a lot of the work in preparing and acquiring datasets and also ignore how models are deployed. However, two notable features in Kaggle are its Kernels and forums/blogs. Kernels are Python or R scripts from other people. These scripts often have very interesting approaches to machine learning tasks. It is well worth following a competition and just looking at how other competitors approach these problems. The second notable feature is the forums/blogs on Kaggle. Again, some interesting approaches are discussed on the competition forums, and after every competition, there's usually a blog post from one of the winning competitions discussing their approach in winning the competition.</p>
<p><span>Going back to R, another fantastic resource is the RStudio website. These guys do fantastic work in keeping R relevant to data science and machine learning. RStudio put a lot of output back into the R ecosystem; for example, the excellent work by Hadley Wickham, their Chief Scientist. The founder of RStudio (J.J. Allaire) is the author of the R API's to TensorFlow and Keras. We have used some of their excellent tools in this book, including RStudio IDE, RShiny, RMarkdown, the tidy universe of packages, and so on. Here are some links with examples that you should check out:</span></p>
<ul>
<li><a href="https://keras.rstudio.com/">https://keras.rstudio.com/</a></li>
<li><a href="https://keras.rstudio.com/articles/examples/index.html">https://keras.rstudio.com/articles/examples/index.html</a></li>
<li><a href="https://tensorflow.rstudio.com/">https://tensorflow.rstudio.com/</a></li>
<li><a href="https://tensorflow.rstudio.com/tfestimators/">https://tensorflow.rstudio.com/tfestimators/</a></li>
<li><a href="https://tensorflow.rstudio.com/tensorflow/">https://tensorflow.rstudio.com/tensorflow/</a></li>
<li><a href="https://tensorflow.rstudio.com/tools/">https://tensorflow.rstudio.com/tools/</a></li>
<li><a href="https://tensorflow.rstudio.com/learn/resources.html">https://tensorflow.rstudio.com/learn/resources.html</a></li>
</ul>
<p>My final suggestion is looking at research papers. Here are a number of good papers to begin with:</p>
<ul>
<li>Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton.<span> </span><em>ImageNet Classification with Deep Convolutional Neural Networks</em>. Advances in neural information processing systems. 2012.</li>
<li>Szegedy, Christian, et al.<span> </span><em>Going Deeper with Convolutions</em>. Cvpr, 2015.</li>
<li>LeCun, Yann, et al.<span> </span><em>Learning Algorithms for Classification: A Comparison on Handwritten Digit Recognition</em>. Neural networks: the statistical mechanics perspective 261 (1995): 276.</li>
<li>Zeiler, Matthew D., and Rob Fergus.<span> </span><em>Visualizing and Understanding Convolutional Networks</em>. European conference on computer vision. Springer, Cham, 2014.</li>
<li>Srivastava, Nitish, et al.<span> </span><em>Dropout: A Simple Way to Prevent Neural Networks from Overfitting</em>. The Journal of Machine Learning Research 15.1 (2014): 1929-1958.</li>
<li><span>Simonyan, Karen, and Andrew Zisserman.<span> </span><em>Very deep convolutional networks for large-scale image recognition</em>. </span>arXiv preprint arXiv:1409.1556<span> (2014).</span></li>
<li><span>Szegedy, Christian, et al. <em>Going deeper with convolutions</em>. </span>Proceedings of the IEEE conference on computer vision and pattern recognition<span>. 2015.</span></li>
<li><span>He, Kaiming, et al. <em>Deep residual learning for image recognition</em>. </span>Proceedings of the IEEE conference on computer vision and pattern recognition<span><span>. 2016.</span></span></li>
<li><span>Goodfellow, Ian, et al. <em>Generative adversarial nets</em>. </span>Advances in neural information processing systems<span>. 2014.</span></li>
<li><span>LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. Deep learning. </span>nature<span> 521.7553 (2015): 436.</span></li>
<li><span>Goldberg, Yoav. <em>A primer on neural network models for natural language processing</em>. </span>Journal of Artificial Intelligence Research<span> 57 (2016): 345-420.</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, the reader has seen some advanced deep learning techniques. First, we looked at some image classification models and looked at some historical models. Next, we loaded an existing model with pre-trained weights into R and used it to classify a new image. We looked at transfer learning, which allows us to reuse an existing model as a base on which to build a deep learning model for new data. We built an image classifier model that could train on image files. This <span>model also showed us how to use data augmentation and callbacks, which are used in many deep learning models. Finally, we demonstrated how we can build a model in R and create a REST endpoint for a prediction API that can be used from other applications or across the web.</span></p>
<p><span>We have come to the end of the book, and I really hope it was useful to you. R is a great language for data science and I believe it is easier to use and allows you to develop machine learning prototypes faster than the main alternative, Python. Now that it has support for some excellent deep learning frameworks in MXNet, Keras and TensorFlow, I believe that R will continue to be an excellent choice for data scientists and machine learning practioners.</span></p>


            </article>

            
        </section>
    </body></html>