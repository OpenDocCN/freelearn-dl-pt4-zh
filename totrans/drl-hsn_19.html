<html><head></head><body>
<div id="sbo-rt-content"><section class="level2 chapterHead" id="chapter-19-reinforcement-learning-with-human-feedback">
<h1 class="chapterNumber">19</h1>
<h1 class="chapterTitle" id="sigil_toc_id_422">
<span id="x1-34900019"/>Reinforcement Learning with Human Feedback
    </h1>
<p>In this chapter, we‚Äôll take a look at a relatively recent method that addresses situations when the desired behavior is hard to define via the explicit reward function ‚Äì <span class="cmbx-10x-x-109">reinforcement learning with human feedback </span>(<span class="cmbx-10x-x-109">RLHF</span>) . This is<span id="dx1-349001"/> also related to exploration (as the method allows humans to push learning in a new direction), the problem we discussed in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="ch022.xhtml#x1-32800018"><span class="cmti-10x-x-109">18</span></a>. Surprisingly, the method, initially developed for a very specific subproblem in the RL domain, turned out to be enormously successful in the <span id="dx1-349002"/><span class="cmbx-10x-x-109">large language</span> <span class="cmbx-10x-x-109">models </span>(<span class="cmbx-10x-x-109">LLMs</span>). Nowadays, RLHF is at the core of modern LLM training pipelines, and without it, the recent fascinating progress wouldn‚Äôt have been possible.</p>
<p>As this book is not about LLMs and modern chatbots, we will focus purely on the original paper from OpenAI and Google by Christiano et al., <span class="cmti-10x-x-109">Deep</span> <span class="cmti-10x-x-109">reinforcement learning from human preferences </span>[<span id="x1-349003"/><a href="#">Chr+17</a>], which describes the RLHF method applied to RL problems and environments. But in the overview of the method, I will explain a bit about how this method is used in LLM training.</p>
<p>In this chapter, we will:</p>
<ul>
<li>
<p>Take a look at human feedback in RL to address problems with unclear reward objectives and exploration.</p>
</li>
<li>
<p>Implement an RLHF pipeline from scratch and check it on the SeaQuest Atari game to teach it new behavior.</p>
</li>
</ul>
<section class="level3 sectionHead" id="reward-functions-in-complex-environments">
<h1 class="heading-1" id="sigil_toc_id_314"> <span id="x1-35000019.1"/>Reward functions in complex environments</h1>
<p>Before we go into the details of the RLHF method, let‚Äôs <span id="dx1-350001"/>start by discussing the underlying motivation of the concept. As we discussed in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="ch005.xhtml#x1-190001"><span class="cmti-10x-x-109">1</span></a>, the <span class="cmti-10x-x-109">reward</span> is the core concept in RL. Without a reward, we‚Äôre blind ‚Äî all the methods we‚Äôve already discussed are heavily dependent on the reward value provided by the environment:</p>
<ul>
<li>
<p>In value-based methods (<span class="cmti-10x-x-109">Part 2 </span>of the book), we used the reward to approximate the <span class="cmmi-10x-x-109">Q</span>-value to evaluate the actions and choose the most prominent one.</p>
</li>
<li>
<p>In policy-based methods (<span class="cmti-10x-x-109">Part 3</span>), the reward was used even more directly ‚Äî as a scale for the Policy Gradient. With all the math removed, we basically optimized our policy to prefer actions that bring more accumulated future reward.</p>
</li>
<li>
<p>In black-box methods (<span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="ch021.xhtml#x1-31100017"><span class="cmti-10x-x-109">17</span></a>), we used the reward to make a decision about agent variants: should they be kept for the future or discarded?</p>
</li>
</ul>
<p>In almost all the RL environments we‚Äôve experimented with, the reward function was predefined for us ‚Äî in Atari games, we had the score; in the FrozenLake environment, it was an explicit target position; in simulated robots, it was the distance travelled, etc. The only exception was in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="ch014.xhtml#x1-16900010"><span class="cmti-10x-x-109">10</span></a>, where we implemented the environment (stock trading system) ourselves and had to decide how the reward was to be shaped. But even in that example, it was fairly obvious what should be used as a reward.</p>
<p>Unfortunately, in real-life situations, it is not always that easy to formulate what should be used as a reward. Let‚Äôs look at a couple of examples. If we are training the chatbot to solve a set of tasks, it‚Äôs important to not only ensure the tasks are completed correctly but also consider the style in which they are done. What if we ask the system ‚ÄúWhat‚Äôs the weather forecast for tomorrow?‚Äù and it replies correctly but in a rude manner? Should it be punished for this with a negative reward and to what extent? What should we do in the opposite situation ‚Äî a very polite answer but the information given is wrong? If we just optimize one single criterion (like the correctness of information), we might get a system that ‚Äúworks‚Äù but is not usable in real life ‚Äì just because it is so awkward that nobody wants to use it.</p>
<p>Another example of a ‚Äúsingle optimization factor‚Äù is the transportation of goods from point A to point B. Transport companies don‚Äôt just try to maximize their profits by all means. In addition, they have tons of restrictions and regulations, like driving rules, working hours, labour legislation, etc. If we optimize only one criterion in our system, we might eventually get ‚ÄúDrive through the neighbor‚Äôs fence ‚Äì this is the fastest way.‚Äù So, in real life, having a single value we want to maximize is an exception rather than the norm. Most of the time, we have several parameters that contribute to the final result and we need to find some sort of balance between them. Even in the Atari games we‚Äôve already seen, the score might be calculated as the sum of different ‚Äúsubgoals.‚Äù A very good example of this is the SeaQuest game we experimented with in the previous chapter. If you haven‚Äôt played it before, you can do it in your browser to get a better understanding: <a class="url" href="https://www.retrogames.cz/play_221-Atari2600.php"><span class="cmtt-10x-x-109">https://www.retrogames.cz/play_221-Atari2600.php</span></a>.</p>
<p>In this game, you‚Äôre controlling the submarine and you are scored for the following activities:</p>
<ul>
<li>
<p>Shooting evil fish and enemy submarines</p>
</li>
<li>
<p>Saving divers and bringing them back to the surface</p>
</li>
<li>
<p>Avoiding enemy fire and ships on the surface (they appear in later levels of the game)</p>
</li>
</ul>
<p>As the level of oxygen is limited, your submarine has to go to the surface from time to time to refill the reserves. Most of the modern RL methods have no problem discovering the reward for shooting fish and submarines ‚Äî starting with trial and error, after just a couple of hours of training, the agent learns how to get the reward from firing.</p>
<p>But discovering scoring from saving divers is much trickier, as the reward for them is given only after collecting six divers and getting to the surface. Oxygen replenishment is also hard to discover by trial and error, as our neural network has no prior idea about oxygen, submarines, and how the sudden death of your submarine might be related to the gauge at the bottom of the screen. Our RL method with <span class="cmmi-10x-x-109">ùúñ</span>-greedy exploration could be seen as a newborn baby randomly pushing buttons and being rewarded for correct sequences of actions, which might take lots of time before the correct lengthy sequence has been executed.</p>
<p>As a result, most of the training episodes in SeaQuest are limited by the average score of 300 and 500 game steps. The submarine just dies from a lack of oxygen and random surface visits are too rare to discover that the game might be played for much longer. At the same time, people who haven‚Äôt seen the game before can figure out how to refill the oxygen and save divers in just several minutes of gameplay.</p>
<p>Potentially, we could help our agent and explain somehow why oxygen is important by adding it to the reward function (as an extra reward for refilling the oxygen, for example), but it might start the vicious circle of tweaking the environment here and there ‚Äì exactly those efforts we‚Äôve tried to avoid by using RL methods.</p>
<p>And, as you might already have guessed, RLHF is exactly the approach that allows us to avoid this low-level reward function tweaking, allowing <span id="dx1-350002"/>humans to give feedback to the agent‚Äôs behavior.</p>
</section>
<section class="level3 sectionHead" id="theoretical-background">
<h1 class="heading-1" id="sigil_toc_id_315"> <span id="x1-35100019.2"/>Theoretical background</h1>
<p>Let‚Äôs take a look<span id="dx1-351001"/> at the original RLHF method published in 2017 by OpenAI and Google researchers [<span id="x1-351002"/><a href="#">Chr+17</a>]. Since the publication (and especially after ChatGPT‚Äôs release), this method has been an area of active research. For recent developments, you can the check papers at <a class="url" href="https://github.com/opendilab/awesome-RLHF"><span class="cmtt-10x-x-109">https://github.com/opendilab/awesome-RLHF</span></a>. In addition, we‚Äôll discuss the role of RLHF in the LLM training process.</p>
<section class="level4 subsectionHead" id="method-overview">
<h2 class="heading-2" id="sigil_toc_id_316"> <span id="x1-35200019.2.1"/>Method overview</h2>
<p>The authors<span id="dx1-352001"/> of the paper experimented with two classes of problems: several environments from MuJoCo simulated robotics (similar to the continuous control problems we discussed in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="ch019.xhtml#x1-27200015"><span class="cmti-10x-x-109">15</span></a> and <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="ch020.xhtml#x1-29000016"><span class="cmti-10x-x-109">16</span></a>) and several Atari games.</p>
<p>The core idea is to keep the original RL model, but replace the reward from the environment with a neural network called <span class="cmti-10x-x-109">reward predictor</span>, which is trained on data gathered by humans. This network (represented as rÃÇ (<span class="cmmi-10x-x-109">o,a</span>) in the paper ) takes the observation and the action and returns the float value of immediate reward for the action.</p>
<p>The training data for this reward predictor is not provided directly by humans, but deducted from <span class="cmti-10x-x-109">human preferences</span>: people are shown two short video clips with examples of the agent‚Äôs behavior and asked the question ‚ÄúWhich one is better?‚Äù. In other words, the training data for the reward predictor is two episode segments <span class="cmmi-10x-x-109">œÉ</span><sup><span class="cmr-8">1</span></sup> and <span class="cmmi-10x-x-109">œÉ</span><sup><span class="cmr-8">2</span></sup> (fixed-length sequences of (<span class="cmmi-10x-x-109">o</span><sub><span class="cmmi-8">t</span></sub><span class="cmmi-10x-x-109">,a</span><sub><span class="cmmi-8">t</span></sub>) with observations and actions) and label <span class="cmmi-10x-x-109">Œº </span>from the human indicating which of the two is preferred. The given answer options are ‚Äúfirst,‚Äù ‚Äúsecond,‚Äù ‚Äúboth are good,‚Äù and ‚Äúcannot judge.‚Äù</p>
<p>The network rÃÇ (<span class="cmmi-10x-x-109">o,a</span>) is trained from this data using cross-entropy loss between labels and the function pÃÇ[<span class="cmmi-10x-x-109">œÉ</span><sup><span class="cmr-8">1</span></sup> <span class="cmsy-10x-x-109">‚âª</span><span class="cmmi-10x-x-109">œÉ</span><sup><span class="cmr-8">2</span></sup>], which is an estimation of the probability of the human preferring segment <span class="cmmi-10x-x-109">œÉ</span><sup><span class="cmr-8">1</span></sup> over <span class="cmmi-10x-x-109">œÉ</span><sup><span class="cmr-8">2</span></sup>:</p>
<div class="math-display">
<img alt="œÄ (a |s) = P[At = a|St = s] " class="math-display" height="59" src="../Images/eq71.png" width="350"/>
</div>
<p>In other words, we sum the rewards predicted for every step in the segment, exponentiate every reward, and normalize the sum. The cross-entropy loss is calculated using the standard formula for the binary classification:</p>
<div class="math-display">
<img alt="œÄ (a |s) = P[At = a|St = s] " class="math-display" height="57" src="../Images/eq72.png" width="570"/>
</div>
<p>Values for <span class="cmmi-10x-x-109">Œº</span><sub><span class="cmr-8">1</span></sub> and <span class="cmmi-10x-x-109">Œº</span><sub><span class="cmr-8">2</span></sub> are assigned based on the human‚Äôs judgement. If the first segment was preferred over the second, then <span class="cmmi-10x-x-109">Œº</span><sub><span class="cmr-8">1</span></sub> = 1 and <span class="cmmi-10x-x-109">Œº</span><sub><span class="cmr-8">2</span></sub> = 0. If the second segment was better, then <span class="cmmi-10x-x-109">Œº</span><sub><span class="cmr-8">2</span></sub> = 1 and <span class="cmmi-10x-x-109">Œº</span><sub><span class="cmr-8">1</span></sub> = 0. If the human decided that both segments are good, then both <span class="cmmi-10x-x-109">Œº </span>are set to <span class="cmtt-10x-x-109">0.5</span>. Such a reward model has several benefits in comparison to different approaches:</p>
<ul>
<li>
<p>By using a neural network for reward prediction, we can significantly reduce the required number of labels. An extreme case would be to ask humans to label every action of the policy, but this is prohibitively expensive in the case of RL, where millions of interactions take place within the environment. In the case of high-level goals, this might be an almost impossible thing to do.</p>
</li>
<li>
<p>We give the network feedback not only about good behavior but also about behavior that we don‚Äôt like. If you remember, in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="ch018.xhtml#x1-24700014"><span class="cmti-10x-x-109">14</span></a>, we used the recorded human demonstrations to train the web automation agent. But human demonstrations only show positive examples (‚Äúdo this‚Äù) and have no way of including negative examples (‚Äúdon‚Äôt do that‚Äù). In addition, human demonstrations are harder to collect and might contain more errors.</p>
</li>
<li>
<p>By asking for human preferences, we can handle problems where humans can <span class="cmti-10x-x-109">recognize </span>the behavior we want, but not necessarily <span class="cmti-10x-x-109">reproduce </span>it. For example, controlling the four-legged Ant robot from <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="ch020.xhtml#x1-29000016"><span class="cmti-10x-x-109">16</span></a> might be very challenging for humans. At the same time, we don‚Äôt have problems detecting when the robot is behaving normally or the policy is wrong.</p>
</li>
</ul>
<p>In the RLHF<span id="dx1-352002"/> paper, the authors experimented with different approaches to the reward model training and its usage in the RL training process. In their setup, three different processes were running in parallel:</p>
<ol>
<li>
<div id="x1-352004x1">
<p>The RL training method (A2C) used the current rÃÇ (<span class="cmmi-10x-x-109">o,a</span>) network for reward prediction. Random trajectory segments <span class="cmmi-10x-x-109">œÉ </span>= (<span class="cmmi-10x-x-109">o</span><sub><span class="cmmi-8">i</span></sub><span class="cmmi-10x-x-109">,a</span><sub><span class="cmmi-8">i</span></sub>) were stored in the labeling database.</p>
</div>
</li>
<li>
<div id="x1-352006x2">
<p>Human labelers sampled pairs of segments (<span class="cmmi-10x-x-109">œÉ</span><sup><span class="cmr-8">1</span></sup><span class="cmmi-10x-x-109">,œÉ</span><sup><span class="cmr-8">2</span></sup>) and assigned their labels <span class="cmmi-10x-x-109">Œº</span>, which were stored in the labeling database.</p>
</div>
</li>
<li>
<div id="x1-352008x3">
<p>The reward model rÃÇ (<span class="cmmi-10x-x-109">o,a</span>) was periodically trained on labeled pairs from the database and sent to the RL training process.</p>
</div>
</li>
</ol>
<p>This process is shown in <span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109">¬†</span><a href="#x1-352009r1"><span class="cmti-10x-x-109">19.1</span></a>.</p>
<div class="minipage">
<p><img alt=" DB: TTserrgaaminiennts 11 22 rÀÜrœÉœÉœÉŒºRewla(Labo,,re,aœÉœÉdls),Œº " height="300" src="../Images/B22150_19_01.png" width="600"/> <span id="x1-352009r1"/></p>
<span class="id">Figure¬†19.1: RLHF structure </span>
</div>
<p>As discussed earlier, the paper addressed two classes of problems: Atari games and continuous control. On both classes, the results were not especially spectacular ‚Äî sometimes traditional RL was better than RLHF, sometimes not. But where RLHF really <span id="dx1-352010"/>stood out was the LLM training pipeline. Let‚Äôs briefly discuss why it happened before we start our RLHF experiments.</p>
</section>
<section class="level4 subsectionHead" id="rlhf-and-llms">
<h2 class="heading-2" id="sigil_toc_id_317"> <span id="x1-35300019.2.2"/>RLHF and LLMs</h2>
<p>ChatGPT, released<span id="dx1-353001"/> at the end of 2022, has <span id="dx1-353002"/>quickly become a really big thing. For the general audience, it was even more influential than AlexNet in 2012, as AlexNet was ‚Äútechy stuff‚Äù‚Äîit pushed the boundaries but it was much harder to explain what was so special about it. ChatGPT was different: just a month after release, it had surpassed a user base of 100M users and almost everybody was talking about it.</p>
<p>At the heart of ChatGPT (and any modern LLM) training pipeline is RLHF. So, very quickly, this method of fine-tuning large models has become popular and has grown in terms of research interest. As this is not a book about LLMs, I will just give a quick description of the pipeline and how RLHF is incorporated there, as, from my perspective, this is an interesting use case.</p>
<p>From a high level, LLM training consists of three stages:</p>
<ol>
<li>
<div id="x1-353004x1">
<p><span class="cmbx-10x-x-109">Pretraining</span>: Here, we<span id="dx1-353005"/> perform the initial training of the language model on a huge corpus of texts. Basically, we take all the information we can possibly get and do unsupervised training of the language model. The volume (and costs) are enormous ‚Äî the RedPajama dataset used for LLaMA training contains 1.2 trillion tokens (which is approximately 15 million books).</p>
<p>At this stage, our randomly-initialized model learns regularities and deep connections of the language. But because the data volume is huge, we cannot just curate this data ‚Äî it could be fake news, hate speech posts, and other weird stuff you can easily find on the internet.</p>
</div>
</li>
<li>
<div id="x1-353007x2">
<p><span class="cmbx-10x-x-109">Supervised fine-tuning</span>: In this step, we fine-tune the model on predefined curated example dialogues. The dataset used here is manually created and validated for correctness and the volume is significantly lower ‚Äî around 10K‚Äì100K example dialogues.</p>
<p>This data is normally created by experts in the field and requires lots of effort to make and double-check it.</p>
</div>
</li>
<li>
<div id="x1-353009x3">
<p><span class="cmbx-10x-x-109">RLHF fine-tuning </span>(also known as ‚Äùmodel alignment‚Äù): This step uses the same process we‚Äôve already described: pairs of generated dialogues are presented to users for labeling, the reward model is trained on those labels, and this reward model is used in the RL algorithm to fine-tune the LLM model to follow the human‚Äôs preferences. The number of labeled samples is larger than on the supervised fine-tuning step (around 1M pairs), but because comparing two dialogues is a much simpler task than creating a proper dialogue from scratch, this is not a problem.</p>
</div>
</li>
</ol>
<p>As you might guess, the first step is the most expensive and lengthy: you have to crunch terabytes of texts and feed them through transformers. But at the same time, the <span class="cmti-10x-x-109">importance </span>of the steps is totally different. In the last step, the system not only learns what the best solution to the presented problem is but also has feedback about generating it in a socially acceptable way.</p>
<p>The RLHF method is very suitable for this task ‚Äî with just pairs of dialogues, it can learn the reward model that represents the labelers‚Äô implicit ‚Äúpreference model‚Äù for such a complicated thing as the chatbot. Doing this explicitly (via the reward function, for example) might be a<span id="dx1-353010"/> very challenging problem with lots of uncertainty.</p>
</section>
</section>
<section class="level3 sectionHead" id="rlhf-experiments">
<h1 class="heading-1" id="sigil_toc_id_318"> <span id="x1-35400019.3"/>RLHF experiments</h1>
<p>To get a better<span id="dx1-354001"/> understanding of the pipeline we‚Äôve just discussed, let‚Äôs implement it ourselves (as ‚Äúdoing is the best way to learn something‚Äù). In the previous chapter, we tried the Atari SeaQuest environment, which is tricky from the exploration point of view, so it is logical to take this environment and check what we can achieve with human feedback.</p>
<p>To limit the scope of the chapter and make the example more reproducible, I made the following modifications to the experiments described in the RLHF paper [<span id="x1-354002"/><a href="#">Chr+17</a>]:</p>
<ul>
<li>
<p>I focused on a single SeaQuest environment. The goal was to improve the agent‚Äôs gameplay in comparison to the A2C results we got in <span class="cmti-10x-x-109">Chapter</span><span class="cmti-10x-x-109">¬†</span><a href="ch022.xhtml#x1-32800018"><span class="cmti-10x-x-109">18</span></a> ‚Äî an average score of 400 and episodes of 500 steps (due to the lack of oxygen).</p>
</li>
<li>
<p>Instead of asynchronous labeling and reward model training, I split them into separate steps:</p>
<ol>
<li>
<div id="x1-354004x1">
<p>A2C training was performed, storing trajectory segments in local files. This training might optionally load and use a reward model network, which would allow us to iterate on reward models, labeling more samples after the training.</p>
</div>
</li>
<li>
<div id="x1-354006x2">
<p>The web UI allowed me to label random pairs of trajectory segments, storing the labels in a JSON file.</p>
</div>
</li>
<li>
<div id="x1-354008x3">
<p>The reward model was trained on those segments and labels. The result of the training was stored on disk.</p>
</div>
</li>
</ol>
</li>
<li>
<p>I avoided all the variations with the reward model training: no L2 regularization, no ensemble, etc.</p>
</li>
<li>
<p>The number of labels was significantly smaller: in every experiment, I labeling an extra 100 pairs of episode segments and retrained the models.</p>
</li>
<li>
<p>Actions were explicitly added to the reward model. For the details, check the section <span class="cmti-10x-x-109">Reward model</span>.</p>
</li>
<li>
<p>The reward model was used in A2C training for the fine-tuning of the best mode saved. For context, in the paper, the model was trained from scratch and improved with parallel RLHF labeling and reward model retraining.</p>
</li>
</ul>
<section class="level4 subsectionHead" id="initial-training-using-a2c">
<h2 class="heading-2" id="sigil_toc_id_319"> <span id="x1-35500019.3.1"/>Initial training using A2C</h2>
<p>To get the<span id="dx1-355001"/> first model (let‚Äôs call it ‚Äúversion 0‚Äù or v0 for short), I used standard<span id="dx1-355002"/> A2C code with the same Atari wrappers we‚Äôve already discussed several times in this book so far.</p>
<p>To start the training, you need to run the <span class="cmtt-10x-x-109">Chapter19/01</span><span class="cmtt-10x-x-109">_a2c.py </span>module, and besides basic A2C training, it contains a command-line option that enables the usage of the reward model (which we covered in earlier chapters), but we don‚Äôt need it in this step.</p>
<p>For now, to start the training of the basic model, use the following command line:</p>
<pre class="lstlisting" id="listing-494"><code>Chapter19$ ./01_a2c.py --dev cuda -n v0 --save save/v0 --db-path db-v0 
A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7) 
[Powered by Stella] 
AtariA2C( 
¬†¬†(conv): Sequential( 
¬†¬†¬†(0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4)) 
¬†¬†¬†(1): ReLU() 
¬†¬†¬†(2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2)) 
¬†¬†¬†(3): ReLU() 
¬†¬†¬†(4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1)) 
¬†¬†¬†(5): ReLU() 
¬†¬†¬†(6): Flatten(start_dim=1, end_dim=-1) 
¬†¬†) 
¬†¬†(policy): Sequential( 
¬†¬†¬†(0): Linear(in_features=3136, out_features=512, bias=True) 
¬†¬†¬†(1): ReLU() 
¬†¬†¬†(2): Linear(in_features=512, out_features=18, bias=True) 
¬†¬†) 
¬†¬†(value): Sequential( 
¬†¬†¬†(0): Linear(in_features=3136, out_features=512, bias=True) 
¬†¬†¬†(1): ReLU() 
¬†¬†¬†(2): Linear(in_features=512, out_features=1, bias=True) 
¬†¬†) 
) 
0: Testing model... 
Got best reward 40.00 and steps 213.0 in 10 episodes 
1024: done 1 games, mean reward 0.000, steps 70, speed 312.22 f/s 
1056: done 2 games, mean reward 0.000, steps 72, speed 1188.69 f/s 
1104: done 3 games, mean reward 0.000, steps 75, speed 1216.18 f/s</code></pre>
<p>Here is the<span id="dx1-355032"/> description of the command-line options:</p>
<ul>
<li>
<p><span class="cmtt-10x-x-109">--dev</span>: The<span id="dx1-355033"/> name of the device used for computation.</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">-n</span>: The name of the run, used in TensorBoard.</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">--save</span>: The directory name where the best models after the testing will be stored. Every 100 batches, we perform 10 test episodes of the current model on SeaQuest with disabled reward clipping (to get the original score range) and if the best reward or the count of steps for any of those 10 rounds is better than our previous record, we save the model into the file. Those files will be used later for fine-tuning.</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">--db-path</span>: The directory name where random episode segments will be stored during the training. This data will be used for the labeling and training of the reward model later.</p>
</li>
</ul>
<p>Let‚Äôs discuss the episode segments database (DB for short). Its structure is very simple: every environment used for training (in total, we have 16 of them) has an identifier from <span class="cmtt-10x-x-109">0 </span>to <span class="cmtt-10x-x-109">15</span>, which is used as a subdirectory under the directory given in the <span class="cmtt-10x-x-109">--db-path </span>command-line argument. So, every environment stores random segments independently in its own directory. The storage logic is implemented in a Gym API <span class="cmtt-10x-x-109">Wrapper </span>subclass, which is called <span class="cmtt-10x-x-109">EpisodeRecorderWrapper </span>and is in the <span class="cmtt-10x-x-109">lib/rlhf.py</span> module.</p>
<p>Let‚Äôs take a <span id="dx1-355034"/>look at the source code of the wrapper. Initially, we declare two hyperparameters, <span class="cmtt-10x-x-109">EPISODE</span><span class="cmtt-10x-x-109">_STEPS</span>, which defines the length of segments, and <span class="cmtt-10x-x-109">START</span><span class="cmtt-10x-x-109">_PROB</span>, which is the probability of starting the episode recording:</p>
<div class="tcolorbox" id="tcolobox-414">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-495"><code># how many transitions to store in episode 
EPISODE_STEPS = 50 
# probability to start episode recording 
START_PROB = 0.00005 
 
@dataclass(frozen=True) 
class EpisodeStep: 
    obs: np.ndarray 
    act: int 
 
class EpisodeRecorderWrapper(gym.Wrapper): 
    def __init__(self, env: gym.Env, db_path: pathlib.Path, env_idx: int, 
                 start_prob: float = START_PROB, steps_count: int = EPISODE_STEPS): 
        super().__init__(env) 
        self._store_path = db_path / f"{env_idx:02d}" 
        self._store_path.mkdir(parents=True, exist_ok=True) 
        self._start_prob = start_prob 
        self._steps_count = steps_count 
        self._is_storing = False 
        self._steps: tt.List[EpisodeStep] = [] 
        self._prev_obs = None 
        self._step_idx = 0</code></pre>
</div>
</div>
<p>We store the<span id="dx1-355057"/> episode segment as a list of <span class="cmtt-10x-x-109">EpisodeStep </span>objects, which is just an observation and the action we‚Äôre taking at this step. The method that resets the environment is very simple ‚Äî it updates the wrapper‚Äôs <span class="cmtt-10x-x-109">_step</span><span class="cmtt-10x-x-109">_idx </span>field(which is a counter of the steps we‚Äôve done in this environment) and stores the observation in the <span class="cmtt-10x-x-109">_prev</span><span class="cmtt-10x-x-109">_obs </span>field, depending on the <span class="cmtt-10x-x-109">_is</span><span class="cmtt-10x-x-109">_store </span>field. This field is <span class="cmtt-10x-x-109">True </span>if we‚Äôre in the middle of segment recording.</p>
<p>Our segments have a fixed number of environment steps (50 by default) and they are recorded independent of episode boundaries (in other words, if we started the segment recording shortly before the submarine‚Äôs death, we‚Äôll record the beginning of the next episode after the <span class="cmtt-10x-x-109">reset() </span>method):</p>
<div class="tcolorbox" id="tcolobox-415">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-496"><code>    def reset(self, *, seed: int | None = None, options: dict[str, tt.Any] | None = None) \ 
            -&gt; tuple[WrapperObsType, dict[str, tt.Any]]: 
        self._step_idx += 1 
        res = super().reset(seed=seed, options=options) 
        if self._is_storing: 
            self._prev_obs = deepcopy(res[0]) 
        return res</code></pre>
</div>
</div>
<p>If you want, you can experiment with this logic as, in principle, observations after the end of the episode are independent from observations and actions before the end of the episode. But it will make the handling of episode segment data more complicated, as the length will become variable.</p>
<p>The main logic <span id="dx1-355065"/>of the wrapper<span id="dx1-355066"/> is in the <span class="cmtt-10x-x-109">step() </span>method and it is also not very complicated. On every action, we store the step if we‚Äôre in the middle of recording; otherwise, we generate a random number to make the decision to start the recording:</p>
<div class="tcolorbox" id="tcolobox-416">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-497"><code>    def step(self, action: WrapperActType) -&gt; tuple[ 
        WrapperObsType, SupportsFloat, bool, bool, dict[str, tt.Any] 
    ]: 
        self._step_idx += 1 
        obs, r, is_done, is_tr, extra = super().step(action) 
        if self._is_storing: 
            self._steps.append(EpisodeStep(self._prev_obs, int(action))) 
            self._prev_obs = deepcopy(obs) 
 
            if len(self._steps) &gt;= self._steps_count: 
                store_segment(self._store_path, self._step_idx, self._steps) 
                self._is_storing = False 
                self._steps.clear() 
        elif random.random() &lt;= self._start_prob: 
            # start recording 
            self._is_storing = True 
            self._prev_obs = deepcopy(obs) 
        return obs, r, is_done, is_tr, extra</code></pre>
</div>
</div>
<p>By default, the probability of starting the recording is small (<span class="cmtt-10x-x-109">START</span><span class="cmtt-10x-x-109">_PROB =</span> <span class="cmtt-10x-x-109">0.00005</span>, which is a 0.005% chance), but because of the large number of steps we‚Äôre doing during the training, we still have plenty of segments to label. For example, after 12M environment steps (about 5 hours of training), the database contains 2,500 recorded segments, which occupy 12GB of disk.</p>
<p>The method <span class="cmtt-10x-x-109">step() </span>uses the function <span class="cmtt-10x-x-109">store</span><span class="cmtt-10x-x-109">_segment() </span>to store the list of <span class="cmtt-10x-x-109">EpisodeStep </span>objects, and it is just the <span class="cmtt-10x-x-109">pickle.dumps() </span>call for the list of steps:</p>
<div class="tcolorbox" id="tcolobox-417">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-498"><code>def store_segment(root_path: pathlib.Path, step_idx: int, steps: tt.List[EpisodeStep]): 
    out_path = root_path / f"{step_idx:08d}.dat" 
    dat = pickle.dumps(steps) 
    out_path.write_bytes(dat) 
    print(f"Stored {out_path}")</code></pre>
</div>
</div>
<p>Before we get to the training results, I need to mention one small but important detail about the wrapper‚Äôs usage. To make the labeling easier, the observations we store in the DB are taken <span class="cmti-10x-x-109">before </span>the standard Atari wrappers. This increases the size of the data we have to store, but human labelers will see the original colorful Atari screen <span id="dx1-355090"/>in <span id="dx1-355091"/>the original resolution (160 <span class="cmsy-10x-x-109">√ó </span>192) instead of a downscaled picture in shades of gray.</p>
<p>To achieve that, the wrapper is applied right after the original Gymnasium environment before the Atari wrappers. The following is the relevant piece of code in the <span class="cmtt-10x-x-109">01</span><span class="cmtt-10x-x-109">_a2c.py </span>module:</p>
<div class="tcolorbox" id="tcolobox-418">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-499"><code>    def make_env() -&gt; gym.Env: 
        e = gym.make("SeaquestNoFrameskip-v4") 
        if reward_path is not None: 
            p = pathlib.Path(reward_path) 
            e = rlhf.RewardModelWrapper(e, p, dev=dev, metrics_queue=metrics_queue) 
        if db_path is not None: 
            p = pathlib.Path(db_path) 
            p.mkdir(parents=True, exist_ok=True) 
            e = rlhf.EpisodeRecorderWrapper(e, p, env_idx=env_idx) 
        e = ptan.common.wrappers.wrap_dqn(e) 
        # add time limit after all wrappers 
        e = gym.wrappers.TimeLimit(e, TIME_LIMIT) 
        return e</code></pre>
</div>
</div>
<p>The training process hyperparameters were taken from the paper (LR decrease schedule, network architecture, count of environments, etc). I let it train for 5 hours and 12M observations. The charts with testing results are shown in <span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109">¬†</span><a href="#x1-355105r2"><span class="cmti-10x-x-109">19.2</span></a>.</p>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_19_02.png" width="600"/> <span id="x1-355105r2"/></p>
<span class="id">Figure¬†19.2: The reward (left) and steps (right) during the A2C training </span>
</div>
<p>The best model was able to reach the reward level of 460 (without reward clipping in the environment), which is good but is much worse than the results that could be achieved if you refill the oxygen from time to time.</p>
<p>The video of this model‚Äôs gameplay is available at <a class="url" href="https://youtu.be/R_H3pXu-7cw"><span class="cmtt-10x-x-109">https://youtu.be/R_H3pXu-7cw</span></a>. As you can see from the video, our agent mastered shooting the fish almost perfectly, but got stuck on the local optima of floating at the bottom (maybe because it is safer, as enemy submarines <span id="dx1-355106"/>are not present there) and <span id="dx1-355107"/>has no idea about the oxygen refilling.</p>
<p>You can record your own video from the model file using the tool <span class="cmtt-10x-x-109">01</span><span class="cmtt-10x-x-109">_play.py</span>, which takes the model filename.</p>
</section>
<section class="level4 subsectionHead" id="labeling-process">
<h2 class="heading-2" id="sigil_toc_id_320"> <span id="x1-35600019.3.2"/>Labeling process</h2>
<p>During the A2C training, we got 12GB of 2,500 random<span id="dx1-356001"/> episode segments. Each segment contains 50 steps with screen observations and actions the agent took on every step. Now we are ready for the labeling process of the RLHF pipeline.</p>
<p>During the labeling, we need to randomly sample pairs of episode segments and show them to the human, asking the question ‚ÄúWhich one is better?‚Äù. The answer should be stored for reward model training. Exactly this logic is implemented in <span class="cmtt-10x-x-109">02</span><span class="cmtt-10x-x-109">_label</span><span class="cmtt-10x-x-109">_ui.py</span>.</p>
<p>The UI of the labeling process is implemented as a web application that uses the NiceGUI library (<a class="url" href="https://nicegui.io/"><span class="cmtt-10x-x-109">https://nicegui.io/</span></a>). NiceGUI allows a modern web application UI to be implemented in Python and provides a rich set of interactive UI widgets, like buttons, lists, pop-up dialogs, etc. In principle, you don‚Äôt need to know JavaScript and CSS (but it won‚Äôt harm if you‚Äôre familiar with them). If you have never used NiceGUI before, that‚Äôs not a problem; you just need to install it with the following command in your Python environment:</p>
<pre class="lstlisting" id="listing-500"><code>pip install nicegui==1.4.26</code></pre>
<p>To start the labeling UI (after installing the NiceGUI package), you need to specify the path to the DB with stored episode segments:</p>
<pre class="lstlisting" id="listing-501"><code>Chapter19$ ./02_label_ui.py -d db-v0 
NiceGUI ready to go on http://localhost:8080, http://172.17.0.1:8080, http://172.18.0.1:8080, and http://192.168.10.8:8080</code></pre>
<p>The interface is available via HTTP (so, open it in your browser) and listens on port <span class="cmtt-10x-x-109">8080 </span>on all machine interfaces, which is convenient if you start it on a remote server (but you need to be aware of the possible risk of external access, as the labeling UI has no authentification and authorization at all). If you want to<span id="dx1-356005"/> change the port or limit the scope to the specific network interface, just tweak <span class="cmtt-10x-x-109">02</span><span class="cmtt-10x-x-109">_label</span><span class="cmtt-10x-x-109">_ui.py</span>. Let‚Äôs look at a screenshot of the labelling interface:</p>
<div class="minipage">
<p><img alt="PIC" height="360" src="../Images/file290.png" width="600"/> <span id="x1-356006r3"/></p>
<span class="id">Figure¬†19.3: The labeling UI section with DB information </span>
</div>
<p>This interface is very basic: on the left, there are three links to different sections of the UI functionality:</p>
<ul>
<li>
<p><span class="cmbx-10x-x-109">Overview </span>shows the path to the database, the total count of segments it contains, and the amount of labels already created.</p>
</li>
<li>
<p><span class="cmbx-10x-x-109">Label new data </span>samples random pairs of segments and allows you to label them.</p>
</li>
<li>
<p><span class="cmbx-10x-x-109">Existing labels </span>shows all the labels and allows you to modify the labels if needed.</p>
</li>
</ul>
<p>If needed, the list with links could be hidden or shown by clicking on the top-left button (with three horizontal lines). The most time has been spent on the <span class="cmbx-10x-x-109">Label</span> <span class="cmbx-10x-x-109">new data </span>section, shown in <span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109">¬†</span><span class="cmbx-10x-x-109">??</span>:</p>
<div class="minipage">
<p><img alt="PIC" height="360" src="../Images/file291.png" width="600"/> <span id="x1-356007r4"/></p>
<span class="id">Figure¬†19.4: The interface for adding new labels (for better visualization, refer to https://packt.link/gbp/9781835882702 ) </span>
</div>
<p>Here, we have a <span id="dx1-356008"/>list of 20 randomly sampled pairs of episode segments we can label. When the entry in the list is selected, the interface shows both segments (as animated GIFs generated by the code on the fly). The user can click one of three buttons to add the label:</p>
<ul>
<li>
<p><span class="cmbx-10x-x-109">#1 IS BETTER (1)</span>: Marks the first segment as preferred. Such entries will have <span class="cmmi-10x-x-109">Œº</span><sub><span class="cmr-8">1</span></sub> = 1<span class="cmmi-10x-x-109">.</span>0 and <span class="cmmi-10x-x-109">Œº</span><sub><span class="cmr-8">2</span></sub> = 0<span class="cmmi-10x-x-109">.</span>0 during the reward model training.</p>
</li>
<li>
<p><span class="cmbx-10x-x-109">BOTH ARE GOOD (0)</span>: Marks both segments as equally good (or bad), assigning <span class="cmmi-10x-x-109">Œº</span><sub><span class="cmr-8">1</span></sub> = 0<span class="cmmi-10x-x-109">.</span>5 and <span class="cmmi-10x-x-109">Œº</span><sub><span class="cmr-8">2</span></sub> = 0<span class="cmmi-10x-x-109">.</span>5.</p>
</li>
<li>
<p><span class="cmbx-10x-x-109">#2 IS BETTER (2)</span>: Marks the second segment as preferred (<span class="cmmi-10x-x-109">Œº</span><sub><span class="cmr-8">1</span></sub> = 0<span class="cmmi-10x-x-109">.</span>0 and <span class="cmmi-10x-x-109">Œº</span><sub><span class="cmr-8">2</span></sub> = 1<span class="cmmi-10x-x-109">.</span>0).</p>
</li>
</ul>
<p>Instead of clicking the UI buttons, you can use the keyboard keys 0 (‚Äúboth are good‚Äù), 1 (‚Äúthe first is better‚Äù), or 2 (‚Äúthe second is better‚Äù) to assign the label. Once the label is assigned, the UI automatically selects the next unlabeled entry in the list, so the labeling process could be done with the keyboard only. When you‚Äôre done with all the labels in the list, you can click the <span class="cmbx-10x-x-109">RESAMPLE LIST </span>button to load 20 new samples for labeling.</p>
<p>After every label has been assigned (with UI button clicks or key presses), the labels are stored in the JSON file <span class="cmtt-10x-x-109">labels.json </span>in the root of the DB directory. The file has a trivial JSON-line format where every line is an entry containing paths to both segments (relative to the DB root) and assigned labels:</p>
<pre class="lstlisting" id="listing-502"><code>Chapter19$ head db-v0/labels.json 
{"sample1":"14/00023925.dat","sample2":"10/00606788.dat","label":0} 
{"sample1":"02/01966114.dat","sample2":"10/01667833.dat","label":2} 
{"sample1":"00/02432057.dat","sample2":"06/01410909.dat","label":1} 
{"sample1":"01/02293138.dat","sample2":"11/00997214.dat","label":0} 
{"sample1":"10/00091149.dat","sample2":"11/01262679.dat","label":2} 
{"sample1":"12/01394239.dat","sample2":"04/01792088.dat","label":2} 
{"sample1":"10/01390371.dat","sample2":"09/00077676.dat","label":0} 
{"sample1":"10/01390371.dat","sample2":"09/00077676.dat","label":1} 
{"sample1":"12/02339611.dat","sample2":"00/02755898.dat","label":2} 
{"sample1":"06/00301623.dat","sample2":"06/00112361.dat","label":2}</code></pre>
<p>If needed, existing<span id="dx1-356020"/> labels could be reviewed using the <span class="cmbx-10x-x-109">Existing labels </span>link (shown in <span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109">¬†</span><a href="#x1-356021r5"><span class="cmti-10x-x-109">19.5</span></a>), which shows almost the same interface as <span class="cmbx-10x-x-109">Label new</span> <span class="cmbx-10x-x-109">data</span>, but instead of sampling 20 fresh pairs, it shows already labeled pairs. Those pairs could be changed by clicking the buttons or using the keyboard shortcuts described earlier.</p>
<div class="minipage">
<p><img alt="PIC" height="360" src="../Images/file292.png" width="600"/> <span id="x1-356021r5"/></p>
<span class="id">Figure¬†19.5: The interface for reviewing and editing old labels (for better visualization, refer to https://packt.link/gbp/9781835882702 ) </span>
</div>
<p>During my experiments, I did the first round of labeling 100 pairs paying most attention to the rare cases when the submarine was on the surface (marking them as good) and more frequent situations when oxygen was low (marking them as bad). In other situations, I prefer the segments<span id="dx1-356022"/> where fish were properly hit. With some labels at hand, we‚Äôre ready to go on to the next step: reward model training.</p>
</section>
<section class="level4 subsectionHead" id="reward-model-training">
<h2 class="heading-2" id="sigil_toc_id_321"> <span id="x1-35700019.3.3"/>Reward model training</h2>
<p>The reward<span id="dx1-357001"/> model <span id="dx1-357002"/>network has most of the structure taken from the paper, with the only difference in handling actions. In the paper, the authors do not specify how actions are taken into account besides stating ‚Äú<span class="cmti-10x-x-109">For</span> <span class="cmti-10x-x-109">the reward predictor, we use </span>84 <span class="cmsy-10x-x-109">√ó </span>84 <span class="cmti-10x-x-109">images as inputs (the same as the</span> <span class="cmti-10x-x-109">inputs to the policy), and stack 4 frames for a total </span>84 <span class="cmsy-10x-x-109">√ó </span>84 <span class="cmsy-10x-x-109">√ó </span>4 <span class="cmti-10x-x-109">input</span> <span class="cmti-10x-x-109">tensor.</span>‚Äù From that, I made an assumption that the reward model deducts actions ‚Äúimplicitly‚Äù from the dynamics between the frames. I haven‚Äôt tried this approach in my experiment and instead decided to show the actions to the network explicitly by concatenating one-hot encoding to the vectors obtained from the convolution layers. As an exercise, you can change my code to use the approach from the paper and compare the results. The rest of the architecture and training parameters are the same as in the paper. Let‚Äôs take a look at the reward model network code:</p>
<div class="tcolorbox" id="tcolobox-419">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-503"><code>class RewardModel(nn.Module): 
    def __init__(self, input_shape: tt.Tuple[int, ...], n_actions: int): 
        super().__init__() 
 
        self.conv = nn.Sequential( 
            nn.Conv2d(input_shape[0], 16, kernel_size=7, stride=3), 
            nn.BatchNorm2d(16), 
            nn.Dropout(p=0.5), 
            nn.LeakyReLU(), 
            nn.Conv2d(16, 16, kernel_size=5, stride=2), 
            nn.BatchNorm2d(16), 
            nn.Dropout(p=0.5), 
            nn.LeakyReLU(), 
            nn.Conv2d(16, 16, kernel_size=3, stride=1), 
            nn.BatchNorm2d(16), 
            nn.Dropout(p=0.5), 
            nn.LeakyReLU(), 
            nn.Conv2d(16, 16, kernel_size=3, stride=1), 
            nn.BatchNorm2d(16), 
            nn.Dropout(p=0.5), 
            nn.LeakyReLU(), 
            nn.Flatten(), 
        ) 
        size = self.conv(torch.zeros(1, *input_shape)).size()[-1] 
        self.out = nn.Sequential( 
            nn.Linear(size + n_actions, 64), 
            nn.LeakyReLU(), 
            nn.Linear(64, 1), 
        ) 
 
    def forward(self, obs: torch.ByteTensor, acts: torch.Tensor) -&gt; torch.Tensor: 
        conv_out = self.conv(obs / 255) 
        comb = torch.hstack((conv_out, acts)) 
        out = self.out(comb) 
        return out</code></pre>
</div>
</div>
<p>As you can see, convolution layers are combined with batch normalization, dropout, and the leaky ReLU activation function.</p>
<p>The training of <span id="dx1-357038"/>the reward model is <span id="dx1-357039"/>implemented in <span class="cmtt-10x-x-109">03</span><span class="cmtt-10x-x-109">_reward</span><span class="cmtt-10x-x-109">_train.py </span>and has nothing complicated. We load labeled data from JSON files (you can pass several databases in the command line to use for the training), use 20% of the data for the testing, and compute the binary cross entropy objective, which is implemented in the <span class="cmtt-10x-x-109">calc</span><span class="cmtt-10x-x-109">_loss() </span>function:</p>
<div class="tcolorbox" id="tcolobox-420">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-504"><code>def calc_loss(model: rlhf.RewardModel, s1_obs: torch.ByteTensor, 
              s1_acts: torch.Tensor, s2_obs: torch.ByteTensor, 
              s2_acts: torch.Tensor, mu: torch.Tensor) -&gt; torch.Tensor: 
    batch_size, steps = s1_obs.size()[:2] 
 
    s1_obs_flat = s1_obs.flatten(0, 1) 
    s1_acts_flat = s1_acts.flatten(0, 1) 
    r1_flat = model(s1_obs_flat, s1_acts_flat) 
    r1 = r1_flat.view((batch_size, steps)) 
    R1 = torch.sum(r1, 1) 
 
    s2_obs_flat = s2_obs.flatten(0, 1) 
    s2_acts_flat = s2_acts.flatten(0, 1) 
    r2_flat = model(s2_obs_flat, s2_acts_flat) 
    r2 = r2_flat.view((batch_size, steps)) 
    R2 = torch.sum(r2, 1) 
    R = torch.hstack((R1.unsqueeze(-1), R2.unsqueeze(-1))) 
    loss_t = F.binary_cross_entropy_with_logits(R, mu) 
    return loss_t</code></pre>
</div>
</div>
<p>Initially, our observations and actions tensors had the following structure: (<span class="cmmi-10x-x-109">batch,time,colors,height,width</span>) for observations and (<span class="cmmi-10x-x-109">batch,time,actions</span>) for actions, where <span class="cmti-10x-x-109">time </span>is the sequence‚Äôs time dimension. More concretely, observation tensors had the size 64 <span class="cmsy-10x-x-109">√ó </span>50 <span class="cmsy-10x-x-109">√ó </span>3 <span class="cmsy-10x-x-109">√ó </span>210 <span class="cmsy-10x-x-109">√ó </span>160 and actions had the size 64 <span class="cmsy-10x-x-109">√ó </span>50 <span class="cmsy-10x-x-109">√ó </span>18.</p>
<p>As the first<span id="dx1-357059"/> step in loss<span id="dx1-357060"/> calculation, we flatten the first two dimensions, getting rid of the time dimension and applying the model to compute the reward value rÃÇ(<span class="cmmi-10x-x-109">o,a</span>). After that, we return the time dimension and sum along it according to the paper‚Äôs formula we‚Äôve already discussed. Then our computation of loss is the application of the <span class="cmtt-10x-x-109">torch </span>function to compute the binary cross entropy.</p>
<p>On every epoch of the training, we compute the test loss (on 20% of the data) and save the reward model if the new loss is lower than the previous minimum of the test loss. If the train loss grows for four epochs in a row, we stop the training.</p>
<p>With the number of labels set in the previous section (a couple of hundred), the training is very quick ‚Äî it takes about a dozen epochs and several minutes. The following is the example training process. The command-line argument <span class="cmtt-10x-x-109">-o </span>specifies the directory name where the best model will be saved:</p>
<pre class="lstlisting" id="listing-505"><code>Chapter19$ ./03_reward_train.py --dev cuda -n v0-rw -o rw db-v0 
Namespace(dev=‚Äôcuda‚Äô, name=v0-rw‚Äô, out=‚Äôrw‚Äô, dbs=[‚Äôdb-v0‚Äô]) 
Loaded DB from db-v0 with 149 labels and 2534 paths 
RewardModel( 
¬†¬†(conv): Sequential( 
¬†¬†¬†(0): Conv2d(3, 16, kernel_size=(7, 7), stride=(3, 3)) 
¬†¬†¬†(1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) 
¬†¬†¬†(2): Dropout(p=0.5, inplace=False) 
¬†¬†¬†(3): LeakyReLU(negative_slope=0.01) 
¬†¬†¬†(4): Conv2d(16, 16, kernel_size=(5, 5), stride=(2, 2)) 
¬†¬†¬†(5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) 
¬†¬†¬†(6): Dropout(p=0.5, inplace=False) 
¬†¬†¬†(7): LeakyReLU(negative_slope=0.01) 
¬†¬†¬†(8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1)) 
¬†¬†¬†(9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) 
¬†¬†¬†(10): Dropout(p=0.5, inplace=False) 
¬†¬†¬†(11): LeakyReLU(negative_slope=0.01) 
¬†¬†¬†(12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1)) 
¬†¬†¬†(13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) 
¬†¬†¬†(14): Dropout(p=0.5, inplace=False) 
¬†¬†¬†(15): LeakyReLU(negative_slope=0.01) 
¬†¬†¬†(16): Flatten(start_dim=1, end_dim=-1) 
¬†¬†) 
¬†¬†(out): Sequential( 
¬†¬†¬†(0): Linear(in_features=8978, out_features=64, bias=True) 
¬†¬†¬†(1): LeakyReLU(negative_slope=0.01) 
¬†¬†¬†(2): Linear(in_features=64, out_features=1, bias=True) 
¬†¬†) 
) 
Epoch 0 done, train loss 0.131852, test loss 0.132976 
Save model for 0.13298 test loss 
Epoch 1 done, train loss 0.104426, test loss 0.354560 
Epoch 2 done, train loss 0.159513, test loss 0.170160 
Epoch 3 done, train loss 0.054362, test loss 0.066557 
Save model for 0.06656 test loss 
Epoch 4 done, train loss 0.046695, test loss 0.121662 
Epoch 5 done, train loss 0.055446, test loss 0.064895 
Save model for 0.06490 test loss 
Epoch 6 done, train loss 0.024505, test loss 0.025308 
Save model for 0.02531 test loss 
Epoch 7 done, train loss 0.015864, test loss 0.045814 
Epoch 8 done, train loss 0.024745, test loss 0.054631 
Epoch 9 done, train loss 0.027670, test loss 0.054107 
Epoch 10 done, train loss 0.025979, test loss 0.048673 
Best test loss was less than current for 4 epoches, stop</code></pre>
</section>
<section class="level4 subsectionHead" id="combining-a2c-with-the-reward-model">
<h2 class="heading-2" id="sigil_toc_id_322"> <span id="x1-35800019.3.4"/>Combining A2C with the reward model</h2>
<p>Once the <span id="dx1-358001"/>reward model is trained, we<span id="dx1-358002"/> can<span id="dx1-358003"/> finally try it for use in RL training. To do that, we use the same tool <span class="cmtt-10x-x-109">01</span><span class="cmtt-10x-x-109">_a2c.py </span>but give it a couple of extra arguments:</p>
<ul>
<li>
<p><span class="cmtt-10x-x-109">-r </span>or <span class="cmtt-10x-x-109">--reward</span>: This gives the path to the reward model to be loaded and used. With this option, we don‚Äôt use the environment reward but, instead, use the model to get the reward from the observation and action we decided to take. This is implemented as an additional environment wrapper; we‚Äôll take a look shortly.</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">-m </span>or <span class="cmtt-10x-x-109">--model</span>: This is the path to the actor model (stored on the previous A2C round of training) to be loaded. As I‚Äôm doing fine-tuning with RLHF instead of training with the reward model from scratch, the actor model is needed. In principle, you can try to use the reward model to train from scratch, but my experiments were not very successful.</p>
</li>
<li>
<p><span class="cmtt-10x-x-109">--finetune</span>: This enables the fine-tuning mode: convolution layers are frozen and LR is decreased 10 times. Without those modifications, the actor very quickly unlearns all the prior knowledge and the reward drops to almost zero.</p>
</li>
</ul>
<p>So, to use the reward model we‚Äôve just trained, the command line will look like this:</p>
<p><span class="cmtt-10x-x-109">./01</span><span class="cmtt-10x-x-109">_a2c.py --dev cuda -n v1 -r rw/reward-v0.dat --save save/v1 -m</span> <span class="cmtt-10x-x-109">save/v0/model</span><span class="cmtt-10x-x-109">_rw=460-steps=580.dat --finetune</span></p>
<p>Before checking the experiment results, let‚Äôs take a look at how the reward model is used in the RL training process. To minimize the changes needed, I implemented an environment wrapper, which is added between the original environment and Atari wrappers, because the reward model needs an unscaled full-color game image.</p>
<p>The code of the wrapper is in <span class="cmtt-10x-x-109">lib/rlhf.py </span>and is called <span class="cmtt-10x-x-109">RewardModelWrapper</span>. The constructor of the wrapper loads the model from the data file and assigns a couple of fields. According to the paper, the reward predicted by the reward model is normalized to have zero mean and unit variance, so to do the normalization, the wrapper maintains the last 100 rewards in <span class="cmtt-10x-x-109">collections.deque</span>. Besides normalization, the wrapper can have a queue for metrics to be sent. The metrics contain information about normalization values and the real sum from the underlying environment:</p>
<div class="tcolorbox" id="tcolobox-421">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-506"><code>class RewardModelWrapper(gym.Wrapper): 
    KEY_REAL_REWARD_SUM = "real_reward_sum" 
    KEY_REWARD_MU = "reward_mu" 
    KEY_REWARD_STD = "reward_std" 
 
    def __init__(self, env: gym.Env, model_path: pathlib.Path, dev: torch.device, 
                 reward_window: int = 100, metrics_queue: tt.Optional[queue.Queue] = None): 
        super().__init__(env) 
        self.device = dev 
        assert isinstance(env.action_space, gym.spaces.Discrete) 
        s = env.observation_space.shape 
        self.total_actions = env.action_space.n 
        self.model = RewardModel( 
            input_shape=(s[2], s[0], s[1]), n_actions=self.total_actions) 
        self.model.load_state_dict(torch.load(model_path, map_location=torch.device(‚Äôcpu‚Äô), 
                                              weights_only=True)) 
        self.model.eval() 
        self.model.to(dev) 
        self._prev_obs = None 
        self._reward_window = collections.deque(maxlen=reward_window) 
        self._real_reward_sum = 0.0 
        self._metrics_queue = metrics_queue</code></pre>
</div>
</div>
<p>In the <span class="cmtt-10x-x-109">reset() </span>method, we just remember the observation and reset the reward counter:</p>
<div class="tcolorbox" id="tcolobox-422">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-507"><code>    def reset(self, *, seed: int | None = None, options: dict[str, tt.Any] | None = None) \ 
            -&gt; tuple[WrapperObsType, dict[str, tt.Any]]: 
        res = super().reset(seed=seed, options=options) 
        self._prev_obs = deepcopy(res[0]) 
        self._real_reward_sum = 0.0 
        return res</code></pre>
</div>
</div>
<p>The main logic <span id="dx1-358032"/>of the <span id="dx1-358033"/>wrapper <span id="dx1-358034"/>is in the <span class="cmtt-10x-x-109">step() </span>function, but it is not very complicated: we apply the model to the observation and the action, normalize the reward, and return it instead of the real one. The model application is not very efficient from a performance perspective and could be optimized (as we have several environments working in parallel), but I decided to implement the simple version first, leaving optimizations as an excercise for you:</p>
<div class="tcolorbox" id="tcolobox-423">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-508"><code>    def step(self, action: WrapperActType) -&gt; tuple[ 
        WrapperObsType, SupportsFloat, bool, bool, dict[str, tt.Any] 
    ]: 
        obs, r, is_done, is_tr, extra = super().step(action) 
        self._real_reward_sum += r 
        p_obs = np.moveaxis(self._prev_obs, (2, ), (0, )) 
        p_obs_t = torch.as_tensor(p_obs).to(self.device) 
        p_obs_t.unsqueeze_(0) 
        act = np.eye(self.total_actions)[[action]] 
        act_t = torch.as_tensor(act, dtype=torch.float32).to(self.device) 
        new_r_t = self.model(p_obs_t, act_t) 
        new_r = float(new_r_t.item()) 
 
        # track reward for normalization 
        self._reward_window.append(new_r) 
        if len(self._reward_window) == self._reward_window.maxlen: 
            mu = np.mean(self._reward_window) 
            std = np.std(self._reward_window) 
            new_r -= mu 
            new_r /= std 
            self._metrics_queue.put((self.KEY_REWARD_MU, mu)) 
            self._metrics_queue.put((self.KEY_REWARD_STD, std)) 
 
        if is_done or is_tr: 
            self._metrics_queue.put((self.KEY_REAL_REWARD_SUM, self._real_reward_sum)) 
        self._prev_obs = deepcopy(obs) 
        return obs, new_r, is_done, is_tr, extra</code></pre>
</div>
</div>
<p>The rest of <span id="dx1-358062"/>the<span id="dx1-358063"/> training is<span id="dx1-358064"/> the same. We just inject the new wrapper into the environment-creating function if the reward model file is given in the command line:</p>
<div class="tcolorbox" id="tcolobox-424">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-509"><code>    def make_env() -&gt; gym.Env: 
        e = gym.make("SeaquestNoFrameskip-v4") 
        if reward_path is not None: 
            p = pathlib.Path(reward_path) 
            e = rlhf.RewardModelWrapper(e, p, dev=dev, metrics_queue=metrics_queue) 
        if db_path is not None: 
            p = pathlib.Path(db_path) 
            p.mkdir(parents=True, exist_ok=True) 
            e = rlhf.EpisodeRecorderWrapper(e, p, env_idx=env_idx) 
        e = ptan.common.wrappers.wrap_dqn(e) 
        # add time limit after all wrappers 
        e = gym.wrappers.TimeLimit(e, TIME_LIMIT) 
        return e</code></pre>
</div>
</div>
<p>With this code, we can now combine the previous <span id="dx1-358078"/>model <span id="dx1-358079"/>with the labels we <span id="dx1-358080"/>made before.</p>
</section>
<section class="level4 subsectionHead" id="fine-tuning-with-100-labels">
<h2 class="heading-2" id="sigil_toc_id_323"> <span id="x1-35900019.3.5"/>Fine-tuning with 100 labels</h2>
<p>I ran the <span id="dx1-359001"/>training with the best model from <span id="dx1-359002"/>the basic A2C training, which, on testing, achieved a reward of 460 in 580 steps. In addition, I enabled sampling of episode segments into the new DB directory (v1 in this case), so the full command line was the following:</p>
<p><span class="cmtt-10x-x-109">./01</span><span class="cmtt-10x-x-109">_a2c.py --dev cuda -n v1 -r rw/reward-v0.dat --save save/v1</span> <span class="cmtt-10x-x-109">-m save/v0/model</span><span class="cmtt-10x-x-109">_rw=460-steps=580.dat --finetune --db-path</span> <span class="cmtt-10x-x-109">v1</span> This model started to overfit quite quickly and after 2M steps (3 hours), I stopped the training. <span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109">¬†</span><a href="#x1-359003r6"><span class="cmti-10x-x-109">19.6</span></a> shows the test results (reward and count of steps):</p>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_19_06.png" width="600"/> <span id="x1-359003r6"/></p>
<span class="id">Figure¬†19.6: Test reward (left) and steps (right) during the fine-tuning </span>
</div>
<p><span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109">¬†</span><a href="#x1-359004r7"><span class="cmti-10x-x-109">19.7</span></a> shows the training reward (predicted by the model) and the total loss:</p>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_19_07.png" width="600"/> <span id="x1-359004r7"/></p>
<span class="id">Figure¬†19.7: Training reward (left) and total loss (right) during the fine-tuning </span>
</div>
<p>The best model was stored at the 500K training step and it was able to get a reward of 900 in 1,120 steps. In comparison to the original model, this is quite an improvement.</p>
<p>A video recording of this model is available here: <a class="url" href="https://youtu.be/LnPwuyVrj9g"><span class="cmtt-10x-x-109">https://youtu.be/LnPwuyVrj9g</span></a>. From the gameplay, we see that the agent learned how to refill the oxygen and is now spending some time in the middle of the screen. I also had the impression that it picked divers more intentionally (but I haven‚Äôt done specific labeling for this behavior). So, overall, the method<span id="dx1-359005"/> works <span id="dx1-359006"/>and it is quite impressive that we can teach the agent something new with just 100 labels.</p>
<p>Let‚Äôs try to improve the model further with more labeling.</p>
</section>
<section class="level4 subsectionHead" id="the-second-round-of-the-experiment">
<h2 class="heading-2" id="sigil_toc_id_324"> <span id="x1-36000019.3.6"/>The second round of the experiment</h2>
<p>On the second<span id="dx1-360001"/> round, I did more labeling: 50 pairs from the v0 DB and 50 pairs from segments stored during the fine-tuning (v1 DB). The database generated during the fine-tuning (v1) contains many more segments with the submarine floating on the surface, which confirms that our pipeline is working as expected. During the labeling, I also put more emphasis on oxygen refill segments.</p>
<p>After labeling, I retrained the reward model, which only took several minutes. Then, fine-tuning of the best v1 model (with a reward of 900 and 1,120 steps) was performed using the reward model.</p>
<p><span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109">¬†</span><a href="#x1-360002r8"><span class="cmti-10x-x-109">19.8</span></a> and <span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109">¬†</span><a href="#x1-360003r9"><span class="cmti-10x-x-109">19.9</span></a> contain charts with test results, training the reward, and the loss:</p>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_19_08.png" width="600"/> <span id="x1-360002r8"/></p>
<span class="id">Figure¬†19.8: Test reward (left) and steps (right) during the fine-tuning </span>
</div>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_19_09.png" width="600"/> <span id="x1-360003r9"/></p>
<span class="id">Figure¬†19.9: Training reward (left) and total loss (right) during the fine-tuning </span>
</div>
<p>After 1.5M steps (2 hours), the training got stuck, but the best model wasn‚Äôt better than the best <span id="dx1-360004"/>model of v1: the best model got a reward of 860 in 1,084 steps.</p>
</section>
<section class="level4 subsectionHead" id="the-third-round-of-the-experiment">
<h2 class="heading-2" id="sigil_toc_id_325"> <span id="x1-36100019.3.7"/>The third round of the experiment</h2>
<p>Here, I paid more<span id="dx1-361001"/> attention during the labeling, trying to prioritize not just oxygen refill, but also better fish shooting and diver pickup. Unfortunately, 100 pairs gave just a couple of examples of divers, so more labeling is needed to teach the agent this behavior.</p>
<p>Regarding the divers, it might be that the agent doesn‚Äôt pick them up just because they are very hard to distinguish from the background, so on a grayscale image, they are invisible. To fix that, we can tweak the contrast in our Atari wrappers.</p>
<p>After the reward model retraining, A2C fine-tuning was started. I also ran it for almost 2M steps for 3 hours and the results were interesting. At the end of the training (check <span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109">¬†</span><a href="#x1-361003r10"><span class="cmti-10x-x-109">19.10</span></a> and <span class="cmti-10x-x-109">Figure</span><span class="cmti-10x-x-109">¬†</span><a href="#x1-361004r11"><span class="cmti-10x-x-109">19.11</span></a>), the boat during the testing reached 5,000 steps (which is the limit I set in the environment), but the score was fairly low. Most likely, the submarine just stayed on the surface, which is very safe, but not what we want ‚Äì this could be because of labeled samples. Strangely, when I tried to record the video of those later models, their behavior was different and the number of<span id="dx1-361002"/> steps was much lower, which could be an indication of some testing bug.</p>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_19_10.png" width="600"/> <span id="x1-361003r10"/></p>
<span class="id">Figure¬†19.10: Test reward (left) and steps (right) during the fine-tuning </span>
</div>
<div class="minipage">
<p><img alt="PIC" height="300" src="../Images/B22150_19_11.png" width="600"/> <span id="x1-361004r11"/></p>
<span class="id">Figure¬†19.11: Training reward (left) and total loss (right) during the fine-tuning </span>
</div>
<p>Before the overfitting, the training generated several policies that were better than the v2 models. For example, in this recording, the<span id="dx1-361005"/> agent refilled the oxygen twice and got a score of 1,820 during the 1,613 steps: <a class="url" href="https://youtu.be/DVe_9b3gdxU"><span class="cmtt-10x-x-109">https://youtu.be/DVe_9b3gdxU</span></a>.</p>
</section>
<section class="level4 subsectionHead" id="overall-results-1">
<h2 class="heading-2" id="sigil_toc_id_326"> <span id="x1-36200019.3.8"/>Overall results</h2>
<p>In the<span id="dx1-362001"/> following table, I have summarized the information about the experiment rounds and the results we got.</p>
<div class="table">
<figure class="float">
<div class="center">
<div class="tabular">
<table class="table-container" id="TBL-7">
<tbody>
<tr id="TBL-7-1-" style="vertical-align:baseline;">
<td class="table-cell" id="TBL-7-1-1"><span class="cmbx-10x-x-109">Step </span></td>
<td class="table-cell" id="TBL-7-1-2"><span class="cmbx-10x-x-109">Labels </span></td>
<td class="table-cell" id="TBL-7-1-3"><span class="cmbx-10x-x-109">Reward </span></td>
<td class="table-cell" id="TBL-7-1-4"><span class="cmbx-10x-x-109">Steps </span></td>
<td class="table-cell" id="TBL-7-1-5"><span class="cmbx-10x-x-109">Video </span></td>
</tr>
<tr id="TBL-7-2-" style="vertical-align:baseline;">
<td class="table-cell" id="TBL-7-2-1">Initial</td>
<td class="table-cell" id="TBL-7-2-2">None</td>
<td class="table-cell" id="TBL-7-2-3">460</td>
<td class="table-cell" id="TBL-7-2-4">580</td>
<td class="table-cell" id="TBL-7-2-5"><a class="url" href="https://youtu.be/R_H3pXu-7cw"><span class="cmtt-10x-x-109">https://youtu.be/R_H3pXu-7cw</span></a></td>
</tr>
<tr id="TBL-7-3-" style="vertical-align:baseline;">
<td class="table-cell" id="TBL-7-3-1">v1</td>
<td class="table-cell" id="TBL-7-3-2">100</td>
<td class="table-cell" id="TBL-7-3-3">900</td>
<td class="table-cell" id="TBL-7-3-4">1120</td>
<td class="table-cell" id="TBL-7-3-5"><a class="url" href="https://youtu.be/LnPwuyVrj9g"><span class="cmtt-10x-x-109">https://youtu.be/LnPwuyVrj9g</span></a></td>
</tr>
<tr id="TBL-7-4-" style="vertical-align:baseline;">
<td class="table-cell" id="TBL-7-4-1">v2</td>
<td class="table-cell" id="TBL-7-4-2">200</td>
<td class="table-cell" id="TBL-7-4-3">860</td>
<td class="table-cell" id="TBL-7-4-4">1083</td>
<td class="table-cell" id="TBL-7-4-5"/>
</tr>
<tr id="TBL-7-5-" style="vertical-align:baseline;">
<td class="table-cell" id="TBL-7-5-1">v3</td>
<td class="table-cell" id="TBL-7-5-2">300</td>
<td class="table-cell" id="TBL-7-5-3">1820</td>
<td class="table-cell" id="TBL-7-5-4">1613</td>
<td class="table-cell" id="TBL-7-5-5"><a class="url" href="https://youtu.be/DVe_9b3gdxU"><span class="cmtt-10x-x-109">https://youtu.be/DVe_9b3gdxU</span></a></td>
</tr>
</tbody>
</table>
</div>
<span id="x1-362002r1"/>
<span class="id">Table¬†19.1: Summary of experiment rounds </span>
</div>
</figure>
</div>
<p>As you can see, with just 300 labels, we were able to increase the scoring by almost 4 times. As an exercise, you can try to teach the agent to pick up divers, which might result in a much better score if done properly.</p>
<p>Another experiment that might be worth doing is to fine-tune the original v0 model, instead of the best models from the previous step. It might lead to better results, as the training has more time before overfitting.</p>
</section>
</section>
<section class="level3 sectionHead" id="summary-18">
<h1 class="heading-1" id="sigil_toc_id_327"> <span id="x1-36300019.4"/>Summary</h1>
<p>In this chapter, we‚Äôve taken a look at the recent addition of RLHF to the RL toolbox. This method, at the core of the LLM training pipeline, allows you to increase the quality of models. In the chapter, we implemented RLHF and applied it to the SeaQuest Atari game, which should have illustrated to you how this method could be used in RL pipelines for model improvement.</p>
<p>In the next chapter, we‚Äôll discuss a different family of RL methods: AlphaGo, AlphaZero, and MuZero.</p>
</section>
</section>
</div></body></html>