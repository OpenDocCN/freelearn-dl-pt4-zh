["```py\nfrom datasetsforecast.m3 import M3\ndataset, *_ = M3.load('./data', 'Quarterly')\nq1 = dataset.query('unique_id==\"Q1\"')\n```", "```py\n    from statsforecast import StatsForecast\n    from statsforecast.models import AutoARIMA\n    models = [AutoARIMA(season_length=4)]\n    sf = StatsForecast(\n        df=q1,\n        models=models,\n        freq='Q',\n        n_jobs=1,\n    )\n    ```", "```py\n    forecasts = sf.forecast(h=8, level=[99], \n        fitted=True).reset_index()\n    insample_forecasts = sf.forecast_fitted_values().reset_index()\n    ```", "```py\n    anomalies = insample_forecasts.loc[\n        (\n            insample_forecasts['y'] >= \n            insample_forecasts['AutoARIMA-hi-99']\n        ) | (\n            insample_forecasts['y'] <= \n            insample_forecasts['AutoARIMA-lo-99'])]\n    ```", "```py\n    StatsForecast.plot(insample_forecasts, unique_ids=['Q1'], \n        plot_anomalies=True)\n    ```", "```py\nfrom datetime import datetime\nimport pandas as pd\ndataset = pd.read_csv('assets/datasets/taxi/taxi_data.csv')\nlabels = pd.read_csv('assets/datasets/taxi/taxi_labels.csv')\ndataset['ds'] = pd.Series([datetime.fromtimestamp(x) \n    for x in dataset['timestamp']])\ndataset = dataset.drop('timestamp', axis=1)\ndataset['unique_id'] = 'NYT'\ndataset = dataset.rename(columns={'value': 'y'})\nis_anomaly = []\nfor i, r in labels.iterrows():\n    dt_start = datetime.fromtimestamp(r.start)\n    dt_end = datetime.fromtimestamp(r.end)\n    anomaly_in_period = [dt_start <= x <= dt_end \n        for x in dataset['ds']]\n    is_anomaly.append(anomaly_in_period)\ndataset['is_anomaly']=pd.DataFrame(is_anomaly).any(axis=0).astype(int)\ndataset['ds'] = pd.to_datetime(dataset['ds'])\n```", "```py\n    from neuralforecast import NeuralForecast\n    from neuralforecast.models import NHITS\n    horizon = 1\n    n_lags = 144\n    models = [NHITS(h=horizon,\n        input_size=n_lags,\n        max_steps=30,\n        n_freq_downsample=[2, 1, 1],\n        mlp_units=3 * [[128, 128]],\n        accelerator='cpu')]\n    nf = NeuralForecast(models=models, freq='30T')\n    ```", "```py\n    nf.fit(df=dataset.drop('is_anomaly', axis=1), val_size=n_lags)\n    ```", "```py\n    insample = nf.predict_insample()\n    insample = insample.tail(-n_lags)\n    abs_error = (insample['NHITS'] - insample['y']).abs()\n    ```", "```py\n    preds = pd.DataFrame(\n        {\n            \"Error\": abs_error.values,\n            \"ds\": dataset[\"ds\"].tail(-n_lags),\n            \"is_anomaly\": dataset[\"is_anomaly\"].tail(-n_lags),\n        }\n    )\n    preds = preds.set_index(\"ds\")\n    predicted_anomaly_periods = find_anomaly_periods(\n        preds[\"is_anomaly\"])\n    setup_plot(preds.rename(columns={\"Error\": \"y\"}), \n        predicted_anomaly_periods, \"Error\")\n    ```", "```py\n    Import numpy as np\n    import pandas as pd\n    import lightning.pytorch as pl\n    from pytorch_forecasting import TimeSeriesDataSet\n    from sklearn.model_selection import train_test_split\n    class TaxiDataModule(pl.LightningDataModule):\n        def __init__(self,\n                     data: pd.DataFrame,\n                     n_lags: int,\n                     batch_size: int):\n            super().__init__()\n            self.data = data\n            self.batch_size = batch_size\n            self.n_lags = n_lags\n            self.train_df = None\n            self.test_df = None\n            self.training = None\n            self.validation = None\n            self.predict_set = None\n    ```", "```py\n        def setup(self, stage=None):\n            self.data['timestep'] = np.arange(self.data.shape[0])\n            unique_times = \\\n                self.data['timestep'].sort_values().unique()\n            tr_ind, ts_ind = \\\n                train_test_split(unique_times, test_size=0.4,\n                    shuffle=False)\n            tr_ind, vl_ind = \\\n                train_test_split(tr_ind, test_size=0.1,\n                    shuffle=False)\n            self.train_df = \\\n                self.data.loc[self.data['timestep'].isin(tr_ind), :]\n            self.test_df = \\\n                self.data.loc[self.data['timestep'].isin(ts_ind), :]\n            validation_df = \\\n                self.data.loc[self.data['timestep'].isin(vl_ind), :]\n            self.training = TimeSeriesDataSet(\n                data=self.train_df,\n                time_idx=\"timestep\",\n                target=\"y\",\n                group_ids=['unique_id'],\n                max_encoder_length=self.n_lags,\n                max_prediction_length=1,\n                time_varying_unknown_reals=['y'],\n            )\n            self.validation = \\\n                TimeSeriesDataSet.from_dataset(\n                    self.training, validation_df)\n            self.test = \\\n                TimeSeriesDataSet.from_dataset(\n                    self.training, self.test_df)\n            self.predict_set = \\\n                TimeSeriesDataSet.from_dataset(\n                    self.training, self.data, predict=True)\n    ```", "```py\n        def train_dataloader(self):\n            return self.training.to_dataloader(\n                batch_size=self.batch_size, shuffle=False)\n        def val_dataloader(self):\n            return self.validation.to_dataloader(\n                batch_size=self.batch_size, shuffle=False)\n        def predict_dataloader(self):\n            return self.predict_set.to_dataloader(\n                batch_size=1, shuffle=False)\n    ```", "```py\n    from torch import nn\n    import torch\n    class Encoder(nn.Module):\n        def __init__(self, context_len, n_variables, \n            embedding_dim=2):\n            super(Encoder, self).__init__()\n            self.context_len, self.n_variables = \\\n                context_len, n_variables\n            self.embedding_dim, self.hidden_dim = \\\n                embedding_dim, 2 * embedding_dim\n            self.lstm1 = nn.LSTM(\n                input_size=self.n_variables,\n                hidden_size=self.hidden_dim,\n                num_layers=1,\n                batch_first=True\n            )\n            self.lstm2 = nn.LSTM(\n                input_size=self.hidden_dim,\n                hidden_size=embedding_dim,\n                num_layers=1,\n                batch_first=True\n            )\n        def forward(self, x):\n            batch_size = x.shape[0]\n            x, (_, _) = self.lstm1(x)\n            x, (hidden_n, _) = self.lstm2(x)\n            return hidden_n.reshape((batch_size, \n                self.embedding_dim))\n    ```", "```py\n    class Decoder(nn.Module):\n        def __init__(self, context_len, n_variables=1, input_dim=2):\n            super(Decoder, self).__init__()\n            self.context_len, self.input_dim = \\\n                context_len, input_dim\n            self.hidden_dim, self.n_variables = \\\n                2 * input_dim, n_variables\n            self.lstm1 = nn.LSTM(\n                input_size=input_dim,\n                hidden_size=input_dim,\n                num_layers=1,\n                batch_first=True\n            )\n            self.lstm2 = nn.LSTM(\n                input_size=input_dim,\n                hidden_size=self.hidden_dim,\n                num_layers=1,\n                batch_first=True\n            )\n            self.output_layer = nn.Linear(self.hidden_dim, \n                self.n_variables)\n        def forward(self, x):\n            batch_size = x.shape[0]\n            x = x.repeat(self.context_len, self.n_variables)\n            x = x.reshape((batch_size, self.context_len, \n                self.input_dim))\n            x, (hidden_n, cell_n) = self.lstm1(x)\n            x, (hidden_n, cell_n) = self.lstm2(x)\n            x = x.reshape((batch_size, self.context_len, \n                self.hidden_dim))\n            return self.output_layer(x)\n    ```", "```py\n    import torch\n    class AutoencoderLSTM(pl.LightningModule):\n        def __init__(self, context_len, n_variables, embedding_dim):\n            super().__init__()\n            self.encoder = Encoder(context_len, n_variables, \n                embedding_dim)\n            self.decoder = Decoder(context_len, n_variables, \n                embedding_dim)\n        def forward(self, x):\n            xh = self.encoder(x)\n            rec_x = self.decoder(xh)\n            return rec_x\n        def configure_optimizers(self):\n            return torch.optim.Adam(self.parameters(), lr=0.001)\n    ```", "```py\n        import torch.nn.functional as F\n        def training_step(self, batch, batch_idx):\n            x, y = batch\n            y_pred = self(x['encoder_cont'])\n            loss = F.mse_loss(y_pred, x['encoder_cont'])\n            self.log('train_loss', loss)\n            return loss\n        def validation_step(self, batch, batch_idx):\n            x, y = batch\n            y_pred = self(x['encoder_cont'])\n            loss = F.mse_loss(y_pred, x['encoder_cont'])\n            self.log('val_loss', loss)\n            return loss\n        def predict_step(self, batch, batch_idx):\n            x, y = batch\n            y_pred = self(x['encoder_cont'])\n            loss = F.mse_loss(y_pred, x['encoder_cont'])\n            return loss\n    ```", "```py\n    N_LAGS = 144\n    N_VARIABLES = 1\n    from lightning.pytorch.callbacks import EarlyStopping\n    datamodule = \\\n        TaxiDataModule(\n            data=dataset.drop('is_anomaly', axis=1),\n            n_lags=N_LAGS,\n            batch_size=32)\n    model = AutoencoderLSTM(n_variables=1,\n            context_len=N_LAGS,\n            embedding_dim=4)\n    early_stop_callback = EarlyStopping(monitor=\"val_loss\",\n        min_delta=1e-4,\n        patience=5,\n        verbose=False,\n        mode=\"min\")\n    trainer = pl.Trainer(max_epochs=20,\n        accelerator='cpu',\n        callbacks=[early_stop_callback])\n    trainer.fit(model, datamodule)\n    ```", "```py\n    dl = datamodule.test.to_dataloader(batch_size=1, shuffle=False)\n    preds = trainer.predict(model, dataloaders=dl)\n    preds = pd.Series(np.array([x.numpy() for x in preds]))\n    ```", "```py\npip install pyod\n```", "```py\n    import pandas as pd\n    from sklearn.preprocessing import StandardScaler\n    N_LAGS = 144\n    series = dataset['y']\n    input_data = []\n    for i in range(N_LAGS, series.shape[0]):\n        input_data.append(series.iloc[i - N_LAGS:i].values)\n    input_data = np.array(input_data)\n    input_data_n = StandardScaler().fit_transform(input_data)\n    input_data_n = pd.DataFrame(input_data_n)\n    ```", "```py\n    from pyod.models.auto_encoder_torch import AutoEncoder\n    model = AutoEncoder(\n        hidden_neurons=[144, 4, 4, 144],\n        hidden_activation=\"relu\",\n        epochs=20,\n        batch_norm=True,\n        learning_rate=0.001,\n        batch_size=32,\n        dropout_rate=0.2,\n    )\n    model.fit(input_data_n)\n    anomaly_scores = model.decision_scores_\n    ```", "```py\n    predictions = model.predict(input_data_n)\n    ```", "```py\n    probs = model.predict_proba(input_data_n)[:, 1]\n    probabilities = pd.Series(probs, \\\n        index=series.tail(len(probs)).index)\n    ```", "```py\n    ds = dataset.tail(-144)\n    ds['Predicted Probability'] = probabilities\n    ds = ds.set_index('ds')\n    anomaly_periods = find_anomaly_periods(ds['is_anomaly'])\n    setup_plot(ds, anomaly_periods)\n    ```", "```py\nN_LAGS = 144\nseries = dataset['y']\n```", "```py\n    import pandas as pd\n    from sklearn.preprocessing import StandardScaler\n    import numpy as np\n    input_data = []\n    for i in range(N_LAGS, series.shape[0]):\n        input_data.append(series.iloc[i - N_LAGS:i].values)\n        input_data = np.array(input_data)\n        input_data_n = StandardScaler().fit_transform(input_data)\n        input_data_n = pd.DataFrame(input_data_n)\n    ```", "```py\n    from pyod.models.vae import VAE\n    from tensorflow.keras.losses import mean_squared_error\n    model = VAE(encoder_neurons=[144, 4],\n            decoder_neurons=[4, 144],\n            latent_dim=2,\n            hidden_activation='relu',\n            output_activation='sigmoid',\n            loss=mean_squared_error,\n            optimizer='adam',\n            epochs=20,\n            batch_size=32,\n            dropout_rate=0.2,\n            l2_regularizer=0.1,\n            validation_size=0.1,\n            preprocessing=True,\n            verbose=1)\n    model.fit(input_data_n)\n    ```", "```py\n    anomaly_scores = model.decision_scores_\n    ```", "```py\n    import pandas as pd\n    from sklearn.preprocessing import StandardScaler\n    import numpy as np\n    N_LAGS = 144\n    series = dataset['y']\n    input_data = []\n    for i in range(N_LAGS, series.shape[0]):\n        input_data.append(series.iloc[i - N_LAGS:i].values)\n        input_data = np.array(input_data)\n        input_data_n = StandardScaler().fit_transform(input_data)\n        input_data_n = pd.DataFrame(input_data_n)\n    ```", "```py\n    from pyod.models.anogan import AnoGAN\n    model = AnoGAN(activation_hidden='tanh',\n        dropout_rate=0.2,\n        latent_dim_G=2,\n        G_layers=[20, 10, 3, 10, 20],\n        verbose=1,\n        D_layers=[20, 10, 5],\n        index_D_layer_for_recon_error=1,\n        epochs=20,\n        preprocessing=False,\n        learning_rate=0.001,\n        learning_rate_query=0.01,\n        epochs_query=1,\n        batch_size=32,\n        output_activation=None,\n        contamination=0.1)\n    model.fit(input_data_n)\n    ```", "```py\n    anomaly_scores = model.decision_scores_\n    predictions = model.predict(input_data_n)\n    ```"]