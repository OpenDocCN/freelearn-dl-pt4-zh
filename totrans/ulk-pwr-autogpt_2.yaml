- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From Installation to Your First AI-Generated Text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have finished the first chapter, let’s discuss the must-have requirements
    of **Auto-GPT**.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, before we start, whether you choose to register for an **application
    programming interface** (**API**) account or not at OpenAI is up to you. I first
    recommend trying to install and start Auto-GPT before registering, in case Auto-GPT
    only works in Docker (which could happen as it keeps changing); it may or may
    not be possible for you to run Auto-GPT. However, let’s begin by setting up Auto-GPT
    without the account. Otherwise, you will have an OpenAI account but there is no
    need for it.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will guide you through preparing your machine to run Auto-GPT,
    the installation process, and your first steps with Auto-GPT.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll cover the fundamental concepts, installation, and setup instructions for
    Auto-GPT. We will conclude by explaining how to execute your first AI-automated
    task using Auto-GPT.
  prefs: []
  type: TYPE_NORMAL
- en: The team at Auto-GPT (including me) is working hard on making Auto-GPT as accessible
    as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Recently a new tool has been made by one of the maintainers called **Auto-GPT
    Wizard**. If you struggle with setting up Auto-GPT at any point, this tool is
    meant to automate the installation and make it easier for newbies to get into
    Auto-GPT.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the tool at [https://github.com/Significant-Gravitas/AutoGPT_Wizard](https://github.com/Significant-Gravitas/AutoGPT_Wizard).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn about the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: System requirements and prerequisites
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing and setting up Auto-GPT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Going through the basic concepts and terminology
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First run
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are some system requirements and prerequisites:'
  prefs: []
  type: TYPE_NORMAL
- en: Install VS Code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Install Python.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Install Poetry.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing VS Code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I strongly recommend installing VS Code for usability or using any other IDE
    you see fit for Python. Working as a triage catalyst (reviewer, support, and contributor
    role at Auto-GPT), I have encountered many people who got stuck because they used
    their text editor or even Microsoft Word.
  prefs: []
  type: TYPE_NORMAL
- en: Using advanced text editors configured properly might be adequate for basic
    scripting or editing configuration files, as they can be configured to avoid issues
    with text encoding and incorrect file extensions. However, IDEs such as VS Code
    offer more robust tools and integrations for a seamless development experience,
    especially when dealing with complex projects such as Auto-GPT; but we will have
    to edit JSON files, a `.env` file, and sometimes markdown (`.md`) files. Editing
    those with anything else than an IDE will probably result in the wrong file extension
    being added (for example, `.env` and `settings.json` could become `.env.txt` or
    `settings.json.docx`, which do not work).
  prefs: []
  type: TYPE_NORMAL
- en: As a common tool to be used by many developers and it being free to use, we
    will focus on VS Code.
  prefs: []
  type: TYPE_NORMAL
- en: To not drift off the topic of why else you could use VS Code, Microsoft wrote
    a very good article on why VS Code is worth using. Of course, you can also use
    other IDEs. The main reason I recommend VS Code is that it is open source and
    free to use and also used by most Auto-GPT contributors, making it very easy to
    work with Auto-GPT and some integrated project settings for VS Code.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Python 3.10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you want to run Auto-GPT directly without Docker, you may need to install
    Python 3.10 or enable it as the terminal’s `python` and `python3` alias, to make
    sure that Auto-GPT doesn’t accidentally call a different Python version.
  prefs: []
  type: TYPE_NORMAL
- en: Auto-GPT is developed in Python and it specifically requires Python version
    3.10.x. The *x* in 3.10.x represents any sub-version (for example, 3.10.0, 3.10.6),
    and the software is compatible with these sub-versions.
  prefs: []
  type: TYPE_NORMAL
- en: While Auto-GPT is lightweight in terms of file size, it can be resource-intensive
    depending on the options and plugins you enable. Consequently, it’s essential
    to have a compatible and optimized environment to ensure the smooth operation
    of Auto-GPT and the plugins you may choose to use, as those are all written for
    Python 3.10 and those expected modules that are also for 3.10\. In addition to
    installing Python 3.10, it is recommended to use virtual environments for Auto-GPT
    development. Virtual environments allow you to manage dependencies and Python
    versions on a project-by-project basis, ensuring that Auto-GPT runs in an isolated
    and controlled setting without affecting other Python projects you may be working
    on. This is crucial for maintaining compatibility and avoiding conflicts between
    different project requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Why choose Python 3.10?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Python 3.10 introduces several features and optimizations that are beneficial
    for running Auto-GPT. One such feature is the improved syntax for type hinting.
    In Python 3.10, you can use the pipe symbol, `|`, as a more concise way of indicating
    that a variable can be of multiple types. This is known as the **type** **union
    operator**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this example error message, Auto-GPT is attempting to use this new type union
    syntax, which is not supported in Python versions earlier than 3.10\. This is
    why using Python 3.9 results in a syntax error, as it cannot parse the new syntax.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, Python 3.10 brings performance improvements, better error messages,
    and new features that can be advantageous for complex applications such as Auto-GPT.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, to avoid compatibility issues and take advantage of the new features
    and optimizations, it is crucial to install and set up Python 3.10 correctly before
    running Auto-GPT.
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisites for Python Installation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before installing Python 3.10, it’s important to ensure that your system meets
    the necessary prerequisites. These prerequisites include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sufficient disk space**: Make sure your system has an adequate amount of
    free disk space to accommodate the Python installation and any additional packages
    or libraries you may install in the future.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Checking for existing Python installations**: If you already have a previous
    version of Python installed on your system, it’s recommended to check for any
    potential conflicts or compatibility issues that may arise with Python 3.10\.
    You can do this by running the appropriate version-specific commands or using
    the Python version management tool for your **operating** **system** (**OS**).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By ensuring that your system meets these prerequisites, you can proceed with
    confidence to install Python 3.10 and set up Auto-GPT successfully.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Python 3.10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Auto-GPT is mostly based on Python 3.10 packages; if you try to run it with
    3.9 for example, you will only receive a few exceptions and you will not be able
    to execute Auto-GPT.
  prefs: []
  type: TYPE_NORMAL
- en: 'Running Python 3.10 requires a system that can support this version of the
    programming language. Here are the system requirements and installation instructions
    for each OS:'
  prefs: []
  type: TYPE_NORMAL
- en: For Windows, use the documentation at [https://www.digitalocean.com/community/tutorials/install-python-windows-10](https://www.digitalocean.com/community/tutorials/install-python-windows-10).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To verify the installation, run the following command in the command prompt:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: For installing Python 3.10 on Linux (Ubuntu/Debian), you may have to do a bit
    of research depending on what flavor of Linux you are using. But, as they say,
    with great power comes great responsibility; you may have to research how to enable
    Python 3.10 on your machine.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For Ubuntu and Debian, here is the documentation on how to install 3.10: [https://www.linuxcapable.com/how-to-install-python-3-10-on-ubuntu-linux/](https://www.linuxcapable.com/how-to-install-python-3-10-on-ubuntu-linux/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To verify if the installation was done successfully, run the following command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The system should return Python 3.10.x.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The exact commands and steps might vary slightly based on the specific version
    of each OS. Always refer to the official Python documentation or your OS’s documentation
    for the most correct and up-to-date information.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Poetry
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A new dependency was recently added, which is a bit tricky to install.
  prefs: []
  type: TYPE_NORMAL
- en: 'Documentation on how to install it can be found here: [https://python-poetry.org/docs/#installing-with-pipx](https://python-poetry.org/docs/#installing-with-pipx).'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you struggle to set it up (on Windows, for example), you may also try the
    Wizard script here: [https://github.com/Significant-Gravitas/AutoGPT_Wizard](https://github.com/Significant-Gravitas/AutoGPT_Wizard).'
  prefs: []
  type: TYPE_NORMAL
- en: Additional requirements that may come up
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Please check the official documentation at [https://docs.agpt.co/autogpt/setup/](https://docs.agpt.co/autogpt/setup/)
    to make sure you are not missing anything.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to having a compatible version of Python and poetry installed on
    your system, it is also essential to ensure that your hardware meets specific
    minimum requirements for running Auto-GPT effectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Processor (CPU)**: A modern multi-core processor (Intel Core i5/i7 or AMD
    Ryzen) is recommended for optimal performance when using Auto-GPT'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Memory (RAM)**: A minimum of 8 GB RAM is recommended; however, having more
    memory available will improve performance when working with large datasets or
    complex tasks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Storage**: Ensure sufficient free disk space on your computer’s primary storage
    drive (HDD/SSD) – at least several gigabytes – as Auto-GPT may generate temporary
    files during operation and require additional space for storing generated output
    files'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Internet connection**: A stable internet connection with reasonable bandwidth
    is necessary since Auto-GPT communicates with OpenAI’s API to access GPT models
    and generate text'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GPU support (optional)**: While not strictly required, having a compatible
    NVIDIA or AMD GPU can significantly improve the performance of certain tasks,
    such as using text-to-speech engines such as Silero **Text-to-Speech** (**TTS**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By ensuring that your system meets these requirements and prerequisites, you
    will be well prepared to install and use Auto-GPT effectively.
  prefs: []
  type: TYPE_NORMAL
- en: In the next sections, we will guide you through the installation process for
    Auto-GPT on various OSs and provide an overview of basic concepts and terminology
    related to Auto-GPT and its underlying technology.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that while these system requirements and prerequisites are designed
    to provide a smooth experience when using Auto-GPT, individual needs may vary
    depending on the specific tasks you plan to perform with the tool. For example,
    if you intend to use Auto-GPT for large-scale text generation or complex **natural
    language processing** (**NLP**) tasks, you might benefit from having a more powerful
    CPU or additional memory available.
  prefs: []
  type: TYPE_NORMAL
- en: In any case, it is always a good idea to monitor your system’s performance while
    using Auto-GPT and adjust your hardware configuration as needed. This will help
    ensure that you can make the most of this powerful AI-driven text generation tool
    without meeting performance bottlenecks or other issues.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before installing and setting up Auto-GPT on your system, do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Ensure that your OS (macOS, Linux/Ubuntu/Debian, Windows) meets the minimum
    requirements for running Python 3.10.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install Python 3.10.x, following the instructions provided for each OS.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Verify that Python 3.10.x has been installed correctly by checking its version
    in Terminal (macOS/Linux) or Command Prompt (Windows).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make sure your hardware meets minimum requirements such as processor (CPU),
    memory (RAM), storage space availability, and internet connection stability/bandwidth.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This step is optional. Consider GPU support if planning to use resource-intensive
    features such as text-to-speech engines or a local LLM such as Vicuna or LLAMA
    (this is an interesting topic, as most GPUs cannot handle an LLM that’s usable
    with Auto-GPT).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By following these guidelines carefully and ensuring that your system meets
    all requirements and prerequisites before installing Auto-GPT, you will be well
    prepared for a successful installation process and an enjoyable experience using
    this powerful AI-driven text generation tool.
  prefs: []
  type: TYPE_NORMAL
- en: In our next sections, we will guide you through every step of getting started
    with this incredible software – from installation procedures tailored specifically
    for each OS to understanding the fundamental concepts and terminology that underpin
    Auto-GPT’s functionality.
  prefs: []
  type: TYPE_NORMAL
- en: Installing and setting up Auto-GPT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are the steps to install Auto-GPT:'
  prefs: []
  type: TYPE_NORMAL
- en: Depending on your experience, you may want to either use Git directly and clone
    the repository from [https://github.com/Significant-Gravitas/Auto-GPT.git](https://github.com/Significant-Gravitas/Auto-GPT.git).
    Or, if you are less experienced with the Terminal, you may go to [https://github.com/Significant-Gravitas/Auto-GPT](https://github.com/Significant-Gravitas/Auto-GPT).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the top right, click on `.zip` file, and save it anywhere you want Auto-GPT’s
    folder to be. Then, simply unpack the `.``zip` file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you want to be 100% sure that you are on the most stable version, go to [https://github.com/Significant-Gravitas/Auto-GPT/releases/latest](https://github.com/Significant-Gravitas/Auto-GPT/releases/latest).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pick the latest release (in our case, 0.4.1) download the `.zip` file in the
    **Assets** section of that post, and unzip it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The latest version that I used was release v0.4.7; anything above that version
    may be restructured, for example, 0.5.0 already has the Auto-GPT folder inside
    `Auto-GPT/autogpts/autogpt`. For closer inspection, read the updated `README`
    and documentation inside the repository itself to see which version you are looking
    at.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Installing Auto-GPT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For a fast-growing project, the installation of Auto-GPT may differ, so if you
    have any trouble with the following guide, try to check [https://docs.agpt.co/](https://docs.agpt.co/)
    if anything has changed.
  prefs: []
  type: TYPE_NORMAL
- en: As Auto-GPT on its own comes with a variety of Python dependencies, you may
    now want to navigate to your Auto-GPT folder in a Terminal.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using Auto-GPT with Docker, do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Some developers use Dockerfile directly, but I (as a Docker newbie) recommend
    using `docker-compose.yml`, which some folks have added.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make sure you have Docker installed (go back to *Installing Docker* in the previous
    chapter).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Simply navigate into the Auto-GPT folder and run the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: I am giving `–gpt3only` as an example only to make sure we don’t spend any money
    yet, as I assume you have just created your OpenAI account, which grants a free
    $5 starting bonus.
  prefs: []
  type: TYPE_NORMAL
- en: Using Docker to pull the Auto-GPT image
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here, let’s ensure you have Docker installed on your system. If you are not
    sure, you can jump to [*Chapter 6*](B21128_06.xhtml#_idTextAnchor091), where I
    cover how to set up Docker on your machine and give you some extra tips on using
    Docker with Auto-GPT.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have Docker installed, do the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a project directory for Auto-GPT:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Create the necessary configuration files. You can find templates in the repository.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Pull the latest image from Docker Hub:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Run with Docker according to the instructions given in the documentation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cloning Auto-GPT using Git
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Assuming you have Git installed on your system (doesn’t come natively on Windows
    for example), we will cover the process of cloning Auto-GPT here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s ensure you have Git installed for your OS:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to first clone the repository with the help of the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will navigate to the directory where you downloaded the repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Without Git/Docker
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Download the source code (the `.zip` file) from the latest stable release.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Extract the zipped file into a folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will navigate to the directory where you downloaded the repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: From here, depending on the version you may install, Auto-GPT could be inside
    the `Auto-GPT/autogpts/autogpt` folder, as the main repository was turned into
    more of a framework to be used to create other `Auto-GPT` instances. The Auto-GPT
    project we discuss in this book is inside the folder mentioned previously.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here is how we configuration happens:'
  prefs: []
  type: TYPE_NORMAL
- en: Find the file named `.env.template` in the main Auto-GPT folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a copy of `.env.template` and rename it `.env`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open the `.env` file in a text editor. If you have not already, investigate
    using VS Code, for example, so that you can just open Auto-GPT as a project and
    edit anything you need to.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the line that says `OPENAI_API_KEY=`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter your unique OpenAI API key after the `=` symbol without any quotes or
    spaces.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you use multiple Auto-GPT instances (which can be easily done with just another
    `auto-gpt` folder; it is best to create multiple API keys), you can make sure
    you keep an eye on the costs of each instance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Depending on which GPT model you have access to, you must now change the `FAST_LLM_MODEL`
    and `SMART_LLM_MODEL` attributes the same way we just did with the API key.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To find out which models are available to you, go to [https://platform.openai.com/account/rate-limits](https://platform.openai.com/account/rate-limits).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It only lists the ones you can use.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As of writing this chapter, OpenAI has just released a 16 K model of gpt-3.5-turbo-16k.
    It can carry more tokens/words than GPT-4 can, but I generally feel like the output
    is still worse than GPT-4’s, as Auto-GPT tends to do random tasks that it makes
    up out of nowhere.
  prefs: []
  type: TYPE_NORMAL
- en: The issue lies in the context process ability, although it can work with more
    tokens, GPT-4 has far more parameters than it works with and is much more optimized.
  prefs: []
  type: TYPE_NORMAL
- en: The default number of tokens is 4,000 if you set GPT-3.5-Turbo as a model and
    8,000 tokens if you set GPT-4 as a model, but I do recommend setting those limits
    slightly below those.
  prefs: []
  type: TYPE_NORMAL
- en: For example, 7,000 instead of 8,000 gives less room for memory summarization
    on the `SMART_LLM_MODEL`, while still making sure there are no exceptions where
    more words or tokens slip through to the Chat Completion prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Auto-GPT has introduced customizing options such as disallowing certain commands
    or which text-to-speech engine you want to use.
  prefs: []
  type: TYPE_NORMAL
- en: Having speech enabled makes Auto-GPT talk to you via voice. The choice of which
    TTS engine to use is all yours. I prefer Silero TTS, as it is almost as good as
    ElevenLabs but it is completely free to use; you just need a computer with a powerful
    CPU and/or GPU (you can select whether to use CPU or GPU for the TTS model).
  prefs: []
  type: TYPE_NORMAL
- en: As you already may have noticed, Auto-GPT comes with a huge set of terminologies
    that come from the world of AI and machine learning. We will now cover some of
    the most frequent ones here.
  prefs: []
  type: TYPE_NORMAL
- en: Basic concepts and terminologies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we start using Auto-GPT, let’s review some basic concepts and terminologies
    that will help us understand how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Text generation**: Text generation is the task of creating natural language
    text from given input data or context. For example, given a topic, a genre, or
    a prompt, text generation can produce a paragraph, an article, a story, or a dialogue
    that matches the input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model**: A model is a mathematical representation of a system or a process
    that can be used to make predictions or decisions. In machine learning, a model
    is a function that maps an input to an output. For example, a model can take an
    image as an input and output a label that describes what is in the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Chain of thought**: This concept is centered on the progressive development
    and refining of ideas or solutions through the systematic and sequential application
    of thought processes. In the context of using a tool such as ChatGPT, a “chain
    of thought” approach would involve feeding the output from one query as the input
    to the next query, essentially creating a “chain” of evolving responses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This method allows for the deep exploration of a topic or problem, as each step
    in the chain builds upon the previous, potentially leading to more nuanced and
    sophisticated results. It could be particularly useful in tasks such as developing
    a complex narrative, iteratively refining a model, or exploring multiple angles
    of a problem before settling on a solution.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Tree of thought**: A strategy to retrieve much better results in text generation,
    ChatGPT for example, is to instruct it to solve a problem and provide multiple
    alternatives. This can be achieved by saying “Write four alternatives, assess
    them, and improve.” This simple instruction tells the model to be creative and
    create four alternatives to an already given solution, evaluate them, and then
    encourage it to output an improved solution instead of just one answer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This results in much more accurate output and can be chained and done multiple
    times. For instance, I was working on a new neural cell network prototype and
    asked ChatGPT to help me with the data transformer method that would receive a
    string (text) and apply it to multiple matrixes. The first result was bad and
    wasn’t even correct Python code, but after three or four iterations of saying
    “Write 4 alternatives that may improve that code and improve its strategy, assess
    them, rate them from 1-10, rank them, then improve,” this resulted in very clean
    code and it even gave me improvement ideas on how to make the code more performant
    after the second iteration, which it wouldn’t have done if I just asked it straight
    away.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Forest of thought**: This one builds on top of the principle of the tree
    of thought but as the name already suggests, you have multiple instances that
    think like a group of people. A fantastic explanation can be found in this video
    I watched recently: [https://www.youtube.com/watch?v=y6SVA3aAfco](https://www.youtube.com/watch?v=y6SVA3aAfco).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Neural network**: A neural network is a type of model that consists of interconnected
    units called **neurons**. Each neuron can perform a simple computation on its
    inputs and produce an output. By combining neurons in different layers and configurations,
    a neural network can learn complex patterns and relationships from data. GPT,
    for instance, has multiple neural networks running that all have different tasks
    and consist of multiple layers of neural networks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deep learning**: Developed by OpenAI, **Generative Pre-Trained Transformer
    3** (**GPT-3**) stands as a monumental figure in the realm of NLP. This deep learning
    model, boasting a staggering 175 billion parameters and a vast training dataset
    of 45 terabytes, is renowned for its text generation capabilities, offering coherence
    and versatility across a myriad of topics, genres, and styles. Despite anticipation
    surrounding its successor, GPT-4, which promises enhanced context understanding
    and logical processing, GPT-3 remains a formidable tool, especially for smaller
    tasks. Its recent upgrade to process up to 16 K tokens has notably enhanced output
    quality, although it is advised to avoid overwhelming the model with excessive
    input data to prevent confusion.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GPT-3**: GPT-3 is a deep learning model for NLP that was developed by OpenAI.
    It is one of the largest and most powerful models for text generation, with 175
    billion parameters and 45 terabytes of training data. GPT-3 can generate coherent
    and diverse text for almost any topic, genre, or style. It is continuously improved
    by OpenAI, and although the successor GPT-4 may have more capability in terms
    of context size and logic, it is a faster model and still very useful for small
    tasks. It can now process 16 K tokens, but I found that this strength is more
    useful on outputs and not input data. This means the model gets confused quickly
    when too much information is provided at once.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GPT-4**: This is a successor to GPT-3, which is far more powerful for text
    generation. It has 170 trillion parameters, almost 1,000 times more than GPT-3\.
    This model receives all plugins and a Bing browser functionality, which allows
    it to research information on its own. OpenAI is being very secretive about some
    details, as it is yet unknown how it works in detail. Some resources and papers
    suggest that it works recursively and learns with each input it gets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Auto-GPT**: Auto-GPT is a tool that automates the process of text generation
    using OpenAI’s Chat Completion API, mainly with GPT-4\. It allows you to specify
    your input text and parameters that control the output text, such as length, tone,
    format, and keywords. Auto-GPT then sends your input text and parameters to the
    GPT-3 model via the OpenAI API and receives the generated text as a response.
    Auto-GPT also supplies features to help you edit and improve the generated text,
    such as suggestions, feedback, and rewriting:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Plugins**: Extensions that can be loaded into Auto-GPT to add more functionality.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Headless browser**: A web browser without a graphical user interface, used
    for automated tasks.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Workspace**: The directory where Auto-GPT saves files and data.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**API key**: An API key is a unique identifier used to authenticate a user,
    developer, or calling program during an API request. This key helps in tracking
    and controlling how the API is being used, to prevent abuse and ensure security.
    Essentially, it acts as a password that grants access to specific services or
    data, facilitating seamless and secure communication between different software
    components. It is paramount that API keys are kept confidential to prevent unauthorized
    access and potential misuse.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: First run of Auto-GPT on your machine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To run Auto-GPT, you need to use one of the commands, depending on your OS.
    Use `run.sh` for Linux or macOS, and `run.bat` for Windows. Alternatively, you
    can just run the following on your console. Navigate into the Auto-GPT folder
    (not the one inside – I know the folder structure can be misleading sometimes),
    and execute the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: You may also execute the “autogpt.bat” or “autogpt.sh” script inside the “autogpts/autogpt”
    folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are not sure whether your default Python is Python 3.10, or if the preceding
    command returns an error, you can check that with the `python –V` command. Should
    you get anything but Python 3.10, you can run this instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: For any OS, you can also use `docker-compose` if you have Docker installed.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also pass some arguments to customize your Auto-GPT experience, such
    as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`–gpt3only` to use GPT-3.5 instead of GPT-4'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`–speak` to enable text-to-speech output'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`–continuous` to run Auto-GPT without user authorization (not recommended)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`–debug` to print out debug logs and more'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use `–help` to see the full list of arguments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can also change the Auto-GPT settings in your `.env` file, such as `SMART_LLM_MODEL`
    to choose the language model, `DISABLED_COMMAND_CATEGORIES` to disable command
    groups such as `auto`, and more. You can find the template and explanation of
    each setting in your `.``env.template` file.
  prefs: []
  type: TYPE_NORMAL
- en: When you first start Auto-GPT, you’ll be prompted to provide a name, AI role,
    and goals. These fields are automated by default, meaning you can issue commands
    directly.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, to research Wladastic, the author of *Unlocking the Power of Auto-GPT
    and its Plugins*, and write the results into a text file, you could issue the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Auto-GPT will then try to generate the `ai_settings.yaml` file; if it fails,
    you will be asked to supply the name of the instance, five main goals of `ai_settings`,
    and the role that influences the behavior of the instance.
  prefs: []
  type: TYPE_NORMAL
- en: Make sure to be extremely specific and detailed in your prompts. When using
    Auto-GPT, I tend to edit the `ai_settings.yaml` file manually and it works very
    well with longer instructions as well as more than 5 goals (this is just a default
    thing, as it was developed when only GPT-3.5 was available, which has a much lower
    token limit)
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to research ChatGPT prompting guides to learn how to make Auto-GPT
    as efficient as possible. Unclear and “too short” prompts may result in Auto-GPT
    hallucinating or just doing very wrong stuff such as “research for my homework,”
    which may result in various steps such as asking the user (you) about what exactly
    you want, and this will all generate costs on your OpenAI account.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this comprehensive chapter, we delved into the installation and setup of
    Auto-GPT across various OSs, including Windows, macOS, and Linux, equipping you
    with the essential knowledge to get started. We began by outlining the system
    requirements for each platform and provided detailed instructions for installing
    Python 3.10, which is crucial for running Auto-GPT. Our guide included different
    methods to obtain Auto-GPT, such as cloning the repository via Git or downloading
    it from GitHub as a ZIP file.
  prefs: []
  type: TYPE_NORMAL
- en: Once you had Auto-GPT on your system, we led you through its installation using
    Docker (recommended), Git, or without either. We also explained the process of
    configuring your `.env` file with your unique OpenAI API key and settings for
    GPT models in the `FAST_LLM_MODEL` and `SMART_LLM_MODEL` attributes.
  prefs: []
  type: TYPE_NORMAL
- en: After successfully setting up Auto-GPT, we introduced the fundamental concepts
    of text generation models such as GPT-3/GPT-4 from OpenAI, discussing neural networks,
    deep learning models for NLP, and the text generation tasks these models perform.
  prefs: []
  type: TYPE_NORMAL
- en: The chapter further explored additional Auto-GPT features, including plugins
    that enhance its functionality, headless browsers for automated tasks, workspaces
    for file management, and API keys for secure access to OpenAI’s services.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we demonstrated running your first AI-generated task with Auto-GPT,
    highlighting its ease of use and power as a tool. We concluded the chapter by
    preparing you for our next sections, which will dive deeper into advanced Auto-GPT
    features, such as customization for specific needs and working with various plugins
    to broaden its capabilities. By mastering these aspects and effectively harnessing
    the power of AI-generated text, you’ll be well equipped for a range of tasks,
    from automating content creation to generating engaging narratives based on prompts.
    Stay tuned as we continue exploring the full potential of Auto-GPT in our upcoming
    chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Building upon the foundational knowledge acquired about installing and configuring
    Auto-GPT, as well as understanding text generation models, the next chapter we
    will explore is titled *Mastering Prompt Generation and Understanding How Auto-GPT
    Generates Prompts*. This chapter promises to deepen your understanding of prompt
    generation, a crucial skill for maximizing the potential of Auto-GPT. It will
    demystify the mechanics behind Auto-GPT’s prompt generation and offer guidance
    on crafting effective prompts to enhance your interactions with this advanced
    language model.
  prefs: []
  type: TYPE_NORMAL
