["```py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\nclass Generator(nn.Module):\n    def __init__(self, latent_dim, cube_len, bias=False):\n        super(Generator, self).__init__()\n        self.latent_dim = latent_dim\n        self.cube_len = cube_len\n\n        self.model = nn.Sequential(\n            *self._create_layer(self.latent_dim, self.cube_len*8, 4, stride=2, padding=1, bias=bias, transposed=True),\n            *self._create_layer(self.cube_len*8, self.cube_len*4, 4, stride=2, padding=1, bias=bias, transposed=True),\n            *self._create_layer(self.cube_len*4, self.cube_len*2, 4, stride=2, padding=1, bias=bias, transposed=True),\n            *self._create_layer(self.cube_len*2, self.cube_len, 4, stride=2, padding=1, bias=bias, transposed=True),\n            *self._create_layer(self.cube_len, 1, 4, stride=2, padding=1, bias=bias, transposed=True, last_layer=True)\n        )\n\n    def _create_layer(self, size_in, size_out, kernel_size=4, stride=2, padding=1, bias=False, transposed=True, last_layer=False):\n        layers = []\n        if transposed:\n            layers.append(nn.ConvTranspose3d(size_in, size_out, kernel_size, stride=stride, padding=padding, bias=bias))\n        else:\n            layers.append(nn.Conv3d(size_in, size_out, kernel_size, stride=stride, padding=padding, bias=bias))\n        if last_layer:\n            layers.append(nn.Sigmoid())\n        else:\n            layers.append(nn.BatchNorm3d(size_out))\n            layers.append(nn.ReLU(inplace=True))\n        return layers\n\n    def forward(self, x):\n        x = x.view(-1, self.latent_dim, 1, 1, 1)\n        return self.model(x)\n```", "```py\nclass Discriminator(nn.Module):\n    def __init__(self, cube_len, bias=False):\n        super(Discriminator, self).__init__()\n        self.cube_len = cube_len\n\n        self.model = nn.Sequential(\n            *self._create_layer(1, self.cube_len, 4, stride=2, padding=1, bias=bias, transposed=False),\n            *self._create_layer(self.cube_len, self.cube_len*2, 4, stride=2, padding=1, bias=bias, transposed=False),\n            *self._create_layer(self.cube_len*2, self.cube_len*4, 4, stride=2, padding=1, bias=bias, transposed=False),\n            *self._create_layer(self.cube_len*4, self.cube_len*8, 4, stride=2, padding=1, bias=bias, transposed=False),\n            *self._create_layer(self.cube_len*8, 1, 4, stride=2, padding=1, bias=bias, transposed=False, last_layer=True)\n        )\n\n    def _create_layer(self, size_in, size_out, kernel_size=4, stride=2, padding=1, bias=False, transposed=False, last_layer=False):\n        layers = []\n        if transposed:\n            layers.append(nn.ConvTranspose3d(size_in, size_out, kernel_size, stride=stride, padding=padding, bias=bias))\n        else:\n            layers.append(nn.Conv3d(size_in, size_out, kernel_size, stride=stride, padding=padding, bias=bias))\n        if last_layer:\n            layers.append(nn.Sigmoid())\n        else:\n            layers.append(nn.BatchNorm3d(size_out))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n        return layers\n\n    def forward(self, x):\n        x = x.view(-1, 1, self.cube_len, self.cube_len, self.cube_len)\n        return self.model(x)\n```", "```py\nimport os\nimport time\nfrom datetime import datetime\nimport torch\nfrom torch.optim.lr_scheduler import MultiStepLR\nimport utils\nfrom model_3dgan import Generator as G\nfrom model_3dgan import Discriminator as D\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pickle\n\nclass Model(object):\n    def __init__(self, name, device, data_loader, latent_dim, cube_len):\n        self.name = name\n        self.device = device\n        self.data_loader = data_loader\n        self.latent_dim = latent_dim\n        self.cube_len = cube_len\n        assert self.name == '3dgan'\n        self.netG = G(self.latent_dim, self.cube_len)\n        self.netG.to(self.device)\n        self.netD = D(self.cube_len)\n        self.netD.to(self.device)\n        self.optim_G = None\n        self.optim_D = None\n        self.scheduler_D = None\n        self.criterion = torch.nn.BCELoss()\n\n    def create_optim(self, g_lr, d_lr, alpha=0.5, beta=0.5):\n        self.optim_G = torch.optim.Adam(self.netG.parameters(),\n                                        lr=g_lr,\n                                        betas=(alpha, beta))\n        self.optim_D = torch.optim.Adam(self.netD.parameters(),\n                                          lr=d_lr,\n                                          betas=(alpha, beta))\n        self.scheduler_D = MultiStepLR(self.optim_D, milestones=[500,  \n         1000])\n\n    def train(self, epochs, d_loss_thresh, log_interval=100, \n       export_interval=10, out_dir='', verbose=True):\n        self.netG.train()\n        self.netD.train()\n        total_time = time.time()\n        for epoch in range(epochs):\n            batch_time = time.time()\n            for batch_idx, data in enumerate(self.data_loader):\n                data = data.to(self.device)\n\n                batch_size = data.shape[0]\n                real_label = torch.Tensor(batch_size).uniform_(0.7, \n                  1.2).to(self.device)\n                fake_label = torch.Tensor(batch_size).uniform_(0, \n                  0.3).to(self.device)\n\n                # Train D\n                d_real = self.netD(data)\n                d_real = d_real.squeeze()\n                d_real_loss = self.criterion(d_real, real_label)\n\n                latent = torch.Tensor(batch_size, \n                  self.latent_dim).normal_(0, 0.33).to(self.device)\n                fake = self.netG(latent)\n                d_fake = self.netD(fake.detach())\n                d_fake = d_fake.squeeze()\n                d_fake_loss = self.criterion(d_fake, fake_label)\n\n                d_loss = d_real_loss + d_fake_loss\n\n                d_real_acc = torch.ge(d_real.squeeze(), 0.5).float()\n                d_fake_acc = torch.le(d_fake.squeeze(), 0.5).float()\n                d_acc = torch.mean(torch.cat((d_real_acc, d_fake_acc),0))\n\n                if d_acc <= d_loss_thresh:\n                    self.netD.zero_grad()\n                    d_loss.backward()\n                    self.optim_D.step()\n\n                # Train G\n                latent = torch.Tensor(batch_size, \n                  self.latent_dim).normal_(0, 0.33).to(self.device)\n                fake = self.netG(latent)\n                d_fake = self.netD(fake)\n                d_fake = d_fake.squeeze()\n                g_loss = self.criterion(d_fake, real_label)\n\n                self.netD.zero_grad()\n                self.netG.zero_grad()\n                g_loss.backward()\n                self.optim_G.step()\n\n            if epoch % export_interval == 0:\n                samples = fake.cpu().data[:8].squeeze().numpy()\n                utils.save_voxels(samples, out_dir, epoch)\n            self.scheduler_D.step()\n```", "```py\ndef save_voxels(voxels, path, idx):\n    from mpl_toolkits.mplot3d import Axes3D\n    voxels = voxels[:8].__ge__(0.5)\n    fig = plt.figure(figsize=(32, 16))\n    gs = gridspec.GridSpec(2, 4)\n    gs.update(wspace=0.05, hspace=0.05)\n\n    for i, sample in enumerate(voxels):\n        x, y, z = sample.nonzero()\n        ax = fig.add_subplot(gs[i], projection='3d')\n        ax.scatter(x, y, z, zdir='z', c='red')\n        ax.set_xticklabels([])\n        ax.set_yticklabels([])\n    plt.savefig(path + '/{}.png'.format(str(idx)), bbox_inches='tight')\n    plt.close()\n\n    with open(path + '/{}.pkl'.format(str(idx)), \"wb\") as f:\n        pickle.dump(voxels, f, protocol=pickle.HIGHEST_PROTOCOL)\n```", "```py\nimport os\nimport numpy as np\nimport scipy.ndimage as nd\nimport scipy.io as io\nimport torch\nfrom torch.utils.data import Dataset\n\ndef getVoxelFromMat(path, cube_len=64):\n    voxels = io.loadmat(path)['instance']\n    voxels = np.pad(voxels, (1, 1), 'constant', constant_values=(0, 0))\n    if cube_len != 32 and cube_len == 64:\n        voxels = nd.zoom(voxels, (2, 2, 2), mode='constant', order=0)\n    return voxels\n\nclass ShapeNetDataset(Dataset):\n    def __init__(self, root, cube_len):\n        self.root = root\n        self.listdir = os.listdir(self.root)\n        self.cube_len = cube_len\n\n    def __getitem__(self, index):\n        with open(os.path.join(self.root, self.listdir[index]), \"rb\") as f:\n            volume = np.asarray(getVoxelFromMat(f, self.cube_len), dtype=np.float32)\n        return torch.FloatTensor(volume)\n\n    def __len__(self):\n        return len(self.listdir)\n```", "```py\nimport argparse\nimport os\nimport sys\nimport numpy as np\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.utils.data as DataLoader\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport utils\nfrom build_gan import Model\nfrom datasets import ShapeNetDataset\n\nFLAGS = None    \n\ndef main():\n    device = torch.device(\"cuda:0\" if FLAGS.cuda else \"cpu\")\n    print('Loading data...\\n')\n    dataset = ShapeNetDataset(FLAGS.data_dir, FLAGS.cube_len)\n    dataloader = torch.utils.data.DataLoader(dataset,\n                                             FLAGS.batch_size,\n                                             shuffle=True,\n                                             num_workers=1,\n                                             pin_memory=True)\n\n    print('Creating model...\\n')\n    model = Model(FLAGS.model, device, dataloader, FLAGS.latent_dim, FLAGS.cube_len)\n    model.create_optim(FLAGS.g_lr, FLAGS.d_lr)\n\n    # Train\n    model.train(FLAGS.epochs, FLAGS.d_loss_thresh, FLAGS.log_interval,\n                FLAGS.export_interval, FLAGS.out_dir, True)\n```", "```py\n\nif __name__ == '__main__':\n    from utils import boolean_string\n    parser = argparse.ArgumentParser(description='Hands-On GANs - Chapter 11')\n    parser.add_argument('--model', type=str, default='3dGan',\n                        help='enter `3dGan`.')\n    parser.add_argument('--cube_len', type=int, default='32',\n                        help='one of `cgan` and `infogan`.')\n    parser.add_argument('--cuda', type=boolean_string,\n                        default=True, help='enable CUDA.')\n    parser.add_argument('--train', type=boolean_string,\n                        default=True, help='train mode or eval mode.')\n    parser.add_argument('--data_dir', type=str,\n                        default='~/data', help='Directory for dataset.')\n    parser.add_argument('--out_dir', type=str,\n                        default='output', help='Directory for output.')\n    parser.add_argument('--epochs', type=int, default=200,\n                        help='number of epochs')\n    parser.add_argument('--batch_size', type=int,\n                        default=128, help='size of batches')\n    parser.add_argument('--g_lr', type=float, default=0.0002,\n                        help='G learning rate')\n    parser.add_argument('--d_lr', type=float, default=0.0002,\n                        help='D learning rate')\n    parser.add_argument('--d_loss_thresh', type=float, default=0.7,\n                        help='D loss threshold')\n    parser.add_argument('--latent_dim', type=int,\n                        default=100, help='latent space dimension')\n    parser.add_argument('--export_interval', type=int,\n                        default=10, help='export interval')\n    parser.add_argument('--classes', type=int, default=10,\n                        help='number of classes')\n    parser.add_argument('--img_size', type=int,\n                        default=64, help='size of images')\n    parser.add_argument('--channels', type=int, default=1,\n                        help='number of image channels')\n    parser.add_argument('--log_interval', type=int, default=100,\n                        help='interval between logging and image sampling')\n    parser.add_argument('--seed', type=int, default=1, help='random seed')\n\n    FLAGS = parser.parse_args()\n    FLAGS.cuda = FLAGS.cuda and torch.cuda.is_available()\n\n    if FLAGS.seed is not None:\n        torch.manual_seed(FLAGS.seed)\n        if FLAGS.cuda:\n            torch.cuda.manual_seed(FLAGS.seed)\n        np.random.seed(FLAGS.seed)\n\n    cudnn.benchmark = True\n\n    if FLAGS.train:\n        utils.clear_folder(FLAGS.out_dir)\n\n    log_file = os.path.join(FLAGS.out_dir, 'log.txt')\n    print(\"Logging to {}\\n\".format(log_file))\n    sys.stdout = utils.StdOut(log_file)\n\n    print(\"PyTorch version: {}\".format(torch.__version__))\n    print(\"CUDA version: {}\\n\".format(torch.version.cuda))\n\n    print(\" \" * 9 + \"Args\" + \" \" * 9 + \"| \" + \"Type\" +\n          \" | \" + \"Value\")\n    print(\"-\" * 50)\n    for arg in vars(FLAGS):\n        arg_str = str(arg)\n        var_str = str(getattr(FLAGS, arg))\n        type_str = str(type(getattr(FLAGS, arg)).__name__)\n        print(\" \" + arg_str + \" \" * (20-len(arg_str)) + \"|\" +\n              \" \" + type_str + \" \" * (10-len(type_str)) + \"|\" +\n              \" \" + var_str)\n    main()\n```", "```py\npython main.py --model 3dgan --train True --epochs 1000 --data_dir Data_Directory\n```"]