["```py\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom keras import backend as k\nfrom collections import deque\nimport gym\n```", "```py\nenv = gym.make('CartPole-v1')\n```", "```py\n# Set seed for reproducibility\nseed_val = 456\nnp.random.seed(seed_val)\nenv.seed(seed_val)\nrandom.seed(seed_val)\n```", "```py\nstates = env.observation_space.shape[0]\nprint('Number of states/variables in the cartpole environment', states) \n```", "```py\nactions = env.action_space.n\nprint('Number of responses/classes in the cartpole environment', actions) \n```", "```py\nstate = env.reset() # reset the game\nprint('State of the Cart-Pole after reset', state)\nprint('Shape of state of the Cart-Pole after reset', state.shape)\n```", "```py\naction = 0\nnew_state, reward, done, info = env.step(action) \nprint((new_state, reward, done, info))\n```", "```py\ndef random_actions_game(episodes):\n    for episode in range(episodes):\n        state = env.reset() # reset environment\n        done = False # set done to False\n        score = 0\n        while not done:\n            #env.render() # Display cart pole game on the screen\n            action = random.choice([0,1]) # Choose between 0 or 1\n            new_state, reward, done, info = env.step(action) # perform the action\n            score+=1\n        print('Episode: {} Score: {}'.format(episode+1, score))\n\n# play game\nrandom_actions_game(10) \n```", "```py\n# Discount in Bellman Equation\ngamma = 0.95 \n\n# Epsilon\nepsilon = 1.0\n\n# Minimum Epsilon\nepsilon_min = 0.01\n\n# Decay multiplier for epsilon\nepsilon_decay = 0.99\n\n# Size of deque container\ndeque_len = 20000\n\n# Average score needed over 100 epochs\ntarget_score = 200\n\n# Number of games\nepisodes = 2000\n\n# Data points per episode used to train the agent\nbatch_size = 64\n\n# Optimizer for training the agent\noptimizer = 'adam'\n\n# Loss for training the agent\nloss = 'mse'\n```", "```py\ndef agent(states, actions):\n    \"\"\"Simple Deep Neural Network.\"\"\"\n    model = Sequential()\n    model.add(Dense(16, input_dim=states))\n    model.add(Activation('relu'))\n    model.add(Dense(16))\n    model.add(Activation('relu'))\n    model.add(Dense(16))\n    model.add(Activation('relu'))\n    model.add(Dense(actions))\n    model.add(Activation('linear'))\n    return model\n\n# print summary of the agent\nprint(agent(states, actions).summary())\n```", "```py\ndef agent_action(model, epsilon, state, actions):\n    \"\"\"Define action to be taken.\"\"\"\n    if np.random.rand() <= epsilon:\n        act = random.randrange(actions)\n    else:\n        act = np.argmax(model.predict(state)[0])\n    return act\n```", "```py\ntraining_data = deque(maxlen=deque_len)\n```", "```py\ndef memory(state, new_state, reward, done, action):\n    \"\"\"Function to store data points in the deque container.\"\"\"\n    training_data.append((state, new_state, reward, done, action))\n```", "```py\ndef performance_plot(scores, target_score):\n    \"\"\"Plot the game progress.\"\"\"\n    scores_arr = np.array(scores) # convert list to array\n    scores_arr[np.where(scores_arr > target_score)] = target_score # scores\n    plt.figure(figsize=(20, 5)) # set figure size to 20 by 5\n    plt.title('Plot of Score v/s Episode') # title\n    plt.xlabel('Episodes') # xlabel\n    plt.ylabel('Scores') # ylabel\n    plt.plot(scores_arr)\n    plt.show()\n```", "```py\ndef replay(epsilon, gamma, epsilon_min, epsilon_decay, model, training_data, batch_size=64):\n    \"\"\"Train the agent on a batch of data.\"\"\"\n    idx = random.sample(range(len(training_data)), min(len(training_data), batch_size))\n    train_batch = [training_data[j] for j in idx]\n    for state, new_state, reward, done, action in train_batch:\n        target = reward\n        if not done:\n            target = reward + gamma * np.amax(model.predict(new_state)[0])\n        #print('target', target)\n        target_f = model.predict(state)\n        #print('target_f', target_f)\n        target_f[0][action] = target\n        #print('target_f_r', target_f)\n\n        model.fit(state, target_f, epochs=1, verbose=0)\n    if epsilon > epsilon_min:\n        epsilon *= epsilon_decay\n    return epsilon\n```", "```py\nstate = [[-0.07294358 -0.94589796 0.03188364 1.40490844]]\nnew_state = [[-0.09186154 -1.14140094 0.05998181 1.70738606]]\nreward = 1\ndone = False\naction = 0\n```", "```py\ntarget = reward + gamma * np.amax(model.predict(new_state)[0])\n```", "```py\ndef train(target_score, batch_size, episodes,\n optimizer, loss, epsilon,\n gamma, epsilon_min, epsilon_decay, actions, render=False):\n \"\"\"Training the agent on games.\"\"\"\n print('----Training----')\n k.clear_session()\n\n # define empty list to store the score at the end of each episode\n scores = []\n\n # load the agent\n model = agent(states, actions)\n\n # compile the agent with mean squared error loss\n model.compile(loss=loss, optimizer=optimizer)\n\n for episode in range(1, (episodes+1)):\n # reset environment at the end of each episode\n state = env.reset()\n\n # reshape state to shape 1*4\n state = state.reshape(1, states)\n\n # set done value to False\n done = False\n```", "```py\ndef test(env, model, states, episodes=100, render=False):\n    \"\"\"Test the performance of the DQN agent.\"\"\"\n    scores_test = []\n    for episode in range(1, (episodes+1)):\n        state = env.reset()\n        state = state.reshape(1, states)\n\n        done = False\n        time_step = 0\n\n        while not done:\n            if render:\n                env.render()\n            action = np.argmax(model.predict(state)[0])\n            new_state, reward, done, info = env.step(action)\n            new_state = new_state.reshape(1, states)\n            state = new_state\n            time_step += 1\n        scores_test.append(time_step)\n        if episode % 10 == 0:\n            print('episode {}, score {} '.format(episode, time_step))\n    print('Average score over 100 test games: {}'.format(np.mean(scores_test)))\n\ntest(env, model, states, render=False)\n```", "```py\n\"\"\"This module contains hyperparameters for the DQN model.\"\"\"\n\n# Discount in Bellman Equation\ngamma = 0.95\n# Epsilon\nepsilon = 1.0\n# Minimum Epsilon\nepsilon_min = 0.01\n# Decay multiplier for epsilon\nepsilon_decay = 0.99\n# Size of deque container\ndeque_len = 20000\n# Average score needed over 100 epochs\ntarget_score = 200\n# Number of games\nepisodes = 2000\n# Data points per episode used to train the agent\nbatch_size = 64\n# Optimizer for training the agent\noptimizer = 'adam'\n# Loss for training the agent\nloss = 'mse'\n```", "```py\n\"\"\"This module contains.\"\"\"\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\n\ndef agent(states, actions):\n    \"\"\"Simple Deep Neural Network.\"\"\"\n    model = Sequential()\n    model.add(Dense(16, input_dim=states))\n    model.add(Activation('relu'))\n    model.add(Dense(16))\n    model.add(Activation('relu'))\n    model.add(Dense(16))\n    model.add(Activation('relu'))\n    model.add(Dense(actions))\n    model.add(Activation('linear'))\n    return model\n```", "```py\n\"\"\"This module contains function to test the performance of the DQN model.\"\"\"\nimport numpy as np\n\ndef test(env, model, states, episodes=100, render=False):\n    \"\"\"Test the performance of the DQN agent.\"\"\"\n    scores_test = []\n    for episode in range(1, (episodes+1)):\n        state = env.reset()\n        state = state.reshape(1, states)\n\n        done = False\n        time_step = 0\n\n        while not done:\n            if render:\n                env.render()\n            action = np.argmax(model.predict(state)[0])\n            new_state, reward, done, info = env.step(action)\n            new_state = new_state.reshape(1, states)\n            state = new_state\n            time_step += 1\n        scores_test.append(time_step)\n        if episode % 10 == 0:\n            print('episode {}, score {} '.format(episode, time_step))\n    print('Average score over 100 test games: {}'.format(np.mean(scores_test)))\n```", "```py\n\"\"\"This module is used to train and test the DQN agent.\"\"\"\nimport random\nimport numpy as np\nfrom agent_replay_dqn import agent, agent_action, replay, performance_plot\nfrom hyperparameters_dqn import *\nfrom test_dqn import test\nfrom keras import backend as k\nfrom collections import deque\nimport gym\n\nenv = gym.make('CartPole-v1')\n\n# Set seed for reproducibility\nseed_val = 456\nnp.random.seed(seed_val)\nenv.seed(seed_val)\nrandom.seed(seed_val)\n\nstates = env.observation_space.shape[0]\nactions = env.action_space.n\ntraining_data = deque(maxlen=deque_len)\n\ndef memory(state, new_state, reward, done, action):\n    \"\"\"Function to store data points in the deque container.\"\"\"\n    training_data.append((state, new_state, reward, done, action))\n\ndef train(target_score, batch_size, episodes,\n          optimizer, loss, epsilon,\n          gamma, epsilon_min, epsilon_decay, actions, render=False):\n    \"\"\"Training the agent on games.\"\"\"\n    print('----Training----')\n    k.clear_session()\n```", "```py\nimport numpy as np\nimport gym\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Flatten\nfrom keras.optimizers import Adam\nfrom rl.agents import SARSAAgent\nfrom rl.policy import EpsGreedyQPolicy\n```", "```py\nenv = gym.make('CartPole-v1')\n\n# set seed \nseed_val = 456\nenv.seed(seed_val)\nnp.random.seed(seed_val)\n\nstates = env.observation_space.shape[0]\nactions = env.action_space.n\n```", "```py\ndef agent(states, actions):\n \"\"\"Simple Deep Neural Network.\"\"\"\n model = Sequential()\n model.add(Flatten(input_shape=(1,states)))\n model.add(Dense(16))\n model.add(Activation('relu'))\n model.add(Dense(16))\n model.add(Activation('relu'))\n model.add(Dense(16))\n model.add(Activation('relu'))\n model.add(Dense(actions))\n model.add(Activation('linear'))\n return model\n\nmodel = agent(states, actions)\n```", "```py\n# Define the policy\npolicy = EpsGreedyQPolicy()\n\n# Loading SARSA agent by feeding it the policy and the model\nsarsa = SARSAAgent(model=model, nb_actions=actions, policy=policy)\n\n# compile sarsa with mean squared error loss\nsarsa.compile('adam', metrics=['mse'])\n\n# train the agent for 50000 steps\nsarsa.fit(env, nb_steps=50000, visualize=False, verbose=1)\n```", "```py\n# Evaluate the agent on 100 new episodes\nscores = sarsa.test(env, nb_episodes=100, visualize=False)\n\nprint('Average score over 100 test games: {}'.format(np.mean(scores.history['episode_reward'])))\n```", "```py\n\"\"\"This module implements training and testing of SARSA agent.\"\"\"\nimport gym\nimport numpy as np\nfrom keras.layers import Dense, Activation, Flatten\nfrom keras.models import Sequential\nfrom rl.agents import SARSAAgent\nfrom rl.policy import EpsGreedyQPolicy\n\n# load the environment\nenv = gym.make('CartPole-v1')\n\n# set seed\nseed_val = 456\nenv.seed(seed_val)\nnp.random.seed(seed_val)\n\nstates = env.observation_space.shape[0]\nactions = env.action_space.n\n\ndef agent(states, actions):\n    \"\"\"Agent/Deep Neural Network.\"\"\"\n    model = Sequential()\n    model.add(Flatten(input_shape=(1, states)))\n    model.add(Dense(16))\n    model.add(Activation('relu'))\n    model.add(Dense(16))\n    model.add(Activation('relu'))\n    model.add(Dense(16))\n    model.add(Activation('relu'))\n    model.add(Dense(actions))\n    model.add(Activation('linear'))\n    return model\n\nmodel = agent(states, actions)\n\n# Define the policy\npolicy = EpsGreedyQPolicy()\n# Define SARSA agent by feeding it the policy and the model\nsarsa = SARSAAgent(model=model, nb_actions=actions, nb_steps_warmup=10,\n                   policy=policy)\n# compile sarsa with mean squared error loss\nsarsa.compile('adam', metrics=['mse'])\n# train the agent for 50000 steps\nsarsa.fit(env, nb_steps=50000, visualize=False, verbose=1)\n\n# Evaluate the agent on 100 new episodes.\nscores = sarsa.test(env, nb_episodes=100, visualize=False)\nprint('Average score over 100 test games: {}'\n      .format(np.mean(scores.history['episode_reward'])))\n```"]