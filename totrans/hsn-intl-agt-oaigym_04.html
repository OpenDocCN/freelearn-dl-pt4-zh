<html><head></head><body>
        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Exploring the Gym and its Features</h1>
                
            
            <article>
                
<p class="calibre2">Now that you have a working setup, we will start exploring the various features and options provided by the Gym toolkit. This chapter will walk you through some of the commonly used environments, the tasks they solve, and what it would take for your agent to master a task.</p>
<p class="calibre2">In this chapter, we will explore the following topics:</p>
<ul class="calibre10">
<li class="calibre11"><span>Exploring the various types of Gym environment</span></li>
<li class="calibre11">Understanding the structure of the reinforcement learning loop</li>
<li class="calibre11">Understanding the different observation and action spaces</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Exploring the list of environments and nomenclature</h1>
                
            
            <article>
                
<p class="calibre2">Let's start by picking an environment and understanding the Gym interface. You may already be familiar with the basic function calls to create a Gym environment from the previous chapters, where we used them to test our installations. Here, we will formally go through them.</p>
<p class="calibre2">Let's activate the <kbd class="calibre12">rl_gym_book</kbd> conda environment and open a Python prompt. The first step is to import the Gym Python module using the following line of code:</p>
<pre class="calibre17"><strong class="calibre1">import gym</strong></pre>
<p class="calibre2">We can now use the <kbd class="calibre12">gym.make</kbd> method to create an environment from the available list of environments. You may be asking how to find the list of Gym environments available on your system. We will create a small utility script to generate the list of environments so that you can refer to it later when you need to. Let's create a script named <kbd class="calibre12">list_gym_envs.py</kbd> under the <kbd class="calibre12">~/rl_gym_book/ch4</kbd> directory with the following contents:</p>
<pre class="calibre17">#!/usr/bin/env python<br class="title-page-name"/>from gym import envs<br class="title-page-name"/>env_names = [spec.id for spec in envs.registry.all()]<br class="title-page-name"/>for name in sorted(env_names):<br class="title-page-name"/> print(name)</pre>
<p class="calibre2">This script will print the names of all the environments available through your Gym installation, sorted alphabetically. You can run this script using the following command to see the names of the environments installed and available in your system:</p>
<pre class="calibre17"><strong class="calibre1">(rl_gym_book) praveen@ubntu:~/rl_gym_book/ch4$python list_gym_envs.py</strong></pre>
<p class="calibre2">You will get an output like this. Note that only the first few environment names are shown in it. They may be different, depending on the environments you installed on your system based on what we discussed in <a href="part0056.html#1LCVG0-22c7fc7f93b64d07be225c00ead6ce12" class="calibre9">Chapter 3</a>, <em class="calibre13">Getting Started with OpenAI Gym and Deep Reinforcement Learning</em>:</p>
<div class="cdpaligncenter"><img src="../images/00111.jpeg" class="calibre52"/></div>
<p class="calibre2">From the list of environment names, you may note that there are similar names, with some variations. For example, there are eight different variations for the Alien environment. Let's try to understand the nomenclature before we pick one and start using it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Nomenclature</h1>
                
            
            <article>
                
<p class="calibre2">The presence of the word <em class="calibre13">ram</em> in the environment name means that the observation returned by the environment is the contents of the <strong class="calibre4">Random Access Memory</strong> (<strong class="calibre4">RAM</strong>) of the Atari console on which the game was designed to run.</p>
<p class="calibre2">The presence of the word <em class="calibre13">deterministic</em> in the environment names means that the actions sent to the environment by the agent are performed repeatedly for a <em class="calibre13">deterministic/fixed</em> duration of four frames, and then the resulting state is returned.</p>
<p class="calibre2">The presence of the word <em class="calibre13">NoFrameskip</em> means that the actions sent to the environment by the agent are performed once and the resulting state is returned immediately, without skipping any frames in-between.</p>
<p class="calibre2">By default, if <em class="calibre13">deterministic</em> and <em class="calibre13">NoFrameskip</em> are not included in the environment name, <span class="calibre5">the action sent to the environment is repeatedly performed for a duration of</span> <em class="calibre13">n</em> <span class="calibre5">frames, where</span> <em class="calibre13">n</em> <span class="calibre5">is uniformly sampled from {2,3,4}</span>.</p>
<p class="calibre2">The letter <em class="calibre13">v</em> followed by a number in the environment name represents the version of the environment. This is to make sure that any change to the environment implementation is reflected in its name so that the results obtained by an algorithm/agent in an environment are comparable to the results obtained by another algorithm/agent without any discrepancies.</p>
<p class="calibre2">Let's understand this nomenclature by looking at the Atari Alien environment. The various options available are listed with a description as follows:</p>
<table border="1" class="calibre41">
<tbody class="calibre36">
<tr class="calibre37">
<td class="calibre48"><strong class="calibre1">Version name</strong></td>
<td class="calibre48"><strong class="calibre1">Description</strong></td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12">Alien-ram-v0</kbd></td>
<td class="calibre48">Observation is the RAM contents of the Atari machine with a total size of 128 bytes <span>and the action sent to the environment is repeatedly performed for a duration of</span> <em class="calibre25">n</em> <span>frames, where</span> <em class="calibre25">n</em> <span>is uniformly sampled from {2,3,4}.</span></td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12"><span>Alien-ram-v4</span></kbd></td>
<td class="calibre48"><span>Observation is the RAM contents of the Atari machine with a total size of 128 bytes</span> <span>and the action sent to the environment is repeatedly performed for a duration of</span> <em class="calibre25">n</em> <span>frames, where</span> <em class="calibre25">n</em> <span>is uniformly sampled from {2,3,4}.</span> There's some modification in the environment compared to v0.</td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12"><span>Alien-ramDeterministic-v0</span></kbd></td>
<td class="calibre48">Observation is the RAM contents of the Atari machine with a total size of 128 bytes and the action sent to the environment is repeatedly performed for a duration of four frames.</td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12"><span>Alien-ramDeterministic-v4</span></kbd></td>
<td class="calibre48">Observation is the RAM contents of the Atari machine with a total size of 128 bytes and the action sent to the environment is repeatedly performed for a duration of four frames. <span>There's some</span>Â modification in the environment compared to v0.</td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12"><span>Alien-ramNoFrameskip-v0</span></kbd></td>
<td class="calibre48">Observation is the RAM contents of the Atari machine with a total size of 128 bytes and the action sent to the environment is applied, and the resulting state is returned immediately without skipping any frames.</td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12"><span>Alien-v0</span></kbd></td>
<td class="calibre48">Observation is an RGB image of the screen represented as an array of shape (210, 160, 3) and the action sent to the environment is repeatedly performed for a duration of <em class="calibre25">n</em> frames, where <em class="calibre25">n</em> is uniformly sampled from {2,3,4}.</td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12"><span>Alien-v4</span></kbd></td>
<td class="calibre48"><span>Observation is an RGB image of the screen represented as an array of shape (210, 160, 3) and the action sent to the environment is repeatedly performed for a duration of</span> <em class="calibre25">n</em> <span>frames, where</span> <em class="calibre25">n</em> <span><span>is uniformly sampled from {2,3,4}</span></span>. There's some modification in the environment compared to v0.</td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12"><span>AlienDeterministic-v0</span></kbd></td>
<td class="calibre48"><span>Observation is an RGB image of the screen represented as an array of shape (210, 160, 3) and the action sent to the environment is repeatedly performed for a duration of four</span> <span>frames.</span></td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12"><span>AlienDeterministic-v4</span></kbd></td>
<td class="calibre48"><span>Observation is an RGB image of the screen represented as an array of shape (210, 160, 3) and the action sent to the environment is repeatedly performed for a duration of four</span> <span>frames. There's some modification in the environment compared to v0.</span></td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12"><span>AlienNoFrameskip-v0</span></kbd></td>
<td class="calibre48">Observation is an RGB image of the screen represented as an array of shape (210, 160, 3) and the action sent to the environment is applied, and the resulting state is returned immediately without skipping any frames.</td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12"><span>AlienNoFrameskip-v4</span></kbd></td>
<td class="calibre48"><span>Observation is an RGB image of the screen represented as an array of shape (210, 160, 3) and the action sent to the environment</span> is applied, and the resulting state is returned immediately without skipping any frames. any frames. There's some modificationÂ in the environment compared to v0.</td>
</tr>
</tbody>
</table>
<p class="calibre2">Â </p>
<p class="calibre2">This summary should help you understand the nomenclature of the environments, and it applies to all environments in general. The RAM may be specific to the Atari environments, but you now have an idea of what to expect when you see several related environment names.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Exploring the Gym environments</h1>
                
            
            <article>
                
<p class="calibre2">To make it easy for us to visualize what an environment looks like or what its task is, we will make use of a simple script that can launch any environment and step through it with some randomly sampled actions. You can download the script from this book's code repository under <kbd class="calibre12">ch4</kbd> or create a file named <kbd class="calibre12">run_gym_env.py</kbd> under <kbd class="calibre12">~/rl_gym_book/ch4</kbd> with the following contents:</p>
<pre class="calibre17">#!/usr/bin/env python<br class="title-page-name"/><br class="title-page-name"/>import gym<br class="title-page-name"/>import sys<br class="title-page-name"/><br class="title-page-name"/>def run_gym_env(argv):<br class="title-page-name"/>    env = gym.make(argv[1]) # Name of the environment supplied as 1st argument<br class="title-page-name"/>    env.reset()<br class="title-page-name"/>    for _ in range(int(argv[2])):<br class="title-page-name"/>        env.render()<br class="title-page-name"/>        env.step(env.action_space.sample())<br class="title-page-name"/>    env.close()<br class="title-page-name"/>    <br class="title-page-name"/>if __name__ == "__main__":<br class="title-page-name"/>    run_gym_env(sys.argv)</pre>
<p class="calibre2">This script will take the name of the environment supplied as the first command-line argument and the number of steps to be run. For example, we can run the script like this:</p>
<pre class="calibre17"><strong class="calibre1"><span>(rl_gym_book) praveen@ubntu:~/rl_gym_book/ch4$</span>python run_gym_env.py Alien-ram-v0 2000</strong></pre>
<p class="calibre2">This command will launch the <kbd class="calibre12">Alien-ram-v0</kbd> environment and step through it 2,000 times using random actions sampled from the action space of the environment.</p>
<p class="calibre2">You will see a window pop up with the <kbd class="calibre12">Alien-ram-v0</kbd> environment, like this:</p>
<div class="cdpaligncenter"><img src="../images/00112.jpeg" class="calibre53"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Understanding the Gym interface</h1>
                
            
            <article>
                
<p class="calibre2">Let's continue our Gym exploration by understanding the interface between the Gym environment and the agents that we will develop. To help us with that, let's have another look at the picture we saw in <a href="part0033.html#VF2I0-22c7fc7f93b64d07be225c00ead6ce12" class="calibre9">Chapter 2</a>,Â <em class="calibre13">Reinforcement Learning and Deep Reinforcement Learning</em>, when we were discussing the basics of reinforcement learning:</p>
<div class="cdpaligncenter"><img src="../images/00113.jpeg" class="calibre54"/></div>
<p class="calibre2">Did the picture give you an idea about the interface between the agent and the environment? We will make your understanding secure by going over the description of the interface.</p>
<p class="calibre2">After we <kbd class="calibre12">import gym</kbd> , we <kbd class="calibre12">make</kbd> an environment using the following line of code:</p>
<pre class="calibre17"><strong class="calibre1"> env = gym.make("ENVIRONMENT_NAME") </strong></pre>
<p class="calibre2">Here, <kbd class="calibre12">ENVIRONMENT_NAME</kbd> is the name of the environment we want, chosen from the list of the environments we found installed on our system. From the previous diagram, we can see that the first arrow comes from the environment to the agent, and is named <span class="calibre5">Observation</span>. From <a href="part0033.html#VF2I0-22c7fc7f93b64d07be225c00ead6ce12" class="calibre9">Chapter 2</a>, <em class="calibre13">Reinforcement Learning and Deep Reinforcement Learning</em>, we understand the difference between partially observable environments and fully observable environments, and the difference between state and observation in each case. We get that first observation from the environment by calling <kbd class="calibre12">env.reset()</kbd>. Let's store the observation in a variable named <kbd class="calibre12">obs</kbd> using the following line of code:</p>
<pre class="calibre17"><strong class="calibre1">obs = env.reset()</strong></pre>
<p class="calibre2">Now, the agent has received the observation (the end of the first arrow). It's time for the agent to take an action and send the action to the environment to see what happens. In essence, this is what the algorithms we develop for the agents should figure out! We'll be developing various state-of-the-art algorithms to develop agents in the next and subsequent chapters. Let's continue our journey towards understanding the Gym interface.</p>
<p class="calibre2">Once the action to be taken is decided, we send it to the environment (second arrow in the diagram) using the <kbd class="calibre12">env.step()</kbd> method, which will return four values in this order: <kbd class="calibre12">next_state</kbd>, <kbd class="calibre12">reward</kbd>, <kbd class="calibre12">done</kbd>, and <kbd class="calibre12">info</kbd>:</p>
<ol class="calibre14">
<li value="1" class="calibre11">The <kbd class="calibre12">next_state</kbd> is the resulting state of the environment after the action was taken in the previous state.</li>
</ol>
<div class="packt_infobox">Some environments may internally run one or more steps using the same action before returning the <kbd class="calibre28">next_state</kbd>. We discussed <em class="calibre46">deterministic</em> and <em class="calibre46">NoFrameskip</em> types in the previous section, which are examples of such environments.</div>
<ol start="2" class="calibre14">
<li value="2" class="calibre11">The <kbd class="calibre12">reward</kbd> (third arrow in the diagram) is returned by the environment.</li>
<li value="3" class="calibre11">The <kbd class="calibre12">done</kbd> variable is a Boolean (true or false), which gets a value of true if the episode has terminated/finished (therefore, it is time to reset the environment) and false otherwise. This will be useful for the agent to know when an episode has ended or when the environment is going to be reset to some initial state.</li>
</ol>
<ol start="4" class="calibre14">
<li value="4" class="calibre11">The <kbd class="calibre12">info</kbd> variable returned is an optional variable, which some environments may return with some additional information. Usually, this is not used by the agent to make its decision on which action to take.</li>
</ol>
<p class="calibre2">Here is a consolidated summary of the four values returned by a Gym environment's <kbd class="calibre12">step()</kbd> method, together with their types and a concise description about them:</p>
<table border="1" class="calibre41">
<tbody class="calibre36">
<tr class="calibre37">
<td class="calibre55"><strong class="calibre1">Returned value</strong></td>
<td class="calibre56"><strong class="calibre1">Type</strong></td>
<td class="calibre57"><strong class="calibre1">Description</strong></td>
</tr>
<tr class="calibre37">
<td class="calibre55"><kbd class="calibre12">next_state</kbd> (or observation)</td>
<td class="calibre56"><kbd class="calibre12">Object</kbd></td>
<td class="calibre57">Observation returned by the environment. The object could be the RGB pixel data from the screen/camera, RAM contents, join angles and join velocities of a robot, and so on, depending on the environment.</td>
</tr>
<tr class="calibre37">
<td class="calibre55"><kbd class="calibre12">reward</kbd></td>
<td class="calibre56"><kbd class="calibre12">Float</kbd></td>
<td class="calibre57">Reward for the previous action that was sent to the environment. The range of the <kbd class="calibre12">Float</kbd> value varies with each environment, but irrespective of the environment, a higher reward is always better and the goal of the agent should be to maximize the total reward.</td>
</tr>
<tr class="calibre37">
<td class="calibre55"><kbd class="calibre12">done</kbd></td>
<td class="calibre56"><kbd class="calibre12">Boolean</kbd></td>
<td class="calibre57">Indicates whether the environment is going to be reset in the next step. When the Boolean value is true, it most likely means that the episode has ended (due to loss of life of the agent, timeout, or some other episode termination criteria).</td>
</tr>
<tr class="calibre37">
<td class="calibre55"><kbd class="calibre12">info</kbd></td>
<td class="calibre56"><kbd class="calibre12">Dict</kbd></td>
<td class="calibre57">Some additional information that can optionally be sent out by an environment as a dictionary of arbitrary key-value pairs. The agent we develop should not rely on any of the information in this dictionary for taking action. It may be used (if available) for debugging purposes.</td>
</tr>
</tbody>
</table>
<p class="calibre2">Â </p>
<div class="packt_infobox"><span class="packt_screen">Note that the following code is provided to show the general structure and is not ready to be executed due to the <kbd class="calibre28">ENVIRONMENT_NAME</kbd> and the <kbd class="calibre28">agent.choose_action()</kbd></span> no<span class="packt_screen">t being defined in this snippet.<br class="calibre42"/></span></div>
<p class="calibre2"><span class="calibre5">Let's put all the pieces together and look at them in one place:</span></p>
<pre class="calibre17">import gym<br class="title-page-name"/>env = gym.make("ENVIRONMENT_NAME")<br class="title-page-name"/>obs = env.reset() # The first arrow in the picture<br class="title-page-name"/># Inner loop (roll out)<br class="title-page-name"/>action = agent.choose_action(obs) # The second arrow in the picture<br class="title-page-name"/>next_state, reward, done, info = env.step(action) # The third arrow (and more)<br class="title-page-name"/>obs = next_state<br class="title-page-name"/># Repeat Inner loop (roll out)</pre>
<p class="calibre2"><span class="calibre5">I hope you got a good understanding of one cycle of the interaction between the environment and the agent.</span> This process will repeat until we decide to terminate the cycle after a certain number of episodes or steps have passed. Let's now have a look at a complete example with the inner loop running for <kbd class="calibre12">MAX_STEPS_PER_EPISODE</kbd> and the outer loop running for <kbd class="calibre12">MAX_NUM_EPISODES</kbd> in a <kbd class="calibre12">Qbert-v0</kbd> environment:</p>
<pre class="calibre17">#!/usr/bin/env python<br class="title-page-name"/>import gym<br class="title-page-name"/>env = gym.make("Qbert-v0")<br class="title-page-name"/>MAX_NUM_EPISODES = 10<br class="title-page-name"/>MAX_STEPS_PER_EPISODE = 500<br class="title-page-name"/>for episode in range(MAX_NUM_EPISODES):<br class="title-page-name"/>    obs = env.reset()<br class="title-page-name"/>    for step in range(MAX_STEPS_PER_EPISODE):<br class="title-page-name"/>        env.render()<br class="title-page-name"/>        action = env.action_space.sample()# Sample random action. This will be replaced by our agent's action when we start developing the agent algorithms<br class="title-page-name"/>        next_state, reward, done, info = env.step(action) # Send the action to the environment and receive the next_state, reward and whether done or not<br class="title-page-name"/>        obs = next_state<br class="title-page-name"/><br class="title-page-name"/>        if done is True:<br class="title-page-name"/>            print("\n Episode #{} ended in {} steps.".format(episode, step+1))<br class="title-page-name"/>            break</pre>
<p class="calibre2">When you run this script, you will notice a Qbert screen pop up and Qbert taking random actions and getting a score, as shown here:</p>
<div class="cdpaligncenter"><img src="../images/00114.jpeg" class="calibre58"/></div>
<p class="calibre2">You will also see print statements on the console like the following, depending on when the episode ended. Note that the step numbers you get might be different because the actions are random:</p>
<div class="cdpaligncenter"><img src="../images/00115.jpeg" class="calibre59"/></div>
<p class="calibre2">The boilerplate code is available in this book's code repository under the <kbd class="calibre12">ch4</kbd> folder and is named <kbd class="calibre12">rl_gym_boilerplate_code.py</kbd>. It is indeed boilerplate code, because the overall structure of the program will remain the same. When we build our intelligent agents in subsequent chapters, we will extend this boilerplate code. It is worth taking a while and going through the script line by line to make sure you understand it well.</p>
<p class="calibre2">You may have noticed that in the example code snippets provided in this chapter and in <a href="part0056.html#1LCVG0-22c7fc7f93b64d07be225c00ead6ce12" class="calibre9">Chapter 3</a>, <em class="calibre13">Getting Started with OpenAI Gym and Deep Reinforcement Learning</em>, we used <kbd class="calibre12">env.action_space.sample()</kbd> in place of <kbd class="calibre12">action</kbd> in the previous code. <kbd class="calibre12">env.action_space</kbd> returns the type of the action space (<kbd class="calibre12">Discrete(18)</kbd>, for example, in the case of Alien-v0), and the <kbd class="calibre12">sample()</kbd> method randomly samples a value from that <kbd class="calibre12">action_space</kbd>. That's all it means!</p>
<p class="calibre2">We will now have a closer look at the spaces in the Gym to understand the state space and action spaces of environments.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Spaces in the Gym</h1>
                
            
            <article>
                
<p class="calibre2">We can see that each environment in the Gym is different. Every game environment under the Atari category is also different from the others. For example, in the case of the <kbd class="calibre12">VideoPinball-v0</kbd> environment, the goal is to keep bouncing a ball with two paddles to collect points based on where the ball hits, and to make sure that the ball never falls below the paddles, whereas in the case of <kbd class="calibre12">Alien-v0</kbd><em class="calibre13"><strong class="calibre4">,</strong></em> which is another <span class="calibre5">Atari game</span> <span class="calibre5">environment, the goal is to move through a maze (the rooms in a ship) collecting</span> <em class="calibre13">dots</em><span class="calibre5">, which are equivalent to destroying the eggs of the alien. Aliens can be killed by collecting a pulsar dot and the reward/score increases when that happens. Do you see the variations in the games/environments? How do we know what types of actions are valid in a game?</span></p>
<p class="calibre2">In the VideoPinball environment, naturally, the actions are to move the paddles up or down, whereas in the Alien environment, the actions are to command the player to move left, right, up, or down. Note that there is no "move left" or "move right" action in the case of VideoPinball. When we look at other categories of environment, the variations are even greater. For example, in the case of continuous control environments such as recently release robotics environments with the fetch robot arms, the action is to vary the continuous valued join positions and joint velocities to achieve the task. The same discussion can be had with respect to the values of the observations from the environment. We already saw the different observation object types in the case of Atari (RAM versus RGB images).</p>
<p class="calibre2"/>
<p class="calibre2">This is the motivation for why the <em class="calibre13">spaces</em> (as in mathematics) for the observation and actions are defined for each environment. At the time of the writing of this book, there are six spaces (plus one more called <kbd class="calibre12">prng</kbd> for random seed) that are supported by OpenAI Gym. They are listed in this table, with a brief description of each:</p>
<table border="1" class="calibre41">
<tbody class="calibre36">
<tr class="calibre37">
<td class="calibre48"><strong class="calibre1">Space type</strong></td>
<td class="calibre48"><strong class="calibre1">Description</strong></td>
<td class="calibre48"><strong class="calibre1">Usage Example</strong></td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12">Box</kbd></td>
<td class="calibre48">A box in the <img class="fm-editor-equation69" src="../images/00116.jpeg"/>space (an <em class="calibre25">n</em>-dimensional box) where each coordinate is bounded to lie in the interval defined by [low,high]. Values will be an array of <em class="calibre25">n</em> numbers. The shape defines the <em class="calibre25">n</em> for the space.</td>
<td class="calibre48"><kbd class="calibre12"><span>gym.spaces.Box(low=-100, high=100, shape=(2,))</span></kbd></td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12">Discrete</kbd></td>
<td class="calibre48">Discrete, integer-value space in the interval [0,n-1]. The argument for <kbd class="calibre12">Discrete()</kbd> defines <em class="calibre25">n.</em></td>
<td class="calibre48"><kbd class="calibre12"><span>gym.spaces.Discrete(4)</span></kbd></td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12">Dict</kbd></td>
<td class="calibre48">A dictionary of sample space to create arbitrarily complex space. In the example, a Dict space is created, which consists of two discrete spaces for positions and velocities in three dimensions.</td>
<td class="calibre48"><kbd class="calibre12"><span>gym.spaces.Dict({"position": gym.spaces.Discrete(3), "velocity": gym.spaces.Discrete(3)})</span></kbd></td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12">MultiBinary</kbd></td>
<td class="calibre48"><em class="calibre25">n</em>-dimensional binary space. The argument to <kbd class="calibre12">MultiBinary()</kbd>defines <em class="calibre25">n.</em></td>
<td class="calibre48"><kbd class="calibre12">gym.spaces.MultiBinary(5)</kbd></td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12">MultiDiscrete</kbd></td>
<td class="calibre48">Multi-dimensional discrete space.</td>
<td class="calibre48"><kbd class="calibre12">gym.spaces.MultiDiscrete([-10,10], [0,1])</kbd></td>
</tr>
<tr class="calibre37">
<td class="calibre48"><kbd class="calibre12">Tuple</kbd></td>
<td class="calibre48">A product of simpler spaces.</td>
<td class="calibre48"><kbd class="calibre12">gym.spaces.Tuple((gym.spaces.Discrete(2), spaces.Discrete(2)))</kbd></td>
</tr>
</tbody>
</table>
<p class="calibre2"/>
<p class="calibre2">Â </p>
<p class="calibre2"><kbd class="calibre12">Box</kbd> and <kbd class="calibre12">Discrete</kbd> are the most commonly used action spaces. We now have a basic understanding of the various space types available in the Gym. Let's look at how to find which observation and action spaces an environment uses.</p>
<p class="calibre2">The following script will print the observation and the action space of a given environment, and also optionally print the lower bound and upper bound of the values in the case of aÂ <kbd class="calibre12">Box Space</kbd>. Additionally, it will also print a description/meaning of the possible action in the environment if it is provided by the environment:</p>
<p class="calibre2"/>
<pre class="calibre17">#!/usr/bin/env python<br class="title-page-name"/>import gym<br class="title-page-name"/>from gym.spaces import *<br class="title-page-name"/>import sys<br class="title-page-name"/><br class="title-page-name"/>def print_spaces(space):<br class="title-page-name"/>    print(space)<br class="title-page-name"/>    if isinstance(space, Box): # Print lower and upper bound if it's a Box space<br class="title-page-name"/>        print("\n space.low: ", space.low)<br class="title-page-name"/>        print("\n space.high: ", space.high)<br class="title-page-name"/><br class="title-page-name"/>    <br class="title-page-name"/>if __name__ == "__main__":<br class="title-page-name"/>    env = gym.make(sys.argv[1])<br class="title-page-name"/>    print("Observation Space:")<br class="title-page-name"/>    print_spaces(env.observation_space)<br class="title-page-name"/>    print("Action Space:")<br class="title-page-name"/>    print_spaces(env.action_space)<br class="title-page-name"/>    try:<br class="title-page-name"/>        print("Action description/meaning:",env.unwrapped.get_action_meanings())<br class="title-page-name"/>    except AttributeError:<br class="title-page-name"/>        pass</pre>
<p class="calibre2">Â This script is also available for download in this book's code repository under <kbd class="calibre12">ch4</kbd>, namedÂ <kbd class="calibre12">get_observation_action_space.py</kbd>. You can run the script using the following command, where we supply the name of the environment as the first argument to the script:</p>
<pre class="calibre17"><strong class="calibre1">(rl_gym_book) praveen@ubuntu:~/rl_gym_book/ch4$ python get_observation_action_space.py CartPole-v0</strong></pre>
<p class="calibre2">The script will print an output like this:</p>
<div class="cdpaligncenter"><img src="../images/00117.jpeg" class="calibre60"/></div>
<p class="calibre2">In this example, the script prints that the observation space for theÂ <kbd class="calibre12">CartPole-v0</kbd> environment is <kbd class="calibre12">Box(4,)</kbd><em class="calibre13">,Â </em>which corresponds toÂ <kbd class="calibre12">cart position</kbd>, <kbd class="calibre12">cart velocity</kbd>, <kbd class="calibre12">pole angle</kbd>, and <kbd class="calibre12">pole velocity</kbd> at the tipÂ for the four box values.Â </p>
<p class="calibre2"/>
<p class="calibre2">The action space is printed out to beÂ <kbd class="calibre12">Discrete(2)</kbd>, which corresponds toÂ <em class="calibre13">push cart to leftÂ </em>andÂ <em class="calibre13">push cart to rightÂ </em>for the discrete values <kbd class="calibre12">0</kbd> and <kbd class="calibre12">1</kbd>, respectively.</p>
<p class="calibre2">Let's have a look at another example that has a few more complex spaces. This time, let's run the script with theÂ <kbd class="calibre12">BipedalWalker-v2</kbd><em class="calibre13">Â </em>environment:</p>
<pre class="calibre17"><span>(rl_gym_book) praveen@ubuntu:~/rl_gym_book/ch4$</span><span> </span><strong class="calibre1">python get_observation_action_space.py BipedalWalker-v2</strong></pre>
<p class="calibre2">That produces an output like this:</p>
<div class="cdpaligncenter"><img src="../images/00118.jpeg" class="calibre61"/></div>
<p class="calibre2">A detailed description of the state space of the Bipedal Walker (v2) environment is tabulatedÂ hereÂ for your quick and easy reference:</p>
<table border="1" class="calibre41">
<tbody class="calibre36">
<tr class="calibre37">
<td class="calibre48">
<p class="calibre2"><strong class="calibre4">Index</strong></p>
</td>
<td class="calibre48">
<p class="calibre2"><strong class="calibre4">Name/description</strong></p>
</td>
<td class="calibre48">
<p class="calibre2"><strong class="calibre4">Min</strong></p>
</td>
<td class="calibre48">
<p class="calibre2"><strong class="calibre4">Max</strong></p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre48">
<p class="calibre2">0</p>
</td>
<td class="calibre48">
<p class="calibre2">hull_angle</p>
</td>
<td class="calibre48">
<p class="calibre2">0</p>
</td>
<td class="calibre48">
<p class="calibre2">2*pi</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre48">
<p class="calibre2">1</p>
</td>
<td class="calibre48">
<p class="calibre2">hull_angularVelocity</p>
</td>
<td class="calibre48">
<p class="calibre2">-inf</p>
</td>
<td class="calibre48">
<p class="calibre2">+inf</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre48">
<p class="calibre2">2</p>
</td>
<td class="calibre48">
<p class="calibre2">vel_x</p>
</td>
<td class="calibre48">
<p class="calibre2">-1</p>
</td>
<td class="calibre48">
<p class="calibre2">+1</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre48">
<p class="calibre2">3</p>
</td>
<td class="calibre48">
<p class="calibre2">vel_y</p>
</td>
<td class="calibre48">
<p class="calibre2">-1</p>
</td>
<td class="calibre48">
<p class="calibre2">+1</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre48">
<p class="calibre2">4</p>
</td>
<td class="calibre48">
<p class="calibre2">hip_joint_1_angle</p>
</td>
<td class="calibre48">
<p class="calibre2">-inf</p>
</td>
<td class="calibre48">
<p class="calibre2">+inf</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre48">
<p class="calibre2">5</p>
</td>
<td class="calibre48">
<p class="calibre2">hip_joint_1_speed</p>
</td>
<td class="calibre48">
<p class="calibre2">-inf</p>
</td>
<td class="calibre48">
<p class="calibre2">+inf</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre48">
<p class="calibre2">6</p>
</td>
<td class="calibre48">
<p class="calibre2">knee_joint_1_angle</p>
</td>
<td class="calibre48">
<p class="calibre2">-inf</p>
</td>
<td class="calibre48">
<p class="calibre2">+inf</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre48">
<p class="calibre2">7</p>
</td>
<td class="calibre48">
<p class="calibre2">knee_joint_1_speed</p>
</td>
<td class="calibre48">
<p class="calibre2">-inf</p>
</td>
<td class="calibre48">
<p class="calibre2">+inf</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre48">
<p class="calibre2">8</p>
</td>
<td class="calibre48">
<p class="calibre2">leg_1_ground_contact_flag</p>
</td>
<td class="calibre48">
<p class="calibre2">0</p>
</td>
<td class="calibre48">
<p class="calibre2">1</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre48">
<p class="calibre2">9</p>
</td>
<td class="calibre48">
<p class="calibre2">hip_joint_2_angle</p>
</td>
<td class="calibre48">
<p class="calibre2">-inf</p>
</td>
<td class="calibre48">
<p class="calibre2">+inf</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre48">
<p class="calibre2">10</p>
</td>
<td class="calibre48">
<p class="calibre2">hip_joint_2_speed</p>
</td>
<td class="calibre48">
<p class="calibre2">-inf</p>
</td>
<td class="calibre48">
<p class="calibre2">+inf</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre48">
<p class="calibre2">11</p>
</td>
<td class="calibre48">
<p class="calibre2">knee_joint_2_angle</p>
</td>
<td class="calibre48">
<p class="calibre2">-inf</p>
</td>
<td class="calibre48">
<p class="calibre2">+inf</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre48">
<p class="calibre2">12</p>
</td>
<td class="calibre48">
<p class="calibre2">knee_joint_2_speed</p>
</td>
<td class="calibre48">
<p class="calibre2">-inf</p>
</td>
<td class="calibre48">
<p class="calibre2">+inf</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre48">
<p class="calibre2">13</p>
</td>
<td class="calibre48">
<p class="calibre2">leg_2_ground_contact_flag</p>
</td>
<td class="calibre48">
<p class="calibre2">0</p>
</td>
<td class="calibre48">
<p class="calibre2">1</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre48">
<p class="calibre2">14-23</p>
</td>
<td class="calibre48">
<p class="calibre2">10 lidar readings</p>
</td>
<td class="calibre48">
<p class="calibre2">-inf</p>
</td>
<td class="calibre48">
<p class="calibre2">+inf</p>
</td>
</tr>
</tbody>
</table>
<p class="calibre2">Â </p>
<p class="calibre2">The state space, as you can see, is quite complicated, which is reasonable for a complex bipedal walking robot. It more or less resembles an actual bipedal robot system and sensor configuration that we can find in the real world, such as Boston Dynamics' (part of Alphabet) Atlas bipedal robot, who stole the limelight during the DARPA Robotics Challenge in 2015.</p>
<p class="calibre2">Next, we will look into and understand the action space. A detailed description of the action space for the Bipedal Walker (v2) environment is tabulated here for your quick and easy reference:</p>
<table border="1" class="calibre41">
<tbody class="calibre36">
<tr class="calibre37">
<td class="calibre48">
<p class="calibre2">Index</p>
</td>
<td class="calibre48">
<p class="calibre2">Name/description</p>
</td>
<td class="calibre48">
<p class="calibre2">Min</p>
</td>
<td class="calibre48">
<p class="calibre2">Max</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre48">
<p class="calibre2">0</p>
</td>
<td class="calibre48">
<p class="calibre2">Hip_1 (torque/velocity)</p>
</td>
<td class="calibre48">
<p class="calibre2">-1</p>
</td>
<td class="calibre48">
<p class="calibre2">+1</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre48">
<p class="calibre2">1</p>
</td>
<td class="calibre48">
<p class="calibre2">Knee_1 <span class="calibre5">(torque/velocity)</span></p>
</td>
<td class="calibre48">
<p class="calibre2">-1</p>
</td>
<td class="calibre48">
<p class="calibre2">+1</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre48">
<p class="calibre2">2</p>
</td>
<td class="calibre48">
<p class="calibre2">Hip_2 <span class="calibre5">(torque/velocity)</span></p>
</td>
<td class="calibre48">
<p class="calibre2">-1</p>
</td>
<td class="calibre48">
<p class="calibre2">+1</p>
</td>
</tr>
<tr class="calibre37">
<td class="calibre48">
<p class="calibre2">3</p>
</td>
<td class="calibre48">
<p class="calibre2">Knee_2 <span class="calibre5">(torque/velocity)</span></p>
</td>
<td class="calibre48">
<p class="calibre2">-1</p>
</td>
<td class="calibre48">
<p class="calibre2">+1</p>
</td>
</tr>
</tbody>
</table>
<div class="mce-root2">Action</div>
<p class="calibre2">The torque control is the default control method, which controls the amount of torque applied at the joints.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            <article>
                
<p class="calibre2">In this chapter, we explored the list of Gym environments available on your system, which you installed in the previous chapter, and then understood the naming conventions, or nomenclature, of the environments. We then revisited the agent-environment interaction (the RL loop) diagram and understood how the Gym environment provides the interfaces corresponding to each of the arrows we saw in the image. We then looked at a consolidated summary of the four values returned by the Gym environment's <kbd class="calibre12">step()</kbd> method in a tabulated, easy-to-understand format to <em class="calibre13">reinforce</em>Â your understanding of what they mean!</p>
<p class="calibre2">We also explored in detail the various types of spaces used in the Gym for the observation and action spaces, and we used a script to print out what spaces are used by an environment to understand the Gym environment interfaces better. In our next chapter, we will consolidate all our learning so far to develop our first artificially intelligent agent! Excited?! Flip the page to the next chapter now!</p>


            </article>

            
        </section>
    </body></html>