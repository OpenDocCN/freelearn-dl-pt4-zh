["```py\n    from io import BytesIO\n    from urllib.request import urlopen\n    from zipfile import ZipFile\n    url = 'https://www.hs-coburg.de/fileadmin/hscoburg/WISENT-CIDDS-001.zip'\n    with urlopen(url) as zurl:\n        with ZipFile(BytesIO(zurl.read())) as zfile:\n            zfile.extractall('.')\n    ```", "```py\n    import numpy as np\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    import itertools\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import PowerTransformer\n    from sklearn.metrics import f1_score, classification_report, confusion_matrix\n    from torch_geometric.loader import DataLoader\n    from torch_geometric.data import HeteroData\n    from torch.nn import functional as F\n    from torch.optim import Adam\n    from torch import nn\n    import torch\n    ```", "```py\n    df = pd.read_csv(\"CIDDS-001/traffic/OpenStack/CIDDS-001-internal-week1.csv\")\n    ```", "```py\n    df.head(5)\n    Date first seen Duration Proto Src IP Addr Src Pt Dst IP Addr Dst Pt Packets Bytes Flows Flags Tos class attackType attackID attackDescription\n    2017-03-15 00:01:16.632 0.000 TCP 192.168.100.5 445 192.168.220.16 58844.0 1 108 1 .AP... 0 normal --- --- ---\n    2017-03-15 00:01:16.552 0.000 TCP 192.168.100.5 445 192.168.220.15 48888.0 1 108 1 .AP... 0 normal --- --- ---\n    2017-03-15 00:01:16.551 0.004 TCP 192.168.220.15 48888 192.168.100.5 445.0 2 174 1 .AP... 0 normal --- --- ---\n    2017-03-15 00:01:16.631 0.004 TCP 192.168.220.16 58844 192.168.100.5 445.0 2 174 1 .AP... 0 normal --- --- ---\n    2017-03-15 00:01:16.552 0.000 TCP 192.168.100.5 445 192.168.220.15 48888.0 1 108 1 .AP... 0 normal --- --- ---\n    ```", "```py\n    df = df.drop(columns=['Src Pt', 'Dst Pt', 'Flows', 'Tos', 'class', 'attackID', 'attackDescription'])\n    ```", "```py\n    df['attackType'] = df['attackType'].replace('---', 'benign')\n    df['Date first seen'] = pd.to_datetime(df['Date first seen'])\n    ```", "```py\n    count_labels = df['attackType'].value_counts() / len(df) * 100\n    plt.pie(count_labels[:3], labels=df['attackType'].unique()[:3], autopct='%.0f%%')\n    ```", "```py\n    fig, ((ax1, ax2, ax3)) = plt.subplots(1, 3, figsize=(20,5))\n    df['Duration'].hist(ax=ax1)\n    ax1.set_xlabel(\"Duration\")\n    df['Packets'].hist(ax=ax2)\n    ax2.set_xlabel(\"Number of packets\")\n    pd.to_numeric(df['Bytes'], errors='coerce').hist(ax=ax3)\n    ax3.set_xlabel(\"Number of bytes\")\n    plt.show()\n    ```", "```py\n    df['weekday'] = df['Date first seen'].dt.weekday\n    df = pd.get_dummies(df, columns=['weekday']).rename(columns = {'weekday_0': 'Monday','weekday_1': 'Tuesday','weekday_2': 'Wednesday', 'weekday_3': 'Thursday','weekday_4': 'Friday','weekday_5': 'Saturday','weekday_6': 'Sunday',})\n    ```", "```py\n    df['daytime'] = (df['Date first seen'].dt.second +df['Date first seen'].dt.minute*60 + df['Date first seen'].dt.hour*60*60)/(24*60*60)\n    ```", "```py\n    df = df.reset_index(drop=True)\n    ohe_flags = one_hot_flags(df['Flags'].to_numpy())\n    ohe_flags = df['Flags'].apply(one_hot_flags).to_list()\n    df[['ACK', 'PSH', 'RST', 'SYN', 'FIN']] = pd.DataFrame(ohe_flags, columns=['ACK', 'PSH', 'RST', 'SYN', 'FIN'])\n    ```", "```py\n    temp = pd.DataFrame()\n    temp['SrcIP'] = df['Src IP Addr'].astype(str)\n    temp['SrcIP'][~temp['SrcIP'].str.contains('\\d{1,3}\\.', regex=True)] = '0.0.0.0'\n    temp = temp['SrcIP'].str.split('.', expand=True).rename(columns = {2: 'ipsrc3', 3: 'ipsrc4'}).astype(int)[['ipsrc3', 'ipsrc4']]\n    temp['ipsrc'] = temp['ipsrc3'].apply(lambda x: format(x, \"b\").zfill(8)) + temp['ipsrc4'].apply(lambda x: format(x, \"b\").zfill(8))\n    df = df.join(temp['ipsrc'].str.split('', expand=True)\n                .drop(columns=[0, 17])\n                .rename(columns=dict(enumerate([f'ipsrc_{i}' for i in range(17)])))\n                .astype('int32'))\n    ```", "```py\n    temp = pd.DataFrame()\n    temp['DstIP'] = df['Dst IP Addr'].astype(str)\n    temp['DstIP'][~temp['DstIP'].str.contains('\\d{1,3}\\.', regex=True)] = '0.0.0.0'\n    temp = temp['DstIP'].str.split('.', expand=True).rename(columns = {2: 'ipdst3', 3: 'ipdst4'}).astype(int)[['ipdst3', 'ipdst4']]\n    temp['ipdst'] = temp['ipdst3'].apply(lambda x: format(x, \"b\").zfill(8)) + temp['ipdst4'].apply(lambda x: format(x, \"b\").zfill(8))\n    df = df.join(temp['ipdst'].str.split('', expand=True)\n                .drop(columns=[0, 17])\n                .rename(columns=dict(enumerate([f'ipdst_{i}' for i in range(17)])))\n                .astype('int32'))\n    ```", "```py\n    m_index = df[pd.to_numeric(df['Bytes'], errors='coerce').isnull() == True].index\n    df['Bytes'].loc[m_index] = df['Bytes'].loc[m_index].apply(lambda x: 10e6 * float(x.strip().split()[0]))\n    df['Bytes'] = pd.to_numeric(df['Bytes'], errors='coerce', downcast='integer')\n    ```", "```py\n    df = pd.get_dummies(df, prefix='', prefix_sep='', columns=['Proto', 'attackType'])\n    ```", "```py\n    labels = ['benign', 'bruteForce', 'dos', 'pingScan', 'portScan']\n    df_train, df_test = train_test_split(df, random_state=0, test_size=0.2, stratify=df[labels])\n    df_val, df_test = train_test_split(df_test, random_state=0, test_size=0.5, stratify=df_test[labels])\n    ```", "```py\n    scaler = PowerTransformer()\n    df_train[['Duration', 'Packets', 'Bytes']] = scaler.fit_transform(df_train[['Duration', 'Packets', 'Bytes']])\n    df_val[['Duration', 'Packets', 'Bytes']] = scaler.transform(df_val[['Duration', 'Packets', 'Bytes']])\n    df_test[['Duration', 'Packets', 'Bytes']] = scaler.transform(df_test[['Duration', 'Packets', 'Bytes']])\n    ```", "```py\n    fig, ((ax1, ax2, ax3)) = plt.subplots(1, 3, figsize=(15,5))\n    df_train['Duration'].hist(ax=ax1)\n    ax1.set_xlabel(\"Duration\")\n    df_train['Packets'].hist(ax=ax2)\n    ax2.set_xlabel(\"Number of packets\")\n    df_train['Bytes'].hist(ax=ax3)\n    ax3.set_xlabel(\"Number of bytes\")\n    plt.show()\n    ```", "```py\n    BATCH_SIZE = 16\n    features_host = [f'ipsrc_{i}' for i in range(1, 17)] + [f'ipdst_{i}' for i in range(1, 17)]\n    features_flow = ['daytime', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Duration', 'Packets', 'Bytes', 'ACK', 'PSH', 'RST', 'SYN', 'FIN', 'ICMP ', 'IGMP ', 'TCP  ', 'UDP  ']\n    ```", "```py\n    def create_dataloader(df, subgraph_size=1024):\n    ```", "```py\n        data = []\n        n_subgraphs = len(df) // subgraph_size\n    ```", "```py\n        for i in range(1, n_batches+1):\n            subgraph = df[(i-1)*subgraph_size:i*subgraph_size]\n            src_ip = subgraph['Src IP Addr'].to_numpy()\n            dst_ip = subgraph['Dst IP Addr'].to_numpy()\n    ```", "```py\n    ip_map = {ip:index for index, ip in enumerate(np.unique(np.append(src_ip, dst_ip)))}\n    ```", "```py\n    host_to_flow, flow_to_host = get_connections(ip_map, src_ip, dst_ip)\n    ```", "```py\n            batch = HeteroData()\n            batch['host'].x = torch.Tensor(subgraph[features_host].to_numpy()).float()\n            batch['flow'].x = torch.Tensor(subgraph[features_flow].to_numpy()).float()\n            batch['flow'].y = torch.Tensor(subgraph[labels].to_numpy()).float()\n            batch['host','flow'].edge_index = host_to_flow\n            batch['flow','host'].edge_index = flow_to_host\n            data.append(batch)\n    ```", "```py\n    return DataLoader(data, batch_size=BATCH_SIZE)\n    ```", "```py\n    def get_connections(ip_map, src_ip, dst_ip):\n    ```", "```py\n        src1 = [ip_map[ip] for ip in src_ip]\n        src2 = [ip_map[ip] for ip in dst_ip]\n        src = np.column_stack((src1, src2)).flatten()\n    ```", "```py\n        dst = list(range(len(src_ip)))\n        dst = np.column_stack((dst, dst)).flatten()\n    ```", "```py\n    return torch.Tensor([src, dst]).int(), torch.Tensor([dst, src]).int()\n    ```", "```py\n    train_loader = create_dataloader(df_train)\n    val_loader = create_dataloader(df_val)\n    test_loader = create_dataloader(df_test)\n    ```", "```py\n    import torch_geometric.transforms as T\n    from torch_geometric.nn import Linear, HeteroConv, SAGEConv\n    ```", "```py\n    class HeteroGNN(torch.nn.Module):\n        def __init__(self, dim_h, dim_out, num_layers):\n            super().__init__()\n    ```", "```py\n            self.convs = torch.nn.ModuleList()\n            for _ in range(num_layers):\n                conv = HeteroConv({\n                    ('host', 'to', 'flow'): SAGEConv((-1,-1), dim_h, add_self_loops=False),\n                    ('flow', 'to', 'host'): SAGEConv((-1,-1), dim_h, add_self_loops=False),\n                }, aggr='sum')\n                self.convs.append(conv)\n    ```", "```py\n            self.lin = Linear(dim_h, dim_out)\n    ```", "```py\n        def forward(self, x_dict, edge_index_dict):\n            for conv in self.convs:\n                x_dict = conv(x_dict, edge_index_dict)\n                x_dict = {key: F.leaky_relu(x) for key, x in x_dict.items()}\n            return self.lin(x_dict['flow'])\n    ```", "```py\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = HeteroGNN(dim_h=64, dim_out=5, num_layers=3).to(device)\n    optimizer = Adam(model.parameters(), lr=0.001)\n    ```", "```py\n    @torch.no_grad()\n    def test(loader):\n        model.eval()\n        y_pred = []\n        y_true = []\n        n_subgraphs = 0\n        total_loss = 0\n    ```", "```py\n       for batch in loader:\n            batch.to(device)\n            out = model(batch.x_dict, batch.edge_index_dict)\n            loss = F.cross_entropy(out, batch['flow'].y.float())\n    ```", "```py\n            y_pred.append(out.argmax(dim=1))\n            y_true.append(batch['flow'].y.argmax(dim=1))\n    ```", "```py\n            n_subgraphs += BATCH_SIZE\n            total_loss += float(loss) * BATCH_SIZE\n    ```", "```py\n        y_pred = torch.cat(y_pred).cpu()\n        y_true = torch.cat(y_true).cpu()\n        f1score = f1_score(y_true, y_pred, average='macro')\n    ```", "```py\n        return total_loss/n_subgraphs, f1score, y_pred, y_true\n    ```", "```py\n    model.train()\n    for epoch in range(101):\n        n_subgraphs = 0\n        total_loss = 0\n    ```", "```py\n        for batch in train_loader:\n            optimizer.zero_grad()\n            batch.to(device)\n            out = model(batch.x_dict, batch.edge_index_dict)\n            loss = F.cross_entropy(out, batch['flow'].y.float())\n            loss.backward()\n            optimizer.step()\n            n_subgraphs += BATCH_SIZE\n            total_loss += float(loss) * BATCH_SIZE\n    ```", "```py\n        if epoch % 10 == 0:\n            val_loss, f1score, _, _ = test(val_loader)\n            print(f'Epoch {epoch} | Loss: {total_loss/n_subgraphs:.4f} | Val loss: {val_loss:.4f} | Val F1 score: {f1score:.4f}')\n    ```", "```py\nEpoch 0 | Loss: 0.1006 | Val loss: 0.0072 | Val F1 score: 0.6044\nEpoch 10 | Loss: 0.0020 | Val loss: 0.0021 | Val F1-score: 0.8899\nEpoch 20 | Loss: 0.0015 | Val loss: 0.0015 | Val F1-score: 0.9211\n...\nEpoch 90 | Loss: 0.0004 | Val loss: 0.0008 | Val F1-score: 0.9753\nEpoch 100 | Loss: 0.0004 | Val loss: 0.0009 | Val F1-score: 0.9785\n```", "```py\n    _, _, y_pred, y_true = test(test_loader)\n    print(classification_report(y_true, y_pred, target_names=labels, digits=4))\n                  precision    recall  f1-score   support\n          benign     0.9999    0.9999    0.9999    700791\n      bruteForce     0.9811    0.9630    0.9720       162\n             dos     1.0000    1.0000    1.0000    125164\n        pingScan     0.9413    0.9554    0.9483       336\n        portScan     0.9947    0.9955    0.9951     18347\n        accuracy                         0.9998    844800\n       macro avg     0.9834    0.9827    0.9831    844800\n    weighted avg     0.9998    0.9998    0.9998    844800\n    ```", "```py\ndf_pred = pd.DataFrame([y_pred.numpy(), y_true.numpy()]).T\ndf_pred.columns = ['pred', 'true']\nplt.pie(df_pred['true'][df_pred['pred'] != df_pred['true']].value_counts(), labels=labels, autopct='%.0f%%')\n```"]