- en: Sequence-to-Sequence Models for Building Chatbots
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We're learning a lot and doing some valuable work! In the evolution of our hypothetical
    business use case, this chapter builds directly on [Chapter 4](c6f638a5-96bf-4488-9e14-4fbc9b969a42.xhtml),
    *Building an NLP Pipeline for Building Chatbots*, where we created our **Natural
    Language Processing** (**NLP**) pipeline. The skills we learned so far in computational
    linguistics should give us the confidence to expand past the training examples
    in this book and tackle this next project. We're going to build a more advanced
    chatbot for our hypothetical restaurant chain to automate the process of fielding
    call-in orders.
  prefs: []
  type: TYPE_NORMAL
- en: This requirement would mean that we'd have to combine a number of technologies
    that we've learned so far. But for this project, we'll be interested in learning
    how to make a chatbot that is more contextually aware and robust, so that we could
    integrate it into a larger system in this hypothetical. By demonstrating mastery
    on this training example, we'll have the confidence to execute this in a real
    situation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous chapters, we learned about representational learning methods,
    such as word2vec, and how to use them in combination with a type of deep learning
    algorithm called a **Convolutional Neural Network** (**CNN**). But there are few
    constraints while using CNNs to build language models, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The model will not be able to preserve the state information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The length of sentences needs to be of a fixed size for both input values and
    output values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CNNs are sometimes unable to adequately handle complex sequential contexts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recurrent Neural Networks** (**RNNs**) do better at modeling information
    in sequence'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, to overcome all of these problems, we have an alternative algorithm, which
    is specially designed to handle input data that comes in the form of sequences
    (including sequences of words, or of characters). This class of algorithm is called
    RNN.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Learn about RNN and its various forms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a language model implementation using RNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build our intuition on the **Long Short-Term Memory** (**LSTM**) model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create an LSTM language model implementation and compare it to the RNN model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement an encoder-decoder RNN, based on the LSTM unit, for a simple sequence
    of question-answer tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Define the goal**: Build a more robust chatbot with memory to provide more
    contextually correct responses to questions.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Introducing RNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RNN is a deep learning model architecture specifically designed for sequential
    data. The purpose of this type of model is to extract relevant features of words
    and characters of text by using a small window that traverses the corpus.
  prefs: []
  type: TYPE_NORMAL
- en: RNN applies a non-linear function to each item in the sequence. This is called
    the RNN *ce**ll* or s*tep* and, in our case, the items are words or characters
    in the sequence. The layer's output in an RNN is derived from the output of the
    RNN cell, which is applied to each element in the sequence. With regard to NLP
    and chatbots that use text data as input, the outputs of the model are successive
    characters or words.
  prefs: []
  type: TYPE_NORMAL
- en: Each RNN cell holds an internal memory that summarizes the history of the sequence
    it has seen so far.
  prefs: []
  type: TYPE_NORMAL
- en: 'This diagram helps us to visualize the RNN model architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9712ff5e-a7f3-4dc4-a097-d20b506f745b.png)'
  prefs: []
  type: TYPE_IMG
- en: Vanilla version of RNN model architecture.
  prefs: []
  type: TYPE_NORMAL
- en: At the heart of the purpose of an RNN was the idea to introduce a feedback mechanism
    that enables context modeling through the use of fixed-weight feedback structures.
    What this does is build a connection between the features in the current mapping
    to the previous version. Basically, it employs a strategy of using an earlier
    version of a sequence to instruct a later version.
  prefs: []
  type: TYPE_NORMAL
- en: This is quite clever; however, it's not without its challenges. Exploding and
    vanishing gradients make it extremely frustrating to train these types of modes
    in instances where the problem is of a complex time series nature.
  prefs: []
  type: TYPE_NORMAL
- en: A great reference to dive into that expertly outlines the vanishing and exploding
    gradient problem, and gives a technical explanation of viable solutions, can be
    found in Sepp's work from 1998 ([https://dl.acm.org/citation.cfm?id=355233](https://dl.acm.org/citation.cfm?id=355233)).
  prefs: []
  type: TYPE_NORMAL
- en: 'A second problem that was discovered was that RNNs were picking up only one
    of two temporal structures: either the short-term or long-term structures. But
    what was needed for the best model performance was a model that was able to learn
    from both types of features (short-term and long-term) at the same time. The solution
    came in changing the basic RNN cell for a **Gated Recurrent Unite** (**GRU**)
    or LSTM cell.'
  prefs: []
  type: TYPE_NORMAL
- en: For additional information on the GRU refer to [http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/](http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/) or,
    to learn more on the LSTM, refer to [http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).
  prefs: []
  type: TYPE_NORMAL
- en: We'll explore the LSTM architecture in detail later in this chapter. Let's gain
    some intuition on the value of LSTM that will help us achieve our goal first.
  prefs: []
  type: TYPE_NORMAL
- en: RNN architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will mostly use the LSTM cell, since it has proven better in most NLP tasks.
    The principle benefit of the LSTM in RNN architectures is that it enables model
    training over long sequences, while retaining memory. To solve the gradient problem,
    LSTMs include more gates that effectively control access to the cell state.
  prefs: []
  type: TYPE_NORMAL
- en: We've found that Colah's blog post ([http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/))
    is a great place to go to obtain a good understand the working of LSTMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'These small LSTM units of RNN can be combined in multiple forms to solve various
    kinds of use-cases. RNNs are quite flexible in terms of combining the different
    input and output patterns, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Many to one**: The model takes a complete input sequence to make a single
    prediction. This is used in sentiment models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**One to many**: This model transforms a single input, such as a numerical
    date, to generate a sequence string such as "day", "month", or "year".'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Many to many**: This is a **sequence-to-sequence** (**seq2seq**) model, which takes
    the entire sequence as input into a second sequence form, as Q/A systems do.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This diagram maps out these relationships nicely:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cbaaa3d4-d06a-4f7c-b719-f2a5a35fac9f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this chapter, we will focus on the **many to many** relationship, also known
    as seq2seq architecture, to build a question-answer chatbot. The standard RNN
    approach to solving the seq2seq problem involves three primary components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Encoders**: These transform the input sentences into some abstract encoded
    representation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hidden layer**: Encoded sentence transformation representations are manipulated
    here'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decoders**: These output a decoded target sequence'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s have a look at the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a7479ffd-d841-4e5f-bcce-24455fbaa7c2.png)'
  prefs: []
  type: TYPE_IMG
- en: The illustration of building the encode decoder model which takes input text
    (question) in the encoder, it gets transformed in the intermediate step and gets
    mapped with the decoder which represents the respective text (answer).
  prefs: []
  type: TYPE_NORMAL
- en: Let's build our intuition on RNNs by first implementing basic forms of RNN models.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing basic RNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will implement a language model, using a basic RNN to perform
    sentiment classification. Code files for the model can found at [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter05/1.%20rnn.py](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter05/1.%20rnn.py).
  prefs: []
  type: TYPE_NORMAL
- en: Importing all of the dependencies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This code imports TensorFlow and key dependencies for our RNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Preparing the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The dataset we''ll use in this project is the *Movie Review Data* from Rotten
    Tomatoes ([http://www.cs.cornell.edu/people/pabo/movie-review-data/](http://www.cs.cornell.edu/people/pabo/movie-review-data/)).
    It contains 10,662 example review sentences, with approximately half of them positive
    and half negative. The dataset has a vocabulary of around 20,000 words. We will
    use the `sklearn` wrapper to load the dataset from a raw file and then a `separate_dataset()` helper
    functionto clean the dataset and transform it from its raw form to the separate
    list structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `trainset` is an object that stores all of the text data and the sentiment
    label data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now we will transform the labels into the one-hot encoding.
  prefs: []
  type: TYPE_NORMAL
- en: It's important to understand the dimensions of the one-hot encoding vector.
    Since we have `10662` separate sentences, and two sentiments, `negative` and `positive`,
    our one-hot vector size will be of a size of [*10662, 2*].
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be using a popular `train_test_split()` sklearn wrapper to randomly
    shuffle the data and divide the dataset into two parts: the `training` set and
    the `test` set. Further, with another `build_dataset()` helper function, we will
    create the vocabulary using a word-count-based approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: You can also try to feed any embedding model in here to make the model more
    accurate.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few important things to remember while preparing the dataset for
    the RNN models. We need to add explicitly special tags in the vocabulary to keep
    track of the start of sentences, extra padding, the ends of sentences, and any
    unknown words. Hence, we have reserved the following positions for special tags
    in our vocabulary dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Hyperparameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will define some of the hyperparameters for our model, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Defining a basic RNN cell model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we will create the RNN model, which takes a few input parameters, including
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`size_layer`: The number of units in the RNN cell'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_layers`: The number of hidden layers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`embedded_size`: The size of the embedding'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dict_size`: The vocabulary size'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dimension_output`: The number of classes we need to classify'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`learning_rate`: The learning rate of the optimization algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The architecture of our RNN model consists of the following parts:'
  prefs: []
  type: TYPE_NORMAL
- en: Two placeholders; one to feed sequence data into the model and the second for
    the output
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A variable to store the embedding lookup from the dictionary
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, add the RNN layer with multiple basic RNN cells
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create weight and bias variables
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute `logits`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute loss
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add the Adam Optimizer
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate prediction and accuracy
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This model is similar to the CNN model created in the previous chapter, [Chapter
    4](c6f638a5-96bf-4488-9e14-4fbc9b969a42.xhtml), *Building an NLP Pipeline for
    Building Chatbots*, except for the RNN cell part:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In this model, the data flows from the variables that we created in *Step 1*.
    Then, it moves to the embedding layer defined in *Step 2*, followed by our RNN
    layer, which performs the computation in two hidden layers of RNN cells. Later, `logits`
    are computed by performing a matrix multiplication of the weight, the output from
    the RNN layer, and addition of bias. The last step is that we define the `cost`
    function; we will be using the `softmax_cross_entropy` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is what the complete model looks like after computation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2c8c39ff-aa88-4f69-8940-f677365ff683.png)'
  prefs: []
  type: TYPE_IMG
- en: TensorBoard graph visualization of the RNN architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram represents the structure of the RNN block from the preceding
    screenshot. In this architecture, we have two RNN cells incorporated in hidden
    layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7bc55c85-35ba-45fb-a70a-1b9b1b354159.png)'
  prefs: []
  type: TYPE_IMG
- en: TensorBoard visualization of the RNN block containing 2 hidden layers as defined
    in the code
  prefs: []
  type: TYPE_NORMAL
- en: Training the RNN Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have our model architecture defined, let''s train our model. We
    begin with a TensorFlow graph initialization and execute the training steps as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'While the RNN model is being trained, we can see the logs of each epoch, shown
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/346eb0d5-77eb-4d99-8237-384f178a9ebb.png)'
  prefs: []
  type: TYPE_IMG
- en: Evaluation of the RNN model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at our results. Once the model is trained, we can feed the test
    data that we prepared earlier in this chapter and evaluate the predictions. In
    this case, we will use a few different metrics to evaluate our model: precision,
    recall, and F1-scores.'
  prefs: []
  type: TYPE_NORMAL
- en: To evaluate your model, it is important to choose the right kind of metrics—F1-scores
    are considered more practical compared to the accuracy score.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some key points to help you understand them in simple terms are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy**: The count of correct predictions, divided by the count of total
    examples that have been evaluated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Precision**: High precision means you identified nearly all positives appropriately;
    a low precision score means you often incorrectly predicted a positive when there
    was none.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recall**: High recall means you correctly predicted almost all of the real
    positives present in the data; a low score means you frequently missed positives
    that were present.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**F1-score**: The balanced harmonic mean of recall and precision, giving both
    metrics equal weight. The higher the F-measure, the better.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now we will execute the model by feeding the test data with vocabulary and
    the max length of the text. This will produce the `logits` values which we will
    use to generate the evaluation metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/80fe4448-9a2c-4123-8abc-fbedffae80d0.png)'
  prefs: []
  type: TYPE_IMG
- en: So here, we can see that our average `f1-score` is 66% while using basic RNN
    cells. Let's see if this can be improved on by using other variations of RNN architectures.
  prefs: []
  type: TYPE_NORMAL
- en: LSTM architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The desire to model sequential data more effectively, without the limitations
    of the gradient problem, led researchers to create the LSTM variant of the previous
    RNN model architecture. LSTM achieves better performance because it incorporates
    gates to control the process of memory in the cell. The following diagram shows
    an LSTM cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/19f1c280-e0c8-475d-82e1-c2bd83f4cf7b.png)'
  prefs: []
  type: TYPE_IMG
- en: An LSTM unit (source: http://colah.github.io/posts/2015-08-Understanding-LSTMs)
  prefs: []
  type: TYPE_NORMAL
- en: 'LSTM consist of three primary elements, labeled as **1**, **2**, and **3**
    in the preceding diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The forget gate f(t)**: This gate provides the ability, in the LSTM cell
    architecture, to forget information that is not needed. The sigmoid activation
    accepts the inputs *X(t)* and **h(t-1)**, and effectively decides to remove pieces
    of old output information by passing a *0*. The output of this gate is *f(t)*c(t-1)*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Information from the new input, *X(t),* that is determined to be retained needs
    to be stored in the next step in the cell state. A sigmoid activation is used
    in this process to update or ignore parts of the new information. Next, a vector
    of all possible values for the new input is created by a **tanh** activation function.
    The new cell state is the product of these two values, then this new memory is
    added to the old memory, **c(t-1)**, to give **c(t)**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The last process of the LSTM cell is to determine the final output. A sigmoid
    layer decides which parts of the cell state to output. We then put the cell state
    through a **tanh** activation to generate all of the possible values, and multiply
    it by the output of the sigmoid gate, to produce desired outputs according to
    a non-linear function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These three steps in the LSTM cell process produce a significant result, that
    being that the model can be trained to learn which information to retain in long-term
    memory and which information to forget. Genius!
  prefs: []
  type: TYPE_NORMAL
- en: Implementing an LSTM model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The process that we performed previously, to build the basic RNN model, will
    remain the same, except for the model definition part. So, let's implement this
    and check the performance of the new model.
  prefs: []
  type: TYPE_NORMAL
- en: The code for the model can be viewed at [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter05/2.%20rnn_lstm.py](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter05/2.%20rnn_lstm.py).
  prefs: []
  type: TYPE_NORMAL
- en: Defining our LSTM model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Again, most of the code will remain same—the only the major change will be
    to use `tf.nn.rnn_cell.LSTMCell()`, instead of `tf.nn.rnn_cell.BasicRNNCell()`*.*
    While initializing the LSTM cell, we are using an orthogonal initializer that
    will generate a random orthogonal matrix, which is an effective way of combating
    exploding and vanishing gradients:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'So this is what the architecture of the LSTM model looks like—almost the same,
    compared to the previous basic model, except with the addition of the LSTM cells
    in the **RNN Block**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cd5802d3-ee3e-4755-928e-5d33950de711.png)'
  prefs: []
  type: TYPE_IMG
- en: Training the LSTM model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we''ve established our LSTM intuition and built the model, let''s
    train it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'While the LSTM model is being trained, we can see the logs of each epoch as
    shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9893e58c-b1e5-4d51-8706-2977a0d6a6d7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: You will notice that, even after using the same configurations of the model,
    the training time required for the LSTM-based model will be greater than the RNN
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation of the LSTM model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s again compute the metrics and compare the performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The computed outputs are shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/de3e11c1-3789-421c-9a7b-4f7ac7e52d61.png)'
  prefs: []
  type: TYPE_IMG
- en: So, we can clearly see the boost in the performance of the model! Now, with
    the LSTM, the `f1-score` is bumped to 72% whereas, in our previous basic RNN model,
    it was 66%, which is quite a good improvement of 7%.
  prefs: []
  type: TYPE_NORMAL
- en: Sequence-to-sequence models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll implement a seq2seq model (an encoder-decoder RNN), based
    on the LSTM unit, for a simple sequence-to-sequence question-answer task. This model
    can be trained to map an input sequence (questions) to an output sequence (answers),
    which are not necessarily of the same length as each other.
  prefs: []
  type: TYPE_NORMAL
- en: This type of seq2seq model has shown impressive performance in various other
    tasks such as speech recognition, machine translation, question answering, **Neural
    Machine Translation** (**NMT**), and image caption generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram helps us visualize our seq2seq model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aa874827-debc-49b6-82a0-42aa8ad6390a.png)'
  prefs: []
  type: TYPE_IMG
- en: The illustration of the sequence to sequence (seq2seq) model. Each rectangle
    box is the RNN cell in which blue ones are the encoders and Red been the Decoders.
  prefs: []
  type: TYPE_NORMAL
- en: In the encoder-decoder structure, one RNN (blue) **encodes** the input sequence.
    The encoder emits the context **C**, usually as a simple function of its final
    hidden state. And the second RNN (red) **decoder** calculates the target values
    and generates the output sequence. One essential step is to let the encoder and
    decoder communicate. In the simplest approach, you use the last hidden state of
    the encoder to initialize the decoder. Other approaches let the decoder attend
    to different parts of the encoded input at different timesteps in the decoding
    process.
  prefs: []
  type: TYPE_NORMAL
- en: So, let's get started with data preparation, model building, training, tuning,
    and evaluating our seq2seq model, and see how it performs.
  prefs: []
  type: TYPE_NORMAL
- en: The model file can be found at [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter05/3.%20rnn_lstm_seq2seq.py](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter05/3.%20rnn_lstm_seq2seq.py).
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here, we will build our question-answering system. For the project, we need
    a dataset with question and answer pairs, as shown in the following screenshot.
    Both of the columns contain sequences of words, which is what we need to feed
    into our seq2seq model. Also, note that our sentences can be of dynamic length:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/953818c9-cf3d-48c3-8466-4a296dd7315b.png)'
  prefs: []
  type: TYPE_IMG
- en: The dataset which we prepared with set of questions and answers
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s load them and perform the same data processing using `build_dataset()`*.*
    In the end, we will have a dictionary with words as keys, where the associated
    values are the counts of the word in the respective corpus. Also, we have four
    extras values that we talked about before in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Defining a seq2seq model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will outline the TensorFlow seq2seq model definition. We
    employed an embedding layer to go from integer representation to the vector representation
    of the input. This seq2seq model has four major components: the embedding layer,
    encoders, decoders, and cost/optimizers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the model in graphical form in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c6268a0b-a8f8-4242-aede-907665dbf9b7.png)'
  prefs: []
  type: TYPE_IMG
- en: The TensorBoard visualization of the seq2seq model. This graph shows the connection
    between the encode and the decoder with other relevent components like the optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a formal outline of the TensorFlow seq2seq model definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Hyperparameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have our model definition ready, we will define the hyperparameters.
    We will keep most of the configurations the same as in the previous one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Training the seq2seq model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s train the model. We will need some helper functions for the padding
    of the sentence and to calculate the accuracy of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We initialize our model and iterate the session for the defined number of epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Evaluation of the seq2seq model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, after running the training process for few hours on a GPU, you can see that
    the accuracy has reached a value of `1.0`, and loss has significantly reduced
    to `0.00045`. Let's see how the model performs when we ask some generic questions.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make predictions, we will create a `predict()` function that will take the
    raw text of any size as input and return the response to the question that we
    asked. We did a quick fix to handle the **Out Of Vocab** (**OOV**) words by replacing
    them with the `PAD`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'When the model was trained for the first 50 epochs, we had the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'When the model was trained for 1,136 epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Well! That's impressive, right? Now your model is not just able to understand
    the context, but can also generate answers word by word.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered basic RNN cells, LSTM cells, and the seq2seq model
    in building a language model that can be used for multiple NLP tasks. We implemented
    a chatbot, from scratch, to answer questions by generating a sequence of words
    from the provided dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The experience in this exercise demonstrates the value of LSTM as an often
    necessary component of the RNN. With the LSTM, we were able to see the following
    improvements over past CNN models:'
  prefs: []
  type: TYPE_NORMAL
- en: The LSTM was able to preserve state information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The length of sentences for both inputs and outputs could be variable and different
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The LSTM was able to adequately handle complex context
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Specifically, in this chapter, we did the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Gained an intuition about the RNN and its primary forms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implemented a language model using RNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learned about the LSTM model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implemented the LSTM language model and compared it to the RNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implemented an encoder-decoder RNN based on the LSTM unit for a simple sequence-to-sequence
    question-answer task
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the right training data, it would be possible to use this model to achieve
    the goal of the hypothetical client (the restaurant chain) of building a robust
    chatbot (in combination with other computational linguistic technologies that
    we've explored) that could automate the over-the-phone food ordering process.
  prefs: []
  type: TYPE_NORMAL
- en: Well done!
  prefs: []
  type: TYPE_NORMAL
