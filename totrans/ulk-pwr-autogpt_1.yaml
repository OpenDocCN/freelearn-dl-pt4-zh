- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: Introducing Auto-GPT
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍Auto-GPT
- en: In the *Preface*, I wrote about what Auto-GPT is and where it came from, but
    I was asking myself, “*Why would anyone read* *this book?*”
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在*前言*中，我写了关于Auto-GPT是什么以及它的来源，但我问自己，“*为什么会有人阅读* *这本书？*”
- en: I mean, it is what it is – an automated form of **artificial intelligence**
    (**AI**) that may or may not help you do some tasks or be a fun toy that can be
    very spooky sometimes, right?
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我的意思是，它就是它——一种自动化的**人工智能**（**AI**），它可能帮助你完成一些任务，也可能是一个有时非常神秘的有趣玩具，对吧？
- en: I want you to have a clear understanding of what you can or cannot do with it.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你清楚了解你可以或不可以用它做什么。
- en: Of course, the more creative you get, the more it can do, but sometimes the
    boundaries appear to be more or less random. For example, let’s say you just built
    a house-building robot that for no apparent reason refuses to make the front door
    blue, even though you really want a blue door; it keeps going off-topic or even
    starts explaining what doors are.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你越有创意，它能做的事情就越多，但有时这些边界看起来更多是随机的。例如，假设你刚刚建造了一个房屋建造机器人，它无缘无故拒绝把前门涂成蓝色，尽管你真的想要一个蓝色的门；它总是偏离主题，甚至开始解释什么是门。
- en: Auto-GPT can be very frustrating when it comes to these limitations as they
    come from a combination of OpenAI’s restrictions (which they give in their GPT
    model) and the humans who write and edit Auto-GPT (along with you – the user who
    gives it instructions). What first appears to be a clear instruction can result
    in a very different outcome just by changing one single character.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到这些局限性时，Auto-GPT可能会非常令人沮丧，因为这些限制来源于OpenAI的限制（它们在其GPT模型中给出）以及编写和编辑Auto-GPT的人类（还有你——作为提供指令的用户）。最初看似明确的指令，通过改变一个字符，就可能导致完全不同的结果。
- en: For me, this is what makes it fascinating – you can always expect it to behave
    like a living being that can randomly choose to do otherwise and have its own
    mind.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 对我来说，这就是它迷人的地方——你总是可以期待它像一个活生生的存在，随机选择做出不同的决定，拥有自己的思想。
- en: Note
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Always keep in mind that this is a fast-moving project, so code can and will
    be changed until this book is released. It may also be the case that you bought
    this book much later and Auto-GPT is completely different. Most of the content
    in this book focuses on version 0.4.1, but changes have been made and considered
    regarding version 0.5.0 as well.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 始终记住，这是一个快速发展的项目，因此代码可能会在本书发布之前发生变化。也可能是你很晚才买这本书，而Auto-GPT已经完全不同。本书的大部分内容集中在0.4.1版本，但也考虑到了0.5.0版本的变化。
- en: For example, once I finished the draft of this book, the “Forge” (an idea we
    had at a team meeting) had already been implemented. This was an experiment that
    allowed other developers to build their own Auto-GPT variation.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一旦我完成了这本书的草稿，“Forge”（我们在团队会议上提出的一个想法）已经实现了。这是一个实验，允许其他开发者构建他们自己的Auto-GPT变种。
- en: The Auto-GPT project is a framework that contains Auto-GPT, which we’ll be working
    with in this book, and can start other agents made by other developers. Those
    agents are in the repositories of the programmers who added them, so we won’t
    dive into them here.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Auto-GPT项目是一个框架，包含了我们在本书中将使用的Auto-GPT，并且可以启动其他由开发者创建的代理。这些代理存放在添加它们的程序员的仓库中，因此我们在这里不会深入探讨这些内容。
- en: In this chapter, we aim to introduce you to Auto-GPT, including its history
    and development, as well as LangChain. This chapter will help you understand what
    Auto-GPT is, its significance, and how it has evolved. By the end of this chapter,
    you will have a solid foundation to build upon as we explore more advanced topics
    in the subsequent chapters.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍Auto-GPT，包括它的历史和发展，以及LangChain。本章将帮助你了解Auto-GPT是什么，它的意义，以及它是如何发展的。在本章结束时，你将拥有坚实的基础，作为后续章节探索更高级主题的起点。
- en: 'We will cover the following main topics in this chapter:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主要主题：
- en: Overview of Auto-GPT
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Auto-GPT概述
- en: History and development of Auto-GPT
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Auto-GPT的历史与发展
- en: Introduction to LangChain
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LangChain介绍
- en: Overview of Auto-GPT
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Auto-GPT概述
- en: '**Auto-GPT** is more or less a category of what it already describes:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**Auto-GPT**或多或少是对它已经描述的内容的一个分类：'
- en: “An automated generative pretrained transformer”
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: “一种自动化生成预训练变换器”
- en: This means it automates GPT or ChatGPT. However, in this book, the main focus
    is on Auto-GPT by name. If you haven’t heard of it and just grabbed this book
    out of curiosity, then you’re in the right place!
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着它自动化了GPT或ChatGPT。然而，在本书中，主要关注的是名为Auto-GPT的部分。如果你从未听说过它并且只是出于好奇拿起这本书，那你来对地方了！
- en: '**Auto-GPT** started as an experimental self-prompting AI application that
    is an attempt to create an autonomous system capable of creating “agents” to perform
    various specialized tasks to achieve larger objectives with minimal human input.
    It is based on OpenAI’s GPT and was developed by *Toran Bruce Richards*, who is
    better known by his GitHub handle *Significant Gravitas*.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Now, how does Auto-GPT think? Auto-GPT creates prompts that are fed to **large
    language models** (**LLMs**) and allows AI models to generate original content
    and execute command actions such as browsing, coding, and more. It represents
    a significant step forward in the development of autonomous AI, making it the
    fastest-growing open source project in GitHub’s history (at the time of writing).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Auto-GPT strings together multiple instances of OpenAI’s language model – **GPT**
    – and by doing so creates so-called “agents” that are tasked with simplified tasks.
    These agents work together to accomplish complex goals, such as writing a blog,
    with minimal human intervention.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s talk about how it rose to fame.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: From an experiment to one of the fastest-growing GitHub projects
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Auto-GPT was initially named **Entrepreneur-GPT** and was released on March
    16, 2023\. The initial goal of the project was to give GPT-4 autonomy to see if
    it could thrive in the business world and test its capability to make real-world
    decisions.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: For some time, the development of Auto-GPT remained mostly unnoticed until late
    March 2023\. However, on March 30, 2023, Significant Gravitas tweeted about the
    latest demo of Auto-GPT and posted a demo video, which began to gain traction.
    The real surge in interest came on April 2, 2023, when computer scientist Andrej
    Karpathy quoted one of Significant Gravitas’ tweets, saying that the next frontier
    of prompt engineering was Auto-GPT.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: This tweet went viral, and Auto-GPT became a subject of discussion on social
    media. One of the agents that was created by Auto-GPT, known as **ChaosGPT**,
    became particularly famous when it was humorously assigned the task of “destroying
    humanity,” which contributed to the viral nature of Auto-GPT ([https://decrypt.co/126122/meet-chaos-gpt-ai-tool-destroy-humanity](https://decrypt.co/126122/meet-chaos-gpt-ai-tool-destroy-humanity)).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, we don’t want to destroy humanity; for a reference on what Entrepreneur-GPT
    can do, take a look at the old logs of Entrepreneur-GPT here:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/Significant-Gravitas/Auto-GPT/blob/c6f61db06cde7bd766e521bf7df1dc0c2285ef73/](https://github.com/Significant-Gravitas/Auto-GPT/blob/c6f61db06cde7bd766e521bf7df1dc0c2285ef73/).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: The more creative you are with your prompts and configuration, the more creative
    Auto-GPT will be. This will be covered in [*Chapter 2*](B21128_02.xhtml#_idTextAnchor028)
    when we run our first Auto-GPT instance together.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: LLMs – the core of AI
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although Auto-GPT can be used with other LLMs, it best leverages the power of
    GPT-4, a state-of-the-art language model by OpenAI.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 Auto-GPT 可以与其他 LLM 一起使用，但它最能发挥 GPT-4 的强大功能，GPT-4 是 OpenAI 的一款最先进的语言模型。
- en: It offers a huge advantage for users who don’t own a graphics card that can
    hold models such as GPT-4 equivalents. Although there are many 7-B and 13-B LLMs
    (**B** stands for **billion parameters**) that do compete with ChatGPT, they cannot
    hold enough context in each prompt to be useful or are just not stable enough.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于没有图形卡且无法容纳像 GPT-4 等模型的用户来说，它提供了巨大的优势。尽管有许多 7-B 和 13-B 的 LLMs（**B** 代表 **十亿参数**）与
    ChatGPT 相竞争，但它们无法在每个提示中保持足够的上下文，或者稳定性不足，无法实用。
- en: 'At the time of writing, GPT-4 and GPT-3.5-turbo are both used with Auto-GPT
    by default. Depending on the complexity of the situation, Auto-GPT differs between
    two types of models:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，GPT-4 和 GPT-3.5-turbo 都是 Auto-GPT 的默认设置。根据情况的复杂性，Auto-GPT 在两种模型之间做出选择：
- en: Smart model
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 智能模型
- en: Fast model
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 快速模型
- en: When does Auto-GPT use GPT-3.5-turbo and not GPT-4 all the time?
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在什么情况下，Auto-GPT 会使用 GPT-3.5-turbo 而不是一直使用 GPT-4？
- en: When Auto-GPT goes through its thought process, it uses the *fast model*. For
    example, as Auto-GPT loops through its thoughts, it uses the configured fast model,
    but when it summarizes the content of a website or writes code, it will decide
    to use the smart model.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 当 Auto-GPT 进行思考时，它使用 *快速模型*。例如，当 Auto-GPT 循环思考时，它使用配置好的快速模型，但在总结网站内容或编写代码时，它会选择使用智能模型。
- en: The default for the fast model is GPT-3.5-turbo. Although it isn’t as precise
    as GPT-4, its response time is much better, leading to a more fluent response
    time; GPT-4 can seem stuck if it thinks for too long.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 快速模型的默认设置是 GPT-3.5-turbo。尽管它不如 GPT-4 精确，但它的响应时间更快，从而导致更流畅的响应时间；而 GPT-4 如果思考时间过长，可能会卡住。
- en: OpenAI has also added new functionalities to assist applications such as Auto-GPT.
    One of them is the *ability to call functions*. Before this new feature, Auto-GPT
    had to explain to GPT what a command is and how to formulate it correctly in text.
    This resulted in many errors as GPT sometimes decides to change the syntax of
    the output that’s expected. This was a huge step forward as this feature now reduces
    the complexity of how commands are communicated and executed. This empowers GPT
    to better understand what the context of each task is.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 还为 Auto-GPT 等应用程序添加了新功能。其中之一是 *调用函数的能力*。在这个新功能出现之前，Auto-GPT 必须向 GPT 解释命令是什么以及如何正确地以文本形式表达它。这样就会出现许多错误，因为
    GPT 有时会决定更改预期的输出语法。这是一个巨大的进步，因为这个功能现在减少了命令的沟通和执行的复杂性，使得 GPT 更好地理解每个任务的上下文。
- en: 'So, why don’t we use an LLM directly? Because LLMs are only responsive:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么我们不直接使用 LLM 呢？因为 LLM 只是响应型的：
- en: They cannot fulfill any tasks
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们不能执行任何任务。
- en: Their knowledge is fixed, and they cannot update it themselves
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们的知识是固定的，无法自我更新。
- en: They don’t remember anything; only frameworks that run them can do it
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们不会记住任何事情；只有运行它们的框架可以做到这一点。
- en: How does Auto-GPT make use of LLMs?
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Auto-GPT 如何利用 LLM？
- en: 'Auto-GPT is structured in a way that it takes in an initial prompt from the
    user via the terminal:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Auto-GPT 的结构是通过终端从用户那里获取初始提示：
- en: '![Figure 1.1 – Letting Auto-GPT define its role](img/B21128_01_1.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.1 – 让 Auto-GPT 定义其角色](img/B21128_01_1.jpg)'
- en: Figure 1.1 – Letting Auto-GPT define its role
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.1 – 让 Auto-GPT 定义其角色
- en: 'Here, you can either define a main task or enter `–-manual` to then answer
    questions, as shown here:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，您可以定义一个主要任务，或者输入 `–-manual` 然后回答问题，如下所示：
- en: '![Figure 1.2 – Setting Auto-GPT’s main goals](img/B21128_01_2.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.2 – 设置 Auto-GPT 的主要目标](img/B21128_01_2.jpg)'
- en: Figure 1.2 – Setting Auto-GPT’s main goals
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.2 – 设置 Auto-GPT 的主要目标
- en: 'The main prompt is then saved as an `ai_settings.yaml` file that may look like
    this:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将主要提示保存为 `ai_settings.yaml` 文件，内容可能如下所示：
- en: '[PRE0]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let’s look at some of the AI components in the preceding file:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看前面文件中的一些 AI 组件：
- en: First, we have `ai_goals`, which specifies the main tasks that Auto-GPT must
    undertake. It will use those to decide which individual steps to take. Each iteration
    will decide to follow one of the goals.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们有 `ai_goals`，它指定了 Auto-GPT 必须承担的主要任务。它将使用这些任务来决定采取哪些具体步骤。每次迭代时，Auto-GPT
    会选择一个目标来执行。
- en: Then, we have `ai_name`, which is also taken as a reference and defines parts
    of the behavior or character of the bot. This means that if you call it *AuthorGPT*,
    it will play the role of a GPT-based author, while if you call it *Author*, it
    will try to behave like a person. It is generally hard to tell how it will behave
    because GPT mostly decides what it puts out on its own.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接着，我们有了`ai_name`，它也被作为参考来定义机器人行为或个性的一部分。这意味着，如果你称它为*AuthorGPT*，它将扮演一个基于GPT的作者角色，而如果你称它为*Author*，它将尝试像一个人一样表现。通常很难判断它会如何表现，因为GPT大多数时候是根据自己的决定来生成内容的。
- en: Finally, we have `ai_role`, which can be viewed as a more detailed role description.
    However, in my experience, it only nudges the thoughts slightly. Goals are more
    potent here.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们有了`ai_role`，可以将其视为更详细的角色描述。然而，根据我的经验，它只会稍微推动思维。目标在这里更具影响力。
- en: 'Once this is done, it summarizes what it’s going to do and starts thinking
    correctly:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成，它会总结要做的事情并开始正确思考：
- en: '![Figure 1.3 – Example of Auto-GPT’s thought process](img/B21128_01_3.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图1.3 – Auto-GPT思维过程示例](img/B21128_01_3.jpg)'
- en: Figure 1.3 – Example of Auto-GPT’s thought process
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3 – Auto-GPT思维过程示例
- en: Thinking generally means that it is sending a chat completion request to the
    LLM.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 思考通常意味着它正在向LLM发送聊天完成请求。
- en: This process can be slow – the more tokens that are used, the more processing
    that’s needed. In the *Understanding tokens in LLMs* section, we will take a look
    at what this means.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程可能很慢——使用的令牌越多，需要的处理时间越长。在*理解LLM中的令牌*一节中，我们将看看这意味着什么。
- en: Once Auto-GPT has started “thinking,” it initiates a sequence of AI “conversations.”
    During these conversations, it forms a query, sends it to the LLM, and then processes
    the response. This process repeats until it finds a satisfactory solution or reaches
    the end of its thinking time.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦Auto-GPT开始“思考”，它就会启动一系列的AI“对话”。在这些对话中，它形成一个查询，发送给LLM，然后处理回应。这个过程会不断重复，直到找到一个满意的解决方案或达到思考时间的上限。
- en: 'This entire process produces thoughts. These fall into the following categories:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 整个过程产生了思维，这些思维可以分为以下几类：
- en: Reasoning
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推理
- en: Planning
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规划
- en: Criticism
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批评
- en: Speak
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 说话
- en: Command
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命令
- en: These individual thoughts are then displayed in the terminal and the user is
    asked whether they want to approve the command or not – it’s that simple.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，这些独立的思维会显示在终端上，用户会被询问是否批准该命令——就是这么简单。
- en: Of course, a lot more goes on here, including a prompt being built to create
    that response.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这里发生了更多的事情，包括构建一个提示语来生成回应。
- en: 'Simply put, Auto-GPT passes the name, role, goals, and some background information.
    You can see an example here: [https://github.com/PacktPublishing/Unlocking-the-Power-of-Auto-GPT-and-Its-Plugins/blob/main/Auto-GPT_thoughts_example.md](https://github.com/PacktPublishing/Unlocking-the-Power-of-Auto-GPT-and-Its-Plugins/blob/main/Auto-GPT_thoughts_example.md).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，Auto-GPT传递了名称、角色、目标和一些背景信息。你可以在这里看到一个示例：[https://github.com/PacktPublishing/Unlocking-the-Power-of-Auto-GPT-and-Its-Plugins/blob/main/Auto-GPT_thoughts_example.md](https://github.com/PacktPublishing/Unlocking-the-Power-of-Auto-GPT-and-Its-Plugins/blob/main/Auto-GPT_thoughts_example.md)。
- en: Auto-GPT’s thought process – understanding the one-shot action
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Auto-GPT的思维过程——理解一-shot动作
- en: 'Let’s understand the thought process behind this one-shot action:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们理解一下这个一-shot动作背后的思维过程：
- en: '**Overview of the thought process**: Auto-GPT operates on a one-shot action
    basis. This approach involves processing each data block that’s sent to OpenAI
    as a single chat completion action. The outcome of this process is that a response
    text from GPT is generated that’s crafted based on a specified structure.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**思维过程概述**：Auto-GPT基于一-shot动作进行操作。这个方法涉及将发送给OpenAI的每一个数据块作为一个单独的聊天完成动作进行处理。这个过程的结果是生成一个基于指定结构的GPT响应文本。'
- en: '**Structure and task definition for GPT**: The structure that’s provided to
    GPT encompasses both the task at hand and the format for the response. This dual-component
    structure ensures that GPT’s responses are not only relevant but also adhere to
    the expected conversational format.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GPT的结构和任务定义**：提供给GPT的结构既包括当前任务，也包括响应格式。这个双重组件的结构确保了GPT的回应不仅相关，而且遵循预期的对话格式。'
- en: '**Role assignment in Auto-GPT**: There are two role assignments here:'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Auto-GPT中的角色分配**：这里有两个角色分配：'
- en: '**System role**: The “system” role is crucial in providing context. It functions
    as a vessel for information delivery and maintains the historical thread of the
    conversation with the LLM.'
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**User role**: Toward the end of the process, a “user” role is assigned. This
    role is pivotal in guiding GPT to determine the subsequent command to execute.
    It adheres to a predefined format, ensuring consistency in interactions.'
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ask_user`)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sending messages (`send_message`)
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Browsing (`browse`)
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Executing code (`execute_code`)
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In some instances, Auto-GPT may opt not to select any command. This typically
    occurs in situations of confusion, such as when the provided task is unclear or
    when Auto-GPT completes a task and requires user feedback for further action.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Either way, each response is only one text and just a text that is being autocompleted,
    meaning the LLM only responds once with such a response.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, I have the planner plugin activated; more on plugins
    later:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Each thought property is then displayed to the user and the “speak” output
    is read aloud if text-to-speech is enabled:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The user can now respond in one of the following ways:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '`y`: To accept the execution.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n`: To decline the execution and close Auto-GPT.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`s`: To let Auto-GPT re-evaluate its decisions.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`y -n`: To tell Auto-GPT to just keep going for the number of steps (for example,
    enter `y -5` to allow it to run on its own for 5 steps). Here, `n` is always a
    number.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If the user confirms, the command is executed and the result of that command
    is added as system content:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: At this point, you’re probably wondering what history is in this context and
    why `self`?
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Auto-GPT uses agents and the instance of the agent has its own history that
    acts as a short-term memory. It contains the context of what the previous messages
    and results were.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: The history is trimmed down on every run cycle of the agent to make sure it
    doesn’t reach its token limit.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 'So, why not directly ask the LLM for a solution? There are several reasons
    for this:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: While LLMs are incredibly sophisticated, they cannot solve complex, multi-step
    problems in a single query. Instead, they need to be asked a series of interconnected
    questions that guide them toward a final solution. This is where Auto-GPT shines
    – it can strategically ask these questions and digest the responses.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs can’t maintain their context. They don’t remember previous queries or answers,
    which means they cannot build on past knowledge to answer future questions. Auto-GPT
    compensates for this by maintaining a history of the conversation, allowing it
    to understand the context of previous queries and responses and use that information
    to craft new queries.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While LLMs are powerful tools for generating human-like text, they cannot take
    initiative. They respond to prompts but don’t actively seek out new tasks or knowledge.
    Auto-GPT, on the other hand, is designed to be more proactive. It not only responds
    to the tasks that have been assigned to it but also proactively explores diverse
    ways to accomplish those tasks, making it a true autonomous agent.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Before we delve deeper into how Auto-GPT utilizes LLMs, it’s important to understand
    a key component of how these models process information: **tokens**.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Understanding tokens in LLMs
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tokens are the fundamental building blocks in LLMs such as GPT-3 and GPT-4\.
    They are pieces of knowledge that vary in proximity to each other based on the
    given context. A token can represent a word, a symbol, or even fragments of words.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization in language processing
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When training LLMs, text data is broken down into smaller units, or tokens.
    For instance, the sentence “ChatGPT is great!” would be divided into tokens such
    as `["ChatGPT", "is", "great", "!"]`. The nature of a token can differ significantly
    across languages and coding paradigms:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: In English, a token typically signifies a word or part of a word
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In other languages, a token may represent a syllable or a character
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In programming languages, tokens can include keywords, operators, or variables
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s look at some examples of tokenization:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '`["ChatGPT", "is", "``great", "!"]`.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`print("Hello, World!")` is tokenized as `["print", "(", " ", "Hello", ","
    , " ", "World", "!"", ")"]`.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Balancing detail and computational resources
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tokenization strategies aim to balance detail and computational efficiency.
    More tokens provide greater detail but require more resources for processing.
    This balance is crucial for the model’s ability to understand and generate text
    at a granular level.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Token limits in LLMs
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The token limit signifies the maximum number of tokens that a model such as
    GPT-3 or GPT-4 can handle in a single interaction. This limit is in place due
    to the computational resources needed to process large numbers of tokens.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: The token limit also influences the model’s “attention” capability – its ability
    to prioritize different parts of the input during output generation.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Implications of token limits
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A model with a token limit may not fully process inputs that exceed this limit.
    For example, with a 20-token limit, a 30-token text would need to be broken into
    smaller segments for the model to process them effectively.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: In programming, tokenization aids in understanding code structure and syntax,
    which is vital for tasks such as code generation or interpretation.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: In summary, tokenization is a critical component in **natural language processing**
    (**NLP**), enabling LLMs to interpret and generate text in a meaningful and contextually
    accurate manner.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if you’re using the model to generate Python code and you input
    `["print", "("]` as a token, you’d expect the model to generate tokens that form
    a valid argument to the print function – for example, `[""Hello,` `World!"", ")"]`.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你使用该模型来生成 Python 代码，并将`["print", "("]`作为输入令牌，你会期望模型生成一个有效的参数，以便传递给 `print`
    函数——例如，`[""Hello,` `World!"", ")"]`。
- en: In the following chapters, we will delve deeper into how Auto-GPT works, its
    capabilities, and how you can use it to solve complex problems or automate tasks.
    We will also cover its plugins, which extend its functionality and allow it to
    interact with external systems so that it can order a pizza, for instance.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将深入探讨 Auto-GPT 的工作原理、其能力，以及如何使用它来解决复杂问题或自动化任务。我们还将介绍它的插件，扩展了其功能，并使其能够与外部系统互动，例如它能够点外卖披萨。
- en: In a nutshell, Auto-GPT is like a very smart, very persistent assistant that
    leverages the power of the most advanced AI to accomplish the goals you set for
    it. Whether you’re an AI researcher, a developer, or simply someone who is fascinated
    by the potential of AI, I hope this book will provide you with the knowledge and
    inspiration you need to make the most of Auto-GPT.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，Auto-GPT 就像一个非常智能、非常执着的助手，利用最先进的 AI 技术来完成你为它设定的目标。无论你是 AI 研究员、开发者，还是单纯对
    AI 潜力感到着迷的人，我希望这本书能为你提供你需要的知识和灵感，帮助你最大限度地利用 Auto-GPT。
- en: At the time of writing (June 1, 2023), Auto-GPT can give you feedback not only
    through the terminal. There are a variety of text-to-speech engines that are currently
    built into Auto-GPT. Depending on what you prefer, you can either use the default,
    which is Google’s text-to-speech option, ElevenLabs, macOS’ `say` command (a low-quality
    Siri voice pack), or Silero TTS.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在写作时（2023年6月1日），Auto-GPT 不仅可以通过终端提供反馈。现在有多种文本转语音引擎已内置于 Auto-GPT 中。根据你的喜好，你可以选择默认的
    Google 文本转语音选项、ElevenLabs、macOS 的 `say` 命令（一个低质量的 Siri 语音包），或 Silero TTS。
- en: When it comes to plugins, Auto-GPT becomes even more powerful. Currently, there
    is an official repository for plugins that contains a list of awesome plugins
    such as Planner Plugin, Discord, Telegram, Text Generation for local or different
    LLMs, and more.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在插件方面，Auto-GPT 变得更加强大。目前，官方插件仓库中列出了许多令人赞叹的插件，如 Planner 插件、Discord、Telegram、本地或不同
    LLM 的文本生成插件等。
- en: This modularity makes Auto-GPT the most exciting thing I’ve ever laid my hands
    on.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模块化使得 Auto-GPT 成为我接触过的最令人兴奋的事物。
- en: Launching and advancing Auto-GPT – a story of innovation and community
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推出并推进 Auto-GPT——创新与社区的故事
- en: Auto-GPT’s development began with a bold vision to make the sophisticated technology
    of GPT-4 accessible and user-friendly. This initiative marked the start of an
    ongoing journey, with the project continually evolving through the integration
    of new features and improvements. At its core, Auto-GPT is a collaborative effort,
    continuously shaped by the input of a dedicated community of developers and researchers.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Auto-GPT 的开发始于一个大胆的愿景：使 GPT-4 的复杂技术变得更加易于访问和用户友好。这一举措标志着一个持续不断的旅程的开始，该项目通过不断整合新特性和改进而不断发展。在其核心，Auto-GPT
    是一个合作性的努力，持续受到开发者和研究人员社区贡献的推动。
- en: The genesis of Auto-GPT can be traced back to the discovery of GPT-4’s potential
    for autonomous task completion. This breakthrough was the catalyst for creating
    a platform that could fully utilize GPT-4’s capabilities, offering users extensive
    control and customization options.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Auto-GPT 的起源可以追溯到发现 GPT-4 在自动任务完成方面的潜力。这一突破成为创建一个能够充分利用 GPT-4 能力的平台的催化剂，提供给用户广泛的控制和自定义选项。
- en: 'The project gained initial popularity with an early version known as *Entrepreneur-GPT*,
    a key milestone that showcased Auto-GPT’s capabilities at the time. This phase
    of the project (documented here:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 该项目最初通过一个早期版本 *Entrepreneur-GPT* 获得了关注，这是一个关键的里程碑，展示了当时 Auto-GPT 的能力。项目的这一阶段（详见：
- en: '[https://github.com/PacktPublishing/Unlocking-the-Power-of-Auto-GPT-and-Its-Plugins/blob/main/Entrepreneur-GPT.md](https://github.com/PacktPublishing/Unlocking-the-Power-of-Auto-GPT-and-Its-Plugins/blob/main/Entrepreneur-GPT.md))
    indicates the differences in prompts and functionalities compared to later stages.
    A review of the git history reveals Auto-GPT’s early abilities, including online
    research and using a local database for long-term memory.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: The ascent of Auto-GPT was swift, attracting contributors – including myself
    – early in its development. My experience with this open source project was transformative,
    offering an addictive blend of passion and excitement for innovation. The dedication
    of the contributors brought a sense of pride, especially when you can see your
    work recognized by a wider audience, including popular YouTubers.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: As an open source project, Auto-GPT thrived on voluntary contributions, leading
    to the formation of a team that significantly enhanced its structure. This team
    played a crucial role in managing incoming pull requests and guiding the development
    paths, thereby continually improving Auto-GPT’s core.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Despite its growing popularity, each new release of Auto-GPT brought enhanced
    power and functionality. These releases are stable versions that are meticulously
    tested by the community to ensure they are bug-free and ready for public use.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: A critical component of Auto-GPT’s evolution is its plugins. These play a major
    role in the customization of the platform, allowing users to tailor it to their
    specific needs. Future discussions will delve deeper into these plugins and will
    explore their installation, usage, and impact on enhancing Auto-GPT’s capabilities.
    This exploration is vital as most customization happens through plugins unless
    significant contributions are made directly to the core platform through pull
    requests.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to LangChain
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although *LangChain* itself is not part of Auto-GPT, it is a crucial component
    of Auto-GPT’s development as it focuses on the process using control. This is
    in contrast to Auto-GPT’s emphasis on results without control.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: LangChain is a powerful tool that enables users to build implementations of
    their own Auto-GPT using LLM primitives. It allows for explicit reasoning and
    the potential for Auto-GPT to become an autonomous agent.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: With multiple alternatives of Auto-GPT arising, LangChain has become a part
    of many of them. One such example is AgentGPT.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: LangChain’s unique approach to language processing and control makes it an essential
    part of AgentGPT’s functionality. By combining the strengths of LangChain and
    Auto-GPT, users can create powerful, customized solutions that leverage the full
    potential of GPT.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: The intersection of LangChain and Auto-GPT
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LangChain and Auto-GPT may have different areas of focus, but their shared goal
    of enhancing the capabilities of LLMs creates a natural synergy between them.
    LangChain’s ability to provide a structured, controllable process pairs well with
    Auto-GPT’s focus on autonomous task completion. Together, they provide an integrated
    solution that both controls the method and achieves the goal, striking a balance
    between the process and the result.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: LangChain enables the explicit reasoning potential within Auto-GPT. It provides
    a pathway to transition the model from being a tool for human-directed tasks to
    a self-governing agent capable of making informed, reasoned decisions.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: In addition, LangChain’s control over language processing enhances Auto-GPT’s
    ability to communicate user-friendly information in JSON format, making it an
    even more accessible platform for users. By optimizing language processing and
    control, LangChain significantly improves Auto-GPT’s interaction with users.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: 'You can read more about it: [https://docs.langchain.com/docs/](https://docs.langchain.com/docs/).'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we embarked on the exciting journey of exploring Auto-GPT,
    an innovative AI application that leverages the power of GPT-4 to autonomously
    solve tasks and operate in a browser environment. We delved into the history of
    Auto-GPT, understanding how it evolved from an ambitious experiment to a powerful
    tool that’s transforming the way we interact with AI.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: We also explored the concept of tokens, which play a crucial role in how LLMs
    such as GPT-4 process information. Understanding this fundamental concept will
    help us better comprehend how Auto-GPT interacts with LLMs to generate meaningful
    and contextually relevant responses.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, we touched on the role of LangChain, a tool that complements Auto-GPT
    by providing structured control over language processing. The intersection of
    LangChain and Auto-GPT creates a powerful synergy, enhancing the capabilities
    of Auto-GPT and paving the way for more advanced AI applications.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: As we move forward, we will dive deeper into the workings of Auto-GPT, exploring
    its plugins, installation process, and how to craft effective prompts. We will
    also delve into more advanced topics, such as integrating your own LLM with Auto-GPT,
    setting up Docker, and safely and effectively using continuous mode.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Whether you’re an AI enthusiast, a developer, or simply someone curious about
    the potential of AI, this journey promises to be a fascinating one. So, buckle
    up, and let’s continue to unravel the immense potential of Auto-GPT together!
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
