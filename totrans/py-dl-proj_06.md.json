["```py\nimport keras\nimport numpy as np\nfrom keras import layers\n# Gather data\npath = keras.utils.get_file(\n    'sample.txt',\n    origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\ntext = open(path).read().lower()\nprint('Number of words in corpus:', len(text))\n```", "```py\n# Length of extracted character sequences\nmaxlen = 100\n\n# We sample a new sequence every 5 characters\nstep = 5\n\n# List to hold extracted sequences\nsentences = []\n\n# List to hold the target characters \nnext_chars = []\n\n# Extracting sentences and the next characters.\nfor i in range(0, len(text) - maxlen, step):\n    sentences.append(text[i: i + maxlen])\n    next_chars.append(text[i + maxlen])\nprint('Number of sequences:', len(sentences))\n\n# List of unique characters in the corpus\nchars = sorted(list(set(text)))\n\n# Dictionary mapping unique characters to their index in `chars`\nchar_indices = dict((char, chars.index(char)) for char in chars)\n\n# Converting characters into one-hot encoding.\nx = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\ny = np.zeros((len(sentences), len(chars)), dtype=np.bool)\nfor i, sentence in enumerate(sentences):\n    for t, char in enumerate(sentence):\n        x[i, t, char_indices[char]] = 1\n    y[i, char_indices[next_chars[i]]] = 1\n```", "```py\nmodel = keras.models.Sequential()\nmodel.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\nmodel.add(layers.Dense(len(chars), activation='softmax')) \n\noptimizer = keras.optimizers.RMSprop(lr=0.01)\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer)\n```", "```py\ndef sample(preds, temperature=1.0):\n    preds = np.asarray(preds).astype('float64')\n    preds = np.log(preds) / temperature\n    exp_preds = np.exp(preds)\n    preds = exp_preds / np.sum(exp_preds)\n    probas = np.random.multinomial(1, preds, 1)\n    return np.argmax(probas)\n```", "```py\nfor epoch in range(1, 30):\n    print('epoch', epoch)\n    # Fit the model for 1 epoch \n    model.fit(x, y, batch_size=128, epochs=1, callbacks=callbacks_list)\n\n # Select a text seed randomly\n    start_index = random.randint(0, len(text) - maxlen - 1)\n    generated_text = text[start_index: start_index + maxlen]\n    print('---Seeded text: \"' + generated_text + '\"')\n\n    for temperature in [0.2, 0.5, 1.0, 1.2]:\n        print('------ Selected temperature:', temperature)\n        sys.stdout.write(generated_text)\n\n        # We generate 100 characters\n        for i in range(100):\n            sampled = np.zeros((1, maxlen, len(chars)))\n            for t, char in enumerate(generated_text):\n                sampled[0, t, char_indices[char]] = 1.\n\n            preds = model.predict(sampled, verbose=0)[0]\n            next_index = sample(preds, temperature)\n            next_char = chars[next_index]\n\n            generated_text += next_char\n            generated_text = generated_text[1:]\n\n            sys.stdout.write(next_char)\n            sys.stdout.flush()\n        print()\n```", "```py\nfrom keras.callbacks import ModelCheckpoint\n\nfilepath=\"weights-{epoch:02d}-{loss:.4f}.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\ncallbacks_list = [checkpoint]\n```", "```py\nseed_text = 'i want to generate new text after this '\nprint (seed_text)\n\n# load the network weights \nfilename = \"weights-30-1.545.hdf5\" \nmodel.load_weights(filename) \nmodel.compile(loss='categorical_crossentropy', optimizer='adam') \n\nfor temperature in [0.5]:\n        print('------ temperature:', temperature)\n        sys.stdout.write(seed_text)\n\n        # We generate 400 characters\n        for i in range(40):\n            sampled = np.zeros((1, maxlen, len(chars)))\n            for t, char in enumerate(seed_text):\n                sampled[0, t, char_indices[char]] = 1.\n\n            preds = model.predict(sampled, verbose=0)[0]\n            next_index = sample(preds, temperature)\n            next_char = chars[next_index]\n\n            seed_text += next_char\n            seed_text = seed_text[1:]\n\n            sys.stdout.write(next_char)\n            sys.stdout.flush()\n        print()\n```", "```py\n--- Generating with seed:\nthe \"good old time\" to which it belongs, and as an expressio\"\n------ temperature: 0.2\nthe \"good old time\" to which it belongs, and as an expression of the sense of the stronger and subli\n------ temperature: 0.5 and as an expression of the sense of the stronger and sublication of possess and more spirit and in\n------ temperature: 1.0 e stronger and sublication of possess and more spirit and instinge, and it: he ventlumentles, no dif\n------ temperature: 1.2\nd more spirit and instinge, and it: he ventlumentles, no differific and does amongly domen--whete ac\n```", "```py\nimport numpy as np\nimport codecs\n\n# Class to perform all preprocessing operations\nclass Preprocessing:\n    vocabulary = {}\n    binary_vocabulary = {}\n    char_lookup = {}\n    size = 0\n    separator = '->'\n# This will take the data file and convert data into one hot encoding and dump the vocab into the file.\n    def generate(self, input_file_path):\n        input_file = codecs.open(input_file_path, 'r', 'utf_8')\n        index = 0\n        for line in input_file:\n            for char in line:\n                if char not in self.vocabulary:\n                    self.vocabulary[char] = index\n                    self.char_lookup[index] = char\n                    index += 1\n        input_file.close()\n        self.set_vocabulary_size()\n        self.create_binary_representation()\n\n# This method is to load the vocab into the memory\n    def retrieve(self, input_file_path):\n        input_file = codecs.open(input_file_path, 'r', 'utf_8')\n        buffer = \"\"\n        for line in input_file:\n            try:\n                separator_position = len(buffer) + line.index(self.separator)\n                buffer += line\n                key = buffer[:separator_position]\n                value = buffer[separator_position + len(self.separator):]\n                value = np.fromstring(value, sep=',')\n\n                self.binary_vocabulary[key] = value\n                self.vocabulary[key] = np.where(value == 1)[0][0]\n                self.char_lookup[np.where(value == 1)[0][0]] = key\n\n                buffer = \"\"\n            except ValueError:\n                buffer += line\n        input_file.close()\n        self.set_vocabulary_size()\n\n# Below are some helper functions to perform pre-processing.\n    def create_binary_representation(self):\n        for key, value in self.vocabulary.iteritems():\n            binary = np.zeros(self.size)\n            binary[value] = 1\n            self.binary_vocabulary[key] = binary\n\n    def set_vocabulary_size(self):\n        self.size = len(self.vocabulary)\n        print \"Vocabulary size: {}\".format(self.size)\n\n    def get_serialized_binary_representation(self):\n        string = \"\"\n        np.set_printoptions(threshold='nan')\n        for key, value in self.binary_vocabulary.iteritems():\n            array_as_string = np.array2string(value, separator=',', max_line_width=self.size * self.size)\n            string += \"{}{}{}\\n\".format(key.encode('utf-8'), self.separator, array_as_string[1:len(array_as_string) - 1])\n        return string\n\n```", "```py\nimport tensorflow as tf\nimport pickle\nfrom tensorflow.contrib import rnn\n\n    def build(self, input_number, sequence_length, layers_number, units_number, output_number):\n        self.x = tf.placeholder(\"float\", [None, sequence_length, input_number])\n        self.y = tf.placeholder(\"float\", [None, output_number])\n        self.sequence_length = sequence_length\n```", "```py\n        self.weights = {\n            'out': tf.Variable(tf.random_normal([units_number, output_number]))\n        }\n        self.biases = {\n            'out': tf.Variable(tf.random_normal([output_number]))\n        }\n\n        x = tf.transpose(self.x, [1, 0, 2])\n        x = tf.reshape(x, [-1, input_number])\n        x = tf.split(x, sequence_length, 0)\n\n```", "```py\n        lstm_layers = []\n        for i in range(0, layers_number):\n            lstm_layer = rnn.BasicLSTMCell(units_number)\n            lstm_layers.append(lstm_layer)\n\n        deep_lstm = rnn.MultiRNNCell(lstm_layers)\n\n        self.outputs, states = rnn.static_rnn(deep_lstm, x, dtype=tf.float32)\n\n        print \"Build model with input_number: {}, sequence_length: {}, layers_number: {}, \" \\\n              \"units_number: {}, output_number: {}\".format(input_number, sequence_length, layers_number,\n                                                           units_number, output_number)\n# This method is using to dump the model configurations \n        self.save(input_number, sequence_length, layers_number, units_number, output_number)\n```", "```py\nimport os\nimport argparse\nfrom modules.Model import *\nfrom modules.Batch import *\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--training_file', type=str, required=True)\n    parser.add_argument('--vocabulary_file', type=str, required=True)\n    parser.add_argument('--model_name', type=str, required=True)\n\n    parser.add_argument('--epoch', type=int, default=200)\n    parser.add_argument('--batch_size', type=int, default=50)\n    parser.add_argument('--sequence_length', type=int, default=50)\n    parser.add_argument('--log_frequency', type=int, default=100)\n    parser.add_argument('--learning_rate', type=int, default=0.002)\n    parser.add_argument('--units_number', type=int, default=128)\n    parser.add_argument('--layers_number', type=int, default=2)\n    args = parser.parse_args()\n```", "```py\nbatch = Batch(training_file, vocabulary_file, batch_size, sequence_length)\n```", "```py\n# Building model instance and classifier\n    model = Model(model_name)\n    model.build(input_number, sequence_length, layers_number, units_number, classes_number)\n    classifier = model.get_classifier()\n\n# Building cost functions\n    cost = tf.reduce_mean(tf.square(classifier - model.y))\n    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\n# Computing the accuracy metrics\n    expected_prediction = tf.equal(tf.argmax(classifier, 1), tf.argmax(model.y, 1))\n    accuracy = tf.reduce_mean(tf.cast(expected_prediction, tf.float32))\n # Preparing logs for Tensorboard\n    loss_summary = tf.summary.scalar(\"loss\", cost)\n    acc_summary = tf.summary.scalar(\"accuracy\", accuracy)\n\n    train_summary_op = tf.summary.merge_all()\n    out_dir = \"{}/{}\".format(model_name, model_name)\n    train_summary_dir = os.path.join(out_dir, \"summaries\")\n\n##\n\n# Initializing the session and executing the training\n\ninit = tf.global_variables_initializer()\nwith tf.Session() as sess:\n        sess.run(init)\n        iteration = 0\n\n        while batch.dataset_full_passes < epoch:\n            iteration += 1\n            batch_x, batch_y = batch.get_next_batch()\n            batch_x = batch_x.reshape((batch_size, sequence_length, input_number))\n\n            sess.run(optimizer, feed_dict={model.x: batch_x, model.y: batch_y})\n            if iteration % log_frequency == 0:\n                acc = sess.run(accuracy, feed_dict={model.x: batch_x, model.y: batch_y})\n                loss = sess.run(cost, feed_dict={model.x: batch_x, model.y: batch_y})\n                print(\"Iteration {}, batch loss: {:.6f}, training accuracy: {:.5f}\".format(iteration * batch_size,\n                                                                                           loss, acc))\n        batch.clean()\n```", "```py\nimport argparse\nimport codecs\nfrom modules.Model import *\nfrom modules.Preprocessing import *\nfrom collections import deque\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model_name', type=str, required=True)\n    parser.add_argument('--vocabulary_file', type=str, required=True)\n    parser.add_argument('--output_file', type=str, required=True)\n\n    parser.add_argument('--seed', type=str, default=\"Yeah, oho \")\n    parser.add_argument('--sample_length', type=int, default=1500)\n    parser.add_argument('--log_frequency', type=int, default=100)\n\n```", "```py\n    model = Model(model_name)\n    model.restore()\n    classifier = model.get_classifier()\n\n    vocabulary = Preprocessing()\n    vocabulary.retrieve(vocabulary_file)\n```", "```py\n# Preparing the raw input data \n    for char in seed:\n        if char not in vocabulary.vocabulary:\n            print char,\"is not in vocabulary file\"\n            char = u' '\n        stack.append(char)\n        sample_file.write(char)\n\n# Restoring the models and making inferences\n    with tf.Session() as sess:\n        tf.global_variables_initializer().run()\n\n        saver = tf.train.Saver(tf.global_variables())\n        ckpt = tf.train.get_checkpoint_state(model_name)\n\n        if ckpt and ckpt.model_checkpoint_path:\n            saver.restore(sess, ckpt.model_checkpoint_path)\n\n            for i in range(0, sample_length):\n                vector = []\n                for char in stack:\n                    vector.append(vocabulary.binary_vocabulary[char])\n                vector = np.array([vector])\n                prediction = sess.run(classifier, feed_dict={model.x: vector})\n                predicted_char = vocabulary.char_lookup[np.argmax(prediction)]\n\n                stack.popleft()\n                stack.append(predicted_char)\n                sample_file.write(predicted_char)\n\n                if i % log_frequency == 0:\n                    print \"Progress: {}%\".format((i * 100) / sample_length)\n\n            sample_file.close()\n            print \"Sample saved in {}\".format(output_file)\n```", "```py\nYeah, oho once upon a time, on ir intasd\n\nI got monk that wear your good\nSo heard me down in my clipp\n\nCure me out brick\nCoway got baby, I wanna sheart in faic\n\nI could sink awlrook and heart your all feeling in the firing of to the still hild, gavelly mind, have before you, their lead\nOh, oh shor,s sheld be you und make\n\nOh, fseh where sufl gone for the runtome\nWeaaabe the ligavus I feed themust of hear\n```", "```py\nfrom music21 import converter, instrument, note, chord\nimport glob\n\nnotes = []\n\nfor file in glob.glob(\"/data/*.mid\"):\n    midi = converter.parse(file)\n    notes_to_parse = None\n    parts = instrument.partitionByInstrument(midi)\n    if parts: # file has instrument parts\n        notes_to_parse = parts.parts[0].recurse()\n    else: # file has notes in a flat structure\n        notes_to_parse = midi.flat.notes\n    for element in notes_to_parse:\n        if isinstance(element, note.Note):\n            notes.append(str(element.pitch))\n        elif isinstance(element, chord.Chord):\n            notes.append('.'.join(str(n) for n in element.normalOrder))\n\n```", "```py\nsequence_length = 100\n# get all pitch names\npitchnames = sorted(set(item for item in notes))\n\n# create a dictionary to map pitches to integers\nnote_to_int = dict((note, number) for number, note in enumerate(pitchnames))\nnetwork_input = []\nnetwork_output = []\n# create input sequences and the corresponding outputs\nfor i in range(0, len(notes) - sequence_length, 1):\n    sequence_in = notes[i:i + sequence_length]\n    sequence_out = notes[i + sequence_length]\n    network_input.append([note_to_int[char] for char in sequence_in])\n    network_output.append(note_to_int[sequence_out])\nn_patterns = len(network_input)\n# reshape the input into a format compatible with LSTM layers\nnetwork_input = numpy.reshape(network_input, (n_patterns, sequence_length, 1))\n# normalize input\nnetwork_input = network_input / float(n_vocab)\nnetwork_output = np_utils.to_categorical(network_output)\n```", "```py\nmodel = Sequential()\nmodel.add(LSTM(\n    256,\n    input_shape=(network_input.shape[1], network_input.shape[2]),\n    return_sequences=True\n))\nmodel.add(Dropout(0.5))\nmodel.add(LSTM(512, return_sequences=True))\nmodel.add(Dropout(0.3))\nmodel.add(LSTM(256))\nmodel.add(Dense(256))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(n_vocab))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', \n                  optimizer='rmsprop', \n                  metrics=['accuracy'])\n\n```", "```py\nfilepath = \"weights-{epoch:02d}-{loss:.4f}.hdf5\"\ncheckpoint = ModelCheckpoint(\n    filepath,\n    monitor='loss',\n    verbose=0,\n    save_best_only=True,\n    mode='min'\n)\ncallbacks_list = [checkpoint]\n\nhistory = model.fit(network_input, network_output, epochs=200, batch_size=64, callbacks=callbacks_list)\n```", "```py\nmodel = Sequential()\nmodel.add(LSTM(\n    512,\n    input_shape=(network_input.shape[1], network_input.shape[2]),\n    return_sequences=True\n))\nmodel.add(Dropout(0.5))\nmodel.add(LSTM(512, return_sequences=True))\nmodel.add(Dropout(0.3))\nmodel.add(LSTM(512))\nmodel.add(Dense(256))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(n_vocab))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\n# Load the weights to each node\nmodel.load_weights('weights_file.hdf5')\n```", "```py\n# Randomly selected a note from our processed data\nstart = numpy.random.randint(0, len(network_input)-1)\npattern = network_input[start]\n\nint_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n\nprediction_output = []\n\n# Generate 1000 notes of music\nfor note_index in range(1000):\n    prediction_input = numpy.reshape(pattern, (1, len(pattern), 1))\n    prediction_input = prediction_input / float(n_vocab)\n\n    prediction = model.predict(prediction_input, verbose=0)\n\n    index = numpy.argmax(prediction)\n    result = int_to_note[index]\n    prediction_output.append(result)\n\n    pattern.append(index)\n    pattern = pattern[1:len(pattern)]\n```", "```py\ndef create_midi_file(prediction_output):\n    \"\"\" convert the output from the prediction to notes and create a midi file\"\"\"\n    offset = 0\n    output_notes = []\n\n    for pattern in prediction_output:\n        # pattern is a chord\n        if ('.' in pattern) or pattern.isdigit():\n            notes_in_chord = pattern.split('.')\n            notes = []\n            for current_note in notes_in_chord:\n                new_note = note.Note(int(current_note))\n                new_note.storedInstrument = instrument.Piano()\n                notes.append(new_note)\n            new_chord = chord.Chord(notes)\n            new_chord.offset = offset\n            output_notes.append(new_chord)\n        # pattern is a note\n        else:\n            new_note = note.Note(pattern)\n            new_note.offset = offset\n            new_note.storedInstrument = instrument.Piano()\n            output_notes.append(new_note)\n\n        # increase offset each iteration so that notes do not stack\n        offset += 0.5\n\n    midi_stream = stream.Stream(output_notes)\n\n    midi_stream.write('midi', fp='generated.mid')\n\n```"]