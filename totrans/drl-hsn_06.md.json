["```py\nimport typing as tt \nimport gymnasium as gym \nfrom collections import defaultdict \nfrom torch.utils.tensorboard.writer import SummaryWriter \n\nENV_NAME = \"FrozenLake-v1\" \nGAMMA = 0.9 \nALPHA = 0.2 \nTEST_EPISODES = 20 \n\nState = int \nAction = int \nValuesKey = tt.Tuple[State, Action] \n\nclass Agent: \n    def __init__(self): \n        self.env = gym.make(ENV_NAME) \n        self.state, _ = self.env.reset() \n        self.values: tt.Dict[ValuesKey] = defaultdict(float)\n```", "```py\n def sample_env(self) -> tt.Tuple[State, Action, float, State]: \n        action = self.env.action_space.sample() \n        old_state = self.state \n        new_state, reward, is_done, is_tr, _ = self.env.step(action) \n        if is_done or is_tr: \n            self.state, _ = self.env.reset() \n        else: \n            self.state = new_state \n        return old_state, action, float(reward), new_state\n```", "```py\n def best_value_and_action(self, state: State) -> tt.Tuple[float, Action]: \n        best_value, best_action = None, None \n        for action in range(self.env.action_space.n): \n            action_value = self.values[(state, action)] \n            if best_value is None or best_value < action_value: \n                best_value = action_value \n                best_action = action \n        return best_value, best_action\n```", "```py\n def value_update(self, state: State, action: Action, reward: float, next_state: State): \n        best_val, _ = self.best_value_and_action(next_state) \n        new_val = reward + GAMMA * best_val \n        old_val = self.values[(state, action)] \n        key = (state, action) \n        self.values[key] = old_val * (1-ALPHA) + new_val * ALPHA\n```", "```py\n def play_episode(self, env: gym.Env) -> float: \n        total_reward = 0.0 \n        state, _ = env.reset() \n        while True: \n            _, action = self.best_value_and_action(state) \n            new_state, reward, is_done, is_tr, _ = env.step(action) \n            total_reward += reward \n            if is_done or is_tr: \n                break \n            state = new_state \n        return total_reward\n```", "```py\nif __name__ == \"__main__\": \n    test_env = gym.make(ENV_NAME) \n    agent = Agent() \n    writer = SummaryWriter(comment=\"-q-learning\") \n\n    iter_no = 0 \n    best_reward = 0.0 \n    while True: \n        iter_no += 1 \n        state, action, reward, next_state = agent.sample_env() \n        agent.value_update(state, action, reward, next_state) \n\n        test_reward = 0.0 \n        for _ in range(TEST_EPISODES): \n            test_reward += agent.play_episode(test_env) \n        test_reward /= TEST_EPISODES \n        writer.add_scalar(\"reward\", test_reward, iter_no) \n        if test_reward > best_reward: \n            print(\"%d: Best test reward updated %.3f -> %.3f\" % (iter_no, best_reward, test_reward)) \n            best_reward = test_reward \n        if test_reward > 0.80: \n            print(\"Solved in %d iterations!\" % iter_no) \n            break \n    writer.close()\n```", "```py\nChapter06$ ./01_frozenlake_q_learning.py \n1149: Best test reward updated 0.000 -> 0.500 \n1150: Best test reward updated 0.500 -> 0.550 \n1164: Best test reward updated 0.550 -> 0.600 \n1242: Best test reward updated 0.600 -> 0.650 \n2685: Best test reward updated 0.650 -> 0.700 \n2988: Best test reward updated 0.700 -> 0.750 \n3025: Best test reward updated 0.750 -> 0.850 \nSolved in 3025 iterations!\n```", "```py\nclass FireResetEnv(gym.Wrapper[np.ndarray, int, np.ndarray, int]): \n    def __init__(self, env: gym.Env) -> None: \n        super().__init__(env) \n        assert env.unwrapped.get_action_meanings()[1] == \"FIRE\" \n        assert len(env.unwrapped.get_action_meanings()) >= 3 \n\n    def reset(self, **kwargs) -> AtariResetReturn: \n        self.env.reset(**kwargs) \n        obs, _, terminated, truncated, _ = self.env.step(1) \n        if terminated or truncated: \n            self.env.reset(**kwargs) \n        obs, _, terminated, truncated, _ = self.env.step(2) \n        if terminated or truncated: \n            self.env.reset(**kwargs) \n        return obs, {}\n```", "```py\nclass MaxAndSkipEnv(gym.Wrapper[np.ndarray, int, np.ndarray, int]): \n    def __init__(self, env: gym.Env, skip: int = 4) -> None: \n        super().__init__(env) \n        self._obs_buffer = np.zeros((2, *env.observation_space.shape), \n            dtype=env.observation_space.dtype) \n        self._skip = skip \n\n    def step(self, action: int) -> AtariStepReturn: \n        total_reward = 0.0 \n        terminated = truncated = False \n        for i in range(self._skip): \n            obs, reward, terminated, truncated, info = self.env.step(action) \n            done = terminated or truncated \n            if i == self._skip - 2: \n                self._obs_buffer[0] = obs \n            if i == self._skip - 1: \n                self._obs_buffer[1] = obs \n            total_reward += float(reward) \n            if done: \n                break \n        # Note that the observation on the done=True frame \n        # doesnâ€™t matter \n        max_frame = self._obs_buffer.max(axis=0) \n\n        return max_frame, total_reward, terminated, truncated, info\n```", "```py\nclass WarpFrame(gym.ObservationWrapper[np.ndarray, int, np.ndarray]): \n    def __init__(self, env: gym.Env, width: int = 84, height: int = 84) -> None: \n        super().__init__(env) \n        self.width = width \n        self.height = height \n        self.observation_space = spaces.Box( \n            low=0, high=255, shape=(self.height, self.width, 1), \n            dtype=env.observation_space.dtype, \n        ) \n\n    def observation(self, frame: np.ndarray) -> np.ndarray: \n        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY) \n        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA) \n        return frame[:, :, None]\n```", "```py\nclass BufferWrapper(gym.ObservationWrapper): \n    def __init__(self, env, n_steps): \n        super(BufferWrapper, self).__init__(env) \n        obs = env.observation_space \n        assert isinstance(obs, spaces.Box) \n        new_obs = gym.spaces.Box( \n            obs.low.repeat(n_steps, axis=0), obs.high.repeat(n_steps, axis=0), \n            dtype=obs.dtype) \n        self.observation_space = new_obs \n        self.buffer = collections.deque(maxlen=n_steps) \n\n    def reset(self, *, seed: tt.Optional[int] = None, options: tt.Optional[dict[str, tt.Any]] = None): \n        for _ in range(self.buffer.maxlen-1): \n            self.buffer.append(self.env.observation_space.low) \n        obs, extra = self.env.reset() \n        return self.observation(obs), extra \n\n    def observation(self, observation: np.ndarray) -> np.ndarray: \n        self.buffer.append(observation) \n        return np.concatenate(self.buffer)\n```", "```py\nclass ImageToPyTorch(gym.ObservationWrapper): \n    def __init__(self, env): \n        super(ImageToPyTorch, self).__init__(env) \n        obs = self.observation_space \n        assert isinstance(obs, gym.spaces.Box) \n        assert len(obs.shape) == 3 \n        new_shape = (obs.shape[-1], obs.shape[0], obs.shape[1]) \n        self.observation_space = gym.spaces.Box( \n            low=obs.low.min(), high=obs.high.max(), \n            shape=new_shape, dtype=obs.dtype) \n\n    def observation(self, observation): \n        return np.moveaxis(observation, 2, 0)\n```", "```py\ndef make_env(env_name: str, **kwargs): \n    env = gym.make(env_name, **kwargs) \n    env = atari_wrappers.AtariWrapper(env, clip_reward=False, noop_max=0) \n    env = ImageToPyTorch(env) \n    env = BufferWrapper(env, n_steps=4) \n    return env\n```", "```py\nimport torch \nimport torch.nn as nn \n\nclass DQN(nn.Module): \n    def __init__(self, input_shape, n_actions): \n        super(DQN, self).__init__() \n\n        self.conv = nn.Sequential( \n            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4), \n            nn.ReLU(), \n            nn.Conv2d(32, 64, kernel_size=4, stride=2), \n            nn.ReLU(), \n            nn.Conv2d(64, 64, kernel_size=3, stride=1), \n            nn.ReLU(), \n            nn.Flatten(), \n        ) \n        size = self.conv(torch.zeros(1, *input_shape)).size()[-1] \n        self.fc = nn.Sequential( \n            nn.Linear(size, 512), \n            nn.ReLU(), \n            nn.Linear(512, n_actions) \n        )\n```", "```py\n def forward(self, x: torch.ByteTensor): \n        # scale on GPU \n        xx = x / 255.0 \n        return self.fc(self.conv(xx))\n```", "```py\nimport gymnasium as gym \nfrom lib import dqn_model \nfrom lib import wrappers \n\nfrom dataclasses import dataclass \nimport argparse \nimport time \nimport numpy as np \nimport collections \nimport typing as tt \n\nimport torch \nimport torch.nn as nn \nimport torch.optim as optim \n\nfrom torch.utils.tensorboard.writer import SummaryWriter\n```", "```py\nDEFAULT_ENV_NAME = \"PongNoFrameskip-v4\" \nMEAN_REWARD_BOUND = 19\n```", "```py\nGAMMA = 0.99 \nBATCH_SIZE = 32 \nREPLAY_SIZE = 10000 \nLEARNING_RATE = 1e-4 \nSYNC_TARGET_FRAMES = 1000 \nREPLAY_START_SIZE = 10000\n```", "```py\nEPSILON_DECAY_LAST_FRAME = 150000 \nEPSILON_START = 1.0 \nEPSILON_FINAL = 0.01\n```", "```py\nState = np.ndarray \nAction = int \nBatchTensors = tt.Tuple[ \n    torch.ByteTensor,           # current state \n    torch.LongTensor,           # actions \n    torch.Tensor,               # rewards \n    torch.BoolTensor,           # done || trunc \n    torch.ByteTensor            # next state \n] \n\n@dataclass \nclass Experience: \n    state: State \n    action: Action \n    reward: float \n    done_trunc: bool \n    new_state: State\n```", "```py\nclass ExperienceBuffer: \n    def __init__(self, capacity: int): \n        self.buffer = collections.deque(maxlen=capacity) \n\n    def __len__(self): \n        return len(self.buffer) \n\n    def append(self, experience: Experience): \n        self.buffer.append(experience) \n\n    def sample(self, batch_size: int) -> tt.List[Experience]: \n        indices = np.random.choice(len(self), batch_size, replace=False) \n        return [self.buffer[idx] for idx in indices]\n```", "```py\nclass Agent: \n    def __init__(self, env: gym.Env, exp_buffer: ExperienceBuffer): \n        self.env = env \n        self.exp_buffer = exp_buffer \n        self.state: tt.Optional[np.ndarray] = None \n        self._reset() \n\n    def _reset(self): \n        self.state, _ = env.reset() \n        self.total_reward = 0.0\n```", "```py\n @torch.no_grad() \n    def play_step(self, net: dqn_model.DQN, device: torch.device, \n                  epsilon: float = 0.0) -> tt.Optional[float]: \n        done_reward = None \n\n        if np.random.random() < epsilon: \n            action = env.action_space.sample() \n        else: \n            state_v = torch.as_tensor(self.state).to(device) \n            state_v.unsqueeze_(0) \n            q_vals_v = net(state_v) \n            _, act_v = torch.max(q_vals_v, dim=1) \n            action = int(act_v.item())\n```", "```py\n new_state, reward, is_done, is_tr, _ = self.env.step(action) \n        self.total_reward += reward \n\n        exp = Experience( \n            state=self.state, action=action, reward=float(reward), \n            done_trunc=is_done or is_tr, new_state=new_state \n        ) \n        self.exp_buffer.append(exp) \n        self.state = new_state \n        if is_done or is_tr: \n            done_reward = self.total_reward \n            self._reset() \n        return done_reward\n```", "```py\ndef batch_to_tensors(batch: tt.List[Experience], device: torch.device) -> BatchTensors: \n    states, actions, rewards, dones, new_state = [], [], [], [], [] \n    for e in batch: \n        states.append(e.state) \n        actions.append(e.action) \n        rewards.append(e.reward) \n        dones.append(e.done_trunc) \n        new_state.append(e.new_state) \n    states_t = torch.as_tensor(np.asarray(states)) \n    actions_t = torch.LongTensor(actions) \n    rewards_t = torch.FloatTensor(rewards) \n    dones_t = torch.BoolTensor(dones) \n    new_states_t = torch.as_tensor(np.asarray(new_state)) \n    return states_t.to(device), actions_t.to(device), rewards_t.to(device), \\ \n           dones_t.to(device),  new_states_t.to(device)\n```", "```py\ndef calc_loss(batch: tt.List[Experience], net: dqn_model.DQN, tgt_net: dqn_model.DQN, \n              device: torch.device) -> torch.Tensor: \n    states_t, actions_t, rewards_t, dones_t, new_states_t = batch_to_tensors(batch, device)\n```", "```py\n state_action_values = net(states_t).gather( \n        1, actions_t.unsqueeze(-1) \n    ).squeeze(-1)\n```", "```py\n with torch.no_grad(): \n        next_state_values = tgt_net(new_states_t).max(1)[0]\n```", "```py\n next_state_values[dones_t] = 0.0\n```", "```py\n next_state_values = next_state_values.detach()\n```", "```py\n expected_state_action_values = next_state_values * GAMMA + rewards_t \n    return nn.MSELoss()(state_action_values, expected_state_action_values)\n```", "```py\ndef calc_loss(batch: tt.List[Experience], net: dqn_model.DQN, tgt_net: dqn_model.DQN, \n              device: torch.device) -> torch.Tensor: \n    states_t, actions_t, rewards_t, dones_t, new_states_t = batch_to_tensors(batch, device) \n\n    state_action_values = net(states_t).gather( \n        1, actions_t.unsqueeze(-1) \n    ).squeeze(-1) \n    with torch.no_grad(): \n        next_state_values = tgt_net(new_states_t).max(1)[0] \n        next_state_values[dones_t] = 0.0 \n        next_state_values = next_state_values.detach() \n\n    expected_state_action_values = next_state_values * GAMMA + rewards_t \n    return nn.MSELoss()(state_action_values, expected_state_action_values)\n```", "```py\nif __name__ == \"__main__\": \n    parser = argparse.ArgumentParser() \n    parser.add_argument(\"--dev\", default=\"cpu\", help=\"Device name, default=cpu\") \n    parser.add_argument(\"--env\", default=DEFAULT_ENV_NAME, \n                        help=\"Name of the environment, default=\" + DEFAULT_ENV_NAME) \n    args = parser.parse_args() \n    device = torch.device(args.dev)\n```", "```py\n env = wrappers.make_env(args.env) \n    net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device) \n    tgt_net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)\n```", "```py\n writer = SummaryWriter(comment=\"-\" + args.env) \n    print(net) \n    buffer = ExperienceBuffer(REPLAY_SIZE) \n    agent = Agent(env, buffer) \n    epsilon = EPSILON_START\n```", "```py\n optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE) \n    total_rewards = [] \n    frame_idx = 0 \n    ts_frame = 0 \n    ts = time.time() \n    best_m_reward = None\n```", "```py\n while True: \n        frame_idx += 1 \n        epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)\n```", "```py\n reward = agent.play_step(net, device, epsilon) \n        if reward is not None: \n            total_rewards.append(reward) \n            speed = (frame_idx - ts_frame) / (time.time() - ts) \n            ts_frame = frame_idx \n            ts = time.time() \n            m_reward = np.mean(total_rewards[-100:]) \n            print(f\"{frame_idx}: done {len(total_rewards)} games, reward {m_reward:.3f}, \" \n                  f\"eps {epsilon:.2f}, speed {speed:.2f} f/s\") \n            writer.add_scalar(\"epsilon\", epsilon, frame_idx) \n            writer.add_scalar(\"speed\", speed, frame_idx) \n            writer.add_scalar(\"reward_100\", m_reward, frame_idx) \n            writer.add_scalar(\"reward\", reward, frame_idx)\n```", "```py\n if best_m_reward is None or best_m_reward < m_reward: \n                torch.save(net.state_dict(), args.env + \"-best_%.0f.dat\" % m_reward) \n                if best_m_reward is not None: \n                    print(f\"Best reward updated {best_m_reward:.3f} -> {m_reward:.3f}\") \n                best_m_reward = m_reward \n            if m_reward > MEAN_REWARD_BOUND: \n                print(\"Solved in %d frames!\" % frame_idx) \n                break\n```", "```py\n if len(buffer) < REPLAY_START_SIZE: \n            continue \n        if frame_idx % SYNC_TARGET_FRAMES == 0: \n            tgt_net.load_state_dict(net.state_dict())\n```", "```py\n optimizer.zero_grad() \n        batch = buffer.sample(BATCH_SIZE) \n        loss_t = calc_loss(batch, net, tgt_net, device) \n        loss_t.backward() \n        optimizer.step()\n```", "```py\nChapter06$ ./02_dqn_pong.py --dev cuda \nA.L.E: Arcade Learning Environment (version 0.8.1+53f58b7) \n[Powered by Stella] \nDQN( \nÂ Â (conv): Sequential( \nÂ Â Â (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4)) \nÂ Â Â (1): ReLU() \nÂ Â Â (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2)) \nÂ Â Â (3): ReLU() \nÂ Â Â (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1)) \nÂ Â Â (5): ReLU() \nÂ Â Â (6): Flatten(start_dim=1, end_dim=-1) \nÂ Â ) \nÂ Â (fc): Sequential( \nÂ Â Â (0): Linear(in_features=3136, out_features=512, bias=True) \nÂ Â Â (1): ReLU() \nÂ Â Â (2): Linear(in_features=512, out_features=6, bias=True) \nÂ Â ) \n) \n940: done 1 games, reward -21.000, eps 0.99, speed 1214.95 f/s \n1946: done 2 games, reward -20.000, eps 0.99, speed 1420.09 f/s \nBest reward updated -21.000 -> -20.000 \n2833: done 3 games, reward -20.000, eps 0.98, speed 1416.26 f/s \n3701: done 4 games, reward -20.000, eps 0.98, speed 1421.84 f/s \n4647: done 5 games, reward -20.200, eps 0.97, speed 1421.63 f/s \n5409: done 6 games, reward -20.333, eps 0.96, speed 1395.67 f/s \n6171: done 7 games, reward -20.429, eps 0.96, speed 1411.90 f/s \n7063: done 8 games, reward -20.375, eps 0.95, speed 1404.49 f/s \n7882: done 9 games, reward -20.444, eps 0.95, speed 1388.26 f/s \n8783: done 10 games, reward -20.400, eps 0.94, speed 1283.64 f/s \n9545: done 11 games, reward -20.455, eps 0.94, speed 1376.47 f/s \n10307: done 12 games, reward -20.500, eps 0.93, speed 431.94 f/s \n11362: done 13 games, reward -20.385, eps 0.92, speed 276.14 f/s \n12420: done 14 games, reward -20.214, eps 0.92, speed 276.44 f/s\n```", "```py\n66024: done 68 games, reward -20.162, eps 0.56, speed 260.89 f/s \n67338: done 69 games, reward -20.130, eps 0.55, speed 257.63 f/s \n68440: done 70 games, reward -20.100, eps 0.54, speed 260.17 f/s \n69467: done 71 games, reward -20.113, eps 0.54, speed 260.02 f/s \n70792: done 72 games, reward -20.125, eps 0.53, speed 258.88 f/s \n72031: done 73 games, reward -20.123, eps 0.52, speed 259.54 f/s \n73314: done 74 games, reward -20.095, eps 0.51, speed 258.16 f/s \n74815: done 75 games, reward -20.053, eps 0.50, speed 257.56 f/s \n76339: done 76 games, reward -20.026, eps 0.49, speed 256.79 f/s \n77576: done 77 games, reward -20.013, eps 0.48, speed 257.86 f/s \n78978: done 78 games, reward -19.974, eps 0.47, speed 255.90 f/s \n80093: done 79 games, reward -19.962, eps 0.47, speed 256.84 f/s \n81565: done 80 games, reward -19.938, eps 0.46, speed 256.34 f/s \n83365: done 81 games, reward -19.901, eps 0.44, speed 254.22 f/s \n84841: done 82 games, reward -19.878, eps 0.43, speed 254.80 f/s\n```", "```py\n737860: done 371 games, reward 18.540, eps 0.01, speed 225.22 f/s \n739935: done 372 games, reward 18.650, eps 0.01, speed 232.70 f/s \nBest reward updated 18.610 -> 18.650 \n741910: done 373 games, reward 18.650, eps 0.01, speed 231.66 f/s \n743964: done 374 games, reward 18.760, eps 0.01, speed 231.59 f/s \nBest reward updated 18.650 -> 18.760 \n745939: done 375 games, reward 18.770, eps 0.01, speed 223.45 f/s \nBest reward updated 18.760 -> 18.770 \n747950: done 376 games, reward 18.810, eps 0.01, speed 229.84 f/s \nBest reward updated 18.770 -> 18.810 \n749925: done 377 games, reward 18.810, eps 0.01, speed 228.05 f/s \n752008: done 378 games, reward 18.910, eps 0.01, speed 225.41 f/s \nBest reward updated 18.810 -> 18.910 \n753983: done 379 games, reward 18.920, eps 0.01, speed 229.75 f/s \nBest reward updated 18.910 -> 18.920 \n755958: done 380 games, reward 19.030, eps 0.01, speed 228.71 f/s \nBest reward updated 18.920 -> 19.030 \nSolved in 755958 frames!\n```", "```py\nimport gymnasium as gym \nimport argparse \nimport numpy as np \nimport typing as tt \n\nimport torch \n\nfrom lib import wrappers \nfrom lib import dqn_model \n\nimport collections \n\nDEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n```", "```py\nif __name__ == \"__main__\": \n    parser = argparse.ArgumentParser() \n    parser.add_argument(\"-m\", \"--model\", required=True, help=\"Model file to load\") \n    parser.add_argument(\"-e\", \"--env\", default=DEFAULT_ENV_NAME, \n                        help=\"Environment name to use, default=\" + DEFAULT_ENV_NAME) \n    parser.add_argument(\"-r\", \"--record\", required=True, help=\"Directory for video\") \n    args = parser.parse_args()\n```", "```py\n env = wrappers.make_env(args.env, render_mode=\"rgb_array\") \n    env = gym.wrappers.RecordVideo(env, video_folder=args.record) \n    net = dqn_model.DQN(env.observation_space.shape, env.action_space.n) \n    state = torch.load(args.model, map_location=lambda stg, _: stg) \n    net.load_state_dict(state) \n\n    state, _ = env.reset() \n    total_reward = 0.0 \n    c: tt.Dict[int, int] = collections.Counter()\n```", "```py\n while True: \n        state_v = torch.tensor([state]) \n        q_vals = net(state_v).data.numpy()[0] \n        action = int(np.argmax(q_vals)) \n        c[action] += 1\n```", "```py\n state, reward, is_done, is_trunc, _ = env.step(action) \n        total_reward += reward \n        if is_done or is_trunc: \n            break \n    print(\"Total reward: %.2f\" % total_reward) \n    print(\"Action counts:\", c) \n    env.close()\n```"]