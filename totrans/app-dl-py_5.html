<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Model Architecture</h1>
                </header>
            
            <article>
                
<p class="mce-root">Building on fundamental concepts from C<em>hapter 4</em>, Introduction to Neural Networks and Deep Learning, we now move into a practical problem: can we predict Bitcoin prices using a deep learning model? In this chapter, we will learn how to build a deep learning model that attempts to do that. We will conclude this chapter by putting all of these components together and building a bare-bones yet complete fist version of a deep learning application.</p>
<p>By the end of this chapter, you will be able to:</p>
<ul>
<li>Prepare data for a deep learning model</li>
<li>Choose the right model architecture</li>
<li>Use Keras, a TensorFlow abstraction library</li>
<li>Make predictions with a trained model</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Choosing the Right Model Architecture</h1>
                </header>
            
            <article>
                
<p>Deep learning is a filled undergoing intense research activity. Among other things, researchers are devoted to inventing new neural network architectures that can either tackle new problems or increase the performance of previously implemented architectures. In this section, we study both old and new architectures.</p>
<p>Older architectures have been used to solve a large array of problems and are generally considered the right choice when starting a new project. Newer architectures have shown great successes in specific problems, but are harder to generalize. The latter are interesting as references of what to explore next, but are hardly a good choice when starting a project.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Common Architectures</h1>
                </header>
            
            <article>
                
<p>Considering the many architecture possibilities, there are two popular architectures that have often been used as starting points for a number of applications: <strong>convolutional neural networks</strong> (<strong>CNNs</strong>) and <strong>recurrent neural networks</strong> (<strong>RNNs</strong>). These are foundational networks and should be considered starting points for most projects. We also include descriptions of another three networks, due to their relevance in the field: <strong>Long-short term memory</strong> (<strong>LSTM</strong>) networks, an RNN variant; <strong>generative adversarial networks</strong> (<strong>GANs</strong>); and deep reinforcement learning. These latter architectures have shown great successes in solving contemporary problems, but are somewhat more difficult to use.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Convolutional Neural Networks</h1>
                </header>
            
            <article>
                
<p>Convolutional neural networks have gained notoriety for working with problems that have a grid-like structure. They were originally created to classify images, but have been used in a number of other areas, ranging from speech recognition to self-driving vehicles.</p>
<p>CNN's essential insight is to use closely related data as an element of the training process, instead of only individual data inputs. This idea is particularly effective in the context of images, where a pixel located to the right of another pixel is related to that pixel as well, given that they form part of a larger composition. In this case, that composition is what the network is training to predict. Hence, combining a few pixels together is better than using an individual pixel on its own.</p>
<p>The name <strong>convolution</strong> is given to the mathematical representation of this process:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/8de2852a-e55b-420b-aed1-cbcdfa685677.png" style="width:40.92em;height:23.33em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 1: Illustration of the convolution process Image source: Volodymyr Mnih, et al.<br/></span></div>
<div class="packt_infobox">For more information refer, Human-level control through deep reinforcement learning. February 2015, Nature. Available at: <a href="https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf">https://storage. googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Recurrent Neural Networks</h1>
                </header>
            
            <article>
                
<p>Convolutional neural networks work with a set of inputs that keep altering the weights and biases of the networks' respective layers and nodes. A known limitation of this approach is that its architecture ignores the sequence of these inputs when determining how to change the networks' weights and biases.</p>
<p>Recurrent neural networks were created precisely to address that problem. RNNs are designed to work with sequential data. This means that at every epoch, layers can be influenced by the output of previous layers. The memory of previous observations in a given sequence plays a role in the evaluation of posterior observations.</p>
<p>RNNs have had successful applications in speech recognition due to the sequential nature of that problem. Also, they are used for translation problems. Google Translate's current algorithm—called <strong>Transformer</strong>—uses an RNN to translate text from one language to another.</p>
<div class="packt_infobox">For more information refer, Transformer: A Novel Neural Network Architecture for Language Understanding, by Jakob Uszkoreit, Google Research Blog, August 2017. Available at: <a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html</a>.</div>
<div class="CDPAlignCenter CDPAlign"><img src="assets/fb6536c7-db6a-4a9a-8bdd-c24d002b1c65.png" style="width:81.92em;height:20.75em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 2: Illustration from distill.pub (https://distill.pub/2016/augmented-rnns/)<br/></span></div>
<p>Figure 2 shows that words in English are related to words in French, based on where they appear in a sentence. RNNs are very popular in language translation problems.</p>
<p>Long-short term memory networks are RNN variants created to address the vanishing gradient problem. The vanishing gradient problem is caused by memory components that are too distant from the current step and would receive lower weights due to their distance. LSTMs are a variant of RNNs that contain a memory component—called <strong>forget gate</strong>. That component can be used to evaluate how both recent and old elements affect the weights and biases, depending on where the observation is placed in a sequence.</p>
<div class="packt_infobox">For more details refer, The LSTM architecture was fist introduced by Sepp Hochreiter and Jürgen Schmidhuber in 1997. Current implementations have had several modifiations. For a detailed mathematical explanation of how each component of an LSTM works, we suggest the article <em>Understanding LSTM Networks</em> by Christopher Olah, August 2015, available at <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">http://colah.github.io/posts/2015- 08-Understanding-LSTMs/</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generative Adversarial Networks</h1>
                </header>
            
            <article>
                
<p><strong>Generative adversarial networks</strong> (<strong>GANs</strong>) were invented in 2014 by Ian Goodfellow and his colleagues at the University of Montreal. GANs suggest that, instead of having one neural network that optimizes weights and biases with the objective to minimize its errors, there should be two neural networks that compete against each other for that purpose.</p>
<div class="packt_infobox"><span>For more details refer, Generative Adversarial Networks by Ian Goodfellow,et al, arXiv. June 10, 2014. Available at: <a href="https://arxiv.org/abs/1406.2661">https://arxiv.org/abs/1406.2661</a>.<br/></span></div>
<p>GANs have a network that generates new data (that is, "fake" data) and a network that evaluates the likelihood of the data generated by the fist network to be real or "fake". They compete because both learn: one learns how to better generate "fake" data, and the other learns how to distinguish if the data it is presented with is real or not. They iterate on every epoch until they both converge. That is the point when the network that evaluates generated data cannot distinguish between "fake" and real data any longer.</p>
<p>GANs have been successfully used in fields where data has a clear topological structure. Its original implementation used a GAN to create synthetic images of objects, people's faces, and animals that were similar to real images of those things. This domain of image creation is where GANs are used the most frequently, but applications in other domains occasionally appear in research papers.</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/a256fdab-16a0-4f18-b2ab-907ddbecd12b.png" style="width:47.75em;height:23.08em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 3: Image that shows the result of different GAN algorithms in changing people's faces based on a given emotion. Source: StarGAN Project. Available at <a href="https://github.com/yunjey/StarGAN">https://github.com/yunjey/StarGAN</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep Reinforcement Learning</h1>
                </header>
            
            <article>
                
<p>The original DRL architecture was championed by DeepMind, a Google-owned artificial intelligence research organization based in the UK.</p>
<p>The key idea of DRL networks is that they are unsupervised in nature and that they learn from trial-and-error, only optimizing for a reward function. That is, different than other networks (which use supervised approaches to optimize for how wrong the predictions are, compared to what is known to be right), DRL networks do not know of a correct way of approaching a problem. They are simply given the rules of a system and are then rewarded every time they perform a function correctly. This process, which takes a very large number of iterations, eventually trains networks to excel in a number of tasks.</p>
<div class="packt_infobox">For more information refer, Human-level control through deep reinforcement learning, by Volodymyr Mnih et al., February 2015, Nature. Available at: <a href="https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf">https://storage.googleapis.com/deepmind-media/dqn/ DQNNaturePaper.pdf</a>.</div>
<p>Deep Reinforcement Learning models gained popularity after DeepMind created AlphaGo, a system that plays the game Go better than professional players. DeepMind also created DRL networks that learn how to play video games at a superhuman level, entirely on their own:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/42cf84e8-ec2b-454b-9198-5e47b6bb8c13.png" style="width:44.92em;height:32.42em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 4: Image that represents how the DQN algorithm works<br/></span></div>
<div class="packt_infobox">For more information refer, DQN was created by DeepMind to beat Atari games. The algorithm uses a deep reinforcement learning solution to continuously increase its reward. Image source: <a href="https://keon.io/deep-q-learning/">https://keon.io/ deep-q-learning/</a>.</div>
<div>
<table style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td style="width: 31.9042%"><strong><span>Architecture<br/></span></strong></td>
<td style="width: 29.0958%"><strong>Data Structure</strong></td>
<td style="width: 35%"><strong><span>Successful Applications<br/></span></strong></td>
</tr>
<tr>
<td style="width: 31.9042%"><span>Convolutional neural networks<br/>
(CNNs)<br/></span></td>
<td style="width: 29.0958%"><span>Grid-like topological structure<br/>
(that is, images)<br/></span></td>
<td style="width: 35%"><span>Image recognition and classification<br/></span></td>
</tr>
<tr>
<td style="width: 31.9042%"><span>Recurrent neural network (RNN) and long-short term memory (LSTM) networks<br/></span></td>
<td style="width: 29.0958%"><span>Sequential data (that is, time-series data)<br/></span></td>
<td style="width: 35%"><span>Speech recognition, text generation, and translation<br/></span></td>
</tr>
<tr>
<td style="width: 31.9042%"><span>Generative adversarial networks (GANs)<br/></span></td>
<td style="width: 29.0958%"><span>Grid-like topological structure (that is, images)<br/></span></td>
<td style="width: 35%"><span>Image generation<br/></span></td>
</tr>
<tr>
<td style="width: 31.9042%"><span>Deep reinforcement learning (DRL)<br/></span></td>
<td style="width: 29.0958%"><span>System with clear rules and a clearly defined reward function<br/></span></td>
<td style="width: 35%">
<p class="mce-root"><span>Playing video games and selfdriving vehicles<br/></span></p>
</td>
</tr>
</tbody>
</table>
</div>
<div class="packt_figref CDPAlignCenter CDPAlign">Table 1: Different neural network architectures have shown success in different filelds. The networks' architecture is typically related to the structure of the problem at hand.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data Normalization</h1>
                </header>
            
            <article>
                
<p>Before building a deep learning model, one more step is necessary: data normalization.</p>
<p>Data normalization is a common practice in machine learning systems. Particularly regarding neural networks, researchers have proposed that normalization is an essential technique for training RNNs (and LSTMs), mainly because it decreases the network's training time and increases the network's overall performance.</p>
<div class="packt_infobox"><span>For more information refer,</span> <em>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</em> <span>by Sergey Ioffe et. al., arXiv,March 2015. Available at: <a href="https://arxiv.org/abs/1502.03167">https://arxiv.org/abs/1502.03167</a>.<br/></span></div>
<p>Deciding on a normalization technique varies, depending on the data and the problem at hand. The following techniques are commonly used.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Z-score</h1>
                </header>
            
            <article>
                
<p>When data is normally distributed (that is, Gaussian), one can compute the distance between each observation as a standard deviation from its mean.</p>
<p>This normalization is useful when identifying how distant data points are from more likely occurrences in the distribution. The Z-score is defiled by:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/dea0c619-724b-4811-89d7-7b94ad031f1f.png" style="width:11.83em;height:5.92em;"/></p>
<p><span>Here, <img src="assets/71b9e4fd-3854-4202-8792-5872dca200f2.png" style="width:1.50em;height:1.58em;"/> is the <img src="assets/88fdc3b7-222c-40df-90fd-19de4ef61ac3.png" style="width:1.75em;height:1.67em;"/>observation, <img src="assets/64f15854-2c09-4dc7-8ae2-66cbba67e794.png" style="width:1.33em;height:1.83em;"/>the mean, and <img src="assets/e519bc15-4a32-4535-ad1a-f4803d95c3ea.png" style="width:1.33em;height:1.58em;"/>the standard deviation of the series.<br/></span></p>
<div class="packt_infobox"><span>For more information refer, Standard score article (Z-score). Wikipedia.Available at: <a href="https://en.wikipedia.org/wiki/Standard_score">https://en.wikipedia.org/wiki/Standard_score</a>.<br/></span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Point-Relative Normalization</h1>
                </header>
            
            <article>
                
<p>This normalization computes the difference of a given observation in relation to the fist observation of the series. This kind of normalization is useful to identify trends in relation to a starting point. The point-relative normalization is defined by:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/77bd28e6-aa6b-44ad-94b6-d4a342a0d074.png" style="width:16.17em;height:6.50em;"/></p>
<p>Here, <img src="assets/9d1a43fa-2b2e-4f4c-8b23-362390dae545.png" style="width:1.67em;height:2.08em;"/>is the <img src="assets/ad642e62-6059-4768-b872-19f70caa4a88.png" style="width:2.08em;height:1.83em;"/>observation and<img src="assets/77b76bcb-3fbd-43e7-8c5d-ed149d7e7afb.png" style="width:2.33em;height:2.25em;"/> is the fist observation of the series.</p>
<div class="packt_infobox">As suggested by Siraj Raval in his video, <em>How to Predict Stock Prices Easily Intro to Deep Learning</em> <em>#7</em>, available on YouTube at: <a href="https://www.youtube.com/watch?v=ftMq5ps503w">https://www.youtube.com/watch?v=ftMq5ps503w</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Maximum and Minimum Normalization</h1>
                </header>
            
            <article>
                
<p>This normalization computes the distance between a given observation and the maximum and minimum values of the series. This normalization is useful when working with series in which the maximum and minimum values are not outliers and are important for future predictions.</p>
<p>This normalization technique can be applied with:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/553451e1-0d79-4ddb-a484-4501ea4298dd.png" style="width:18.42em;height:5.67em;"/></p>
<p>Here, <img src="assets/0d60a1c3-779b-4517-a09f-5fc81d3f41d0.png" style="width:2.00em;height:2.50em;"/>is the <img src="assets/4c8b2958-c7c3-4b0e-a6e9-88d3aeee18ca.png" style="width:2.00em;height:1.83em;"/>observation, <em>O</em> represents a vector with all O values, and the functions min (O) and max (O) represent the minimum and maximum values of the series, respectively.</p>
<p>During next Activity, Exploring the Bitcoin Dataset and Preparing Data for Model, we will prepare available Bitcoin data to be used in our LSTM mode. That includes selecting variables of interest, selecting a relevant period, and applying the preceding point-relative normalization technique.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Structuring Your Problem</h1>
                </header>
            
            <article>
                
<p>Compared to researchers, practitioners spend much less time determining which architecture to choose when starting a new deep learning project. Acquiring data that represents a given problem correctly is the most important factor to consider when developing these systems, followed by the understanding of the datasets inherent biases and limitations.</p>
<p>When starting to develop a deep learning system, consider the following questions for reflection:</p>
<ul>
<li><span><strong>Do I have the right data?</strong> This is the hardest challenge when training a deep learning model. First, define your problem with mathematical rules. Use precise definitions and organize the problem in either categories (classification problems) or a continuous scale (regression problems). Now, how can you collect data about those metrics?<br/></span></li>
</ul>
<ul>
<li><strong>Do I have enough data?</strong> Typically, deep learning algorithms have shown to perform much better in large datasets than in smaller ones. Knowing how much data is necessary to train a high-performance algorithm depends on the kind of problem you are trying to address, but aim to collect as much data as you can.</li>
<li><strong>Can I use a pre-trained model?</strong> If you are working on a problem that is a subset of a more general application—but within the same domain—consider using a pretrained model. Pretrained models can give you a head start on tackling the specific patterns of your problem, instead of the more general characteristics of the domain at large. A good place to start is the official TensorFlow repository  <span>(<a href="https://github.com/tensorflow/models">https://github.com/tensorflow/models</a>).<br/></span></li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img src="assets/40b2f22f-501b-4608-b702-668971502661.png" style="width:25.92em;height:44.50em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 5: Decision-tree of key reflection questions to be made at the beginning of a deep learning project<br/></span></div>
<p>In certain circumstances, data may simply not be available. Depending on the case, it may be possible to use a series of techniques to effectively create more data from your input data. This process is known as <strong>data augmentation</strong> and has successful application when working with image recognition problems.</p>
<div class="packt_infobox">A good reference is the article Classifying plankton with deep neural networks available at <a href="http://benanne.github.io/2015/03/17/plankton.html">http://benanne.github.io/2015/03/17/plankton.html</a>. The authors show a series of techniques for augmenting a small set of image data in order to increase the number of training samples the model has.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Activity:Exploring the Bitcoin Dataset and Preparing Data for Model</h1>
                </header>
            
            <article>
                
<p>We will be using a public dataset originally retrieved from CoinMarketCap, a popular website that tracks different cryptocurrency statistics. The dataset has been provided alongside this chapter and will be used.</p>
<p>We will be exploring the dataset using Jupyter Notebooks. Jupyter Notebooks provide Python sessions via a web-browser that allows you to work with data interactively. They are a popular tool for exploring datasets. They will be used in activities throughout this book.</p>
<p><span>Using your terminal, navigate to the directory <kbd>Chapter_5/activity_3</kbd> and execute the following command to start a Jupyter Notebook instance:<br/></span></p>
<pre>     <span>$ jupyter notebook </span></pre>
<p>Now, open the URL provided by the application in your browser. You should be able to see a Jupyter Notebook page with a number of directories from your file system. You should see the following output:</p>
<p><img src="assets/6f2a7e42-7770-466a-b1bd-4d1232d1d6e9.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 6: Terminal image after starting a Jupyter Notebook instance. Navigate to the URL show in a browser, and you should be able to see the Jupyter Notebook landing page.<br/></span></div>
<p>Now, navigate to the directories and click on the file Activity <kbd>Exploring_Bitcoin_ Dataset.ipynb</kbd>. This is a Jupyter Notebook file that will be opened in a new browser tab. The application will automatically start a new Python interactive session for you.</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d520e1b2-f642-4877-9885-3a6d2bf9ab1b.png" style="width:69.08em;height:44.75em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 7: Landing page of your Jupyter Notebook instance<br/></span></div>
<div class="CDPAlignCenter CDPAlign"><img src="assets/0eeb900e-4592-4edd-b97b-2f3516e4b4c9.png" style="width:68.75em;height:42.17em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 8: Image of the Notebook Activity_Exploring_Bitcoin_Dataset.ipynb. You can now interact with that Notebook and make modifications.<br/></span></div>
<p>After opening our Jupyter Notebook, let's now explore the Bitcoin data made available with this chapter.</p>
<p>The dataset <kbd>data/bitcoin_historical_prices.csv</kbd> contains measurements of Bitcoin prices since early 2013. The most recent observation is on November 2017—the dataset comes from <kbd>CoinMarketCap</kbd>, an online service that is updated daily. It contains eight variables, two of which (date and week) describe a time period of the data—these can be used as indices—and six others (<kbd>open</kbd>, <kbd>high</kbd>, <kbd>low</kbd>, <kbd>close</kbd>, <kbd>volume</kbd>, and <kbd>market_ capitalization</kbd>) that can be used to understand how the price and value of Bitcoin has changed over time:</p>
<table style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td style="width: 27.3959%"><strong><span>Variable<br/></span></strong></td>
<td style="width: 72.0232%"><strong><span>Description<br/></span></strong></td>
</tr>
<tr>
<td style="width: 27.3959%"><span><kbd>date</kbd><br/></span></td>
<td style="width: 72.0232%"><span>Date of the observation.<br/></span></td>
</tr>
<tr>
<td style="width: 27.3959%"><span><kbd>iso_week</kbd><br/></span></td>
<td style="width: 72.0232%"><span>Week number for a given year.<br/></span></td>
</tr>
<tr>
<td style="width: 27.3959%"><span><kbd>open</kbd><br/></span></td>
<td style="width: 72.0232%"><span>Open value for a single Bitcoin coin.<br/></span></td>
</tr>
<tr>
<td style="width: 27.3959%"><span><kbd>high</kbd><br/></span></td>
<td style="width: 72.0232%"><span>Highest value achieved during a given day period.<br/></span></td>
</tr>
<tr>
<td style="width: 27.3959%"><span><kbd>low</kbd><br/></span></td>
<td style="width: 72.0232%"><span>Lowest value achieved during a given day period.<br/></span></td>
</tr>
<tr>
<td style="width: 27.3959%"><span><kbd>close</kbd><br/></span></td>
<td style="width: 72.0232%"><span>Value at the close of the transaction day.<br/></span></td>
</tr>
<tr>
<td style="width: 27.3959%"><span><kbd>volume</kbd><br/></span></td>
<td style="width: 72.0232%"><span>The total volume of Bitcoin that was exchanged during that day.<br/></span></td>
</tr>
<tr>
<td style="width: 27.3959%"><span><kbd>market_capitalization</kbd><br/></span></td>
<td style="width: 72.0232%"><span>Market capitalization, which is explained by Market Cap = Price *Circulating Supply.<br/></span></td>
</tr>
</tbody>
</table>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Table 2: Available variables (that is, columns) in the Bitcoin historical prices dataset<br/></span></div>
<p>Using the open Jupyter Notebook instance, let's now explore the time-series of two of those variables: <kbd>close</kbd> and <kbd>volume</kbd>. We will start with those time-series to explore price-fluctuation patterns.</p>
<p>Navigate to the open instance of the Jupyter Notebook Activity <kbd>Exploring_Bitcoin_ Dataset.ipynb</kbd>. Now, execute all cells under the header Introduction. This will import the required libraries and import the dataset into memory.</p>
<p>After the dataset has been imported into memory, move to the Exploration section. You will find a snippet of code that generates a time-series plot for the close variable. Can you generate the same plot for the <kbd>volume</kbd> variable?</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f68fa8a7-9123-44f9-93fd-afbe5ac1a1b3.png" style="width:70.17em;height:51.75em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 9: Time-series plot of the closing price for Bitcoin from the close variable. Reproduce this plot,but using t</span><span>he volume variable </span><span>in a new cell below this one.</span></div>
<p>You will have most certainly noticed that both variables surge in 2017. This reflects the current phenomenon that both the prices and value of Bitcoin have been continuously growing since the beginning of that year.</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6d7278be-375b-4533-b4b7-22f1cd2daf27.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 10: Closing price of Bitcoin coins in USD. Notice an early spike by late 2013 and early 2014. Also, notice how the recent prices have skyrocketed since the beginning of 2017.<span><br/></span></div>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c239d72f-ae1c-4050-b983-c461063fde59.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 11: The volume of transactions of Bitcoin coins (in USD) shows that starting in 2017, a trend starts in which a significantly larger amount of Bitcoin is being transacted in the market. The total daily volume varies much more than daily closing prices.<span><br/></span></div>
<p>Also, we notice that for many years, Bitcoin prices did not fluctuate as much as in recent years. While those periods can be used by a neural network to understand certain patterns, we will be excluding older observations, given that we are interested in predicting future prices for not-too-distant periods. Let's filter the data for 2016 and 2017 only.</p>
<p>Navigate to the Preparing Dataset for Model section. We will use the pandas API for filtering the data for the years 2016 and 2017. Pandas provides an intuitive API for performing this operation:</p>
<pre>     bitcoin_recent = bitcoin[bitcoin['date'] &gt;= '2016-01-01']</pre>
<p>The variable <kbd>bitcoin_recent</kbd> now has a copy of our original bitcoin dataset, but filtered to the observations that are newer or equal to January 1, 2016.</p>
<p>As our final step, we now normalize our data using the point-relative normalization technique described in the <em>Data Normalization</em> section. We will only normalize two variables (close and volume), because those are the variables that we are working to predict.</p>
<p>In the same directory containing this chapter, we have placed a script called <kbd>normalizations.py</kbd>. That script contains the three normalization techniques described in this chapter. We import that script into our Jupyter Notebook and apply the functions to our series.</p>
<p>Navigate to the Preparing Dataset for Model section. Now, use the <kbd>iso_week</kbd> variable to group all the day observations from a given week using the pandas method <kbd>groupby()</kbd> . We can now apply the normalization function <kbd>normalizations.point_relative_ normalization()</kbd> directly to the series within that week. We store the output of that normalization as a new variable in the same pandas dataframe using:</p>
<pre><span>     bitcoin_recent['close_point_relative_normalization'] =<br/>     bitcoin_recent.groupby('iso_week')['close'].apply(<br/>     lambda x: normalizations.point_relative_normalization(x))</span></pre>
<p>The variable <kbd>close_point_relative_normalization</kbd> now contains the normalized data for the variable <kbd>close</kbd>. Do the same with the variable <kbd>volume</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d8daa1c0-9449-464b-abeb-05250c0ac72c.png" style="width:51.25em;height:33.92em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 12: Image of Jupyter Notebook focusing on section where the normalization function is applied.<br/></span></div>
<p>The normalized close variable contains an interesting variance pattern every week. We will be using that variable to train our LSTM model.</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/8412f325-783f-478b-8862-adc14683b416.png" style="width:51.08em;height:16.42em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 13: Plot that displays the series from the normalized variable close_point_relative_normalization.<br/></span></div>
<p>In order to evaluate how well our model performs, we need to test its accuracy versus some other data. We do that by creating two datasets: a training set and a test set. In this activity, we will use 80 percent of the dataset to train our LSTM model and 20 percent to evaluate its performance.</p>
<p>Given that the data is continuous and in the form of a time series, we use the last 20 percent of available weeks as a test set and the fist 80 percent as a training set:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/ad7ee5d6-370e-49b2-bad4-3871d4146d5b.png" style="width:70.92em;height:46.83em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 14: Using weeks to create a training and a test set<br/></span></div>
<p><span>Finally, navigate to the Storing Output section and save the filtered variable to disk, as follows:</span></p>
<pre><span>     test_dataset.to_csv('data/test_dataset.csv', index=False)<br/>     train_dataset.to_csv('data/train_dataset.csv', index=False)<br/>     bitcoin_recent.to_csv('data/bitcoin_recent.csv', index=False)</span></pre>
<p>In this section, we explored the Bitcoin dataset and prepared it for a deep learning model.</p>
<p>We learned that during the year 2017, the prices of Bitcoin skyrocketed. This phenomenon takes a long time to take place—and may be influenced by a number of external factors that this data alone doesn't explain (for instance, the emergence of other cryptocurrencies). We also used the point-relative normalization technique to process the Bitcoin dataset in weekly chunks. We do this to train an LSTM network to learn the weekly patterns of Bitcoin price changes so that it can predict a full week into the future. However, Bitcoin statistics show significant fluctuations on a weekly basis. Can we predict the price of Bitcoin in the future?</p>
<p>What will those prices be seven days from now? We will be building a deep learning model to explore that question in our next section using Keras.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using Keras as a TensorFlow Interface</h1>
                </header>
            
            <article>
                
<p>This section focuses on Keras. We are using Keras because it simplifies the TensorFlow interface into general abstractions. In the backend, the computations are still performed in TensorFlow—and the graph is still built using TensorFlow components—but the interface is much simpler. We spend less time worrying about individual components, such as variables and operations, and spend more time building the network as a computational unit. Keras makes it easy to experiment with different architectures and hyperparameters, moving more quickly towards a performant solution.</p>
<p>As of TensorFlow 1.4.0 (November 2017), Keras is now officially distributed with TensorFlow as <kbd>tf.keras</kbd>. This suggests that Keras is now tightly integrated with TensorFlow and that it will likely continue to be developed as an open source tool for a long period of time.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model Components</h1>
                </header>
            
            <article>
                
<p>As we have seen in C<em>hapter 4</em>, <em>Introduction to Neural Networks and Deep Learning</em>, LSTM networks also have input, hidden, and output layers. Each hidden layer has an activation function which evaluates that layer's associated weights and biases. As expected, the network moves data sequentially from one layer to another and evaluates the results by the output at every iteration (that is, an epoch).</p>
<p>Keras provides intuitive classes that represent each one of those components:</p>
<table style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td style="width: 48.3553%"><strong><span>Component<br/></span></strong></td>
<td style="width: 48.6447%"><strong><span>Keras Class<br/></span></strong></td>
</tr>
<tr>
<td style="width: 48.3553%"><span>High-level abstraction of a complete sequential neural network.<br/></span></td>
<td style="width: 48.6447%"><span><kbd>keras.models.Sequential()</kbd><br/></span></td>
</tr>
<tr>
<td style="width: 48.3553%"><span>Dense, fully-connected layer.<br/></span></td>
<td style="width: 48.6447%"><span><kbd>keras.layers.core.Dense()</kbd><br/></span></td>
</tr>
<tr>
<td style="width: 48.3553%"><span>Activation function.<br/></span></td>
<td style="width: 48.6447%"><span><kbd>keras.layers.core.Activation()</kbd><br/></span></td>
</tr>
<tr>
<td style="width: 48.3553%"><span>LSTM recurrent neural network. This class contains components that are exclusive to this architecture, most of which are abstracted by Keras.<br/></span></td>
<td style="width: 48.6447%"><span><kbd>keras.layers.recurrent.LSTM()</kbd><br/></span></td>
</tr>
</tbody>
</table>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Table 3: Description of key components from the Keras API. We will be using these components to build a deep learning model.<br/></span></div>
<p>Keras' <kbd>keras.models.Sequential()</kbd> component represents a whole sequential neural network. That Python class can be instantiated on its own, then have other components added to it subsequently.</p>
<p>We are interested in building an LSTM network because those networks perform well with sequential data—and time-series is a kind of sequential data. Using Keras, the complete LSTM network would be implemented as follows:</p>
<pre><span>     from keras.models import Sequential<br/>     from keras.layers.recurrent import LSTM<br/>     from keras.layers.core import Dense, Activation<br/><br/>     model = Sequential()<br/><br/>     model.add(LSTM(<br/>     units=number_of_periods,<br/>     input_shape=(period_length, number_of_periods)<br/>     return_sequences=False), stateful=True)<br/>     model.add(Dense(units=period_length))<br/>     model.add(Activation("linear"))<br/>     model.compile(loss="mse", optimizer="rmsprop")<br/></span></pre>
<div class="packt_figref CDPAlignCenter CDPAlign"><em>Snippet 1</em><span>: LSTM implementation using Keras<br/></span></div>
<p><span>This implementation will be further optimized in <em>Chapter 6</em>, <em>Model Evaluation and Optimization</em>.<br/></span></p>
<p>Keras abstraction allows for one to focus on the key elements that make a deep learning system more performant: what the right sequence of components is, how many layers and nodes to include, and which activation function to use. All of these choices are determined by either the order in which components are added to the instantiated <kbd>keras.models. Sequential()</kbd> class or by parameters passed to each component instantiation (that is, <kbd>Activation("linear")</kbd> ). The final model<kbd>.compile()</kbd> step builds the neural network using TensorFlow components.</p>
<p>After the network is built, we train our network using the <kbd>model.fit()</kbd> method. This will yield a trained model that can be used to make predictions:</p>
<pre><span>     model.fit(<br/>     X_train, Y_train,<br/>     batch_size=32, epochs=epochs)<br/></span></pre>
<div class="packt_figref CDPAlignCenter CDPAlign"><em>Snippet</em> 2.1: Usage of <kbd>model.fit()</kbd><span><br/></span></div>
<p>The variables <kbd>X_train</kbd> and <kbd>Y_train</kbd> are, respectively, a set used for training and a smaller set used for evaluating the loss function (that is, testing how well the network predicts data).</p>
<p>Finally, we can make predictions using the <kbd>model.predict()</kbd> method:</p>
<pre>      <span>model.predict(x=X_train)</span></pre>
<div class="CDPAlignCenter packt_figref CDPAlign"><em>Snippet</em> 2.2: Usage of <kbd>model.predict()</kbd><span><br/></span></div>
<p>The previous steps cover the Keras paradigm for working with neural networks. Despite the fact that different architectures can be dealt with in very different ways, Keras simplifies the interface for working with different architectures by using three components - network architecture, fit, and predict:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b5e7c37a-bb2b-4af7-91f9-5ad1c648a012.png" style="width:37.08em;height:6.92em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 15: The Keras neural network paradigm: A. design a neural network architecture, B. Train a neural network (or Fit), and C. Make predictions</div>
<p>Keras allows for much greater control within each of those steps. However, its focus is to make it as easy as possible for users to create neural networks in as little time as possible. That means that we can start with a simple model, then add complexity to each one of the steps above to make that initial model perform better.</p>
<p>We will take advantage of that paradigm during our upcoming activity and chapters. In the next activity, we will create the simplest LSTM network possible. Then, in C<em>hapter 6</em>, <em>Model Evaluation and Optimization</em>, we will continuously evaluate and alter that network to make it more robust and performant.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Activity:Creating a TensorFlow Model Using Keras</h1>
                </header>
            
            <article>
                
<p>In this activity, we will create an LSTM model using Keras.</p>
<p>Keras serves as an interface for lower-level programs; in this case, TensorFlow. When we use Keras to design our neural network, that neural network is <em>compiled</em> as a TensorFlow computation graph.</p>
<p>Navigate to the open instance of the Jupyter Notebook <kbd>Activity_4_Creating_a_ TensorFlow_Model_Using_Keras.ipynb</kbd>. Now, execute all cells under the header <strong>Building a Model</strong>. In that section, we build our fist LSTM model parametrizing two values: the input size of the training observation (1 equivalent for a single day) and the output size for the predicted period—in our case, seven days:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/3cc954a0-193b-4adb-9fa2-fc4ccf80e5fb.png" style="width:68.83em;height:45.58em;"/></p>
<p>Use the Jupyter Notebook <kbd>Activity_4_Creating_a_TensorFlow_Model_Using_Keras.ipynb</kbd> to build the same model from the <em>Model Components</em> section, parametrizing the period length of input and of output to allow for experimentation.</p>
<p>After the model is compiled, we proceed to storing it as an <kbd>h5 file</kbd> on disk. It is a good practice to store versions of your model on disk occasionally, so that you keep a version of the model architecture alongside its predictive capabilities.</p>
<p>Still on the same Jupyter Notebook, navigate to the header <strong>Saving Model</strong>. In that section, we will store the model as a file on disk with the following command:</p>
<pre><span>     model.save('bitcoin_lstm_v0.h5')</span></pre>
<p><span>The model '<kbd>bitcoin_lstm_v0.h5</kbd>' hasn't been trained yet. When saving a model without prior training, one effectively only saves the architecture of the model. That same model can later be loaded by using Keras' <kbd>load_model()</kbd> function, as follows:<br/></span></p>
<pre><span>     1 model = keras.models.load_model('bitcoin_lstm_v0.h5')</span></pre>
<div class="packt_infobox">You may encounter the following warning when loading the Keras library: Using TensorFlow backend. Keras can be configured to use an other backend instead of TensorFlow (that is, Theano). In order to avoid this message, you can create a file called <kbd>keras.json</kbd> and configure its backend there. The correct configuration of that file depends on your system. Hence, it is recommended that you visit Keras' official documentation on the topic at <a href="https://keras.io/backend/">https://keras.io/backend/</a><a href="https://keras.io/backend/">.</a></div>
<p>In this section, we have learned how to build a deep learning model using Keras, an interface for TensorFlow. We studied core components from Keras and used those components to build the fist version of our Bitcoin price-predicting system based on an LSTM model.</p>
<p>In our next section, we will discuss how to put all the components from this chapter together into a (nearly complete) deep learning system. That system will yield our very fist predictions, serving as a starting point for future improvements.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">From Data Preparation to Modeling</h1>
                </header>
            
            <article>
                
<p>This section focuses on the implementation aspects of a Deep Learning system. We will use the Bitcoin data from, <em>Choosing the Right Model Architecture</em> and the Keras knowledge from, <em>Using Keras as a TensorFlow Interface</em> to put both of these components together. This section concludes the chapter by building a system that reads data from a disk and feeds it into a model as a single piece of software.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training a Neural Network</h1>
                </header>
            
            <article>
                
<p>Neural networks can take long periods of time to train. Many factors affect how long that process may take. Among them, three factors are commonly considered the most important:</p>
<ul>
<li>The network's architecture</li>
<li>How many layers and neurons the network has</li>
<li>How much data there is to be used in the training process</li>
</ul>
<p>Other factors may also greatly impact how long a network takes to train, but most of the optimization that a neural network can have when addressing a business problem comes from exploring those three.</p>
<p>We will be using the normalized data from our previous section. Recall that we have stored the training data in a file called <kbd>train_dataset.csv</kbd>. We will load that dataset into memory using pandas for easy exploration:</p>
<pre><span>    import pandas as pd<br/>    train = pd.read_csv('data/train_dataset.csv')</span></pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/a32c3f11-9d76-4325-8350-37b8eefd48eb.png" style="width:81.58em;height:19.25em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 17: Table showing the first five rows of the training dataset loaded from the <kbd>train_d–ataset.csv</kbd> file<br/></span></div>
<p>We will be using the series from the variable <kbd>close_point_relative_normalization</kbd>, which is a normalized series of the Bitcoin closing prices—from the variable close—since the beginning of 2016.</p>
<p>The variable <kbd>close_point_relative_normalization</kbd> has been normalized on a weekly basis. Each observation from the week's period is made relative to the difference from the closing prices on the fist day of the period. This normalization step is important and will help our network train faster.</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b3c97bde-f62a-43c2-8f94-5377459c1fb9.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 18: Plot that displays the series from the normalized variable close_point_relative_normalization. This variable will be used to train our LSTM model.<span><br/></span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reshaping Time-Series Data</h1>
                </header>
            
            <article>
                
<p>Neural networks typically work with vectors and tensors, both mathematical objects that organize data in a number of dimensions. Each neural network implemented in Keras will have either a vector or a tensor that is organized according to a specification as input. At fist, understanding how to reshape the data into the format expected by a given layer can be confusing. To avoid confusion, it is advised to start with a network with as little components as possible, then add components gradually. Keras' official documentation (under the section <strong>Layers</strong>) is essential for learning about the requirements for each kind of layer.</p>
<div class="packt_infobox">The Keras official documentation is available at <a href="https://keras.io/layers/core/">https://keras.io/ layers/core/</a>. That link takes you directly to the Layers section.</div>
<div class="packt_infobox"><kbd>NumPy</kbd> is a popular Python library used for performing numerical computations. It is used by the deep learning community to manipulate vectors and tensors and prepare them for deep learning systems. In particular, the <kbd>numpy.reshape()</kbd> method is very important when adapting data for deep learning models. That model allows for the manipulation of NumPy arrays, which are Python objects analogous to vectors and tensors.</div>
<p>We now organize the prices from the variable <kbd>close_point_relative_normalization</kbd> using the weeks of both 2016 and 2017. We create distinct groups containing seven observations each (one for each day of the week) for a total of 77 complete weeks.</p>
<p>We do that because we are interested in predicting the prices of a week's worth of trading.</p>
<div class="packt_infobox">We use the ISO standard to determine the beginning and the end of a week. Other kinds of organizations are entirely possible. This one is simple and intuitive to follow, but there is room for improvement.</div>
<p>LSTM networks work with three-dimensional tensors. Each one of those dimensions represents an important property for the network. These dimensions are:</p>
<ul>
<li><strong>Period length</strong>: The period length, that is, how many observations there are on a period</li>
<li><strong>Number of periods</strong>: How many periods are available in the dataset</li>
<li><strong>Number of features</strong>: Number of features available in the dataset</li>
</ul>
<p>Our data from the variable <kbd>close_point_relative_normalization</kbd> is currently a one dimensional vector—we need to reshape it to match those three dimensions.</p>
<p>We will be using a week's period. Hence, our period length is seven days (period length = 7). We have 77 complete weeks available in our data. We will be using the very last of those weeks to test our model against during its training period. That leaves us with 76 distinct weeks (number of periods = 76). Finally, we will be using a single feature in this network (number of features = 1)—we will include more features in future versions.</p>
<p>How can we reshape the data to match those dimensions? We will be using a combination of base Python properties and the <kbd>reshape()</kbd> from the <kbd>numpy library</kbd>. First, we create the 76 distinct week groups with seven days each using pure Python:</p>
<pre><span>     group_size = 7<br/>     samples = list()<br/>     for i in range(0, len(data), group_size):<br/>     sample = list(data[i:i + group_size])<br/></span><span>     if len(sample) == group_size:<br/>     samples.append(np.array(sample).reshape(group_size, 1).tolist())<br/><br/>     data = np.array(samples) </span></pre>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Snippet 3: Python code snippet that creates distinct week groups<br/></span></div>
<p>The resulting variable data is a variable that contains all the right dimensions. The Keras LSTM layer expects these dimensions to be organized in a specific order: number of features, number of observations, and period length.</p>
<p>Let's reshape our dataset to match that format:</p>
<pre><span>     X_train = data[:-1,:].reshape(1, 76, 7)<br/>     Y_validation = data[-1].reshape(1, 7)</span></pre>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Snippet 4: Python code snippet that creates distinct week groups<br/></span></div>
<div class="packt_infobox">Each Keras layer will expect its input to be organized in specific ways. However, Keras will reshape data accordingly in most cases. Always refer to the Keras documentation on layers (<a href="https://keras.io/layers/core/">https://keras.io/layers/ core/</a>) before adding a new layer or if you encounter issues with the shape layers expect.</div>
<p><em>Snippet 4</em> also selects the very last week of our set as a validation set (<kbd>via data[-1]</kbd> ). We will be attempting to predict the very last week in our dataset by using the preceding 76 weeks. The next step is to use those variables to fit our model:</p>
<pre>      <span>model.fit(x=X_train, y=Y_validation, epochs=100) </span></pre>
<div class="CDPAlignCenter CDPAlign packt_figref"><em>Snippet 5</em>: Snippet shows how to train our model<span><br/></span></div>
<p>LSTMs are computationally expensive models. They may take up to file minutes to train with our dataset in a modern computer. Most of that time is spent at the beginning of the computation, when the algorithm creates the full computation graph. The process gains speed after it starts training:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/01635e04-55ec-49fd-9de8-911f20ea04f7.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 19: Graph that shows the results of the loss function evaluated at each epoch<br/></span></div>
<div class="packt_infobox">This compares what the model predicted at each epoch, then compares with the real data using a technique called mean-squared error. This plot shows those results.</div>
<p><span>At a glance, our network seems to perform very well: it starts with a very small error rate that continuously decreases. Now, what do our predictions tell us?<br/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Making Predictions</h1>
                </header>
            
            <article>
                
<p>After our network has been trained, we can now proceed to making predictions. We will be making predictions for a future week beyond our time period.</p>
<p>Once we have trained our model with model.fit(), making predictions is trivial:</p>
<pre><span>     model.predict(x=X_train)</span></pre>
<div class="CDPAlignCenter CDPAlign packt_figref"><em>Snippet 6</em>: Making a prediction using the same data that we previously used for training</div>
<p>We use the same data for making predictions as the data used for training (<kbd>the X_train variable</kbd>). If we have more data available, we can use that instead—given that we reshape it to the format the LSTM requires.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Overfitting</h1>
                </header>
            
            <article>
                
<p>When a neural network overfits to a validation set, it means that it learns patterns present in the training set, but is unable to generalize it to unseen data (for instance, the test set). During our next chapter, we will learn how to avoid over-fitting and create a system for both evaluating our network and increasing its performance:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f4b1d78c-54be-448e-aa90-9d0cb95d865a.png" style="width:68.42em;height:20.75em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 20: After de-normalization, our LSTM model predicted that in late July 2017, the prices of Bitcoin would increase from $2,200 to roughly $2,800, a 30 percent increase in a single week</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Activity:Assembling a Deep Learning System</h1>
                </header>
            
            <article>
                
<p>In this activity, we bring together all the essential pieces for building a basic deep learning system: data, model, and prediction.</p>
<p>We will continue to use Jupyter Notebooks, and will use the data prepared in previous exercises (<kbd>data/train_dataset.csv</kbd>) as well as the model that we stored locally (<kbd>bitcoin_ lstm_v0.h5</kbd>).</p>
<ol>
<li>After starting a Jupyter Notebook instance, navigate to the Notebook called <kbd>Activity_5_Assembling_a_Deep_Learning_System.ipynb</kbd> and open it. Execute the cells from the header to load the required components and then navigate to the header <strong>Shaping Data</strong>:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/bc02ce10-84f8-40b3-886a-1720b68227ec.png" style="width:73.00em;height:44.75em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 21: Plot that displays the series from the normalized variable close_point_relative_normalization</div>
<div class="packt_infobox">The <kbd>close_point_relative_normalization</kbd> variable will be used to train our LSTM model.</div>
<p style="padding-left: 60px"><span>We will start by loading the dataset we prepared during our previous activities. We use pandas to load that dataset into memory.<br/></span></p>
<ol start="2">
<li><span>Load the training dataset into memory using pandas, as follows:</span></li>
</ol>
<pre><span>      train = pd.read_csv('data/train_dataset.csv')</span></pre>
<ol start="3">
<li>Now, quickly inspect the dataset by executing the following command:</li>
</ol>
<pre class="mce-root">      train.head()</pre>
<p style="padding-left: 60px">As explained in this chapter, LSTM networks require tensors with three dimensions. These dimensions are: period length, number of periods, and number of features.</p>
<p style="padding-left: 60px">Now, proceed to creating weekly groups, then rearrange the resulting array to match those dimensions.</p>
<ol start="4">
<li>Feel free to use the provided function <kbd>create_groups()</kbd> to perform this operation:</li>
</ol>
<pre>       create_groups(data=train, group_size=7)</pre>
<p style="padding-left: 60px">The default values for that function are 7 days. What would happen if you changed that number to a different value, for instance, 10?</p>
<p style="padding-left: 60px">Now, make sure to the data into two sets: training and validation. We do this by assigning the last week from the Bitcoin prices dataset to the evaluation set. We then train the network to evaluate that last week.</p>
<p style="padding-left: 60px">Separate the last week of the training data and reshape it using <kbd>numpy.reshape()</kbd> . Reshaping is important, as the LSTM model only accepts data organized in this way:</p>
<pre>       X_train = data[:-1,:].reshape(1, 76, 7)<br/>       Y_validation = data[-1].reshape(1, 7)</pre>
<p style="padding-left: 60px">Our data is now ready to be used in training. Now we load our previously saved model and train it with a given number of epochs.</p>
<ol start="5">
<li>Navigate to the header <strong>Load Our Model</strong> and load our previously trained model:</li>
</ol>
<pre>      model = load_model('bitcoin_lstm_v0.h5')</pre>
<ol start="6">
<li><span>And now, train that model with our training data <kbd>X_train</kbd> and <kbd>Y_validation</kbd>:</span></li>
</ol>
<pre><span>      history = model.fit(<br/>      x=X_train, y=Y_validation,<br/>      batch_size=32, epochs=100)</span></pre>
<p class="mce-root" style="padding-left: 60px"><span>Notice that we store the logs of the model in a variable called history. The model logs are useful for exploring specific variations in its training accuracy and to understand how well the loss function is performing:<br/></span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/019c6080-5184-4600-a077-cc5315e19925.png" style="width:63.83em;height:44.00em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 22: Section of Jupyter Notebook where we load our earlier model and train it with new data</div>
<div style="padding-left: 60px"><span><span>Finally, let's make a prediction with our trained model.</span></span></div>
<ol start="7">
<li><span>Using the same <kbd>data X_train</kbd>, call the following method:</span></li>
</ol>
<pre><span>      model.predict(x=X_train)</span></pre>
<ol start="8">
<li>The model immediately returns a list of normalized values with the prediction for the next seven days. Use the <kbd>denormalize()</kbd> function to turn the data into US Dollar values. Use the latest values available as a reference for scaling the predicted results:</li>
</ol>
<pre>       <span>denormalized_prediction = denormalize(predictions, last_weeks_value)</span></pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/020c46f7-38af-4919-a8b8-d7ce2ede143a.png" style="width:54.67em;height:33.33em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 23: Section of Jupyter Notebook where we predict the prices of Bitcoin for the next seven days.</div>
<div class="CDPAlignCenter CDPAlign packt_figref">Our predictions suggest a great price surge of about 30 percent.</div>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b15b2b04-3e9c-4349-8c52-f6b4a8660e64.png" style="width:63.33em;height:18.83em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 24: Projection of Bitcoin prices for seven days in the future using the LSTM model we just built<span><br/></span></div>
<div class="packt_infobox">We combine both time-series in this graph: the real data (before the line) and the predicted data (after the line). The model shows variance similar to the patterns seen before and it suggests a price increase during the following seven days period.</div>
<ol start="9">
<li>After you are done experimenting, save your model with the following command:</li>
</ol>
<pre>      model.save('bitcoin_lstm_v0_trained.h5')</pre>
<p style="padding-left: 60px">We will save this trained network for future reference and compare its performance with other models.</p>
<p style="padding-left: 60px">The network may have learned patterns from our data, but how can it do that with such a simple architecture and so little data? LSTMs are powerful tools for learning patterns from data. However, we will learn in our next sessions that they can also suffer from <em>overfitting</em>, a phenomenon common in neural networks in which they learn patterns from the training data that are useless when predicting real-world patterns. We will learn how to deal with that and how to improve our network to make useful predictions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have assembled a complete deep learning system: from data to prediction. The model created in this activity needs a number of improvements before it can be considered useful. However, it serves as a great starting point from which we will continuously improve.</p>
<p>Our next chapter will explore techniques for measuring the performance of our model and will continue to make modifications until we reach a model that is both useful and robust.</p>


            </article>

            
        </section>
    </body></html>