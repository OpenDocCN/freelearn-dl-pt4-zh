<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Markov Decision Process</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will talk about another application of HMMs known as <strong>Markov Decision Process</strong> (<strong>MDP</strong>). In the case of MDPs, we introduce a reward to our model, and any sequence of states taken by the process results in a specific reward. We will also introduce the concept of discounts, which will allow us to control how short-sighted or far-sighted we want our agent to be. The goal of the agent would be to maximize the total reward that it can get.</p>
<p>In this chapter, we will be covering the following topics:</p>
<ul>
<li>Reinforcement learning</li>
<li>The Markov reward process</li>
<li>Markov decision processes</li>
<li>Code example</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reinforcement learning</h1>
                </header>
            
            <article>
                
<p>Reinforcement learning is a different paradigm in machine learning where an agent tries to learn to behave optimally in a defined environment by making decisions/actions and observing the outcome of that decision. So, in the case of reinforcement learning, the agent is not really from some given dataset, but rather, by interacting with the environment, the agent tries to learn by observing the effects of its actions. The environment is defined in such a way that the agent gets rewards if its action gets it closer to the goal. </p>
<p>Humans are known to learn in this way. For example, consider a child in front of a fireplace where the child is the agent and the space around the child is the environment. Now, if the child moves its hand towards the fire, it feels the warmth, which feels good and, in a way, the child (or the agent) is rewarded for the action of moving its hand close to the fire. But if the child moves its hand too close to the fire, its hand will burn, hence receiving a negative reward. Using these rewards, the child is able to figure out the optimal distance to keep its hand from the fire. Reinforcement learning tries to imitate exactly this kind of system in order to train the agent to learn to optimize its goal in the given environment. </p>
<p>Making this more formal, to train an agent we will need to have an environment which represents the world in which the agent should be able to take actions. For each of these actions, the environment should return observations which contain information about the reward, telling it how good or bad the action was. The observation should also have information regarding the next state of the agent in the environment. And, based on these observations, the agent tries to figure out the optimal way to reach its goal. <em>Figure 1</em> shows the interaction between an agent and an environment:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6ea82be0-74db-4985-b571-e29668558f2a.png" style="width:28.58em;height:11.00em;"/></p>
<p>The thing that makes reinforcement learning fundamentally different from other algorithms is that there is no supervisor. Instead, there is only a reward signal giving feedback to the agent about the action it took. Another important thing to mention here is that an environment can be constructed in such a way that the reward is delayed, which can make the agent wander around before reaching its goal. It is also possible that the agent might have to go through a lot of negative feedback before reaching its goal. </p>
<p>In the case of supervised learning, we are given the dataset, which basically tells our learning algorithm the right answers in different situations. Our learning algorithm then looks over all these different situations, and the solutions in those cases, and tries to generalize based upon it. Hence, we also expect that the dataset given to use is <strong>independent and identically distributed</strong> (<strong>IID</strong>). But in the case of reinforcement learning the data is not IID, the data generated depends on the path the agent took, and, hence, it depends on the actions taken by the agent. Hence, reinforcement learning is an active learning process in which the actions taken by the agent influence the environment which in turn influences the data generated by the environment. </p>
<p class="mce-root"/>
<p>We can take a very simple example to better understand how a reinforcement learning agent and environment behave. Consider an agent trying to learn to play Super Mario Bros:</p>
<ol>
<li>The agent will receive the initial state from the environment. In the case of Super Mario, this would be the current frame of the game.</li>
<li>Having received the state information, the agent will try to take an action. Let's say the action the agent took is to move to the right.</li>
<li>When the environment receives this action it will return the next state based on it. The next state would also be a frame, but the frame could be of a dying Mario if there was an enemy next to Mario in the previous state. Otherwise, the frame would just have shown Mario to have moved one step to the right. The environment will also return the rewards based on the action. If there was an enemy on the right of Mario the rewards could be, let's say -5 (since the action killed Mario) or could be +1 if the Mario moved towards finishing the level. </li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reward hypothesis</h1>
                </header>
            
            <article>
                
<p>The whole concept of reinforcement learning is based on something called <strong>reward hypothesis</strong>. According to reward hypothesis, all goals can be defined by the maximization of the expected cumulative reward. </p>
<p>At any given time <em>t</em>, the total cumulative reward can be given by:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/2458c36c-959a-4168-adfe-73035ba18602.png" style="width:13.58em;height:5.92em;"/></p>
<p>But, in reality, the rewards which are closer to the current state are more probable that the ones which are further away. To deal with this, we would introduce another term called the <strong>discount rate,</strong> <img class="fm-editor-equation" src="assets/0f5e1c1c-8fea-4597-ad98-a1ffa23cf86e.png" style="width:0.83em;height:1.25em;"/>. The value of the discount rate is always between 0 and 1. A large value of <img class="fm-editor-equation" src="assets/7e86090e-61af-42bb-993e-e105248db956.png" style="width:0.83em;height:1.25em;"/> means a smaller discount, which make the agent care more about the long-term rewards, whereas, for smaller values of <img class="fm-editor-equation" src="assets/058b9431-3af7-4648-b9ea-184921bdc7e9.png" style="width:0.83em;height:1.25em;"/>, the agent cares more about the short-term rewards. With the discount rate term, we can now define our cumulative reward as:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/1e708f38-37af-4060-8246-3ba7c185487e.png" style="width:12.42em;height:4.58em;"/></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">State of the environment and the agent</h1>
                </header>
            
            <article>
                
<p>As we saw earlier, the agent interacts with the environment at intervals of time <em>t = 0, 1, 2, 3, ..,</em> and, at each time instance, the agent gets the state of the environment <em>S<sub>t</sub></em> based on which it takes an action <em>A<sub>t</sub></em> and gets a reward <em>R<sub>t+1</sub></em>. This sequence of state, action and rewards over time is known as the <strong>history of the agen</strong><strong>t</strong> and is given as:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/9ca92514-748f-4bf9-8d99-bad282d7763d.png" style="width:23.25em;height:1.75em;"/></p>
<p>Ideally, we would like the action taken by the agent to be based on its total history, but it is generally unfeasible because the history of the agent can be huge. Therefore, we define the state in a way such that it is a summary of all the history of the agent:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/6698b496-3ddc-40bd-ac20-19db2f9c5c4b.png" style="width:7.50em;height:1.75em;"/></p>
<p>Basically, we are defining the state to be a function of the history of the agent. It's important to notice that the environment state is the state that the environment uses to determine its next state, based on the action, and give out rewards. Also, it is private to the environment.</p>
<p>On the other hand, the agent state is the state that the agent uses to decide on the next action. The agent state is its internal representation, and can be any function of the history as mentioned before.</p>
<p>We use a Markov state to represent an agent's state, which basically means that the current state of the agent is able to summarize all the history, and the next action of the agent will depend only on the current state of the agent. Hence,</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/9eed4f10-981c-4cee-b55f-64575586d4b8.png" style="width:20.92em;height:1.75em;"/></p>
<p>In this chapter, we will only be considering the case when the agent is directly able to observe the environment's state. This results in the observation from the environment to be both the current state of the agent as well as the environment. This special case is also commonly known as <strong>MDP</strong>.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Components of an agent</h1>
                </header>
            
            <article>
                
<p>In this section, we will formally define the different kinds of components that an agent can have.</p>
<ul>
<li class="CDPAlignLeft CDPAlign"><strong>Policy</strong>: It is a conditional over the action given the state. Based on this conditional distribution, the agent chooses its action at any given state. It is possible for the policy to be deterministic: <em>a = π(s)</em> or stochastic: <img class="fm-editor-equation" src="assets/26fa3ff6-449d-4502-ba5a-58f7b5ffbb1a.png" style="width:12.50em;height:1.25em;"/>.</li>
<li><strong>Value function</strong>: The value function tries to predict the reward to expect on taking a given action in a given state. It is given as:<br/>
<img class="fm-editor-equation" src="assets/9cd0fb06-8243-45fc-8474-230d7d871484.png" style="width:19.92em;height:1.25em;"/>.<br/>
where <em>E</em> is the expectation and <img class="fm-editor-equation" src="assets/06b33013-eeab-4b47-92e5-64ae385ff31e.png" style="width:0.83em;height:1.25em;"/> is the discount factor.</li>
<li><strong>Model</strong>: The model is the agent's representation of the environment. It is defined using a transition function <em>P</em> which predicts the next state of the environment:<br/>
<img class="fm-editor-equation" src="assets/55f70d9a-aaef-4be0-beee-f6f8cdc0a70a.png" style="width:13.92em;height:1.25em;"/>.<br/>
and a reward function which predicts the reward associated with any given action at a given state: <img class="fm-editor-equation" src="assets/87664401-62c1-42c9-a764-6582d6bfa877.png" style="width:15.75em;height:1.50em;"/>.</li>
</ul>
<p>Based on these components, agents can be classified into the following five categories:</p>
<ul>
<li><strong>Value-based agents</strong>: <span>Have an i</span>mplicit policy and the agent takes decisions for actions based on the value function</li>
<li><strong>Policy-based agents</strong>: <span>Have an e</span>xplicit policy and the agent searches for the most optimal action-value function</li>
<li><strong>Actor-critic agents</strong>: Combination of both value-based and policy-based agents</li>
<li><strong>Model-based agents</strong>: Try to build a model based on the environment</li>
<li><strong>Model-free agents</strong>: Don't try to learn the environment, rather they try to learn the policy and value function</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Markov reward process</h1>
                </header>
            
            <article>
                
<p>In the previous section, we gave an introduction to MDP. In this section, we will define the problem statement formally and see the algorithms for solving it.</p>
<p>An MDP is used to define the environment in reinforcement learning and almost all reinforcement learning problems can be defined using an MDP.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>For understanding MDPs we need to use the concept of the <strong>Markov reward process</strong> (<strong>MRP</strong>). An MRP is a stochastic process which extends a Markov chain by adding a reward rate to each state. We can also define an additional variable to keep a track of the accumulated reward over time. Formally, an MRP is defined by <img class="fm-editor-equation" src="assets/7488784d-0670-480e-a673-8320989866c4.png" style="width:4.92em;height:1.17em;"/> where <em>S</em> is a finite state space, <em>P</em> is the state transition probability function, <em>R</em> is a reward function, and <img class="fm-editor-equation" src="assets/b829afec-9f1c-4b23-8a78-f77547dbc951.png" style="width:0.83em;height:1.25em;"/> is the discount rate:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a0b927f4-6354-4751-ac3a-13c348781d1a.png" style="width:13.92em;height:1.75em;"/></p>
<p>where <img class="fm-editor-equation" src="assets/5ff856d1-0e22-4ad3-ac30-6867098e1371.png" style="width:1.08em;height:1.50em;"/> denotes the expectation. And the term <em>R<sub>s</sub></em> here denotes the expected reward at the state <em>s</em>.</p>
<p>In the case of MRPs, we can define the expected return when starting from a state <em>s</em> as:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/2f5e4c4d-cff7-45a4-86bb-8ca971bd66c3.png" style="width:13.17em;height:1.75em;"/></p>
<p>where <em>G<sub>t</sub></em> is the cumulative gain as we had defined in the previous section:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/53d2fd3d-43c8-45ec-8f65-bda4e3592f97.png" style="width:22.92em;height:6.75em;"/></p>
<p>Now, for maximizing the cumulative reward, the agent will try to get the most expected sum of rewards from every state it goes into. To do that we need to find the optimal value function. We will see the algorithm for doing that in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bellman equation</h1>
                </header>
            
            <article>
                
<p>Using the Bellman equation, we decompose our value function into two separate parts, one representing the immediate reward and the other term representing the future rewards. From our previous definitions, the immediate reward is represented as <em>R<sub>t+1</sub></em> and the future rewards by <img class="fm-editor-equation" src="assets/d9847e5a-6360-4875-8b40-3f4296a066c1.png" style="width:4.67em;height:1.33em;"/> where:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/fbee6c17-72b3-4084-bb61-b9d622a90e7b.png" style="width:13.17em;height:1.75em;"/></p>
<p class="mce-root"/>
<p>Let's now unroll <em>G<sub>t</sub></em> and substitute <em>G<sub>t+1</sub></em> in it:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a8d00aeb-1527-47c2-88e6-8960ec6a5d58.png" style="width:25.92em;height:5.50em;"/></p>
<p>Now, since we know that:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c34fe84e-b0f3-4d8c-b9c9-cd2386ff72b1.png" style="width:18.58em;height:1.67em;"/></p>
<p>Using this identity we have:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e81848b6-cc68-4947-96d1-58d008685f5a.png" style="width:21.50em;height:1.67em;"/></p>
<p>And this gives us the Bellman equation for MRPs:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/5ddd6fb3-0ebf-44d8-b385-b49dd34cbe0b.png" style="width:20.00em;height:1.58em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">MDP</h1>
                </header>
            
            <article>
                
<p>Now that we have a basic understanding of MRPs, we can move on to MDPs. An MDP is an MRP which also involved decisions. All the states in the environment are also Markov, hence the next state is only dependent on the current state. Formally, an MDP can be represented using <img class="fm-editor-equation" src="assets/57989e16-1d7b-45a9-b644-35221cfb5099.png" style="width:7.08em;height:1.33em;"/> where <em>S</em> is the state space, <em>A</em> is the action set, <em>P</em> is the state transition probability function, <em>R</em> is the reward function, and <img class="fm-editor-equation" src="assets/ada7f4d2-3aba-46a2-9172-6b188b3457dd.png" style="width:0.83em;height:1.25em;"/> is the discount rate. The state transition probability function <em>P</em> and the reward function <em>R</em> are formally defined as:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/f4c103f7-991b-427d-bcc1-e686de7e028d.png" style="width:19.25em;height:3.42em;"/></p>
<p>We can also formally define a policy <em>π</em> as:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/dfa1a5a9-553b-4755-8c4a-6add32b46623.png" style="width:14.75em;height:1.50em;"/></p>
<p>Since the states in the MDP are considered to be Markov, the MDP policies depend only on the current state, which means that the policies are stationary that is, <img class="fm-editor-equation" src="assets/04f513a8-31a0-4304-8c53-5d2ad9d73d74.png" style="width:11.00em;height:1.42em;"/>. This means that whenever the agent falls into the same state, it will take the decision based on the same policy it had decided before. The decision function can be made stochastic so that the agent doesn't keep on taking the same decisions and hence is able to explore the environment.</p>
<p>Now, since we want to use the Bellman Equation in the case of MDP, we will recover an MRP from the MDPs. Given an MDP, <img class="fm-editor-equation" src="assets/b35de36f-a812-4f4d-a6a8-56112f1f568f.png" style="width:9.92em;height:1.33em;"/> and a policy <img class="fm-editor-equation" src="assets/f40666b4-eb4f-4fff-a06c-76f3b1f83173.png" style="width:0.92em;height:0.92em;"/> the state sequence <em>S<sub>1</sub>, S<sub>2</sub>, ...</em> is a Markov Process <em>(S,P)</em> on the policy <em>π</em>. The state and reward sequence <em>S1, R1, S2, R2, ... </em>is also an MRP given by <img class="fm-editor-equation" src="assets/d82e40a8-d340-4e3c-b9d7-41eb27928ae2.png" style="width:6.00em;height:1.42em;"/> where:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/f901d745-4aeb-4c7c-a33d-26d0e9786154.png" style="width:11.83em;height:3.17em;"/></p>
<p>We can similarly formulate our reward function as:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a46a80a3-643f-4fb5-8c7f-91ea46ee8a84.png" style="width:9.92em;height:2.92em;"/></p>
<p>And, since we know that the state-value function <em>Vπ(s)</em> of an MDP is the expected return starting from state <em>S</em> and then following the policy <em><span>π</span></em>, the value function is given as:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/15bcda57-42d2-41de-8726-9027a70ffe97.png" style="width:29.25em;height:4.92em;"/></p>
<p>Also, the action-value function can be given as:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/7c0a8568-b3be-4d46-9928-0d7ed49b47f6.png" style="width:46.00em;height:4.92em;"/></p>
<p>Having these values, we can again derive the Bellman expectation equation in the case of MDPs. We again start by decomposing the state-value function into immediate reward and future rewards:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/92216e16-9342-4643-9bc0-4b636cedfcc7.png" style="width:22.08em;height:1.58em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>And similar to the case of MRPs, the action-value function can also be decomposed as:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/7c5528ee-2aa8-4ef0-a8ff-8f8a7e10f534.png" style="width:31.58em;height:1.58em;"/></p>
<p>And as we have multiple actions from each state <em>S</em> and the policy defines the probability distribution over the actions, we will need to average over it to get the Bellman expectation equation:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c407fadd-243a-482f-87bf-3e00d4e38a55.png" style="width:17.00em;height:3.67em;"/></p>
<p>We can also average over all the possible action-values to know how good being in a given state <em>S</em> is:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a4aa3925-892e-4d08-a5f8-09c0fcc04420.png" style="width:20.83em;height:3.75em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Code example</h1>
                </header>
            
            <article>
                
<p>In the following code example we implement a simple MDP:</p>
<pre>import numpy as np<br/>import random<br/><br/><br/>class MDP(object):<br/>  """ <br/>    Defines a Markov Decision Process containing:<br/>  <br/>    - States, s <br/>    - Actions, a<br/>    - Rewards, r(s,a)<br/>    - Transition Matrix, t(s,a,_s)<br/><br/>    Includes a set of abstract methods for extended class will<br/>    need to implement.<br/><br/>  """<br/> <br/>  def __init__(self, states=None, actions=None, rewards=None, transitions=None, <br/>        discount=.99, tau=.01, epsilon=.01):<br/>    """<br/>    Parameters:<br/>    -----------<br/>    states: 1-D array<br/>        The states of the environment<br/><br/>    actions: 1-D array<br/>        The possible actions by the agent.<br/><br/>    rewards: 2-D array<br/>        The rewards corresponding to each action at each state of the environment.<br/><br/>    transitions: 2-D array<br/>        The transition probabilities between the states of the environment.<br/><br/>    discount: float<br/>        The discount rate for the reward.<br/>    """    <br/>    self.s = np.array(states)<br/>    self.a = np.array(actions)<br/>    self.r = np.array(rewards)<br/>    self.t = np.array(transitions)<br/>    <br/>    self.discount = discount<br/>    self.tau = tau<br/>    self.epsilon = epsilon<br/><br/>    # Value iteration will update this<br/>    self.values = None<br/>    self.policy = None<br/><br/>  def getTransitionStatesAndProbs(self, state, action):<br/>    """<br/>      Returns the list of transition probabilities<br/>    """<br/>    return self.t[state][action][:]<br/><br/>  def getReward(self, state):<br/>    """<br/>      Gets reward for transition from state-&gt;action-&gt;nextState.<br/>    """<br/>    return self.r[state]<br/><br/><br/>  def takeAction(self, state, action):<br/>    """<br/>      Take an action in an MDP, return the next state<br/><br/>      Chooses according to probability distribution of state transitions,<br/>      contingent on actions.<br/>    """<br/>    return np.random.choice(self.s, p=self.getTransitionStatesAndProbs(state, action)) <br/><br/><br/>  def valueIteration(self):<br/>    """<br/>      Performs value iteration to populate the values of all states in<br/>      the MDP. <br/><br/>    """<br/><br/>    # Initialize V_0 to zero<br/>    self.values = np.zeros(len(self.s))<br/>    self.policy = np.zeros([len(self.s), len(self.a)])<br/><br/>    policy_switch = 0<br/><br/>    # Loop until convergence<br/>    while True:<br/><br/>      # To be used for convergence check<br/>      oldValues = np.copy(self.values)<br/><br/>      for i in range(len(self.s)-1):<br/><br/>        self.values[i] = self.r[i] + np.max(self.discount * \<br/>              np.dot(self.t[i][:][:], self.values))<br/><br/>      # Check Convergence<br/>      if np.max(np.abs(self.values - oldValues)) &lt;= self.epsilon:<br/>        break<br/><br/><br/><br/>  def extractPolicy(self):<br/>    """<br/>      Extract policy from values after value iteration runs.<br/>    """<br/><br/>    self.policy = np.zeros([len(self.s),len(self.a)])<br/><br/>    for i in range(len(self.s)-1):<br/><br/>      state_policy = np.zeros(len(self.a))<br/><br/>      state_policy = self.r[i] + self.discount* \<br/>            np.dot(self.t[i][:][:], self.values)<br/><br/>      # Softmax the policy <br/>      state_policy -= np.max(state_policy)<br/>      state_policy = np.exp(state_policy / float(self.tau))<br/>      state_policy /= state_policy.sum()<br/><br/>      self.policy[i] = state_policy<br/><br/><br/><br/>  def simulate(self, state):<br/><br/>    """ <br/>      Runs the solver for the MDP, conducts value iteration, extracts policy,<br/>      then runs simulation of problem.<br/><br/>      NOTE: Be sure to run value iteration (solve values for states) and to<br/>       extract some policy (fill in policy vector) before running simulation<br/>    """<br/>    <br/>    # Run simulation using policy until terminal condition met<br/>    <br/>    while not self.isTerminal(state):<br/><br/>      # Determine which policy to use (non-deterministic)<br/>      policy = self.policy[np.where(self.s == state)[0][0]]<br/>      p_policy = self.policy[np.where(self.s == state)[0][0]] / \<br/>            self.policy[np.where(self.s == state)[0][0]].sum()<br/><br/>      # Get the parameters to perform one move<br/>      stateIndex = np.where(self.s == state)[0][0]<br/>      policyChoice = np.random.choice(policy, p=p_policy)<br/>      actionIndex = np.random.choice(np.array(np.where(self.policy[state][:] == policyChoice)).ravel())<br/><br/>      # Take an action, move to next state<br/>      nextState = self.takeAction(stateIndex, actionIndex)<br/><br/>      print "In state: {}, taking action: {}, moving to state: {}".format(<br/>        state, self.a[actionIndex], nextState)<br/><br/>      # End game if terminal state reached<br/>      state = int(nextState)<br/>      if self.isTerminal(state):<br/><br/>        # print "Terminal state: {} has been reached. Simulation over.".format(state)<br/>        return state</pre>
<p>Using this MDP, we can now code up a simple betting game:</p>
<pre>class BettingGame(MDP):<br/><br/> """ <br/> Defines the Betting Game:<br/><br/> Problem: A gambler has the chance to make bets on the outcome of <br/> a fair coin flip. If the coin is heads, the gambler wins as many<br/> dollars back as was staked on that particular flip - otherwise<br/> the money is lost. The game is won if the gambler obtains $100,<br/> and is lost if the gambler runs out of money (has 0$). This gambler<br/> did some research on MDPs and has decided to enlist them to assist<br/> in determination of how much money should be bet on each turn. Your <br/> task is to build that MDP!<br/><br/> Params: <br/><br/> pHead: Probability of coin flip landing on heads<br/> - Use .5 for fair coin, <span>otherwise </span>choose a bias [0,1]<br/><br/> """<br/><br/> def __init__(self, pHeads=.5, discount=.99, epsilon=.1, tau=.0001):<br/><br/> MDP.__init__(self,discount=discount,tau=tau,epsilon=epsilon)<br/> self.pHeads = pHeads<br/> self.setBettingGame(pHeads)<br/> self.valueIteration()<br/> self.extractPolicy()<br/><br/> # Edge case fix: Policy for $1<br/> self.policy[1][:] = 0<br/> self.policy[1][1] = 1.0<br/><br/> def isTerminal(self, state):<br/> """<br/> Checks if MDP is in terminal state.<br/> """<br/> return True if state is 100 or state is 0 else False<br/><br/> def setBettingGame(self, pHeads=.5):<br/><br/> """ <br/> Initializes the MDP to the starting conditions for <br/> the betting game. <br/><br/> Params:<br/> pHeads = Probability that coin lands on head<br/> - .5 for fair coin, otherwise choose bias<br/><br/> """<br/><br/> # This is how much we're starting with<br/> self.pHeads = pHeads<br/><br/> # Initialize all possible states<br/> self.s = np.arange(102)<br/><br/> # Initialize possible actions<br/> self.a = np.arange(101)<br/><br/> # Initialize rewards<br/> self.r = np.zeros(101)<br/> self.r[0] = -5<br/> self.r[100] = 10<br/><br/> # Initialize transition matrix<br/> temp = np.zeros([len(self.s),len(self.a),len(self.s)])<br/><br/> # List comprehension using tHelper to determine probabilities for each index<br/> self.t = [self.tHelper(i[0], i[1], i[2], self.pHeads) for i,x in np.ndenumerate(temp)]<br/> self.t = np.reshape(self.t, np.shape(temp))<br/> <br/> for x in range(len(self.a)):<br/><br/> # Remember to add -1 to value it, and policy extract<br/> # Send the end game states to the death state!<br/> self.t[100][x] = np.zeros(len(self.s))<br/> self.t[100][x][101] = 1.0<br/> self.t[0][x] = np.zeros(len(self.s))<br/> self.t[0][x][101] = 1.0<br/><br/><br/> def tHelper(self, x, y, z, pHeads):<br/><br/> """ <br/> Helper function to be used in a list comprehension to quickly<br/> generate the transition matrix. Encodes the necessary conditions<br/> to compute the necessary probabilities.<br/><br/> Params:<br/> x,y,z indices<br/> pHeads = probability coin lands on heads<br/><br/> """<br/> <br/> # If you bet no money, you will always have original amount<br/> if x + y is z and y is 0:<br/> return 1.0<br/><br/> # If you bet more money than you have, no chance of any outcome<br/> elif y &gt; x and x is not z:<br/> return 0<br/><br/> # If you bet more money than you have, returns same state with 1.0 prob.<br/> elif y &gt; x and x is z:<br/> return 1.0<br/><br/> # Chance you lose<br/> elif x - y is z:<br/> return 1.0 - pHeads<br/><br/> # Chance you win<br/> elif x + y is z:<br/> return pHeads<br/><br/> # Edge Case: Chance you win, and winnings go over 100<br/> elif x + y &gt; z and z is 100:<br/> return pHeads<br/><br/><br/> else:<br/> return 0 <br/><br/> return 0<br/><br/></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p><span>In this chapter, we started with a short introduction to Reinforcement Learning. We talked about agents, rewards and our learning goals in reinforcement learning. In the next section, we introduced MRP which is one of the main concepts underlying MDP. Having an understanding of MRP we next introduce the concepts of MDP along with a code example.</span></p>


            </article>

            
        </section>
    </body></html>