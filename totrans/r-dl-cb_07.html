<html><head></head><body>
        <section id="7OT2Q1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Reinforcement Learning</h1>
                
            
            <article>
                
<p class="calibre2">The current chapter will introduce Reinforcement Learning. We will cover the following topics:</p>
<ul class="calibre12">
<li class="calibre13">Setting up a Markov Decision Process</li>
<li class="calibre13">Performing model-based learning</li>
<li class="calibre13">Performing model-free learning</li>
</ul>


            </article>

            
        </section>
    

        <section id="7PRJC1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Introduction</h1>
                
            
            <article>
                
<p class="calibre2"><strong class="calibre1">Reinforcement Learning</strong> (<strong class="calibre1">RL</strong>) is an area in machine learning that is inspired by psychology, such as how agents (software programs) can take actions in order to maximize cumulative rewards.</p>
<p class="calibre2">The RL is reward-based learning where the reward comes at the end or is distributed during the learning. For example, in chess, the reward will be assigned to winning or losing the game whereas in games such as tennis, every point won is a reward. Some of the commercial examples of RL are DeepMind from Google uses RL to master parkour. Similarly, Tesla is developing AI-driven technology using RL. An example of reinforcement architecture is shown in the following figure:</p>
<div class="cdpaligncenter"><img class="image-border92" src="../images/00111.jpeg"/></div>
<div class="packt_figref">Interaction of an agent with environment in Reinforcement Learning</div>
<p class="calibre2">The basic notations for RL are as follows:</p>
<ul class="calibre12">
<li class="calibre13"><strong class="calibre1">T(s, a, s')</strong>: Represents the transition model for reaching state <em class="calibre9">s'</em> when action <em class="calibre9">a</em> is taken at state <em class="calibre9">s</em></li>
<li class="calibre13"><img src="../images/00120.gif" class="calibre48"/>: Represents a policy which defines what action to take at every possible state <img src="../images/00114.gif" class="calibre48"/></li>
<li class="calibre13"><strong class="calibre1">R(s)</strong>: Denotes the reward received by agent at state <em class="calibre9">s</em></li>
</ul>
<p class="calibre2">The current chapter will look at how to set up reinforcement models using R. The next sub-section will introduce <kbd class="calibre10">MDPtoolbox</kbd> from R.</p>


            </article>

            
        </section>
    

        <section id="7QQ3U1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Setting up a Markov Decision Process</h1>
                
            
            <article>
                
<p class="calibre2">The <strong class="calibre1">Markov Decision Process</strong> (<strong class="calibre1">MDP</strong>) forms the basis of setting up RL, where the outcome of a decision is semi-controlled; that is, it is partly random and partly controlled (by the decision-maker). An MDP is defined using a set of possible states (<strong class="calibre1">S</strong>), a set of possible actions (<strong class="calibre1">A</strong>), a real-values reward function (<strong class="calibre1">R</strong>), and a set of transition probabilities from one state to another state for a given action (<strong class="calibre1">T</strong>). In addition, the effects of an action performed on one state depends only on that state and not on its previous states.</p>


            </article>

            
        </section>
    

        <section id="7ROKG1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Getting ready</h1>
                
            
            <article>
                
<p class="calibre2">In this section, let us define an agent travelling across a 4 x 4 grid, as shown in following figure:</p>
<div class="cdpaligncenter"><img class="image-border93" src="../images/00025.gif"/></div>
<div class="packt_figref">A sample 4 x 4 grid of 16 states</div>
<p class="calibre2">This grid has 16 states (<em class="calibre9">S1</em>, <em class="calibre9">S2</em>....<em class="calibre9">S16</em>). In each state, the agent can perform four actions (<em class="calibre9">up</em>, <em class="calibre9">right</em>, <em class="calibre9">down</em>, <em class="calibre9">left</em>). However, the agent will be restricted to some actions based on the following constraints:</p>
<ul class="calibre12">
<li class="calibre13">The states across the edges shall be restricted to actions which point only toward states in the grid. For example, an agent in <em class="calibre9">S1</em> is restricted to the <em class="calibre9">right</em> or <em class="calibre9">down</em> action.</li>
<li class="calibre13">Some state transitions have barriers, marked in red. For example, the agent cannot go <em class="calibre9">down</em> from <em class="calibre9">S2</em> to <em class="calibre9">S3</em>.</li>
</ul>
<p class="calibre2">Each state is also assigned to a reward. The objective of the agent is to reach the destination with minimum moves, thereby achieving the maximum reward. Except state <em class="calibre9">S15</em> with a reward value of 100, all the remaining states have a reward value of <em class="calibre9">-1</em>.</p>
<p class="calibre2">Here, we will use the <kbd class="calibre10">MDPtoolbox</kbd> package in R.</p>


            </article>

            
        </section>
    

        <section>

                            <header id="7SN522-a0a93989f17f4d6cb68b8cfd331bc5ab">
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<p class="calibre2">This section will show you how to set up RL models using <kbd class="calibre10">MDPtoolbox</kbd> in R:</p>
<ol class="calibre15">
<li value="1" class="calibre13">Install and load the required package:</li>
</ol>
<pre class="calibre23">
Install.packages("MDPtoolbox") 
library(MDPtoolbox) 
</pre>
<ol start="2" class="calibre15">
<li value="2" class="calibre13">Define the transition probabilities for action. Here, each row denotes <kbd class="calibre10">from state</kbd> and each column denotes <kbd class="calibre10">to state</kbd>. As we have 16 states, the transition probability matrix of each action shall be a 16 x 16 matrix, with each row adding upto 1:</li>
</ol>
<pre class="calibre23">
up&lt;- matrix(c(1      ,     0     ,     0     ,     0     ,     0     ,     0   ,     0     ,     0     ,     0     ,     0     ,     0     ,     0     ,   0     ,     0     ,     0     ,     0     , 
               0.7   ,     0.2   ,     0     ,     0     ,     0     ,     0.1   ,     0     ,     0     ,     0     ,     0     ,     0     ,     0     ,   0     ,     0     ,     0     ,     0     , 
               0     ,     0     ,     0.8   ,     0.05  ,     0     ,     0   ,     0.15  ,     0     ,     0     ,     0     ,     0     ,     0     ,   0     ,     0     ,     0     ,     0     , 
               0     ,     0     ,     0.7   ,     0.3   ,     0     ,     0   ,     0     ,     0     ,     0     ,     0     ,     0     ,     0     ,   0     ,     0     ,     0     ,     0     , 
               0.1   ,     0     ,     0     ,     0     ,     0.7   ,     0.1   ,     0     ,     0     ,     0.1   ,     0     ,     0     ,     0     ,   0     ,     0     ,     0     ,     0     , 
               0     ,     0.05  ,     0     ,     0     ,     0.7   ,     0.15   ,     0.1   ,     0     ,     0     ,     0     ,     0     ,     0     ,   0     ,     0     ,     0     ,     0     , 
               0     ,     0     ,     0.05  ,     0     ,     0     ,     0.7   ,     0.15  ,     0.05  ,     0     ,     0     ,     0.05  ,     0     ,   0     ,     0     ,     0     ,     0     , 
               0     ,     0     ,     0     ,     0     ,     0     ,     0   ,     0.7   ,     0.2   ,     0     ,     0     ,     0     ,     0.1   ,   0     ,     0     ,     0     ,     0     , 
               0     ,     0     ,     0     ,     0     ,     0.05  ,     0   ,     0     ,     0     ,     0.85  ,     0.05  ,     0     ,     0     ,   0.05  ,     0     ,     0     ,     0     , 
               0     ,     0     ,     0     ,     0     ,     0     ,     0   ,     0     ,     0     ,     0.7   ,     0.2   ,     0.05  ,     0     ,   0     ,     0.05  ,     0     ,     0     , 
               0     ,     0     ,     0     ,     0     ,     0     ,     0   ,     0.05  ,     0     ,     0     ,     0.7   ,     0.2   ,     0     ,   0     ,     0     ,     0.05  ,     0     , 
               0     ,     0     ,     0     ,     0     ,     0     ,     0   ,     0     ,     0.05  ,     0     ,     0     ,     0     ,     0.9   ,   0     ,     0     ,     0     ,     0.05  , 
               0     ,     0     ,     0     ,     0     ,     0     ,     0   ,     0     ,     0     ,     0.1   ,     0     ,     0     ,     0     ,   0.9   ,     0     ,     0     ,     0     , 
               0     ,     0     ,     0     ,     0     ,     0     ,     0   ,     0     ,     0     ,     0     ,     0.1   ,     0     ,     0     ,   0.7   ,     0.2   ,     0     ,     0     , 
               0     ,     0     ,     0     ,     0     ,     0     ,     0   ,     0     ,     0     ,     0     ,     0     ,     0.05  ,     0     ,   0     ,     0.8   ,     0.15  ,     0     , 
               0     ,     0     ,     0     ,     0     ,     0     ,     0   ,     0     ,     0     ,     0     ,     0     ,     0     ,     0     ,   0     ,     0     ,     0.8   ,     0.2   ), 
nrow=16, ncol=16, byrow=TRUE) 
left&lt;- matrix(c(1    ,     0     ,     0     ,     0     ,     0     ,     0   ,     0     ,     0     ,     0     ,     0     ,     0     ,     0     ,   0     ,     0     ,     0     ,     0     , 
                 0.05      ,     0.9   ,     0     ,     0     ,     0     ,   0.05  ,     0     ,     0     ,     0     ,     0     ,     0     ,     0   ,     0     ,     0     ,     0     ,     0     , 
                 0   ,     0     ,     0.9   ,     0.05  ,     0     ,     0   ,     0.05  ,     0     ,     0     ,     0     ,     0     ,     0     ,   0     ,     0     ,     0     ,     0     , 
                 0   ,     0     ,     0.05  ,     0.9   ,     0     ,     0   ,     0     ,     0.05  ,     0     ,     0     ,     0     ,     0     ,   0     ,     0     ,     0     ,     0     , 
                 0.8 ,     0     ,     0     ,     0     ,     0.1   ,     0.05   ,     0     ,     0     ,     0.05  ,     0     ,     0     ,     0     ,   0     ,     0     ,     0     ,     0     , 
                 0   ,     0.8   ,     0     ,     0     ,     0.05  ,     0.1   ,     0.05  ,     0     ,     0     ,     0     ,     0     ,     0     ,   0     ,     0     ,     0     ,     0     , 
                 0   ,     0     ,     0.8   ,     0     ,     0     ,     0.05   ,     0.1   ,     0.05  ,     0     ,     0     ,     0     ,     0     ,   0     ,     0     ,     0     ,     0     , 
                 0   ,     0     ,     0     ,     0     ,     0     ,     0   ,     0.1   ,     0.8   ,     0     ,     0     ,     0     ,     0.1   ,   0     ,     0     ,     0     ,     0     , 
                 0   ,     0     ,     0     ,     0     ,     0.8   ,     0   ,     0     ,     0     ,     0.1   ,     0.05  ,     0     ,     0     ,   0.05  ,     0     ,     0     ,     0     , 
                 0   ,     0     ,     0     ,     0     ,     0     ,     0.8   ,     0     ,     0     ,     0.05  ,     0.1   ,     0.05  ,     0     ,   0     ,     0     ,     0     ,     0     , 
                 0   ,     0     ,     0     ,     0     ,     0     ,     0   ,     0.8   ,     0     ,     0     ,     0.1   ,     0.1   ,     0     ,   0     ,     0     ,     0     ,     0     , 
                 0   ,     0     ,     0     ,     0     ,     0     ,     0   ,     0     ,     0.8   ,     0     ,     0     ,     0     ,     0.2   ,   0     ,     0     ,     0     ,     0     , 
                 0   ,     0     ,     0     ,     0     ,     0     ,     0   ,     0     ,     0     ,     0.8   ,     0     ,     0     ,     0     ,   0.2   ,     0     ,     0     ,     0     , 
                 0   ,     0     ,     0     ,     0     ,     0     ,     0   ,     0     ,     0     ,     0     ,     0.8   ,     0     ,     0     ,   0.05  ,     0.1   ,     0.05  ,     0     , 
                 0   ,     0     ,     0     ,     0     ,     0     ,     0   ,     0     ,     0     ,     0     ,     0     ,     0.8   ,     0     ,   0     ,     0.05  ,     0.1   ,     0.05  , 
                 0   ,     0     ,     0     ,     0     ,     0     ,     0   ,     0     ,     0     ,     0     ,     0     ,     0     ,     0.8   ,   0     ,     0     ,     0.05  ,     0.15), 
nrow=16, ncol=16, byrow=TRUE) 
down&lt;- matrix(c(0.1  ,     0.8   ,     0     ,     0     ,     0.1   ,     0   ,     0     ,     0     ,     0     ,     0     ,     0     ,     0     ,   0     ,     0     ,     0     ,     0     , 
                 0.05      ,     0.9   ,     0     ,     0     ,     0     ,   0.05  ,     0     ,     0     ,     0     ,     0     ,     0     ,     0   ,     0     ,     0     ,     0     ,     0     , 
                 0   ,     0     ,     0.1   ,     0.8   ,     0     ,     0   ,     0.1   ,     0     ,     0     ,     0     ,     0     ,     0     ,   0     ,     0     ,     0     ,     0     , 
                 0   ,     0     ,     0.1   ,     0.9   ,     0     ,     0   ,     0     ,     0     ,     0     ,     0     ,     0     ,     0     ,   0     ,     0     ,     0     ,     0     , 
                 0.05      ,     0     ,     0     ,     0     ,     0.15  ,   0.8   ,     0     ,     0     ,     0     ,     0     ,     0     ,     0   ,     0     ,     0     ,     0     ,     0     , 
                 0   ,     0     ,     0     ,     0     ,     0     ,     0.2   ,     0.8   ,     0     ,     0     ,     0     ,     0     ,     0     ,   0     ,     0     ,     0     ,     0     , 
                 0   ,     0     ,     0     ,     0     ,     0     ,     0   ,     0.2   ,     0.8   ,     0     ,     0     ,     0     ,     0     ,   0     ,     0     ,     0     ,     0     , 
                 0   ,     0     ,     0     ,     0     ,     0     ,     0   ,     0.1   ,     0.9   ,     0     ,     0     ,     0     ,     0     ,   0     ,     0     ,     0     ,     0     , 
                 0   ,     0     ,     0     ,     0     ,     0.05  ,     0   , <span>    0     ,     0     ,     0.1   ,     0.8   ,     0     ,     0     ,   0.05  ,     0     ,     0     ,     0     , 
                 0   ,     0     ,     0     ,     0     ,     0     ,     0   ,     0     ,     0     ,     0     ,     0.2   ,     0.8   ,     0     ,   0     ,     0     ,     0     ,     0     , 
                 0   ,     0     ,     0     ,     0     ,     0     ,     0   ,     0     ,     0     ,     0     ,     0.05  ,     0.8   ,     0     ,   0     ,     0     ,     0.05  ,     0     , 
                 0   ,     0     ,     0     ,     0     ,     0     ,     0   ,     0     ,     0.05  ,     0     ,     0     ,     0     ,     0.9   ,   0     ,     0     ,     0     ,     0.05  , 
                 0   ,     0     ,     0     ,     0     ,     0     ,     0   ,     0     ,     0     ,     0     ,     0     ,     0     ,     0     ,   0.2   ,     0.8   ,     0     ,     0     , 
                 0   ,     0     ,     0     ,     0     ,     0     ,     0   ,     0     ,     0     ,     0     ,     0     ,     0     ,     0     ,   0.05  ,     0.15  ,     0.8   ,     0     , 
                 0   ,     0     ,     0     ,     0     ,     0     ,     0   ,     0     ,     0     ,     0     ,     0     ,     0     ,     0     ,   0     ,     0     ,     0.2   ,     0.8   , 
                 0   ,     0     ,     0     ,     0     ,     0     ,     0   ,     0     ,     0     ,     0     ,     0     ,     0     ,     0     ,   0     ,     0     ,     0     ,     1), 
nrow=16, ncol=16, byrow=TRUE) 
right&lt;- matrix(c(0.2 ,     0.1   ,     0     ,     0     ,     0.7   ,     0   ,     0     ,     0     ,     0     ,     0     ,     0     ,     0     ,   0     ,     0     ,     0     ,     0     , 
                  0.1      ,     0.1   ,     0     ,     0     ,     0     ,   0.8   ,     0     ,     0     ,     0     ,     0     ,     0     ,     0   ,     0     ,     0     ,     0     ,     0     , 
                  0  ,     0     ,     0.2   ,     0     ,     0     ,     0   ,     0.8   ,     0     ,     0     ,     0     ,     0     ,     0     ,   0     ,     0     ,     0     ,     0     , 
                  0  ,     0     ,     0.1   ,     0.9   ,     0     ,     0   ,     0     ,     0     ,     0     ,     0     ,     0     ,     0     ,   0     ,     0     ,     0     ,     0     , 
                  0  ,     0     ,     0     ,     0     ,     0.2   ,     0.1   ,     0     ,     0     ,     0.7   ,     0     ,     0     ,     0     ,   0     ,     0     ,     0     ,     0     , 
                  0  ,     0     ,     0     ,     0     ,     0     ,     0.9   ,     0.1   ,     0     ,     0     ,     0     ,     0     ,     0     ,   0     ,     0     ,     0     ,     0     , 
                  0  ,     0     ,     0     ,     0     ,     0     ,     0.05   ,     0.1   ,     0     ,     0     ,     0     ,     0.85  ,     0     ,   0     ,     0     ,     0     ,     0     , 
                  0  ,     0     ,     0     ,     0     ,     0     ,     0   ,     0.1   ,     0.2   ,     0     ,     0     ,     0     ,     0.7   ,   0     ,     0     ,     0     ,     0     , 
                  0  ,     0     ,     0     ,     0     ,     0     ,     0   ,     0     ,     0     ,     0.2   ,     0     ,     0     ,     0     ,   0.8   ,     0     ,     0     ,     0     , 
                  0  ,     0     ,     0     ,     0     ,     0     ,     0   ,     0     ,     0     ,     0     ,     0.1   ,     0     ,     0     ,   0     ,     0.9   ,     0     ,     0     , 
                  0  ,     0     ,     0     ,     0     ,     0     ,     0   ,     0     ,     0     ,     0     ,     0     ,     0.1   ,     0     ,   0     ,     0     ,     0.9   ,     0     , 
                  0  ,     0     ,     0     ,     0     ,     0     ,     0   ,     0     ,     0     ,     0     ,     0     ,     0     ,     0.2   ,   0     ,     0     ,     0     ,     0.8   , 
                  0  ,     0     ,     0     ,     0     ,     0     ,     0   ,     0     ,     0     ,     0     ,     0     ,     0     ,     0     ,   1     ,     0     ,     0     ,     0     , 
                  0  ,     0     ,     0     ,     0     ,     0     ,     0   ,     0     ,     0     ,     0     ,     0     ,     0     ,     0     ,   0     ,     1     ,     0     ,     0     , 
                  0  ,     0     ,     0     ,     0     ,     0     ,     0   ,     0     ,     0     ,     0     ,     0     ,     0     ,     0     ,   0     ,     0     ,     1     ,     0     , 
                  0  ,     0     ,     0     ,     0     ,     0     ,     0   ,     0     ,     0     ,     0     ,     0     ,     0     ,     0     ,   0     ,     0     ,     0     ,     1), 
nrow=16, ncol=16, byrow=TRUE) 
</span></pre>
<ol start="3" class="calibre15">
<li value="3" class="calibre13">Define a list of transition probability matrices:</li>
</ol>
<pre class="calibre23">
TPMs &lt;- list(up=up, left=left, 
down=down, right=right) 
</pre>
<ol start="4" class="calibre15">
<li value="4" class="calibre13">Define a reward matrix of dimensions: 16 (number of states) x 4 (number of actions):</li>
</ol>
<pre class="calibre23">
Rewards&lt;- matrix(c(-1, -1, -1, -1, 
              -1, -1, -1, -1, 
              -1, -1, -1, -1, 
              -1, -1, -1, -1, 
              -1, -1, -1, -1, 
              -1, -1, -1, -1,
              -1, -1, -1, -1, 
              -1, -1, -1, -1, 
              -1, -1, -1, -1, 
              -1, -1, -1, -1, 
              -1, -1, -1, -1, 
              -1, -1, -1, -1, 
              -1, -1, -1, -1, 
              -1, -1, -1, -1, 
              100, 100, 100, 100, 
              -1, -1, -1, -1), 
nrow=16, ncol=4, byrow=TRUE) 
</pre>
<ol start="5" class="calibre15">
<li value="5" class="calibre13">Test whether the defined <kbd class="calibre10">TPMs</kbd> and <kbd class="calibre10">Rewards</kbd> satisfy a well-defined MDP. If it returns an empty string, then the MDP is valid:</li>
</ol>
<pre class="calibre23">
mdp_check(TPMs, Rewards) 
</pre>


            </article>

            
        </section>
    

        <section id="7TLLK1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Performing model-based learning</h1>
                
            
            <article>
                
<p class="calibre2">As the name suggests, the learning is augmented using a predefined model. Here, the model is represented in the form of transition probabilities and the key objective is to determine the optimal policy and value functions using these predefined model attributes (that is, <kbd class="calibre10">TPMs</kbd>). The policy is defined as a learning mechanism of an agent, traversing across multiple states. In other words, identifying the best action of an agent in a given state, to traverse to a next state, is termed a policy.</p>
<p class="calibre2">The objective of the policy is to maximize the cumulative reward of transitioning from the start state to the destination state, defined as follows, where <em class="calibre9">P(s)</em> is the cumulative policy <em class="calibre9">P</em> from a start state <em class="calibre9">s</em>, and <em class="calibre9">R</em> is the reward of transitioning from state <em class="calibre9">st</em> to state <em class="calibre9">s<sub class="calibre30">t+1</sub></em> by performing an action at.</p>
<div class="cdpaligncenter"><img src="../images/00028.gif" class="calibre39"/></div>
<p class="calibre2">The value function is of two types: the state-value function and the state-action value function. In the state-value function, for a given policy, it is defined as an expected reward to be in a particular state (including start state), whereas in the state-action value function, for a given policy, it is defined as an expected reward to be in a particular state (including the start state) and undertake a particular action.</p>
<div class="packt_infobox">Now, a policy is said to be optimal provided it returns the maximum expected cumulative reward, and its corresponding states are termed optimal state-value functions or its corresponding states and actions are termed optimal state-action value functions.</div>
<p class="calibre2">In model-based learning, the following iterative steps are performed in order to obtain an optimum policy, as shown in the following figure:</p>
<div class="cdpaligncenter"><img class="image-border94" src="../images/00044.jpeg"/></div>
<div class="packt_figref">Iterative steps to find an optimum policy</div>
<p class="calibre2">In this section, we shall evaluate the policy using the state-value function. In each iteration, the policies are dynamically evaluated using the Bellman equation, as follows, where <em class="calibre9">V<sub class="calibre30">i</sub></em> denotes the value at iteration <em class="calibre9">i</em>, <em class="calibre9">P</em> denotes an arbitrary policy of a given state <em class="calibre9">s</em> and action <em class="calibre9">a</em>, <em class="calibre9">T</em> denotes the transition probability from state <em class="calibre9">s</em> to state <em class="calibre9">s'</em> due to an action <em class="calibre9">a</em>, <em class="calibre9">R</em> denotes the reward at state <em class="calibre9">s'</em> while traversing from the state <em class="calibre9">s</em> post an action <em class="calibre9">a</em>, and <img src="../images/00057.gif" class="calibre48"/> denotes a discount factor in the range of (0,1). The discount factor ensures higher importance to starting learning steps than later.</p>
<div class="cdpaligncenter"><br class="title-page-tagline"/>
<img src="../images/00031.gif" class="calibre39"/></div>


            </article>

            
        </section>
    

        <section id="7UK661-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<p class="calibre2">This section shows you how to set up model-based RL:</p>
<ol class="calibre15">
<li value="1" class="calibre13">Run the policy iteration using the state-action value function with the discount factor <em class="calibre9">Υ = 0.9</em>:</li>
</ol>
<pre class="calibre23">
mdp_policy&lt;- mdp_policy_iteration(P=TPMs, R=Rewards, discount=0.9) 
</pre>
<ol start="2" class="calibre15">
<li value="2" class="calibre13">Get the best (optimum) policy P* as shown in the following figure. The arrows marked in green show the direction of traversing <em class="calibre9">S1</em> to <em class="calibre9">S15</em>:</li>
</ol>
<pre class="calibre23">
mdp_policy$policy 
names(TPMs)[mdp_policy$policy] 
</pre>
<div class="cdpaligncenter"><img class="image-border95" src="../images/00034.gif"/></div>
<div class="packt_figref">Optimum policy using model-based iteration with an optimum path from <em class="calibre9">S1</em> to <em class="calibre9">S15</em></div>
<ol start="3" class="calibre15">
<li value="3" class="calibre13">Get the optimum value function V* for each state and plot them as shown in the following figure:</li>
</ol>
<pre class="calibre23">
mdp_policy$V 
names(mdp_policy$V) &lt;- paste0("S",1:16) 
barplot(mdp_policy$V,col="blue",xlab="states",ylab="Optimal value",main="Value function of the optimal Policy",width=0.5) 
</pre>
<div class="cdpaligncenter"><img class="image-border96" src="../images/00150.gif"/></div>
<div class="packt_figref">Value functions of the optimal policy</div>


            </article>

            
        </section>
    

        <section id="7VIMO1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Performing model-free learning</h1>
                
            
            <article>
                
<p class="calibre2">Unlike model-based learning, where dynamics of transitions are explicitly provided (as transition probabilities from one state to another state), in model-free learning, the transitions are supposed to be deduced and learned directly from the interaction between states (using actions) rather explicitly provided. Widely used frameworks of mode-free learning are <strong class="calibre1">Monte Carlo</strong> methods and the <strong class="calibre1">Q-learning</strong> technique. The former is simple to implement but convergence takes time, whereas the latter is complex to implement but is efficient in convergence due to off-policy learning.</p>


            </article>

            
        </section>
    

        <section id="80H7A1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Getting ready</h1>
                
            
            <article>
                
<p class="calibre2">In this section, we will implement the Q-learning algorithm in R. The simultaneous exploration of the surrounding environment and exploitation of existing knowledge is termed off-policy convergence. For example, an agent in a particular state first explores all the possible actions of transitioning into next states and observes the corresponding rewards, and then exploits current knowledge to update the existing state-action value using the action generating the maximum possible reward.</p>
<p class="calibre2">The Q learning returns a 2D Q-table of the size of the number of states x the number of actions. The values in the Q-table are updated based on the following formula, where <em class="calibre9">Q</em> denotes the value of state <em class="calibre9">s</em> and action <em class="calibre9">a</em>, <em class="calibre9">r'</em> denotes the reward of the next state for a selected action <em class="calibre9">a</em>, <em class="calibre9">Υ</em> denotes the discount factor, and <em class="calibre9">α</em> denotes the learning rate:</p>
<div class="cdpaligncenter"><img src="../images/00071.gif" class="calibre39"/></div>
<p class="calibre2">The framework for Q-learning is shown in the following figure:</p>
<div class="packt_figref"><img class="image-border97" src="../images/00078.jpeg"/></div>
<div class="packt_figref">Framework of Q-learning</div>


            </article>

            
        </section>
    

        <section id="81FNS1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<p class="calibre2">The section provide steps for how to set up Q-learning:</p>
<ol class="calibre15">
<li value="1" class="calibre13">Define 16 states:</li>
</ol>
<pre class="calibre23">
states &lt;- c("s1", "s2", "s3", "s4", "s5", "s6", "s7", "s8", "s9", "s10", "s11", "s12", "s13", "s14", "s15", "s16") 
</pre>
<ol start="2" class="calibre15">
<li value="2" class="calibre13">Define four actions:</li>
</ol>
<pre class="calibre23">
actions&lt;- c("up", "left", "down", "right") 
</pre>
<ol start="3" class="calibre15">
<li value="3" class="calibre13">Define the <kbd class="calibre10">transitionStateAction</kbd> function, which can simulate the transitions from one state <em class="calibre9">s</em> to another state <em class="calibre9">s'</em> using an action <em class="calibre9">a</em>. The function takes in the current state <em class="calibre9">s</em> and selected action <em class="calibre9">a</em>, and it returns the next state <em class="calibre9">s'</em> and corresponding reward <em class="calibre9">r'</em>. In case of constrained action, the next state returned is the current state <em class="calibre9">s</em> and the existing reward <em class="calibre9">r</em>:</li>
</ol>
<pre class="calibre23">
transitionStateAction&lt;- function(state, action) { 
  # The default state is the existing state in case of constrained action 
next_state&lt;- state 
if (state == "s1"&amp;&amp; action == "down") next_state&lt;- "s2" 
if (state == "s1"&amp;&amp; action == "right") next_state&lt;- "s5" 
if (state == "s2"&amp;&amp; action == "up") next_state&lt;- "s1" 
if (state == "s2"&amp;&amp; action == "right") next_state&lt;- "s6" 
if (state == "s3"&amp;&amp; action == "right") next_state&lt;- "s7" 
if (state == "s3"&amp;&amp; action == "down") next_state&lt;- "s4" 
if (state == "s4"&amp;&amp; action == "up") next_state&lt;- "s3" 
if (state == "s5"&amp;&amp; action == "right") next_state&lt;- "s9" 
if (state == "s5"&amp;&amp; action == "down") next_state&lt;- "s6" 
if (state == "s5"&amp;&amp; action == "left") next_state&lt;- "s1" 
if (state == "s6"&amp;&amp; action == "up") next_state&lt;- "s5" 
if (state == "s6"&amp;&amp; action == "down") next_state&lt;- "s7" 
if (state == "s6"&amp;&amp; action == "left") next_state&lt;- "s2" 
if (state == "s7"&amp;&amp; action == "up") next_state&lt;- "s6" 
if (state == "s7"&amp;&amp; action == "right") next_state&lt;- "s11" 
if (state == "s7"&amp;&amp; action == "down") next_state&lt;- "s8" 
if (state == "s7"&amp;&amp; action == "left") next_state&lt;- "s3" 
if (state == "s8"&amp;&amp; action == "up") next_state&lt;- "s7" 
if (state == "s8"&amp;&amp; action == "right") next_state&lt;- "s12" 
if (state == "s9"&amp;&amp; action == "right") next_state&lt;- "s13" 
if (state == "s9"&amp;&amp; action == "down") next_state&lt;- "s10" 
if (state == "s9"&amp;&amp; action == "left") next_state&lt;- "s5" 
if (state == "s10"&amp;&amp; action == "up") next_state&lt;- "s9" 
if (state == "s10"&amp;&amp; action == "right") next_state&lt;- "s14" 
if (state == "s10"&amp;&amp; action == "down") next_state&lt;- "s11" 
if (state == "s11"&amp;&amp; action == "up") next_state&lt;- "s10" 
if (state == "s11"&amp;&amp; action == "right") next_state&lt;- "s15" 
if (state == "s11"&amp;&amp; action == "left") next_state&lt;- "s7" 
if (state == "s12"&amp;&amp; action == "right") next_state&lt;- "s16" 
if (state == "s12"&amp;&amp; action == "left") next_state&lt;- "s8" 
if (state == "s13"&amp;&amp; action == "down") next_state&lt;- "s14" 
if (state == "s13"&amp;&amp; action == "left") next_state&lt;- "s9" 
if (state == "s14"&amp;&amp; action == "up") next_state&lt;- "s13" 
if (state == "s14"&amp;&amp; action == "down") next_state&lt;- "s15" 
if (state == "s14"&amp;&amp; action == "left") next_state&lt;- "s10" 
if (state == "s15"&amp;&amp; action == "up") next_state&lt;- "s14" 
if (state == "s15"&amp;&amp; action == "down") next_state&lt;- "s16" 
if (state == "s15"&amp;&amp; action == "left") next_state&lt;- "s11" 
if (state == "s16"&amp;&amp; action == "up") next_state&lt;- "s15" 
if (state == "s16"&amp;&amp; action == "left") next_state&lt;- "s12" 
  # Calculate reward 
if (next_state == "s15") { 
reward&lt;- 100 
  } else { 
reward&lt;- -1 
  } 
 
return(list(state=next_state, reward=reward)) 
} 
</pre>
<ol start="4" class="calibre15">
<li value="4" class="calibre13">Define a function to perform Q-learning using <kbd class="calibre10">n</kbd> iterations:</li>
</ol>
<pre class="calibre23">
Qlearning&lt;- function(n, initState, termState, 
epsilon, learning_rate) { 
  # Initialize a Q-matrix of size #states x #actions with zeroes 
Q_mat&lt;- matrix(0, nrow=length(states), ncol=length(actions), 
dimnames=list(states, actions)) 
  # Run n iterations of Q-learning 
for (i in 1:n) { 
Q_mat&lt;- updateIteration(initState, termState, epsilon, learning_rate, Q_mat) 
  } 
return(Q_mat) 
} 
   updateIteration&lt;- function(initState, termState, epsilon, learning_rate, Q_mat) { 
state&lt;- initState # set cursor to initial state 
while (state != termState) { 
    # Select the next action greedily or randomnly 
if (runif(1) &gt;= epsilon) { 
action&lt;- sample(actions, 1) # Select randomnly 
    } else { 
action&lt;- which.max(Q_mat[state, ]) # Select best action 
    } 
    # Extract the next state and its reward 
response&lt;- transitionStateAction(state, action) 
    # Update the corresponding value in Q-matrix (learning) 
Q_mat[state, action] &lt;- Q_mat[state, action] + learning_rate * 
      (response$reward + max(Q_mat[response$state, ]) - Q_mat[state, action]) 
state&lt;- response$state # update with next state 
  } 
return(Q_mat) 
} 
</pre>
<ol start="5" class="calibre15">
<li value="5" class="calibre13">Set learning parameters such as <kbd class="calibre10">epsilon</kbd> and <kbd class="calibre10">learning_rate</kbd>:</li>
</ol>
<pre class="calibre23">
epsilon&lt;- 0.1 
learning_rate&lt;- 0.9 
</pre>
<ol start="6" class="calibre15">
<li value="6" class="calibre13">Get the Q-table after 500k iterations:</li>
</ol>
<pre class="calibre23">
Q_mat&lt;- Qlearning(500, "s1", "s15", epsilon, learning_rate) 
Q_mat 
</pre>
<ol start="7" class="calibre15">
<li value="7" class="calibre13">Get the best (optimum) policy P*, as shown in the following figure. The arrows marked in green shows the direction of traversing <em class="calibre9">S1</em> to <em class="calibre9">S15:</em></li>
</ol>
<pre class="calibre23">
actions[max.col(Q_mat)] 
</pre>
<div class="cdpaligncenter"><img class="image-border98" src="../images/00082.gif"/></div>
<div class="packt_figref">Optimum policy using model-free iteration with an optimum path from <em class="calibre9">S1</em> to <em class="calibre9">S15</em></div>
<div class="title-page-tagline"/>


            </article>

            
        </section>
    </body></html>