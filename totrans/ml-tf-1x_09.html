<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Cruise Control - Automation</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will create a production system, from training to serving a model. Our system will have the ability to distinguish between 37 different species of dogs and cat. A user can upload an image to our system to receive the results. The system can also receive feedback from the user and automatically train itself every day to improve results.</p>
<p>This chapter will focus on several areas:</p>
<ul>
<li>How to apply transfer learning to a new dataset</li>
<li>How to serve a production model with TensorFlow Serving</li>
<li>Creating a system with crowd-sourced labeling of the dataset and automatic fine-tuning the model on user data</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">An overview of the system</h1>
                </header>
            
            <article>
                
<p>The following diagram provides an overview of our system:</p>
<div class="CDPAlignCenter CDPAlign"><img height="186" width="300" src="assets/f70d3bc2-a815-4a92-8c76-8ac66ffc649f.png"/></div>
<p>In this system, we will use an initial dataset to train a convolutional neural network model on a training server. Then, the model will be served in a production server with TensorFlow Serving. On the production server, there will be a Flask server that allows users to upload a new image and correct the label if the model goes wrong. At a defined time in the day, the training server will combine all the user-labeled images with the current dataset to automatically fine-tune the model and send it to the production server. Here is the wireframe of the web interface that allows users to upload and receive the result:</p>
<div class="CDPAlignCenter CDPAlign"><img height="281" width="662" class="image-border" src="assets/9972d10a-5ab1-456c-849f-1490e4f346ff.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up the project</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will fine-tune a VGG model that has been trained on <kbd>ImageNet</kbd> data with 1,000 classes. We have provided an initial project with a pretrained VGG model and some utility files. You can go ahead and download the code from <a href="https://github.com/mlwithtf/mlwithtf/tree/master/chapter_09"><span class="URLPACKT">https://github.com/mlwithtf/mlwithtf/tree/master/chapter_09</span></a><span class="URLPACKT">.</span><a href="https://github.com/mlwithtf/mlwithtf/tree/master/chapter_09"/></p>
<p>In the folder <kbd>chapter-09</kbd>, you will have the following structure:</p>
<pre><strong>- data</strong>
<strong>--VGG16.npz</strong>
<strong>- samples_data</strong>
<strong>- production</strong>
<strong>- utils</strong>
<strong>--__init__.py</strong>
<strong>--debug_print.py</strong>
<strong>- README.md</strong></pre>
<p>There are two files that you should understand:</p>
<ul>
<li><kbd>VGG16.npz</kbd> is the pre-trained model that is exported from the Caffe model. <a href="1cae2bb8-19d3-4640-aae6-d31d66afb605.xhtml">Chapter 11</a>, <em>Going Further - 21 Problems</em> will show you how to create this file from the Caffe model. In this chapter, we will use this as the initial values for our model. You can download this file from the <kbd>README.md</kbd> in the <kbd>chapter_09</kbd> folder.</li>
<li><kbd>production</kbd> is the Flask server that we created to serve as a web interface for users to upload and correct the model.</li>
<li><kbd>debug_print.py</kbd> contains some methods that we will use during this chapter to understand the network structure.</li>
<li><kbd>samples_data</kbd> contains some images of cats, dogs, and cars that we will use throughout the chapter.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Loading a pre-trained model to speed up the training</h1>
                </header>
            
            <article>
                
<p>In this section, let's focus on loading the pre-trained model in TensorFlow. We will use the VGG-16 model proposed by K. Simonyan and A. Zisserman from the University of Oxford.</p>
<p>VGG-16 is a very deep neural network with lots of convolution layers followed by max-pooling and fully connected layers. In the <kbd>ImageNet</kbd> challenge, the top-5 classification error of the VGG-16 model on the validation set of 1,000 image classes is 8.1% in a single-scale approach:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="214" width="375" class="image-border" src="assets/98634c06-1c4f-4443-a69b-1a6df6d9436b.png"/></div>
<p>First, create a file named <kbd>nets.py</kbd> in the <kbd>project</kbd> directory. The following code defines the graph for the VGG-16 model:</p>
<pre style="padding-left: 30px">    import tensorflow as tf 
    import numpy as np 
 
 
    def inference(images): 
    with tf.name_scope("preprocess"): 
        mean = tf.constant([123.68, 116.779, 103.939],  
    dtype=tf.float32, shape=[1, 1, 1, 3], name='img_mean') 
        input_images = images - mean 
    conv1_1 = _conv2d(input_images, 3, 3, 64, 1, 1,   
    name="conv1_1") 
    conv1_2 = _conv2d(conv1_1, 3, 3, 64, 1, 1, name="conv1_2") 
    pool1 = _max_pool(conv1_2, 2, 2, 2, 2, name="pool1") 
 
    conv2_1 = _conv2d(pool1, 3, 3, 128, 1, 1, name="conv2_1") 
    conv2_2 = _conv2d(conv2_1, 3, 3, 128, 1, 1, name="conv2_2") 
    pool2 = _max_pool(conv2_2, 2, 2, 2, 2, name="pool2") 
 
    conv3_1 = _conv2d(pool2, 3, 3, 256, 1, 1, name="conv3_1") 
    conv3_2 = _conv2d(conv3_1, 3, 3, 256, 1, 1, name="conv3_2") 
    conv3_3 = _conv2d(conv3_2, 3, 3, 256, 1, 1, name="conv3_3") 
    pool3 = _max_pool(conv3_3, 2, 2, 2, 2, name="pool3") 
 
    conv4_1 = _conv2d(pool3, 3, 3, 512, 1, 1, name="conv4_1") 
    conv4_2 = _conv2d(conv4_1, 3, 3, 512, 1, 1, name="conv4_2") 
    conv4_3 = _conv2d(conv4_2, 3, 3, 512, 1, 1, name="conv4_3") 
    pool4 = _max_pool(conv4_3, 2, 2, 2, 2, name="pool4") 
 
    conv5_1 = _conv2d(pool4, 3, 3, 512, 1, 1, name="conv5_1") 
    conv5_2 = _conv2d(conv5_1, 3, 3, 512, 1, 1, name="conv5_2") 
    conv5_3 = _conv2d(conv5_2, 3, 3, 512, 1, 1, name="conv5_3") 
    pool5 = _max_pool(conv5_3, 2, 2, 2, 2, name="pool5") 
 
    fc6 = _fully_connected(pool5, 4096, name="fc6") 
    fc7 = _fully_connected(fc6, 4096, name="fc7") 
    fc8 = _fully_connected(fc7, 1000, name='fc8', relu=False) 
    outputs = _softmax(fc8, name="output") 
    return outputs </pre>
<p>In the preceding code, there are a few things that you should note:</p>
<ul>
<li><kbd>_conv2d</kbd>, <kbd>_max_pool</kbd>, <kbd>_fully_connected</kbd> and <kbd>_softmax</kbd> are methods that define the convolution, max pooling, fully connected, and softmax layers, respectively. We will implement these methods shortly.</li>
<li>In the <kbd>preprocess</kbd> name scope, we define a constant tensor, <kbd>mean</kbd>, which is subtracted from the input image. This is the mean vector that the VGG-16 model is trained on in order to make the image zero mean.</li>
<li>We then define the convolution, max pooling, and fully connected layers with the parameters.</li>
<li>In the <kbd>fc8</kbd> layers, we don't apply ReLU activation to the outputs and we send the outputs to a <kbd>softmax</kbd> layer to compute the probability over 1,000 classes.</li>
</ul>
<p>Now, we will implement <kbd>_conv2d</kbd>, <kbd>_max_pool</kbd>, <kbd>_fully_connected</kbd>, and <kbd>_softmax</kbd> in the <kbd>nets.py</kbd> file.</p>
<p>The following code is the code for the <kbd>_conv2d</kbd> and <kbd>_max_pool</kbd> methods:</p>
<pre style="padding-left: 60px"> def _conv2d(input_data, k_h, k_w, c_o, s_h, s_w, name, relu=True,  <br/> padding="SAME"): 
    c_i = input_data.get_shape()[-1].value 
    convolve = lambda i, k: tf.nn.conv2d(i, k, [1, s_h, s_w, 1],  <br/> padding=padding) 
    with tf.variable_scope(name) as scope: 
        weights = tf.get_variable(name="kernel", shape=[k_h, k_w,  <br/> c_i, c_o], 
                                   <br/> initializer=tf.truncated_normal_initializer(stddev=1e-1,  <br/> dtype=tf.float32)) 
        conv = convolve(input_data, weights) 
        biases = tf.get_variable(name="bias", shape=[c_o],  <br/> dtype=tf.float32, 
                                  <br/> initializer=tf.constant_initializer(value=0.0)) 
        output = tf.nn.bias_add(conv, biases) 
        if relu: 
            output = tf.nn.relu(output, name=scope.name) 
        return output 
 def _max_pool(input_data, k_h, k_w, s_h, s_w, name,  <br/> padding="SAME"): 
    return tf.nn.max_pool(input_data, ksize=[1, k_h, k_w, 1], 
                          strides=[1, s_h, s_w, 1], padding=padding,  <br/> name=name) </pre>
<p>Most of the preceding code is self-explanatory if you have read <a href="ff9f54f4-c5eb-4ea8-bc0c-da5021479d77.xhtml">Chapter 4</a>, <em>Cats and Dogs</em>, but there are some lines that deserve a bit of explanation:</p>
<ul>
<li><kbd>k_h</kbd> and <kbd>k_w</kbd> are the height and weights of the kernel</li>
<li><kbd>c_o</kbd> means channel outputs, which is the number of feature maps of the convolution layers</li>
<li><kbd>s_h</kbd> and <kbd>s_w</kbd> are the stride parameters for the <kbd>tf.nn.conv2d</kbd> and <kbd>tf.nn.max_pool</kbd> layers</li>
<li><kbd>tf.get_variable</kbd> is used instead of <kbd>tf.Variable</kbd> because we will need to use <kbd>get_variable</kbd> again when we load the pre-trained weights</li>
</ul>
<p>Implementing the <kbd>fully_connected</kbd> layers and <kbd>softmax</kbd> layers are quite easy:</p>
<pre style="padding-left: 60px"> def _fully_connected(input_data, num_output, name, relu=True): 
    with tf.variable_scope(name) as scope: 
        input_shape = input_data.get_shape() 
        if input_shape.ndims == 4: 
            dim = 1 
            for d in input_shape[1:].as_list(): 
                dim *= d 
            feed_in = tf.reshape(input_data, [-1, dim]) 
        else: 
            feed_in, dim = (input_data, input_shape[-1].value) 
        weights = tf.get_variable(name="kernel", shape=[dim,  <br/> num_output], 
                                   <br/> initializer=tf.truncated_normal_initializer(stddev=1e-1,  <br/> dtype=tf.float32)) 
        biases = tf.get_variable(name="bias", shape=[num_output], <br/> dtype=tf.float32, 
                                  <br/> initializer=tf.constant_initializer(value=0.0)) 
        op = tf.nn.relu_layer if relu else tf.nn.xw_plus_b 
        output = op(feed_in, weights, biases, name=scope.name) 
        return output 
 def _softmax(input_data, name): 
    return tf.nn.softmax(input_data, name=name) </pre>
<p>Using the <kbd>_fully_connected</kbd> method, we first check the number of dimensions of the input data in order to reshape the input data into the correct shape. Then, we create <kbd>weights</kbd> and <kbd>biases</kbd> variables with the  <kbd>get_variable</kbd> method. Finally, we check the <kbd>relu</kbd> parameter to decide whether we should apply <kbd>relu</kbd> to the output with the <kbd>tf.nn.relu_layer</kbd> or <kbd>tf.nn.xw_plus_b</kbd>. <kbd>tf.nn.relu_layer</kbd> will compute <kbd>relu(matmul(x, weights) + biases)</kbd>. <kbd>tf.nn.xw_plus_b</kbd> but will only compute <kbd>matmul(x, weights) + biases</kbd>.</p>
<p>The final method in this section is used to load the pre-trained <kbd>caffe</kbd> weights into the defined variables:</p>
<pre>   def load_caffe_weights(path, sess, ignore_missing=False): 
    print("Load caffe weights from ", path) 
    data_dict = np.load(path).item() 
    for op_name in data_dict: 
        with tf.variable_scope(op_name, reuse=True): 
            for param_name, data in   
    data_dict[op_name].iteritems(): 
                try: 
                    var = tf.get_variable(param_name) 
                    sess.run(var.assign(data)) 
                except ValueError as e: 
                    if not ignore_missing: 
                        print(e) 
                        raise e </pre>
<p>In order to understand this method, we must know how the data is stored in the pre-trained model, <kbd>VGG16.npz</kbd>. We have created a simple code to print all the variables in the pre-trained model. You can put the following code at the end of <kbd>nets.py</kbd> and run it with Python <kbd>nets.py</kbd>:</p>
<pre>    if __name__ == "__main__": 
    path = "data/VGG16.npz" 
    data_dict = np.load(path).item() 
    for op_name in data_dict: 
        print(op_name) 
        for param_name, data in     ].iteritems(): 
            print("\t" + param_name + "\t" + str(data.shape)) </pre>
<p>Here are a few lines of the results:</p>
<pre><strong>conv1_1</strong>
    <strong>weights (3, 3, 3, 64)</strong>
    <strong>biases  (64,)</strong>
<strong>conv1_2</strong>
   <strong> weights (3, 3, 64, 64)</strong>
   <strong> biases  (64,)</strong></pre>
<p>As you can see, <kbd>op_name</kbd> is the name of the layers, and we can access the <kbd>weights</kbd> and <kbd>biases</kbd> of each layer with <kbd>data_dict[op_name]</kbd>.</p>
<p>Let's take a look at <kbd>load_caffe_weights</kbd>:</p>
<ul>
<li>We use it with <kbd>tf.variable_scope</kbd> with <kbd>reuse=True</kbd> in the parameters so that we can get the exact variables for <kbd>weights</kbd> and <kbd>biases</kbd> that were defined in the graph. After that, we run the assign method to set the data for each variable.</li>
<li>The <kbd>get_variable</kbd> method will give <kbd>ValueError</kbd> if the variable name is not defined. Therefore, we will use the <kbd>ignore_missing</kbd> variable to decide whether we should raise an error or not.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Testing the pre-trained model</h1>
                </header>
            
            <article>
                
<p>We have already created a VGG16 neural network. In this section, we will try to use the pre-trained model to perform the classifications of cars, cats, and dogs to check whether the model has been loaded successfully.</p>
<p>In the <kbd>nets.py</kbd> file, we need to replace the current <kbd>__main__</kbd> code with the following code:</p>
<pre>    import os 
    from utils import debug_print 
    from scipy.misc import imread, imresize 
 
 
    if __name__ == "__main__": 
    SAMPLES_FOLDER = "samples_data" 
    with open('%s/imagenet-classes.txt' % SAMPLES_FOLDER, 'rb') as   
    infile: 
     class_labels = map(str.strip, infile.readlines()) 
 
    inputs = tf.placeholder(tf.float32, [None, 224, 224, 3],   
    name="inputs") 
    outputs = inference(inputs) 
 
    debug_print.print_variables(tf.global_variables()) 
    debug_print.print_variables([inputs, outputs]) 
 
    with tf.Session() as sess: 
     load_caffe_weights("data/VGG16.npz", sess,   
    ignore_missing=False) 
 
        files = os.listdir(SAMPLES_FOLDER) 
        for file_name in files: 
            if not file_name.endswith(".jpg"): 
                continue 
            print("=== Predict %s ==== " % file_name) 
            img = imread(os.path.join(SAMPLES_FOLDER, file_name),  
            mode="RGB") 
            img = imresize(img, (224, 224)) 
 
            prob = sess.run(outputs, feed_dict={inputs: [img]})[0] 
            preds = (np.argsort(prob)[::-1])[0:3] 
 
            for p in preds: 
                print class_labels[p], prob[p]</pre>
<p>In the preceding code, there are several things that you should note:</p>
<ul>
<li>We use the <kbd>debug_print</kbd>.<kbd>print_variables</kbd> helper method to visualize all the variables by printing the variable names and shapes.</li>
<li>We define a placeholder named <kbd>inputs</kbd> with the shape <kbd>[None, 224, 224, 3]</kbd>, which is the required input size of the VGG16 model:</li>
</ul>
<pre>      We get the model graph with outputs = inference(inputs). </pre>
<ul>
<li>In <kbd>tf.Session()</kbd>, we call the <kbd>load_caffe_weights</kbd> method with <kbd>ignore_missing=False</kbd> to ensure that we can load all the weights and biases of the pre-trained model.</li>
<li>The image is loaded and resized with the <kbd>imread</kbd> and <kbd>imresize</kbd> methods from <kbd>scipy</kbd>. Then, we use the <kbd>sess.run</kbd> method with the <kbd>feed_dict</kbd> dictionary and receive the predictions.</li>
<li>The following results are the predictions for <kbd>car.jpg</kbd>, <kbd>cat.jpg</kbd>, and <kbd>dog.jpg</kbd> in the <kbd>samples_data</kbd> that we provided at the beginning of the chapter:</li>
</ul>
<pre>    <strong>== Predict car.jpg ==== </strong>
    <strong>racer, race car, racing car 0.666172</strong>
    <strong>sports car, sport car 0.315847</strong>
    <strong>car wheel 0.0117961</strong>
    <strong>=== Predict cat.jpg ==== </strong>
    <strong>Persian cat 0.762223</strong>
    <strong>tabby, tabby cat 0.0647032</strong>
    <strong>lynx, catamount 0.0371023</strong>
    <strong>=== Predict dog.jpg ==== </strong>
    <strong>Border collie 0.562288</strong>
    <strong>collie 0.239735</strong>
    <strong>Appenzeller 0.0186233</strong></pre>
<p>The preceding results are the exact labels of these images. This means that we have successfully loaded the pre-trained VGG16 model in TensorFlow. In the next section, we will show you how to fine-tune the model on our dataset.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training the model for our dataset</h1>
                </header>
            
            <article>
                
<p>In this section, we will work through the process of creating the dataset, fine-tuning the model, and exporting the model for production.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction to the Oxford-IIIT Pet dataset</h1>
                </header>
            
            <article>
                
<p>The Oxford-IIIT Pet dataset contains 37 species of dogs and cats. Each class has 200 images with large variations in scale, pose, and lighting. The ground truth data has annotations for species, head position, and pixel segmentation for each image. In our application, we only use the species name as the class name for the model:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="100" width="431" class="image-border" src="assets/682fb671-be0e-4fa6-8895-23a01be38c5b.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dataset Statistics</h1>
                </header>
            
            <article>
                
<p>The following are the dataset for dogs and cats breed:</p>
<ol start="1">
<li> Dog breeds:</li>
</ol>
<div class="CDPAlignCenter CDPAlign">
<table style="width: 567px;height: 1137px">
<tbody>
<tr>
<td>
<p><strong>Breed</strong></p>
</td>
<td><strong>Total</strong></td>
</tr>
<tr>
<td><span>American Bulldog</span></td>
<td>200</td>
</tr>
<tr>
<td><span>American Pit Bull Terrier </span></td>
<td>200</td>
</tr>
<tr>
<td><span>Basset Hound</span></td>
<td>200</td>
</tr>
<tr>
<td><span>Beagle </span></td>
<td>200</td>
</tr>
<tr>
<td><span>Boxer </span></td>
<td>199</td>
</tr>
<tr>
<td><span>Chihuahua</span></td>
<td>200</td>
</tr>
<tr>
<td><span>English Cocker Spaniel</span></td>
<td>196</td>
</tr>
<tr>
<td><span>English Setter </span></td>
<td>200</td>
</tr>
<tr>
<td><span>German Shorthaired</span></td>
<td>200</td>
</tr>
<tr>
<td><span>Great Pyrenees </span></td>
<td>200</td>
</tr>
<tr>
<td><span>Havanese</span></td>
<td>200</td>
</tr>
<tr>
<td>Japanese Chin  </td>
<td>200</td>
</tr>
<tr>
<td><span>Keeshond</span></td>
<td>199</td>
</tr>
<tr>
<td><span>Leonberger </span></td>
<td>200</td>
</tr>
<tr>
<td><span>Miniature Pinscher</span></td>
<td>200</td>
</tr>
<tr>
<td><span>Newfoundland </span></td>
<td>196</td>
</tr>
<tr>
<td><span>Pomeranian</span></td>
<td>200</td>
</tr>
<tr>
<td><span>Pug</span></td>
<td>200</td>
</tr>
<tr>
<td><span>Saint Bernard</span></td>
<td>200</td>
</tr>
<tr>
<td><span>Samoyed</span></td>
<td>200</td>
</tr>
<tr>
<td><span>Scottish Terrier</span></td>
<td>199</td>
</tr>
<tr>
<td><span>Shiba Inu</span></td>
<td>200</td>
</tr>
<tr>
<td><span>Staffordshire Bull Terrier</span></td>
<td>189</td>
</tr>
<tr>
<td><span>Wheaten Terrier</span></td>
<td>200</td>
</tr>
<tr>
<td>Yorkshire Terrier</td>
<td>200</td>
</tr>
<tr>
<td><strong>Total</strong></td>
<td><strong>4978</strong></td>
</tr>
</tbody>
</table>
</div>
<ol start="2">
<li>Cat breeds:</li>
</ol>
<div class="CDPAlignRight CDPAlign">
<table style="width: 568px;height: 506px">
<tbody>
<tr>
<td><strong>Breed</strong></td>
<td><strong>Count</strong></td>
</tr>
<tr>
<td><span>Abyssinian</span></td>
<td>198</td>
</tr>
<tr>
<td>Bengal</td>
<td>200</td>
</tr>
<tr>
<td>Birman</td>
<td>200</td>
</tr>
<tr>
<td>Bombay</td>
<td>184</td>
</tr>
<tr>
<td>British Shorthair</td>
<td>200</td>
</tr>
<tr>
<td>Egyptian Mau</td>
<td>190</td>
</tr>
<tr>
<td>Maine Coon</td>
<td>200</td>
</tr>
<tr>
<td>Persian</td>
<td>200</td>
</tr>
<tr>
<td>Ragdoll</td>
<td>200</td>
</tr>
<tr>
<td>Russian Blue</td>
<td>200</td>
</tr>
<tr>
<td>Siamese</td>
<td>199</td>
</tr>
<tr>
<td>Sphynx</td>
<td>200</td>
</tr>
<tr>
<td><strong>Total</strong></td>
<td><strong>2371</strong></td>
</tr>
</tbody>
</table>
</div>
<ol start="3">
<li>Total pets:</li>
</ol>
<table style="width: 677px;height: 164px">
<tbody>
<tr>
<td><strong>Family</strong></td>
<td><strong>Count</strong></td>
</tr>
<tr>
<td>Cat</td>
<td>2371</td>
</tr>
<tr>
<td>Dog</td>
<td>4978</td>
</tr>
<tr>
<td><strong>Total</strong></td>
<td><strong>7349</strong></td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Downloading the dataset</h1>
                </header>
            
            <article>
                
<p>We can get the dataset from the website of the University of Oxford at <a href="http://www.robots.ox.ac.uk/~vgg/data/pets/"><span class="URLPACKT">http://www.robots.ox.ac.uk/~vgg/data/pets/</span></a>. We need to download the dataset and ground truth data as <kbd>images.tar.gz</kbd> and <kbd>annotations.tar.gz</kbd>. We store the TAR files in the <kbd>data/datasets</kbd> folder and extract all the <kbd>.tar</kbd> files. Make sure that the <kbd>data</kbd> folder has the following structure:</p>
<pre><strong>- data</strong>
<strong>-- VGG16.npz</strong>
<strong>-- datasets</strong>
<strong>---- annotations</strong>
<strong>------ trainval.txt</strong>
<strong>---- images</strong>
<strong>------ *.jpg</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing the data</h1>
                </header>
            
            <article>
                
<p>Before starting the training process, we need to pre-process the dataset into a simpler format, which we will use in the further automatic fine-tuning.</p>
<p>First, we make a Python package with named scripts in the <kbd>project</kbd> folder. Then, we create a Python file named <kbd>convert_oxford_data.py</kbd> and add the following code:</p>
<pre>    import os 
    import tensorflow as tf 
    from tqdm import tqdm 
    from scipy.misc import imread, imsave 
 
    FLAGS = tf.app.flags.FLAGS 
 
    tf.app.flags.DEFINE_string( 
    'dataset_dir', 'data/datasets', 
    'The location of Oxford IIIT Pet Dataset which contains    
     annotations and images folders' 
    ) 
 
    tf.app.flags.DEFINE_string( 
    'target_dir', 'data/train_data', 
    'The location where all the images will be stored' 
    ) 
 
    def ensure_folder_exists(folder_path): 
    if not os.path.exists(folder_path): 
        os.mkdir(folder_path) 
    return folder_path 
 
    def read_image(image_path): 
    try: 
        image = imread(image_path) 
        return image 
    except IOError: 
        print(image_path, "not readable") 
    return None </pre>
<p>In this code, we use <kbd>tf.app.flags.FLAGS</kbd> to parse arguments so that we can customize the script easily. We also create two <kbd>helper</kbd> methods to make a directory and read images.</p>
<p>Next, we add the following code to convert the Oxford dataset into our preferred format:</p>
<pre style="padding-left: 60px"> def convert_data(split_name, save_label=False): 
    if split_name not in ["trainval", "test"]: 
    raise ValueError("split_name is not recognized!") 
    target_split_path =  <br/>    ensure_folder_exists(os.path.join(FLAGS.target_dir, split_name)) 
    output_file = open(os.path.join(FLAGS.target_dir, split_name +  <br/>    ".txt"), "w") 
 
    image_folder = os.path.join(FLAGS.dataset_dir, "images") 
    anno_folder = os.path.join(FLAGS.dataset_dir, "annotations") 
 
    list_data = [line.strip() for line in open(anno_folder + "/" +  <br/>    split_name + ".txt")] 
 
    class_name_idx_map = dict() 
    for data in tqdm(list_data, desc=split_name): 
      file_name,class_index,species,breed_id = data.split(" ") 
      file_label = int(class_index) - 1 
 
      class_name = "_".join(file_name.split("_")[0:-1]) 
      class_name_idx_map[class_name] = file_label 
 
      image_path = os.path.join(image_folder, file_name + ".jpg") 
      image = read_image(image_path) 
      if image is not None: 
      target_class_dir =  <br/>       ensure_folder_exists(os.path.join(target_split_path,    <br/>       class_name)) 
      target_image_path = os.path.join(target_class_dir,  <br/>       file_name + ".jpg") 
            imsave(target_image_path, image) 
            output_file.write("%s %s\n" % (file_label,  <br/>            target_image_path)) 
 
    if save_label: 
        label_file = open(os.path.join(FLAGS.target_dir,  <br/>        "labels.txt"), "w") 
        for class_name in sorted(class_name_idx_map,  <br/>        key=class_name_idx_map.get): 
        label_file.write("%s\n" % class_name) 
 
 
 def main(_): 
    if not FLAGS.dataset_dir: 
    raise ValueError("You must supply the dataset directory with  <br/>    --dataset_dir") 
 
    ensure_folder_exists(FLAGS.target_dir) 
    convert_data("trainval", save_label=True) 
    convert_data("test") 
 
 
 if __name__ == "__main__": 
    tf.app.run() </pre>
<p>Now, we can run the <kbd>scripts</kbd> with the following code:</p>
<pre><strong>python scripts/convert_oxford_data.py --dataset_dir data/datasets/ --target_dir data/train_data.</strong></pre>
<p>The script reads the Oxford-IIIT dataset ground truth <kbd>data</kbd> and creates a new <kbd>dataset</kbd> in <kbd>data/train_data</kbd> with the following structure:</p>
<pre><strong>- train_data</strong>
<strong>-- trainval.txt</strong>
<strong>-- test.txt</strong>
<strong>-- labels.txt</strong>
<strong>-- trainval</strong>
<strong>---- Abyssinian</strong>
<strong>---- ...</strong>
<strong>-- test</strong>
<strong>---- Abyssinian</strong>
<strong>---- ...</strong></pre>
<p>Let's discuss these a bit:</p>
<ul>
<li><kbd>labels.txt</kbd> contains a list of 37 species in our dataset.</li>
<li><kbd>trainval.txt</kbd> contains a list of the images that we will use in the training process, with the format <kbd>&lt;class_id&gt; &lt;image_path&gt;</kbd>.</li>
<li><kbd>test.txt</kbd> contains a list of the images that we will use to check the accuracy of the model. The format of <kbd>test.txt</kbd> is the same as <kbd>trainval.txt</kbd>.</li>
<li><kbd>trainval</kbd> and <kbd>test</kbd> folders contain 37 sub-folders, which are the names of each class and contains all the images of each class.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up input pipelines for training and testing</h1>
                </header>
            
            <article>
                
<p>TensorFlow allows us to create a reliable input pipeline for quick and easy training. In this section, we will implement <kbd>tf.TextLineReader</kbd> to read the train and test text files. We will use <kbd>tf.train.batch</kbd> to read and preprocess images in parallel.</p>
<p>First, we need to create a new Python file named <kbd>datasets.py</kbd> in the <kbd>project</kbd> directory and add the following code:</p>
<pre>    import tensorflow as tf 
    import os 
 
    def load_files(filenames): 
    filename_queue = tf.train.string_input_producer(filenames) 
    line_reader = tf.TextLineReader() 
    key, line = line_reader.read(filename_queue) 
    label, image_path = tf.decode_csv(records=line, 
                                         
    record_defaults=[tf.constant([], dtype=tf.int32),   
    tf.constant([], dtype=tf.string)], 
                                      field_delim=' ') 
    file_contents = tf.read_file(image_path) 
    image = tf.image.decode_jpeg(file_contents, channels=3) 
 
    return image, label </pre>
<p>In the <kbd>load_files</kbd> method, we use the <kbd>tf.TextLineReader</kbd> to read each line of the text file, such as <kbd>trainval.txt, test.txt</kbd>. <kbd>tf.TextLineReader</kbd> needs a queue of strings to read, so we use <kbd>tf.train.string_input_producer</kbd> to store the filenames. After that, we pass the line variable into <kbd>tf.decode_cvs</kbd> in order to get the <kbd>label</kbd> and <kbd>filename</kbd>. The image can be easily read with <kbd>tf.image.decode_jpeg</kbd>.</p>
<p>Now that we can load the image, we can move forward and create <kbd>image</kbd> batches and <kbd>label</kbd> batches for <kbd>training</kbd>.</p>
<p>In <kbd>datasets.py</kbd>, we need to add a new method:</p>
<pre style="padding-left: 60px"> def input_pipeline(dataset_dir, batch_size, num_threads=8,   
    is_training=True, shuffle=True): 
    if is_training: 
        file_names = [os.path.join(dataset_dir, "trainval.txt")] 
    else: 
        file_names = [os.path.join(dataset_dir, "test.txt")] 
    image, label = load_files(file_names) 
 
    image = preprocessing(image, is_training) 
 
    min_after_dequeue = 1000 
    capacity = min_after_dequeue + 3 * batch_size 
    if shuffle: 
     image_batch, label_batch = tf.train.shuffle_batch( 
     [image, label], batch_size, capacity,  
     min_after_dequeue, num_threads 
      ) 
    else: 
        image_batch, label_batch = tf.train.batch( 
            [image, label], batch_size, num_threads, capacity 
            ) 
    return image_batch, label_batch</pre>
<p>We first load the <kbd>image</kbd> and <kbd>label</kbd> with the <kbd>load_files</kbd> method. Then, we pass the image through a new preprocessing method, which we will implement shortly. Finally, we pass the <kbd>image</kbd> and <kbd>label</kbd> into <kbd>tf.train.shuffle_batch</kbd> for training and <kbd>tf.train.batch</kbd> for testing:</p>
<pre style="padding-left: 60px"> def preprocessing(image, is_training=True, image_size=224,  <br/> resize_side_min=256, resize_side_max=312): 
    image = tf.cast(image, tf.float32) 
 
    if is_training: 
        resize_side = tf.random_uniform([], minval=resize_side_min,  <br/>        maxval=resize_side_max+1, dtype=tf.int32) 
        resized_image = _aspect_preserving_resize(image,  <br/>        resize_side) 
 
        distorted_image = tf.random_crop(resized_image, [image_size,  <br/>        image_size, 3]) 
 
        distorted_image =  <br/>        tf.image.random_flip_left_right(distorted_image) 
  
        distorted_image =  <br/>        tf.image.random_brightness(distorted_image, max_delta=50) 
 
        distorted_image = tf.image.random_contrast(distorted_image,  <br/>        lower=0.2, upper=2.0) 
 
        return distorted_image 
    else: 
        resized_image = _aspect_preserving_resize(image, image_size) 
        return tf.image.resize_image_with_crop_or_pad(resized_image,  <br/>        image_size, image_size)</pre>
<p>There are two different approaches to preprocessing in training and testing. In training, we need to augment data to create more training data from the current dataset. There are a few techniques that are used in the preprocessing method:</p>
<ul>
<li>The images in the dataset can have different image resolutions, but we only need 224x224 images. Therefore, we need to resize the image to a reasonable size before performing <kbd>random_crop</kbd>. The following diagram describes how cropping works. The <kbd>_aspect_preserving_resize</kbd> method will be implemented shortly:</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img height="106" width="400" src="assets/062d0bd1-3f15-4966-8b71-b3603ad59f01.png"/></div>
<ul>
<li>After cropping the image, we pass the image through <kbd>tf.image.random_flip_left_right</kbd>, <kbd>tf.image.random_brightness</kbd> and <kbd>tf.image.random_contrast</kbd> to distort the image and create a new training sample.</li>
<li>In the testing routine, we only need to resize the image with <kbd>_aspect_preserving_resize</kbd> and <kbd>tf.image.resize_image_with_crop_or_pad</kbd>. <kbd>tf.image.resize_image_with_crop_or_pad</kbd> allows us to crop centrally or pad the image to the target <kbd>width</kbd> and <kbd>height</kbd>.</li>
</ul>
<p>Now, we need to add the last two methods into <kbd>datasets.py</kbd>, as shown here:</p>
<pre>    def _smallest_size_at_least(height, width, smallest_side): 
      smallest_side = tf.convert_to_tensor(smallest_side,   
      dtype=tf.int32) 
 
      height = tf.to_float(height) 
      width = tf.to_float(width) 
      smallest_side = tf.to_float(smallest_side) 
 
      scale = tf.cond(tf.greater(height, width), 
                    lambda: smallest_side / width, 
                    lambda: smallest_side / height) 
      new_height = tf.to_int32(height * scale) 
      new_width = tf.to_int32(width * scale) 
      return new_height, new_width 
 
 
    def _aspect_preserving_resize(image, smallest_side): 
      smallest_side = tf.convert_to_tensor(smallest_side,   
      dtype=tf.int32) 
      shape = tf.shape(image) 
      height = shape[0] 
      width = shape[1] 
      new_height, new_width = _smallest_size_at_least(height, width,   
      smallest_side) 
      image = tf.expand_dims(image, 0) 
      resized_image = tf.image.resize_bilinear(image, [new_height,   
      new_width], align_corners=False) 
      resized_image = tf.squeeze(resized_image) 
      resized_image.set_shape([None, None, 3]) 
      return resized_image </pre>
<p>Up to this section, we had to do a lot of work to prepare the <kbd>dataset</kbd> and <kbd>input</kbd> pipelines. In the following section, we will define the model for our <kbd>dataset</kbd>, <kbd>loss</kbd>, <kbd>accuracy</kbd> and <kbd>training</kbd> operations to perform the <kbd>training</kbd> routine.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining the model</h1>
                </header>
            
            <article>
                
<p>Our application will need to classify <kbd>37</kbd> classes of dogs and cats. The VGG16 Model supports 1,000 different classes. In our application, we will reuse all layers up to the <kbd>fc7</kbd> layer and train the last layer from scratch. In order to make the model output <kbd>37</kbd> classes, we need to modify the inference method in <kbd>nets.py</kbd> as follows:</p>
<pre>    def inference(images, is_training=False): 
    # 
    # All the code before fc7 are not modified. 
    # 
    fc7 = _fully_connected(fc6, 4096, name="fc7") 
    if is_training: 
        fc7 = tf.nn.dropout(fc7, keep_prob=0.5) 
    fc8 = _fully_connected(fc7, 37, name='fc8-pets', relu=False) 
    return fc8</pre>
<ul>
<li>We add a new parameter, <kbd>is_training</kbd>, to the method. After the <kbd>fc7</kbd> layer, we add a <kbd>tf.nn.dropout</kbd> layer if the inference is training. This dropout layer can help the model regularize better with unseen data and avoid overfitting.</li>
<li>The number of outputs in the <kbd>fc8</kbd> layer is changed from 1,000 to 37. Besides, the name of the <kbd>fc8</kbd> layer must be changed to another name; in this case, we choose <kbd>fc8-pets</kbd>. If we don't change the name of the <kbd>fc8</kbd> layer, <kbd>load_caffe_weights</kbd> will still find the new layers and assign the original weights, which is not the same size as our new <kbd>fc8</kbd> layer.</li>
<li>The <kbd>softmax</kbd> layer at the end of the inference method is also removed because the <kbd>loss</kbd> function that we will use later only needs unnormalized outputs.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining training operations</h1>
                </header>
            
            <article>
                
<p>We will define all the operations in a new Python file named <kbd>models.py</kbd>. First, let's create some operations to compute <kbd>loss</kbd> and <kbd>accuracy</kbd>:</p>
<pre style="padding-left: 60px"> def compute_loss(logits, labels): 
   labels = tf.squeeze(tf.cast(labels, tf.int32)) 
 
   cross_entropy =   
   tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,    
   labels=labels) 
   cross_entropy_mean = tf.reduce_mean(cross_entropy) 
   tf.add_to_collection('losses', cross_entropy_mean) 
 
   return tf.add_n(tf.get_collection('losses'),    
   name='total_loss') 
 
 
 def compute_accuracy(logits, labels): 
   labels = tf.squeeze(tf.cast(labels, tf.int32)) 
   batch_predictions = tf.cast(tf.argmax(logits, 1), tf.int32) 
   predicted_correctly = tf.equal(batch_predictions, labels) 
   accuracy = tf.reduce_mean(tf.cast(predicted_correctly,   
   tf.float32)) 
   return accuracy</pre>
<p>In these methods, <kbd>logits</kbd> is the output of the model and <kbd>labels</kbd> is the ground truth data from the <kbd>dataset</kbd>. In the <kbd>compute_loss</kbd> method, we use <kbd>tf.nn.sparse_softmax_cross_entropy_with_logits</kbd> so we don't need to normalize the <kbd>logits</kbd> with <kbd>softmax</kbd> methods. Besides, we don't need to make the <kbd>labels</kbd> a one-hot vector. In the <kbd>compute_accuracy</kbd> method, we compare the max value in <kbd>logits</kbd> with <kbd>tf.argmax</kbd> and compare it with the <kbd>labels</kbd> to get the <kbd>accuracy</kbd>.</p>
<p>Next, we are going to define the operations for the <kbd>learning_rate</kbd> and the <kbd>optimizer</kbd>:</p>
<pre style="padding-left: 60px"> def get_learning_rate(global_step, initial_value, decay_steps,          
   decay_rate): 
   learning_rate = tf.train.exponential_decay(initial_value,   
   global_step, decay_steps, decay_rate, staircase=True) 
   return learning_rate 
 
 
 def train(total_loss, learning_rate, global_step, train_vars): 
 
   optimizer = tf.train.AdamOptimizer(learning_rate) 
 
   train_variables = train_vars.split(",") 
 
   grads = optimizer.compute_gradients( 
       total_loss, 
       [v for v in tf.trainable_variables() if v.name in   
       train_variables] 
       ) 
   train_op = optimizer.apply_gradients(grads,   
   global_step=global_step) 
   return train_op </pre>
<p>In the <kbd>train</kbd> method, we configure the <kbd>optimizer</kbd> to only <kbd>compute</kbd> and apply <kbd>gradients</kbd> to some variables defined in the <kbd>train_vars</kbd> string. This allows us to only update the <kbd>weights</kbd> and <kbd>biases</kbd> for the last layer, <kbd>fc8</kbd>, and freeze other layers. <kbd>train_vars</kbd> is a string that contains a list of variables split by commas, for example, <kbd>models/fc8-pets/weights:0,models/fc8-pets/biases:0</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Performing the training process</h1>
                </header>
            
            <article>
                
<p>Now we are ready to train the model. Let's create a Python file named <kbd>train.py</kbd> in the <kbd>scripts</kbd> folder. First, we need to define some parameters for the <kbd>training</kbd> routines:</p>
<pre style="padding-left: 60px"> import tensorflow as tf 
 import os 
 from datetime import datetime 
 from tqdm import tqdm 
 
 import nets, models, datasets 
 
 # Dataset 
 dataset_dir = "data/train_data" 
 batch_size = 64 
 image_size = 224 
 
 # Learning rate 
 initial_learning_rate = 0.001 
 decay_steps = 250 
 decay_rate = 0.9 
 
 # Validation 
 output_steps = 10  # Number of steps to print output 
 eval_steps = 20  # Number of steps to perform evaluations 
 
 # Training 
 max_steps = 3000  # Number of steps to perform training 
 save_steps = 200  # Number of steps to perform saving checkpoints 
 num_tests = 5  # Number of times to test for test accuracy 
 max_checkpoints_to_keep = 3 
 save_dir = "data/checkpoints" 
 train_vars = 'models/fc8-pets/weights:0,models/fc8-pets/biases:0' 
 
 # Export 
 export_dir = "/tmp/export/" 
 export_name = "pet-model" 
 export_version = 2 </pre>
<p>These variables are self-explanatory. Next, we need to define some operations for <kbd>training</kbd>, as follows:</p>
<pre style="padding-left: 60px"> images, labels = datasets.input_pipeline(dataset_dir, batch_size,   <br/> is_training=True) 
 test_images, test_labels = datasets.input_pipeline(dataset_dir,  <br/> batch_size, is_training=False) 
 
 with tf.variable_scope("models") as scope: 
    logits = nets.inference(images, is_training=True) 
    scope.reuse_variables() 
    test_logits = nets.inference(test_images, is_training=False) 
 
 total_loss = models.compute_loss(logits, labels) 
 train_accuracy = models.compute_accuracy(logits, labels) 
 test_accuracy = models.compute_accuracy(test_logits, test_labels) 
  
 global_step = tf.Variable(0, trainable=False) 
 learning_rate = models.get_learning_rate(global_step,  <br/> initial_learning_rate, decay_steps, decay_rate) 
 train_op = models.train(total_loss, learning_rate, global_step,  <br/> train_vars) 
 
 saver = tf.train.Saver(max_to_keep=max_checkpoints_to_keep) 
 checkpoints_dir = os.path.join(save_dir,  <br/> datetime.now().strftime("%Y-%m-%d_%H-%M-%S")) 
 if not os.path.exists(save_dir): 
    os.mkdir(save_dir) 
 if not os.path.exists(checkpoints_dir): 
    os.mkdir(checkpoints_dir) </pre>
<p>These operations are created by calling our defined methods in <kbd>datasets.py</kbd>, <kbd>nets.py</kbd>, and <kbd>models.py</kbd>. In this code, we create an input pipeline for training and another pipeline for testing. After that, we create a new <kbd>variable_scope</kbd> named <kbd>models</kbd> and create <kbd>logits</kbd> and <kbd>test_logits</kbd> with the <kbd>nets.inference</kbd> method. You must make sure that <kbd>scope.reuse_variables</kbd> is added because we want to reuse the <kbd>weights</kbd> and <kbd>biases</kbd> from training in testing. Finally, we create a <kbd>saver</kbd> and some directories to save the checkpoints every <kbd>save_steps</kbd>.</p>
<p>The last part of the <kbd>training</kbd> routine is the <kbd>training</kbd> loop:</p>
<pre style="padding-left: 60px"> with tf.Session() as sess: 
    sess.run(tf.global_variables_initializer()) 
    coords = tf.train.Coordinator() 
    threads = tf.train.start_queue_runners(sess=sess, coord=coords) 
 
    with tf.variable_scope("models"): 
       nets.load_caffe_weights("data/VGG16.npz", sess,  <br/>       ignore_missing=True) 
 
    last_saved_test_accuracy = 0 
    for i in tqdm(range(max_steps), desc="training"): 
                  _, loss_value, lr_value = sess.run([train_op,    <br/>                  total_loss,  learning_rate]) 
 
      if (i + 1) % output_steps == 0: 
          print("Steps {}: Loss = {:.5f} Learning Rate =  <br/>          {}".format(i + 1, loss_value, lr_value)) 
 
      if (i + 1) % eval_steps == 0: 
          test_acc, train_acc, loss_value =  <br/>          sess.run([test_accuracy, train_accuracy, total_loss]) 
          print("Test accuracy {} Train accuracy {} : Loss =  <br/>          {:.5f}".format(test_acc, train_acc, loss_value)) 
 
      if (i + 1) % save_steps == 0 or i == max_steps - 1: 
          test_acc = 0 
          for i in range(num_tests): 
              test_acc += sess.run(test_accuracy) 
          test_acc /= num_tests 
      if test_acc &gt; last_saved_test_accuracy: 
            print("Save steps: Test Accuracy {} is higher than  <br/>            {}".format(test_acc, last_saved_test_accuracy)) 
             last_saved_test_accuracy = test_acc 
             saved_file = saver.save(sess, 
                                         <br/>     os.path.join(checkpoints_dir, 'model.ckpt'), 
                  global_step=global_step) 
          print("Save steps: Save to file %s " % saved_file) 
      else: 
          print("Save steps: Test Accuracy {} is not higher  <br/>                than {}".format(test_acc, last_saved_test_accuracy)) 
 
    models.export_model(checkpoints_dir, export_dir, export_name,  <br/>    export_version) 
 
    coords.request_stop() 
    coords.join(threads) </pre>
<p>The <kbd>training</kbd> loop is easy to understand. First, we load the pre-trained <kbd>VGG16</kbd> model with <kbd>ignore_missing</kbd> set to <kbd>True</kbd> because we replaced the name of the <kbd>fc8</kbd> layer before. Then, we loop for <kbd>max_steps</kbd> steps, print the <kbd>loss</kbd> every <kbd>output_steps</kbd>, and print the <kbd>test_accuracy</kbd> every <kbd>eval_steps</kbd>. Every <kbd>save_steps</kbd>, we check and save the checkpoint if the current test accuracy is higher than the previous. We still need to create <kbd>models.export_model</kbd> to export the model for serving after <kbd>training</kbd>. However, you may want to check whether the <kbd>training</kbd> routine works before moving forward. Let's comment out the following line:</p>
<pre>    models.export_model(checkpoints_dir, export_dir, export_name,  <br/>    export_version) </pre>
<p>Then, run the <kbd>training</kbd> script with this command:</p>
<pre><strong>python scripts/train.py</strong></pre>
<p>Here is some output in the console. First, our script loads the pre-trained model. Then, it will output the <kbd>loss</kbd>:</p>
<pre><strong>('Load caffe weights from ', 'data/VGG16.npz')</strong>
<strong>training:   0%|▏                | 9/3000 [00:05&lt;24:59,  1.99it/s]</strong>
<strong>Steps 10: Loss = 31.10747 Learning Rate = 0.0010000000475</strong>
<strong>training:   1%|▎                | 19/3000 [00:09&lt;19:19,  2.57it/s]</strong>
<strong>Steps 20: Loss = 34.43741 Learning Rate = 0.0010000000475</strong>
<strong>Test accuracy 0.296875 Train accuracy 0.0 : Loss = 31.28600</strong>
<strong>training:   1%|▍                | 29/3000 [00:14&lt;20:01,  2.47it/s]</strong>
<strong>Steps 30: Loss = 15.81103 Learning Rate = 0.0010000000475</strong>
<strong>training:   1%|▌                | 39/3000 [00:18&lt;19:42,  2.50it/s]</strong>
<strong>Steps 40: Loss = 14.07709 Learning Rate = 0.0010000000475</strong>
<strong>Test accuracy 0.53125 Train accuracy 0.03125 : Loss = 20.65380</strong>  </pre>
<p>Now, let's stop the <kbd>training</kbd> and uncomment the <kbd>export_model</kbd> method. We need the <kbd>models.export_model</kbd> method to export the latest model that has the highest test accuracy to the <kbd>export_dir</kbd> folder with the name <kbd>export_name</kbd> and the version <kbd>export_version</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exporting the model for production</h1>
                </header>
            
            <article>
                
<pre style="padding-left: 60px"> def export_model(checkpoint_dir, export_dir, export_name,  <br/> export_version): 
    graph = tf.Graph() 
    with graph.as_default(): 
        image = tf.placeholder(tf.float32, shape=[None, None, 3]) 
        processed_image = datasets.preprocessing(image,  <br/>        is_training=False) 
        with tf.variable_scope("models"): 
         logits = nets.inference(images=processed_image,  <br/>          is_training=False) 
 
        model_checkpoint_path =  <br/>        get_model_path_from_ckpt(checkpoint_dir) 
        saver = tf.train.Saver() 
 
        config = tf.ConfigProto() 
        config.gpu_options.allow_growth = True 
        config.gpu_options.per_process_gpu_memory_fraction = 0.7 
 
        with tf.Session(graph=graph) as sess: 
            saver.restore(sess, model_checkpoint_path) 
            export_path = os.path.join(export_dir, export_name,  <br/>            str(export_version)) 
            export_saved_model(sess, export_path, image, logits) 
            print("Exported model at", export_path)</pre>
<p>In the <kbd>export_model</kbd> method, we need to create a new graph to run in production. In production, we don't need all the variables, as in <kbd>training</kbd>, and we don't need an input pipeline. However, we need to export the model with the <kbd>export_saved_model</kbd> method, as follows:</p>
<pre style="padding-left: 60px"> def export_saved_model(sess, export_path, input_tensor,  <br/> output_tensor): 
    from tensorflow.python.saved_model import builder as  <br/> saved_model_builder 
    from tensorflow.python.saved_model import signature_constants 
    from tensorflow.python.saved_model import signature_def_utils 
    from tensorflow.python.saved_model import tag_constants 
    from tensorflow.python.saved_model import utils 
    builder = saved_model_builder.SavedModelBuilder(export_path) 
 
    prediction_signature = signature_def_utils.build_signature_def( 
        inputs={'images': utils.build_tensor_info(input_tensor)}, 
        outputs={ 
            'scores': utils.build_tensor_info(output_tensor) 
        }, 
        method_name=signature_constants.PREDICT_METHOD_NAME) 
 
    legacy_init_op = tf.group( 
        tf.tables_initializer(), name='legacy_init_op') 
    builder.add_meta_graph_and_variables( 
        sess, [tag_constants.SERVING], 
        signature_def_map={ 
          'predict_images': 
           prediction_signature, 
        }, 
        legacy_init_op=legacy_init_op) 
 
    builder.save() </pre>
<p>With this method, we can create a metagraph of the model for serving in production. We will cover how to serve the model in a later section. Now, let's run the <kbd>scripts</kbd> to automatically train and export after 3,000 steps:</p>
<pre><strong>python scripts/train.py</strong></pre>
<p>On our system, with Core i7-4790 CPU and one TITAN-X GPU, the training routine takes 20 minutes to finish. Here are a few of the last outputs in our console:</p>
<pre><strong>Steps 3000: Loss = 0.59160 Learning Rate = 0.000313810509397</strong>
<strong>Test accuracy 0.659375 Train accuracy 0.853125: Loss = 0.25782</strong>
<strong>Save steps: Test Accuracy 0.859375 is not higher than 0.921875</strong>
<strong>training: 100%|██████████████████| 3000/3000 [23:40&lt;00:00,  1.27it/s]</strong>
    <strong>I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:01:00.0)</strong>
    <strong>('Exported model at', '/home/ubuntu/models/pet-model/1')</strong></pre>
<p>Great! We have a model with 92.18% test accuracy. We also have the exported model as a <kbd>.pb</kbd> file. The <kbd>export_dir</kbd> folder will have the following structure:</p>
<pre><strong>- /home/ubuntu/models/</strong>
<strong>-- pet_model</strong>
<strong>---- 1</strong>
<strong>------ saved_model.pb</strong>
<strong>------ variables</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Serving the model in production</h1>
                </header>
            
            <article>
                
<p>In production, we need to create an endpoint so our users can send the image and receive the result. In TensorFlow, we can easily serve our model with TensorFlow Serving. In this section, we will install TensorFlow Serving and create a Flask app that allows users to upload their images via a web interface.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up TensorFlow Serving</h1>
                </header>
            
            <article>
                
<p>In your production server, you need to install TensorFlow Serving and its prerequisites. You can visit the official website of TensorFlow Serving at <a href="https://tensorflow.github.io/serving/setup"><span class="URLPACKT">https://tensorflow.github.io/serving/setup</span></a>. Next, we will use the standard TensorFlow Model Server provided in TensorFlow Serving to serve the model. First, we need to build the <kbd>tensorflow_model_server</kbd> with the following command:</p>
<pre><strong>bazel build   <br/>//tensorflow_serving/model_servers:tensorflow_model_server</strong></pre>
<p>Copy all the files from <kbd>/home/ubuntu/models/pet_model</kbd> in your training server into your production server. In our setup, we choose <kbd>/home/ubuntu/productions</kbd> as our folder to store all the production models. The <kbd>productions</kbd> folder will have the following structure:</p>
<pre><strong>- /home/ubuntu/productions/</strong>
<strong>-- 1</strong>
<strong>---- saved_model.pb</strong>
<strong>---- variables</strong></pre>
<p>We will use <kbd>tmux</kbd> to keep the model server running. Let's install <kbd>tmux</kbd> with this command:</p>
<pre><strong>sudo apt-get install tmux</strong></pre>
<p>Run a <kbd>tmux</kbd> session with this command:</p>
<pre><strong>tmux new -s serving</strong></pre>
<p>In the <kbd>tmux</kbd> session, let's change directory to the <kbd>tensorflow_serving</kbd> directory and run the following command:</p>
<pre>    <strong>bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server --port=9000 --model_name=pet-model --model_base_path=/home/ubuntu/productions</strong></pre>
<p>The output of the console should look like this:</p>
<pre>    <strong>2017-05-29 13:44:32.203153: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:274] Loading SavedModel: success. Took 537318 microseconds.</strong>
    <strong>2017-05-29 13:44:32.203243: I tensorflow_serving/core/loader_harness.cc:86] Successfully loaded servable version {name: pet-model version: 1}</strong>
    <strong>2017-05-29 13:44:32.205543: I tensorflow_serving/model_servers/main.cc:298] Running ModelServer at 0.0.0.0:9000 ...</strong>  </pre>
<p>As you can see, the model is running on host <kbd>0.0.0.0</kbd> and port <kbd>9000</kbd>. In the next section, we will create a simple Python client to send an image to this server via gRPC.</p>
<p>You should also note that the current serving is only using CPU on the production server. Building TensorFlow Serving with GPUs is beyond the scope of this chapter. If you prefer serving with GPUs, you may want to read <a href="8022db02-d24f-4620-9da7-ae53df279306.xhtml">Appendix A</a><a href="8022db02-d24f-4620-9da7-ae53df279306.xhtml"/><em><a href="8022db02-d24f-4620-9da7-ae53df279306.xhtml"/>, Advanced Installation</em>, which explains how to build TensorFlow and TensorFlow Serving with GPU support.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running and testing the model</h1>
                </header>
            
            <article>
                
<p>In the project repository, we have already provided a package named <kbd>production</kbd>. In that package, we need to copy the <kbd>labels.txt</kbd> file into our <kbd>dataset</kbd>, create a new Python file, <kbd>client.py</kbd>, and add the following code:</p>
<pre>    import tensorflow as tf 
    import numpy as np 
    from tensorflow_serving.apis import prediction_service_pb2,     
    predict_pb2 
    from grpc.beta import implementations 
    from scipy.misc import imread 
    from datetime import datetime 
 
 
    class Output: 
    def __init__(self, score, label): 
        self.score = score 
        self.label = label 
 
    def __repr__(self): 
        return "Label: %s Score: %.2f" % (self.label, self.score) 
 
 
    def softmax(x): 
    return np.exp(x) / np.sum(np.exp(x), axis=0) 
 
 
    def process_image(path, label_data, top_k=3): 
    start_time = datetime.now() 
    img = imread(path) 
 
    host, port = "0.0.0.0:9000".split(":") 
    channel = implementations.insecure_channel(host, int(port)) 
    stub =  
    prediction_service_pb2.beta_create_PredictionService_stub(channel) 
 
    request = predict_pb2.PredictRequest() 
    request.model_spec.name = "pet-model" 
    request.model_spec.signature_name = "predict_images" 
 
    request.inputs["images"].CopyFrom( 
        tf.contrib.util.make_tensor_proto( 
            img.astype(dtype=float), 
            shape=img.shape, dtype=tf.float32 
        ) 
    ) 
 
    result = stub.Predict(request, 20.) 
    scores =    
    tf.contrib.util.make_ndarray(result.outputs["scores"])[0] 
    probs = softmax(scores) 
    index = sorted(range(len(probs)), key=lambda x: probs[x],  
    reverse=True) 
 
    outputs = [] 
    for i in range(top_k): 
        outputs.append(Output(score=float(probs[index[i]]),  
        label=label_data[index[i]])) 
 
    print(outputs) 
    print("total time", (datetime.now() -   
    start_time).total_seconds()) 
    return outputs 
 
    if __name__ == "__main__": 
    label_data = [line.strip() for line in   
    open("production/labels.txt", 'r')] 
    process_image("samples_data/dog.jpg", label_data) 
    process_image("samples_data/cat.jpg", label_data) </pre>
<p>In this code, we create a <kbd>process_image</kbd> method that will read the image from an image path and use some TensorFlow methods to create a tensor and send it to the model server with gRPC. We also create an <kbd>Output</kbd> class so that we can easily return it to the <kbd>caller</kbd> method. At the end of the method, we print the output and the total time so that we can debug it more easily. We can run this Python file to see if the <kbd>process_image</kbd> works:</p>
<pre><strong>python production/client.py</strong></pre>
<p>The output should look like this:</p>
<pre>    <strong>[Label: saint_bernard Score: 0.78, Label: american_bulldog Score: 0.21, Label: staffordshire_bull_terrier Score: 0.00]</strong>
    <strong>('total time', 14.943942)</strong>
    <strong>[Label: Maine_Coon Score: 1.00, Label: Ragdoll Score: 0.00, Label: Bengal Score: 0.00]</strong>
    <strong>('total time', 14.918235)</strong></pre>
<p>We get the correct result. However, the time to process is almost 15 seconds for each image. The reason is that we are using TensorFlow Serving in CPU mode. As we mentioned earlier, you can build TensorFlow Serving with GPU support in <a href="8022db02-d24f-4620-9da7-ae53df279306.xhtml">Appendix A</a>, <a href="8022db02-d24f-4620-9da7-ae53df279306.xhtml"/><span><em>Advanced Installation</em></span>. If you follow that tutorial, you will have the following result:</p>
<pre>    <strong>[Label: saint_bernard Score: 0.78, Label: american_bulldog Score: 0.21, Label: staffordshire_bull_terrier Score: 0.00]</strong>
    <strong>('total time', 0.493618)</strong>
    <strong>[Label: Maine_Coon Score: 1.00, Label: Ragdoll Score: 0.00, Label: Bengal Score: 0.00]</strong>
    <strong>('total time', 0.023753)</strong></pre>
<p>The time to process in the first calling time is 493 ms. However, the later calling time will be only about 23 ms, which is so much quicker than the CPU version.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Designing the web server</h1>
                </header>
            
            <article>
                
<p>In this section, we will set up a Flask server to allow users to upload their images and set the correct label if our model is mistaken. We have provided the code needed in the production package. Implementing a Flask server with database support is beyond the scope of this chapter. In this section, we will describe all the main points about Flask so you can follow and understand better.</p>
<p>The main flow that allows users to upload and correct labels can be described in the following wireframe.</p>
<p>This flow is implemented with the following routes:</p>
<table class="MsoTableGrid">
<tbody>
<tr>
<td>
<p><strong>Route</strong></p>
</td>
<td>
<p><strong>Method</strong></p>
</td>
<td>
<p><strong>Description</strong></p>
</td>
</tr>
<tr>
<td>
<p><kbd>/</kbd></p>
</td>
<td>
<p>GET</p>
</td>
<td>
<p>This route returns a web form for users to upload the image.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>/upload_image</kbd></p>
</td>
<td>
<p>POST</p>
</td>
<td>
<p>This route gets the image from POST data, saves it to the upload directory, and calls <kbd>process_image</kbd> in our <kbd>client.py</kbd> to recognize the image and save the result to the database.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>/results&lt;result_id&gt;</kbd></p>
</td>
<td>
<p>GET</p>
</td>
<td>
<p>This route returns the result of the corresponding row in the database.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>/results&lt;result_id&gt;</kbd></p>
</td>
<td>
<p>POST</p>
</td>
<td>
<p>This route saves the label from, user to the database so that we can fine-tune the model later.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>/user-labels</kbd></p>
</td>
<td>
<p>GET</p>
</td>
<td>
<p>This route returns a list of all the user-labeled images. In the fine-tune process, we will call this route to get the list of labeled images.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>/model</kbd></p>
</td>
<td>
<p>POST</p>
</td>
<td>
<p>This route allows the fine-tune process from the training server to serve a new trained model. This route receives a link of the zipped model, a version number, a checkpoint name, and a model name.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>/model</kbd></p>
</td>
<td>
<p>GET</p>
</td>
<td>
<p>This route returns the latest model in the database. The fine-tune process will call this to know which is the latest model and fine-tune from it.</p>
</td>
</tr>
</tbody>
</table>
<p>We should run this server in a <kbd>tmux</kbd> session with the following command:</p>
<pre><strong>tmux new -s "flask"</strong>
<strong>python production/server.py</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Testing the system</h1>
                </header>
            
            <article>
                
<p>Now, we can access the server via <kbd>http://0.0.0.0:5000</kbd>.</p>
<p>First, you will see a form to choose and submit an image.</p>
<p>The website will be redirected to the <kbd>/results</kbd> page with the corresponding image and its results. The user label field is empty. There is also a short form at the end so that you can submit the corrected label of the model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Automatic fine-tune in production</h1>
                </header>
            
            <article>
                
<p>After running the system for a while, we will have some user-labeled images. We will create a fine-tune process to automatically run every day and fine-tune the latest model with new data.</p>
<p>Let's create a file named <kbd>finetune.py</kbd> in the scripts folder.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Loading the user-labeled data</h1>
                </header>
            
            <article>
                
<p>First, we will add the code to download all user-labeled images from the production server:</p>
<pre>    import tensorflow as tf 
    import os 
    import json 
    import random 
    import requests 
    import shutil 
    from scipy.misc import imread, imsave 
    from datetime import datetime 
    from tqdm import tqdm 
 
    import nets, models, datasets 
 
 
    def ensure_folder_exists(folder_path): 
    if not os.path.exists(folder_path): 
        os.mkdir(folder_path) 
    return folder_path 
 
 
    def download_user_data(url, user_dir, train_ratio=0.8): 
    response = requests.get("%s/user-labels" % url) 
    data = json.loads(response.text) 
 
    if not os.path.exists(user_dir): 
        os.mkdir(user_dir) 
    user_dir = ensure_folder_exists(user_dir) 
    train_folder = ensure_folder_exists(os.path.join(user_dir,   
    "trainval")) 
    test_folder = ensure_folder_exists(os.path.join(user_dir,   
    "test")) 
 
    train_file = open(os.path.join(user_dir, 'trainval.txt'), 'w') 
    test_file = open(os.path.join(user_dir, 'test.txt'), 'w') 
 
    for image in data: 
        is_train = random.random() &lt; train_ratio 
        image_url = image["url"] 
        file_name = image_url.split("/")[-1] 
        label = image["label"] 
        name = image["name"] 
 
        if is_train: 
          target_folder =  
          ensure_folder_exists(os.path.join(train_folder, name)) 
        else: 
          target_folder =   
          ensure_folder_exists(os.path.join(test_folder, name)) 
 
        target_file = os.path.join(target_folder, file_name) +   
        ".jpg" 
 
        if not os.path.exists(target_file): 
            response = requests.get("%s%s" % (url, image_url)) 
            temp_file_path = "/tmp/%s" % file_name 
            with open(temp_file_path, 'wb') as f: 
                for chunk in response: 
                    f.write(chunk) 
 
            image = imread(temp_file_path) 
            imsave(target_file, image) 
            os.remove(temp_file_path) 
            print("Save file: %s" % target_file) 
 
        label_path = "%s %s\n" % (label, target_file) 
        if is_train: 
            train_file.write(label_path) 
        else: 
            test_file.write(label_path) </pre>
<p>In <kbd>download_user_data</kbd>, we call the <kbd>/user-labels</kbd> endpoint to get the list of user-labeled images. The JSON has the following format:</p>
<pre>   [ 
    { 
     "id": 1,  
     "label": 0,  
     "name": "Abyssinian",  
     "url": "/uploads/2017-05-23_14-56-45_Abyssinian-cat.jpeg" 
    },  
    { 
     "id": 2,  
      "label": 32,  
      "name": "Siamese",  
     "url": "/uploads/2017-05-23_14-57-33_fat-Siamese-cat.jpeg" 
    } 
   ] </pre>
<p>In this JSON, <kbd>label</kbd> is the label that the user has chosen, and URL is the link to download the image from. For every image, we will download it into the <kbd>tmp</kbd> folder and use <kbd>imread</kbd> and <kbd>imsave</kbd> from <kbd>scipy</kbd> to make sure that the image is in JPEG format. We also create a <kbd>trainval.txt</kbd> and <kbd>test.txt</kbd> file, as in the training dataset.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Performing a fine-tune on the model</h1>
                </header>
            
            <article>
                
<p>In order to fine-tune the model, we need to know which one is the latest model and its corresponding checkpoint to restore <kbd>weights</kbd> and <kbd>biases</kbd>. Therefore, we call the <kbd>/model</kbd> endpoint to get the checkpoint name and a version number:</p>
<pre>    def get_latest_model(url): 
    response = requests.get("%s/model" % url) 
    data = json.loads(response.text) 
    print(data) 
    return data["ckpt_name"], int(data["version"]) </pre>
<p>The response JSON should look like this:</p>
<pre>    { 
     "ckpt_name": "2017-05-26_02-12-49",  
     "id": 10,  
     "link": "http://1.53.110.161:8181/pet-model/8.zip",  
     "name": "pet-model",  
     "version": 8 
    } </pre>
<p>Now, we will implement the code to fine-tune the model. Let's start with some parameters:</p>
<pre>    # Server info 
    URL = "http://localhost:5000" 
    dest_api = URL + "/model" 
 
    # Server Endpoints 
    source_api = "http://1.53.110.161:8181" 
 
    # Dataset 
    dataset_dir = "data/train_data" 
    user_dir = "data/user_data" 
    batch_size = 64 
    image_size = 224 
 
    # Learning rate 
    initial_learning_rate = 0.0001 
    decay_steps = 250 
    decay_rate = 0.9 
 
    # Validation 
    output_steps = 10  # Number of steps to print output 
    eval_steps = 20  # Number of steps to perform evaluations 
 
    # Training 
    max_steps = 3000  # Number of steps to perform training 
    save_steps = 200  # Number of steps to perform saving    
    checkpoints 
    num_tests = 5  # Number of times to test for test accuracy 
    max_checkpoints_to_keep = 1 
    save_dir = "data/checkpoints" 
    train_vars = 'models/fc8-pets/weights:0,models/fc8- 
    pets/biases:0' 
 
    # Get the latest model 
    last_checkpoint_name, last_version = get_latest_model(URL) 
    last_checkpoint_dir = os.path.join(save_dir,   
    last_checkpoint_name) 
 
    # Export 
    export_dir = "/home/ubuntu/models/" 
    export_name = "pet-model" 
    export_version = last_version + 1 </pre>
<p>Then, we will implement the fine-tune loop. In the following code, we call <kbd>download_user_data</kbd> to download all the user-labeled images and pass <kbd>user_dir</kbd> into <kbd>input_pipeline</kbd> so that it will load the new images:</p>
<pre>    # Download user-labels data 
    download_user_data(URL, user_dir) 
 
    images, labels = datasets.input_pipeline(dataset_dir,     
    batch_size, is_training=True, user_dir=user_dir) 
    test_images, test_labels =    
    datasets.input_pipeline(dataset_dir, batch_size,    
    is_training=False, user_dir=user_dir) 
 
     with tf.variable_scope("models") as scope: 
     logits = nets.inference(images, is_training=True) 
     scope.reuse_variables() 
     test_logits = nets.inference(test_images, is_training=False) 
 
    total_loss = models.compute_loss(logits, labels) 
    train_accuracy = models.compute_accuracy(logits, labels) 
    test_accuracy = models.compute_accuracy(test_logits,  
    test_labels) 
 
    global_step = tf.Variable(0, trainable=False) 
    learning_rate = models.get_learning_rate(global_step,      
    initial_learning_rate, decay_steps, decay_rate) 
    train_op = models.train(total_loss, learning_rate,  
    global_step, train_vars) 
 
    saver = tf.train.Saver(max_to_keep=max_checkpoints_to_keep) 
    checkpoint_name = datetime.now().strftime("%Y-%m-%d_%H-%M-%S") 
    checkpoints_dir = os.path.join(save_dir, checkpoint_name) 
    if not os.path.exists(save_dir): 
      os.mkdir(save_dir) 
    if not os.path.exists(checkpoints_dir): 
      os.mkdir(checkpoints_dir) 
 
    with tf.Session() as sess: 
      sess.run(tf.global_variables_initializer()) 
      coords = tf.train.Coordinator() 
      threads = tf.train.start_queue_runners(sess=sess,   
      coord=coords) 
 
    saver.restore(sess,  
    models.get_model_path_from_ckpt(last_checkpoint_dir)) 
    sess.run(global_step.assign(0)) 
 
    last_saved_test_accuracy = 0 
    for i in range(num_tests): 
        last_saved_test_accuracy += sess.run(test_accuracy) 
    last_saved_test_accuracy /= num_tests 
    should_export = False 
    print("Last model test accuracy    
    {}".format(last_saved_test_accuracy)) 
    for i in tqdm(range(max_steps), desc="training"): 
        _, loss_value, lr_value = sess.run([train_op, total_loss,   
        learning_rate]) 
 
     if (i + 1) % output_steps == 0: 
       print("Steps {}: Loss = {:.5f} Learning Rate =   
       {}".format(i + 1, loss_value, lr_value)) 
 
        if (i + 1) % eval_steps == 0: 
          test_acc, train_acc, loss_value =  
          sess.run([test_accuracy, train_accuracy, total_loss]) 
            print("Test accuracy {} Train accuracy {} : Loss =  
            {:.5f}".format(test_acc, train_acc, loss_value)) 
 
        if (i + 1) % save_steps == 0 or i == max_steps - 1: 
          test_acc = 0 
          for i in range(num_tests): 
            test_acc += sess.run(test_accuracy) 
            test_acc /= num_tests 
 
        if test_acc &gt; last_saved_test_accuracy: 
          print("Save steps: Test Accuracy {} is higher than  
          {}".format(test_acc, last_saved_test_accuracy)) 
          last_saved_test_accuracy = test_acc 
          saved_file = saver.save(sess, 
                                      
        os.path.join(checkpoints_dir, 'model.ckpt'), 
                                        global_step=global_step) 
                should_export = True 
                print("Save steps: Save to file %s " % saved_file) 
            else: 
                print("Save steps: Test Accuracy {} is not higher  
       than {}".format(test_acc, last_saved_test_accuracy)) 
 
    if should_export: 
        print("Export model with accuracy ",  
        last_saved_test_accuracy) 
        models.export_model(checkpoints_dir, export_dir,   
        export_name, export_version) 
        archive_and_send_file(source_api, dest_api,  
        checkpoint_name, export_dir, export_name, export_version) 
      coords.request_stop() 
      coords.join(threads)</pre>
<p>Other parts are quite similar to the training loop. However, instead of loading the weights from the <kbd>caffe</kbd> model, we use the checkpoint of the latest model and run the test a few times to get its test accuracy.</p>
<p>At the end of the fine-tune loop, we need a new method named <kbd>archive_and_send_file</kbd> to make an archive from the <kbd>exported</kbd> model and send the link to the production server:</p>
<pre>    def make_archive(dir_path): 
    return shutil.make_archive(dir_path, 'zip', dir_path) 
 
 
    def archive_and_send_file(source_api, dest_api, ckpt_name,    
    export_dir, export_name, export_version): 
    model_dir = os.path.join(export_dir, export_name,    
    str(export_version)) 
    file_path = make_archive(model_dir) 
    print("Zip model: ", file_path) 
 
    data = { 
        "link": "{}/{}/{}".format(source_api, export_name,  
     str(export_version) + ".zip"), 
        "ckpt_name": ckpt_name, 
        "version": export_version, 
        "name": export_name, 
    } 
     r = requests.post(dest_api, data=data) 
    print("send_file", r.text) </pre>
<p>You should note that we create a link with the <kbd>source_api</kbd> parameter, which is the link to the training server, <kbd><span class="URLPACKT">http://1.53.110.161:8181</span></kbd>. We will set up a simple Apache Server to support this function. However, in reality, we suggest that you upload the archived model to cloud storage such as Amazon S3. Now, we will show you the simplest way with Apache.</p>
<p>We need to install Apache with the following command:</p>
<pre><strong>sudo apt-get install apache2</strong></pre>
<p>Now, in <kbd>/etc/apache2/ports.conf</kbd>, on line 6, we need to add this code to make <kbd>apache2</kbd> listen on port <kbd>8181</kbd>:</p>
<pre>    Listen 8181 </pre>
<p>Then, add the following code at the beginning of <kbd>/etc/apache2/sites-available/000-default.conf</kbd> to support downloading from the <kbd>/home/ubuntu/models</kbd> directory:</p>
<pre>    &lt;VirtualHost *:8181&gt; 
      DocumentRoot "/home/ubuntu/models" 
      &lt;Directory /&gt; 
        Require all granted 
      &lt;/Directory&gt; 
    &lt;/VirtualHost&gt; </pre>
<p>Finally, we need to restart the <kbd>apache2</kbd> server:</p>
<pre><strong>sudo service apache2 restart</strong></pre>
<p>Up to now, we have set up all the code to perform fine-tuning. Before running the fine-tuning for the first time, we need to send a <kbd>POST</kbd> request to the <kbd>/model</kbd> endpoint with the information about our first model because we have already copied the model to the production server.</p>
<p>In the <kbd>project</kbd> repository, let's run the <kbd>finetune</kbd> script:</p>
<pre><strong>python scripts/finetune.py</strong></pre>
<p>The last few lines in the console will look like the following:</p>
<pre>    <strong>Save steps: Test Accuracy 0.84 is higher than 0.916875</strong>
    <strong>Save steps: Save to file data/checkpoints/2017-05-29_18-46-43/model.ckpt-2000</strong>
    <strong>('Export model with accuracy ', 0.916875000000004)</strong>
    <strong>2017-05-29 18:47:31.642729: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:01:00.0)</strong>
    <strong>('Exported model at', '/home/ubuntu/models/pet-model/2')</strong>
    <strong>('Zip model: ', '/home/ubuntu/models/pet-model/2.zip')</strong>
    <strong>('send_file', u'{\n  "ckpt_name": "2017-05-29_18-46-43", \n  "id": 2, \n  "link": "http://1.53.110.161:8181/pet-model/2.zip", \n  "name": "pet-model", \n  "version": 2\n}\n')</strong></pre>
<p>As you can see, the new model has a test accuracy of 91%. The model is also exported and archived to <kbd>/home/ubuntu/models/pet-model/2.zip</kbd>. The code is also calling the <kbd>/model</kbd> endpoint to post the link to the production server. In the logging of the Flask app in the production server, we will get the following results:</p>
<pre><strong>('Start downloading', u'http://1.53.110.161:8181/pet-model/2.zip')</strong>
<strong>('Downloaded file at', u'/tmp/2.zip')</strong>
<strong>('Extracted at', u'/home/ubuntu/productions/2')</strong>
<strong>127.0.0.1 - - [29/May/2017 18:49:05] "POST /model HTTP/1.1" 200 -</strong></pre>
<p>This means that our Flask app had downloaded the <kbd>2.zip</kbd> file from the training server and extracted the content to <kbd>/home/ubuntu/productions/2</kbd>. In the <kbd>tmux</kbd> session for TensorFlow Serving, you will also get the following results:</p>
<pre>    <strong>2017-05-29 18:49:06.234808: I tensorflow_serving/core/loader_harness.cc:86] Successfully loaded servable version {name: pet-model version: 2}</strong>
    <strong>2017-05-29 18:49:06.234840: I tensorflow_serving/core/loader_harness.cc:137] Quiescing servable version {name: pet-model version: 1}</strong>
    <strong>2017-05-29 18:49:06.234848: I tensorflow_serving/core/loader_harness.cc:144] Done quiescing servable version {name: pet-model version: 1}</strong>
    <strong>2017-05-29 18:49:06.234853: I tensorflow_serving/core/loader_harness.cc:119] Unloading servable version {name: pet-model version: 1}</strong>
    <strong>2017-05-29 18:49:06.240118: I ./tensorflow_serving/core/simple_loader.h:226] Calling MallocExtension_ReleaseToSystem() with 645327546</strong>
    <strong>2017-05-29 18:49:06.240155: I tensorflow_serving/core/loader_harness.cc:127] Done unloading servable version {name: pet-model version: 1}</strong></pre>
<p>This output indicates that the TensorFlow model server has successfully loaded <kbd>version 2</kbd> of the <kbd>pet-model</kbd> and unloaded <kbd>version 1</kbd>. This also means that we have served the new model, which was trained on the training server and sent to the production server via the <kbd>/model</kbd> endpoint.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up cronjob to run every day</h1>
                </header>
            
            <article>
                
<p>Finally, we need to set up the fine-tuning to run every day and automatically upload the new model to the server. We can achieve this easily by creating a <kbd>crontab</kbd> in the training server.</p>
<p>First, we need to run the <kbd>crontab</kbd> command:</p>
<pre><strong>crontab -e</strong></pre>
<p>Then, we can just add the following line to define the time that we want <kbd>finetune.py</kbd> to run:</p>
<pre><strong>0 3 * * * python /home/ubuntu/project/scripts/finetune.py</strong></pre>
<p>As we defined, the Python command will run at 3 a.m. every day.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have implemented a complete real-life production, from training to serving a deep learning model. We also created a web interface in a Flask app so that users can upload their images and receive results. Our model can automatically be fine-tuned every day to improve the quality of the system. There are a few things that you can consider to improve the overall system:</p>
<ul>
<li>The model and checkpoints should be saved in cloud storage.</li>
<li>The Flask app and TensorFlow Serving should be managed by another, better process management system, such as Supervisor.</li>
<li>There should be a web interface so that the team can approve the labels that users select. We shouldn't rely completely on users to decide the training set.</li>
<li>TensorFlow Serving should be built with GPU support to achieve the best performance.</li>
</ul>


            </article>

            
        </section>
    </body></html>