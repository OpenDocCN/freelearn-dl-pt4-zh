["```py\npip install -U pandas numpy scikit-learn sktime torch pytorch-forecasting pytorch-lightning gluonts neuralforecast\n```", "```py\nimport numpy as np\nimport pandas as pd\nfrom gluonts.dataset.repository.datasets import get_dataset\nfrom pytorch_forecasting import TimeSeriesDataSet\nimport lightning.pytorch as pl\nfrom sklearn.model_selection import train_test_split\ndataset = get_dataset('nn5_daily_without_missing', regenerate=False)\nN_LAGS = 7\nHORIZON = 7\ndatamodule = GlobalDataModule(data=dataset,\n    n_lags=N_LAGS,\n    horizon=HORIZON)\n```", "```py\n    datamodule.setup()\n    ```", "```py\n    from pytorch_forecasting import NBeats\n    model = NBeats.from_dataset(\n        dataset=datamodule.training,\n        stack_types=['trend', 'seasonality'],\n        num_blocks=[3, 3],\n        num_block_layers=[4, 4],\n        widths=[256, 2048],\n        sharing=[True],\n        backcast_loss_ratio=1.0,\n    )\n    ```", "```py\n    import lightning.pytorch as pl\n    from lightning.pytorch.callbacks import EarlyStopping\n    early_stop_callback = EarlyStopping(monitor=\"val_loss\",\n        min_delta=1e-4,\n        patience=10,\n        verbose=False,\n        mode=\"min\")\n    trainer = pl.Trainer(\n        max_epochs=30,\n        accelerator=\"auto\",\n        enable_model_summary=True,\n        gradient_clip_val=0.01,\n        callbacks=[early_stop_callback],\n    )\n    ```", "```py\n    trainer.fit(\n        model,\n        train_dataloaders=datamodule.train_dataloader(),\n        val_dataloaders=datamodule.val_dataloader(),\n    )\n    ```", "```py\n    best_model_path = trainer.checkpoint_callback.best_model_path\n    best_model = NBeats.load_from_checkpoint(best_model_path)\n    ```", "```py\n    predictions = best_model.predict(datamodule.test.to_dataloader(batch_size=1, shuffle=False))\n    actuals = torch.cat(\n        [y[0] for x, y in iter(\n            datamodule.test.to_dataloader(batch_size=1, \n                shuffle=False))])\n    ```", "```py\n    (actuals - predictions).abs().mean()\n    ```", "```py\n    forecasts = best_model.predict(datamodule.predict_dataloader())\n    ```", "```py\n    raw_predictions = best_model.predict\n        (datamodule.val_dataloader(),\n        mode=\"raw\",\n        return_x=True)\n    best_model.plot_interpretation(x=raw_predictions[1],\n        output=raw_predictions[0],\n        idx=0)\n    ```", "```py\ndatamodule = GlobalDataModule(data=dataset,\n    n_lags=N_LAGS,\n    horizon=HORIZON,\n    batch_size=32,\n    test_size=0.2)\ndatamodule.setup()\n```", "```py\nfrom lightning.pytorch.tuner import Tuner\nimport lightning.pytorch as pl\nfrom pytorch_forecasting import NBeats\ntrainer = pl.Trainer(accelerator=\"auto\", gradient_clip_val=0.01)\ntuner = Tuner(trainer)\nmodel = NBeats.from_dataset(\n    dataset=datamodule.training,\n    stack_types=['trend', 'seasonality'],\n    num_blocks=[3, 3],\n    num_block_layers=[4, 4],\n    widths=[256, 2048],\n    sharing=[True],\n    backcast_loss_ratio=1.0,\n)\n```", "```py\nlr_optim = tuner.lr_find(model,\n    train_dataloaders=datamodule.train_dataloader(),\n    val_dataloaders=datamodule.val_dataloader(),\n    min_lr=1e-5)\n```", "```py\nlr_optim.suggestion()\nfig = lr_optim.plot(show=True, suggest=True)\nfig.show()\n```", "```py\npip install gluonts pytorch\n```", "```py\nfrom gluonts.dataset.repository.datasets import get_dataset\ndataset = get_dataset(\"nn5_daily_without_missing\", regenerate=False)\n```", "```py\nprint(dataset.metadata)\n```", "```py\nfrom gluonts.transform import AddAgeFeature\ntransformation_with_age = Chain([\n    AddAgeFeature(output_field=\"age\",\n    target_field=\"target\",\n    pred_length=dataset.metadata.prediction_length)\n])\ntransformed_train_with_age = TransformedDataset(dataset.train, \n    transformation_with_age)\n```", "```py\ntraining_data = list(dataset.train)\nprint(training_data[0])\n```", "```py\nfrom gluonts.torch.model.simple_feedforward import SimpleFeedForwardEstimator\nestimator_with_age = SimpleFeedForwardEstimator(\n    hidden_dimensions=[10],\n    prediction_length=dataset.metadata.prediction_length,\n    context_length=100,\n    trainer_kwargs={'max_epochs': 100}\n)\n```", "```py\npredictor_with_age = estimator_with_age.train\n    (transformed_train_with_age)\n```", "```py\nforecast_it_with_age, ts_it_with_age = make_evaluation_predictions(\n    dataset=dataset.test,\n    predictor=predictor_with_age,\n    num_samples=100,\n)\nforecasts_with_age = list(forecast_it_with_age)\ntss_with_age = list(ts_it_with_age)\nfig, ax = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\nts_entry_with_age = tss_with_age[0]\nax[0].plot(ts_entry_with_age[-150:].to_timestamp())\nforecasts_with_age[0].plot(show_label=True, ax=ax[0])\nax[0].set_title(\"Forecast with AddAgeFeature\")\nax[0].legend()\n```", "```py\nfrom gluonts.dataset.repository.datasets import get_dataset\ndataset = get_dataset(\"nn5_daily_without_missing\", regenerate=False)\n```", "```py\n    from gluonts.dataset.common import ListDataset\n    from gluonts.dataset.common import FieldName\n    train_ds = ListDataset(\n        [\n            {FieldName.TARGET: entry[\"target\"], \n                FieldName.START: entry[\"start\"]}\n            for entry in dataset.train\n        ],\n        freq=dataset.metadata.freq,\n    )\n    ```", "```py\n    from gluonts.torch.model.deepar import DeepAREstimator\n    N_LAGS=7\n    HORIZON=7\n    estimator = DeepAREstimator(\n        prediction_length=HORIZON,\n        context_length=N_LAGS,\n        freq=dataset.metadata.freq,\n        trainer_kwargs={\"max_epochs\": 100},\n    )\n    ```", "```py\n    predictor = estimator.train(train_ds)\n    ```", "```py\n    forecast_it, ts_it = make_evaluation_predictions(\n        dataset=dataset.test,\n        predictor=predictor,\n        num_samples=100,\n    )\n    forecasts = list(forecast_it)\n    tss = list(ts_it)\n    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n    ts_entry = tss[0]\n    ax.plot(ts_entry[-150:].to_timestamp())\n    forecasts[0].plot(show_label=True, ax=ax, intervals=())\n    ax.set_title(\"Forecast with DeepAR\")\n    ax.legend()\n    plt.tight_layout()\n    plt.show()\n    ```", "```py\n    from gluonts.dataset.repository.datasets import get_dataset\n    import pandas as pd\n    from sklearn.preprocessing import StandardScaler\n    import matplotlib.pyplot as plt\n    from neuralforecast.core import NeuralForecast\n    from neuralforecast.models import VanillaTransformer\n    dataset = get_dataset(\"nn5_daily_without_missing\", regenerate=False)\n    N_LAGS = 7\n    HORIZON = 7\n    ```", "```py\n    data_list = list(dataset.train)\n    data_list = [\n        pd.Series(\n            ds[\"target\"],\n            index=pd.date_range(\n                start=ds[\"start\"].to_timestamp(),\n                freq=ds[\"start\"].freq,\n                periods=len(ds[\"target\"]),\n            ),\n        )\n        for ds in data_list\n    ]\n    tseries_df = pd.concat(data_list, axis=1)\n    tseries_df[tseries_df.columns] = \n        \\StandardScaler().fit_transform(tseries_df)\n    tseries_df = tseries_df.reset_index()\n    df = tseries_df.melt(\"index\")\n    df.columns = [\"ds\", \"unique_id\", \"y\"]\n    df[\"ds\"] = pd.to_datetime(df[\"ds\"])\n    ```", "```py\n    model = [\n        VanillaTransformer(\n            h=HORIZON,\n            input_size=N_LAGS,\n            max_steps=100,\n            val_check_steps=5,\n            early_stop_patience_steps=3,\n        ),\n    ]\n    nf = NeuralForecast(models=model, freq=\"D\")\n    Y_df = df[df[\"unique_id\"] == 0]\n    Y_train_df = Y_df.iloc[:-2*HORIZON]\n    Y_val_df = Y_df.iloc[-2*HORIZON:-HORIZON]\n    training_df = pd.concat([Y_train_df, Y_val_df])\n    nf.fit(df=training_df, val_size=HORIZON)\n    ```", "```py\n    forecasts = nf.predict()\n    Y_df = df[df[\"unique_id\"] == 0]\n    Y_hat_df = forecasts[forecasts.index == 0].reset_index()\n    Y_hat_df = Y_test_df.merge(Y_hat_df, how=\"outer\", \n        on=[\"unique_id\", \"ds\"])\n    plot_df = pd.\n        concat([Y_train_df, Y_val_df, Y_hat_df]).set_index(\"ds\")\n    plot_df = plot_df.iloc[-150:]\n    fig, ax = plt.subplots(1, 1, figsize=(20, 7))\n    plot_df[[\"y\", \"VanillaTransformer\"]].plot(ax=ax, linewidth=2)\n    ax.set_title(\"First Time Series Forecast with Transformer\", fontsize=22)\n    ax.set_ylabel(\"Value\", fontsize=20)\n    ax.set_xlabel(\"Timestamp [t]\", fontsize=20)\n    ax.legend(prop={\"size\": 15})\n    ax.grid()\n    plt.show()\n    ```", "```py\nfrom gluonts.dataset.common import ListDataset, FieldName\nfrom gluonts.dataset.repository.datasets import get_dataset\ndataset = get_dataset(\"nn5_daily_without_missing\", regenerate=False)\ntrain_ds = ListDataset(\n    [\n        {FieldName.TARGET: entry[\"target\"], FieldName.START: entry[\"start\"]}\n        for entry in dataset.train\n    ],\n    freq=dataset.metadata.freq,\n)\n```", "```py\n    from gluonts.torch.model.tft import TemporalFusionTransformerEstimator\n    N_LAGS = 7\n    HORIZON = 7\n    estimator = TemporalFusionTransformerEstimator(\n        prediction_length=HORIZON,\n        context_length=N_LAGS,\n        freq=dataset.metadata.freq,\n        trainer_kwargs={\"max_epochs\": 100},\n    )\n    ```", "```py\n    predictor = estimator.train(train_ds)\n    ```", "```py\n    from gluonts.evaluation import make_evaluation_predictions\n    forecast_it, ts_it = make_evaluation_predictions(\n        dataset=dataset.test,\n        predictor=predictor,\n        num_samples=100,\n    )\n    ```", "```py\n    import matplotlib.pyplot as plt\n    ts_entry = tss[0]\n    ax.plot(ts_entry[-150:].to_timestamp())\n    forecasts[0].plot(show_label=True, ax=ax, intervals=())\n    ax.set_title(\"Forecast with Temporal Fusion Transformer\")\n    ax.legend()\n    plt.tight_layout()\n    plt.show()\n    ```", "```py\nfrom gluonts.dataset.repository.datasets import get_dataset\ndataset = get_dataset('nn5_daily_without_missing')\n```", "```py\n    import pandas as pd\n    from sklearn.preprocessing import StandardScaler\n    data_list = list(dataset.train)\n    data_list = [pd.Series(ds['target'],\n        index=pd.date_range(start=ds['start'].to_timestamp(),\n            freq=ds['start'].freq,\n            periods=len(ds['target'])))\n        for ds in data_list]\n    tseries_df = pd.concat(data_list, axis=1)\n    tseries_df[tseries_df.columns] = \\\n        StandardScaler().fit_transform(tseries_df)\n    tseries_df = tseries_df.reset_index()\n    df = tseries_df.melt('index')\n    df.columns = ['ds', 'unique_id', 'y']\n    df['ds'] = pd.to_datetime(df['ds'])\n    n_time = len(df.ds.unique())\n    val_size = int(.2 * n_time)\n    ```", "```py\n    from neuralforecast.core import NeuralForecast\n    from neuralforecast.models import Informer\n    N_LAGS = 7\n    HORIZON = 7\n    model = [Informer(h=HORIZON,\n        input_size=N_LAGS,\n        max_steps=1000,\n        val_check_steps=25,\n        early_stop_patience_steps=10)]\n    nf = NeuralForecast(models=model, freq='D')\n    ```", "```py\n    nf.fit(df=df, val_size=val_size)\n    ```", "```py\n    forecasts = nf.predict()\n    forecasts.head()\n    ```", "```py\nval_size = int(.1 * n_time)\ntest_size = int(.1 * n_time)\n```", "```py\nfrom neuralforecast.models import Informer, VanillaTransformer\nmodels = [\n    Informer(h=HORIZON,\n        input_size=N_LAGS,\n        max_steps=1000,\n        val_check_steps=10,\n        early_stop_patience_steps=15),\n    VanillaTransformer(h=HORIZON,\n        input_size=N_LAGS,\n        max_steps=1000,\n        val_check_steps=10,\n        early_stop_patience_steps=15),\n]\n```", "```py\nfrom neuralforecast.core import NeuralForecast\nnf = NeuralForecast(\n    models=models,\n    freq='D')\ncv = nf.cross_validation(df=df,\n    val_size=val_size,\n    test_size=test_size,\n    n_windows=None)\n```", "```py\nfrom neuralforecast.losses.numpy import mae\nmae_informer = mae(cv['y'], cv['Informer'])\nmae_transformer = mae(cv['y'], cv['VanillaTransformer'])\n```"]