- en: Improving Your RNN Performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter goes through some techniques for improving your recurrent neural
    network model. Often, the initial results from your model can be disappointing,
    so you need to find ways of improving them. This can be done with various methods
    and tools, but we will focus on two main areas:'
  prefs: []
  type: TYPE_NORMAL
- en: Improving the RNN model performance with data and tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing the TensorFlow library for better results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First, we will see how more data, as well as tuning the hyperparameters, can
    yield significantly better results. Then our focus will shift to getting the most
    out of the built-in TensorFlow functionality. Both approaches are applicable to
    any task that involves the neural network model, so the next time you want to
    do image recognition with convolutional networks or fix a rescaled image with
    GAN, you can apply the same techniques for perfecting your model.
  prefs: []
  type: TYPE_NORMAL
- en: Improving your RNN model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When working on a problem using RNN (or any other network), your process looks
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f0060b1e-ad15-412c-97bd-61882cb9331a.png)'
  prefs: []
  type: TYPE_IMG
- en: First, you come up with an **idea for the model**, its hyperparameters, the
    number of layers, how deep the network should be, and so on. Then the model is **implemented
    and trained** in order to produce some results. Finally, these results are **assessed**
    and the necessary modifications are made. It is rarely the case that you'll receive
    meaningful results from the first run. This cycle may occur multiple times until
    you are satisfied with the outcome.
  prefs: []
  type: TYPE_NORMAL
- en: 'Considering this approach, one important question comes to mind: *How can we
    change the model so the next cycle produces better results?*'
  prefs: []
  type: TYPE_NORMAL
- en: This question is tightly connected to your understanding of the network's results.
    Let's discuss that now.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you already know, in the beginning of each model training, you need to prepare
    lots of quality data. This step should happen before the **Idea** part of the
    aforementioned cycle. Then, during the Idea stage, you should come up with the
    actual neural network and its characteristics. After that comes the Code stage,
    where you use your data to supply the model and perform the actual training. There
    is something important to keep in mind—*once your data is collected, you need
    to split it into 3 parts: training (80%), validation (10%) and testing (10%)*.'
  prefs: []
  type: TYPE_NORMAL
- en: The Code stage only uses the training part of your data. Then, the Experiment stage
    uses the validation part to evaluate the model. Based on the results of these
    two operations, we will make the necessary changes.
  prefs: []
  type: TYPE_NORMAL
- en: You should use the testing data **only** after you have gone through all the
    necessary cycles and have identified that your model is performing well. The testing
    data will help you understand the rate of accuracy you are receiving on unseen
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the end of each cycle, you need to determine how good your model is. Based
    on the results, you will see that your model is always either **underfitting**
    (**high bias**) or **overfitting** (**high variance**) the data (by varying degrees).
    You should aim for both the bias and variance to be low, so there is almost no
    underfitting or overfitting. The next diagram may help you understand this concept
    better:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cf47792e-c51b-4d37-b84f-97657b1f4f3d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Examining the preceding diagram, we can state the following definitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Underfitting (high bias)**: This occurs when the network is not influenced
    enough by the training data, and generalizes the prediction'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Just Right (low bias, low variance)**: This occurs when the network makes
    quality predictions, both during training and in the general case during testing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Overfitting (high variance)**: This occurs when the network is influenced
    by the training data too much, and makes false decisions on new entries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The preceding diagram may be helpful to understand the concepts of high bias
    and high variance, but it is difficult to apply this to real examples. The problem
    is that we normally deal with data of more than two dimensions. That is why we
    will be using the loss (error) function values produced by the model to make the
    same evaluation for higher dimensional data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say we are evaluating the Spanish-to-English translator neural network
    from [Chapter 4](982f956b-d1b6-446c-85b1-71f55faf114f.xhtml), *Creating a Spanish-to-English
    Translator*. We can assume that the lowest possible error on that task can be
    produced by a human, and it is 1.5%. Now we will evaluate the results based on
    all the error combinations that our network can give:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Training data error: ~2%; Validation data error: ~14%: **high variance**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Training data error: ~14%; Validation data error: ~15%: **high bias**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Training data error: ~14%; Validation data error: ~30%: **high variance, high
    bias**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Training data error:~2%; Validation data error: ~2.4%: **low variance, low
    bias**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The desired output is having low variance and low bias. Of course, it takes
    a lot of time and effort to get this kind of improvement, but in the end, it is
    worth doing.
  prefs: []
  type: TYPE_NORMAL
- en: You have now got familiar with how to read your model results and evaluate the
    model's performance. Now, let's see what can be done to **lower both the variance
    and the bias of the model**.
  prefs: []
  type: TYPE_NORMAL
- en: '***How can we lower the variance? (fixing overfitting)***'
  prefs: []
  type: TYPE_NORMAL
- en: A very useful approach is to collect and transform more data. This will generalize
    the model and make it perform well on both the training and validation sets.
  prefs: []
  type: TYPE_NORMAL
- en: '***How can we lower the bias?* *(fixing underfitting)***'
  prefs: []
  type: TYPE_NORMAL
- en: This can be done by increasing the network depth—that is, changing the numbers
    of layers and of hidden units, and tuning the hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will cover both of these approaches and see how to use them effectively
    to improve our neural network's performance.
  prefs: []
  type: TYPE_NORMAL
- en: Improving performance with data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A large amount of quality data is critical for the success of any deep learning
    model. A good comparison can be made to other algorithms, where an increased volume
    of data does not necessarily improve performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b802c1b6-d305-48c4-a183-2a451e703b50.png)'
  prefs: []
  type: TYPE_IMG
- en: But this doesn't mean that gathering more data is always the right approach.
    For example, if our model suffers from underfitting, more data won't increase
    the performance. On the other hand, solving the overfitting problem can be done
    using exactly that approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'Improving the model performance with data comes in three steps: **selecting
    data**, **processing data**, and **transforming data**. It is important to note
    that all three steps should be done according to your specific problem. For some
    tasks, such as recognizing digits inside an image, a nicely formatted dataset
    can be found easily. For more concrete tasks (e.g. analyzing images from plants),
    you may need to experiment and come up with non-trivial decisions.'
  prefs: []
  type: TYPE_NORMAL
- en: Selecting data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is a pretty straightforward technique. You either collect more dataor invent
    more training examples.
  prefs: []
  type: TYPE_NORMAL
- en: Finding more data can be done using an online collection of datasets ([https://skymind.ai/wiki/open-datasetshttps://skymind.ai/wiki/open-datasets](https://skymind.ai/wiki/open-datasetshttps://skymind.ai/wiki/open-datasets)). Other
    methods are to scrape web pages, or use the advanced options of Google Search ([https://www.google.com/advanced_search](https://www.google.com/advanced_search)).
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, inventing or augmenting data is a challenging and complex
    problem, especially if we are trying to generate text or images. For example,
    a new approach ([https://www.quora.com/What-data-augmentation-techniques-are-available-for-deep-learning-on-text](https://www.quora.com/What-data-augmentation-techniques-are-available-for-deep-learning-on-text)) for
    augmenting text was created recently. It is done by translating an English sentence
    to another language and then back to English. This way we are getting two slightly
    different but meaningful sentences, which increases and diversifies our dataset substantially.
    Another interesting technique for augmenting data, specifically for RNN language
    models, can be found in the paper on *Data Noising as Smoothing in Neural Network
    Language Models* ([https://arxiv.org/abs/1703.02573](https://arxiv.org/abs/1703.02573)).
  prefs: []
  type: TYPE_NORMAL
- en: Processing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After you have selected the required data, the time comes for processing. This
    can be done with these three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Formatting**: This involves converting the data into the most suitable format
    for your application. Imagine, for example, that your data is the text from thousands
    of PDF files. You should extract the text and covert the data into CSV format.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cleaning**: Often, it is the case that your data may be incomplete. For example,
    if you have scraped book metadata from the internet, some entries may have missing
    data (such as ISBN, date of writing, and so on). Your job is to decide whether
    to fix or discard the metadata for the whole book.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sampling**: Using a small part of the dataset can reduce computational time
    and speed up your training cycles while you are determining the model accuracy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The order of the preceding steps is not determined, and you may revisit them
    multiple times.
  prefs: []
  type: TYPE_NORMAL
- en: Transforming data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, you need to transform the data using techniques such as scaling, decomposition,
    and feature selection. First, it is good to plot/visualize your data using Matplotlib
    (a Python library) or TensorFlow's TensorBoard ([https://www.tensorflow.org/guide/summaries_and_tensorboard](https://www.tensorflow.org/guide/summaries_and_tensorboard)).
  prefs: []
  type: TYPE_NORMAL
- en: '*Scaling* is a technique that converts every entry into a number within a specific
    range (0-1) without mitigating its effectiveness. Normally, scaling is done within the
    bounds of your activation functions. If you are using sigmoid activation functions,
    rescale your data to values between 0 and 1\. If you''re using the hyperbolic
    tangent (tanh), rescale to values between -1 and 1\. This applies to inputs (x)
    and outputs (y).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Decomposition* is a technique of splitting some features into their components
    and using them instead. For example, the feature time may have minutes and hours,
    but we care only about the minutes.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Feature selection* is one of the most important decisions you would make when
    building your model. A great tutorial to follow when deciding how to choose the
    most appropriate features is Jason Brownlee''s *An Introduction to Feature Selection*
    ([https://machinelearningmastery.com/an-introduction-to-feature-selection/](https://machinelearningmastery.com/an-introduction-to-feature-selection/)).'
  prefs: []
  type: TYPE_NORMAL
- en: Processing and transforming data can be accomplished using the vast selection
    of Python libraries, such as NumPy, among others. They turn out to be pretty handy
    when it comes to data manipulation.
  prefs: []
  type: TYPE_NORMAL
- en: After you have gone through all of the preceding steps (probably multiple times),
    you can move forward to building your neural network model.
  prefs: []
  type: TYPE_NORMAL
- en: Improving performance with tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After selecting, processing, and transforming your data, it's time for a second
    optimization technique—hyperparameter tuning. This approach is one of the most
    important components in building your model and you need to spend the time necessary
    to execute it well.
  prefs: []
  type: TYPE_NORMAL
- en: Every neural network model has parameters and hyperparameters. These are two
    distinct sets of values. Parameters are learned by the model during training,
    such as weights and biases. On the other hand, hyperparameters are predefined
    values that are selected after careful observation. In a standard recurrent neural
    network, the set of hyperparameters includes the number of hidden units, number
    of layers, RNN model type, sequence length, batch size, number of epochs (iterations),
    and the learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: Your task is to identify the best of all possible combinations so that the network
    performs pretty well. This is a pretty challenging task and often takes a lot
    of time (hours, days, even months) and computational power.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following Andrew Ng''s tutorial on hyperparameter tuning ([https://www.coursera.org/lecture/deep-neural-network/hyperparameters-tuning-in-practice-pandas-vs-caviar-DHNcc](https://www.coursera.org/lecture/deep-neural-network/hyperparameters-tuning-in-practice-pandas-vs-caviar-DHNcc)),
    we can separate this process into two different techniques: *Pandas* vs *Caviar*.'
  prefs: []
  type: TYPE_NORMAL
- en: The *Pandas* approach follows the way pandas (that is, the animal) raise their
    children. We initialize our model with a specific set of parameters, and then
    improve these values after every training operation until we achieve delightful
    results. This approach is ideal if you lack computational power and multiple GPUs
    to train neural networks simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: The *Caviar* approach follows the way fish reproduce. We introduce multiple
    models at once (using different sets of parameters) and train them at the same
    time, while tracking the results. This technique will likely require access to
    more computational power.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now the question becomes: *How can we decide what should be included in our
    set of hyperparameters?*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Summarizing a great article on hyperparameters optimization ([http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html#tree-structured-parzen-estimators-tpe](http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html#tree-structured-parzen-estimators-tpe)),
    we can define five ways for tuning:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Grid search**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Random search**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hand-tuning**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bayesian optimization**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tree-structured Parzen Estimators** (**TPE**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During the beginning phase of your deep learning journey, you will mostly be utilizing
    grid search, random search, and hand-tuning. The last two techniques are more
    complex in terms of understanding and implementation. We will cover both of them
    in the following section, but bear in mind that, for trivial tasks, you can go
    with normal hand-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Grid search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is the most straightforward way of finding the right hyperparameters.
    It follows the approach in this graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3e6f5dba-6b9c-42ba-a2bc-6ac19146dfae.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we generate all the possible combinations of values for the hyperparameters
    and perform separate training cycles. This works for small neural networks, but
    is impractical for more complex tasks. That is why we should use the better approach
    listed in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Random search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This technique is similar to grid search. You can follow the graph here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/99c9f7e1-d9fe-48a9-8a2c-62eebf40cc6d.png)'
  prefs: []
  type: TYPE_IMG
- en: Instead of taking all the possible combinations, we sample a smaller set of
    random values and use these values to train the model. If we see that a particular
    group of closely positioned dots tends to perform better, we can examine this
    region more closely and focus on it.
  prefs: []
  type: TYPE_NORMAL
- en: Hand-tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bigger networks normally require more time for training. This is why the aforementioned
    approaches are not ideal for such situations. In these cases, we often use the
    hand-tuning technique. The idea is to initially try one set of values, and then
    evaluate the performance. Then, our intuition, as well as our learning experience,
    may lead to ideas on a specific sequence of changes. We perform those tweaks and
    learn new things about the model. After several iterations, we have a good understanding
    of what needs to change for future improvement.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This approach is a way of learning the hyperparameters without the need to manually
    determine different values. It uses a Gaussian process that utilizes a set of
    previously evaluated parameters, and the resultant accuracy, to make an assumption
    about unobserved parameters. An acquisition function uses this information to
    suggest the next set of parameters. For more information, I suggest watching Professor
    Hinton's lecture on *Bayesian optimization of Hyper Parameters *([https://www.youtube.com/watch?v=cWQDeB9WqvU](https://www.youtube.com/watch?v=cWQDeB9WqvU)).
  prefs: []
  type: TYPE_NORMAL
- en: Tree-structured Parzen Estimators (TPE)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The idea behind this approach is that, at each iteration, TPE collects new observations,
    and at the end of the iteration, the algorithm decides which set of parameters
    it should try next. For more information, I suggest taking a look at this amazing
    article on *Hyperparameters optimization for Neural Networks* ([http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html#tree-structured-parzen-estimators-tpe](http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html#tree-structured-parzen-estimators-tpe)).
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing the TensorFlow library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section focuses mostly on practical advice that can be directly implemented
    in your code. The TensorFlow team has provided a large set of tools that can be
    utilized to improve your performance. These techniques are constantly being updated
    to achieve better results. I strongly recommend watching TensorFlow's video on
    training performance from the 2018 TensorFlow conference ([https://www.youtube.com/watch?v=SxOsJPaxHME](https://www.youtube.com/watch?v=SxOsJPaxHME)).
    This video is accompanied by nicely aggregated documentation, which is also a
    must-read ([https://www.tensorflow.org/performance/](https://www.tensorflow.org/performance/)).
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's dive into more details around what you can do to achieve faster and
    more reliable training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first start with an illustration from TensorFlow that presents the general
    steps of training a neural network. You can divide this process into three phases:
    **data processing**, **performing training**, and** optimizing gradients**:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data processing** **(step 1)**: This phase includes fetching the data (locally
    or from a network) and transforming it to fit our needs. These transformations
    might include augmentation, batching, and so on. Normally, these operations are
    done on the **CPU**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Perform training (steps 2a, 2b and 2c)**: This phase includes computing the
    forward pass during training, which requires a specific neural network model—LSTM,
    GPU, or a basic RNN in our case. These operations utilize powerful **GPUs** and
    **TPUs**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Optimize gradients (step 3)**: This phase includes the process of minimizing
    the loss function with the aim of optimizing the weights. The operation is again
    performed on **GRUs** and **TPUs**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This graph illustrates the above steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/48d71258-83c8-4cae-bb5f-4d6f279f9125.png)'
  prefs: []
  type: TYPE_IMG
- en: Next, let's explain how to improve each of these steps.
  prefs: []
  type: TYPE_NORMAL
- en: Data processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You need to examine if loading and transforming the data is the bottleneck of
    your performance. You can do this with several approaches, some of which involve
    estimating the time it takes to perform these tasks, as well as tracking the CPU
    usage.
  prefs: []
  type: TYPE_NORMAL
- en: After you have determined that these operations are slowing down the performance
    of your model, it's time to apply some useful techniques to speed things up.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we said, these operations (loading and transforming data) should be performed
    on the CPU, rather than the GPU, so that you free up the latter for training.
    To ensure this, wrap your code as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Then, you need to focus on both the process of loading (fetching) and transforming
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: Improving data loading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The TensorFlow team has been working hard to make this as easy as possible by
    providing the `tf.data` API ([https://www.tensorflow.org/performance/performance_guide](https://www.tensorflow.org/performance/performance_guide)), which
    works incredibly well. To learn more about it and understand how to use it efficiently,
    I recommend watching TensorFlow's talk on `tf.data` ([https://www.youtube.com/watch?v=uIcqeP7MFH0](https://www.youtube.com/watch?v=uIcqeP7MFH0)).
    This API should always be used, instead of the standard `feed_dict` approach you
    have seen so far.
  prefs: []
  type: TYPE_NORMAL
- en: Improving data transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Transformations can come in different forms, for example, cropping images,
    splitting text, and rendering and batching files. TensorFlow offers solutions
    for these techniques. For example, if you are cropping images before training,
    it is good to use `tf.image.decode_and_crop_jpeg`, which decodes only the part
    of the image required. Another optimization can be made in the batching process.
    The TensorFlow library offers two methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The second method is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s clarify these lines:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Batch normalization is performed to a neural network model to speed up the
    process of training. Refer to this amazing article, *Batch Normalization in Neural
    Networks*, for more details: [https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c](https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `fused` parameter indicates whether or not the method should combine the
    multiple operations, required for batch normalization, into a single kernel.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `data_format` parameter refers to the structure of the Tensor passed to
    a given Operation (such as summation, division, training, and so on). A good explanation
    can be found under *Data formats* in the TensorFlow performance guide ([https://www.tensorflow.org/performance/](https://www.tensorflow.org/performance/)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing the training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let's move on to the phase of performing the training. Here, we are using
    one of TensorFlow's built-in functions for initializing recurrent neural network
    cells and calculating their weights using the preprocessed data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on your situation, different techniques for optimizing your training
    may be more appropriate:'
  prefs: []
  type: TYPE_NORMAL
- en: For small and experimental models, you can use `tf.nn.rnn_cell.BasicLSTMCell`.
    Unfortunately, this is highly inefficient and takes up more memory than the following
    optimized versions. That is why using it is **not** recommended, unless you are
    just experimenting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An optimized version of the previous code is `tf.contrib.rnn.LSTMBlockFusedCell`.
    It should be used when you don't have access to GPUs or TPUs and want run a more
    efficient cell.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The best set of cells to use is under `tf.contrib.cudnn_rnn.*` (`CuddnnCompatibleGPUCell`
    for GPU cells and more). They are highly optimized to run on GPUs and perform
    significantly better than the preceding ones.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, you should always perform the training using `tf.nn.dynamic_rnn` (see
    the TensorFlow documentation: [https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn))
    and pass the specific cell. This method optimizes the training of the recurrent
    neural networks by occasionally swapping memory between GPUs and CPUs to enable
    training of large sequences.'
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing gradients
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The last optimization technique will actually improve the performance of our
    backpropagation algorithm. Recall from the previous chapters that your goal during
    training is to minimize the loss function by adjusting the weights and biases
    of the model. Adjusting (optimizing) these weights and biases can be accomplished
    with different built-in TensorFlow optimizers, such as `tf.train.AdamOptimizer` and `tf.train.GradientDescentOptimizer`.
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow offers the ability to distribute this process across multiple TPUs
    using this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Here, `existing_optimizer = tf.train.AdamOptimizer()`, and your training step
    will look like `train_step = optimizer.minimize(loss)`.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered a lot of new and exciting approaches for optimizing
    your model's performance, both on a general level, and specifically, using the
    TensorFlow library.
  prefs: []
  type: TYPE_NORMAL
- en: The first part covered techniques for improving your RNN performance by selecting,
    processing, and transforming your data, as well as tuning your hyperparameters.
    You also learned how to understand your model in more depth, and now know what
    should be done to make it work better.
  prefs: []
  type: TYPE_NORMAL
- en: The second part was specifically focused on practical ways of improving your
    model's performance using the built-in TensorFlow functions. The team at TensorFlow
    seeks to make it as easy as possible for you to quickly achieve the results you
    want by providing distributed environments and optimization techniques with just
    a few lines of code.
  prefs: []
  type: TYPE_NORMAL
- en: Combining both of the techniques covered in this chapter will enhance your knowledge
    in deep learning and let you experiment with more complicated models without worrying
    about performance issues. The knowledge you gained is applicable for any neural
    network model, so you can confidently apply exactly the same technique for broader
    sets of problems.
  prefs: []
  type: TYPE_NORMAL
- en: External links
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Open source data collection: [https://skymind.ai/wiki/open-datasets ](https://skymind.ai/wiki/open-datasets)or
    awesome-public-datasets GitHub repo: [https://github.com/awesomedata/awesome-public-datasets](https://github.com/awesomedata/awesome-public-datasets)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google Search Advanced: [https://www.google.com/advanced_search](https://www.google.com/advanced_search)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Augmenting text: [https://www.quora.com/What-data-augmentation-techniques-are-available-for-deep-learning-on-text](https://www.quora.com/What-data-augmentation-techniques-are-available-for-deep-learning-on-text)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data Noising as Smoothing in Neural Network Language Models: [https://arxiv.org/abs/1703.02573](https://arxiv.org/abs/1703.02573)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorBoard: [https://www.tensorflow.org/guide/summaries_and_tensorboard](https://www.tensorflow.org/guide/summaries_and_tensorboard)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An Introduction to Feature Selection: [https://machinelearningmastery.com/an-introduction-to-feature-selection/](https://machinelearningmastery.com/an-introduction-to-feature-selection/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Andrew Ng's course on hyperparameters tuning: [https://www.coursera.org/lecture/deep-neural-network/hyperparameters-tuning-in-practice-pandas-vs-caviar-DHNcc](https://www.coursera.org/lecture/deep-neural-network/hyperparameters-tuning-in-practice-pandas-vs-caviar-DHNcc)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hyperparameters optimization for Neural Networks: [http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html#tree-structured-parzen-estimators-tpe ](http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html#tree-structured-parzen-estimators-tpe)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bayesian Optimization of Hyper Parameters: [https://www.youtube.com/watch?v=cWQDeB9WqvU](https://www.youtube.com/watch?v=cWQDeB9WqvU)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training performance (TensorFlow Summit 2018): [https://www.youtube.com/watch?v=SxOsJPaxHME](https://www.youtube.com/watch?v=SxOsJPaxHME)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow Performance Guide: [https://www.tensorflow.org/performance/](https://www.tensorflow.org/performance/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: tf.data API: [https://www.tensorflow.org/performance/performance_guide](https://www.tensorflow.org/performance/performance_guide)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.data` (TensorFlow Summit 2018): [https://www.youtube.com/watch?v=uIcqeP7MFH0](https://www.youtube.com/watch?v=uIcqeP7MFH0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Batch Normalization in Neural Networks: [https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c](https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
