- en: Markov Decision Process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will talk about another application of HMMs known as **Markov
    Decision Process** (**MDP**). In the case of MDPs, we introduce a reward to our
    model, and any sequence of states taken by the process results in a specific reward.
    We will also introduce the concept of discounts, which will allow us to control
    how short-sighted or far-sighted we want our agent to be. The goal of the agent
    would be to maximize the total reward that it can get.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will be covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Markov reward process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Markov decision processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code example
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reinforcement learning is a different paradigm in machine learning where an
    agent tries to learn to behave optimally in a defined environment by making decisions/actions
    and observing the outcome of that decision. So, in the case of reinforcement learning,
    the agent is not really from some given dataset, but rather, by interacting with
    the environment, the agent tries to learn by observing the effects of its actions.
    The environment is defined in such a way that the agent gets rewards if its action
    gets it closer to the goal.
  prefs: []
  type: TYPE_NORMAL
- en: Humans are known to learn in this way. For example, consider a child in front
    of a fireplace where the child is the agent and the space around the child is
    the environment. Now, if the child moves its hand towards the fire, it feels the
    warmth, which feels good and, in a way, the child (or the agent) is rewarded for
    the action of moving its hand close to the fire. But if the child moves its hand
    too close to the fire, its hand will burn, hence receiving a negative reward.
    Using these rewards, the child is able to figure out the optimal distance to keep
    its hand from the fire. Reinforcement learning tries to imitate exactly this kind
    of system in order to train the agent to learn to optimize its goal in the given
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Making this more formal, to train an agent we will need to have an environment
    which represents the world in which the agent should be able to take actions.
    For each of these actions, the environment should return observations which contain
    information about the reward, telling it how good or bad the action was. The observation
    should also have information regarding the next state of the agent in the environment.
    And, based on these observations, the agent tries to figure out the optimal way
    to reach its goal. *Figure 1* shows the interaction between an agent and an environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6ea82be0-74db-4985-b571-e29668558f2a.png)'
  prefs: []
  type: TYPE_IMG
- en: The thing that makes reinforcement learning fundamentally different from other
    algorithms is that there is no supervisor. Instead, there is only a reward signal
    giving feedback to the agent about the action it took. Another important thing
    to mention here is that an environment can be constructed in such a way that the
    reward is delayed, which can make the agent wander around before reaching its
    goal. It is also possible that the agent might have to go through a lot of negative
    feedback before reaching its goal.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of supervised learning, we are given the dataset, which basically
    tells our learning algorithm the right answers in different situations. Our learning
    algorithm then looks over all these different situations, and the solutions in
    those cases, and tries to generalize based upon it. Hence, we also expect that
    the dataset given to use is **independent and identically distributed** (**IID**).
    But in the case of reinforcement learning the data is not IID, the data generated
    depends on the path the agent took, and, hence, it depends on the actions taken
    by the agent. Hence, reinforcement learning is an active learning process in which
    the actions taken by the agent influence the environment which in turn influences
    the data generated by the environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can take a very simple example to better understand how a reinforcement
    learning agent and environment behave. Consider an agent trying to learn to play
    Super Mario Bros:'
  prefs: []
  type: TYPE_NORMAL
- en: The agent will receive the initial state from the environment. In the case of
    Super Mario, this would be the current frame of the game.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Having received the state information, the agent will try to take an action.
    Let's say the action the agent took is to move to the right.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When the environment receives this action it will return the next state based
    on it. The next state would also be a frame, but the frame could be of a dying
    Mario if there was an enemy next to Mario in the previous state. Otherwise, the
    frame would just have shown Mario to have moved one step to the right. The environment
    will also return the rewards based on the action. If there was an enemy on the
    right of Mario the rewards could be, let's say -5 (since the action killed Mario)
    or could be +1 if the Mario moved towards finishing the level.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reward hypothesis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The whole concept of reinforcement learning is based on something called **reward
    hypothesis**. According to reward hypothesis, all goals can be defined by the
    maximization of the expected cumulative reward.
  prefs: []
  type: TYPE_NORMAL
- en: 'At any given time *t*, the total cumulative reward can be given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2458c36c-959a-4168-adfe-73035ba18602.png)'
  prefs: []
  type: TYPE_IMG
- en: 'But, in reality, the rewards which are closer to the current state are more
    probable that the ones which are further away. To deal with this, we would introduce
    another term called the **discount rate,** ![](img/0f5e1c1c-8fea-4597-ad98-a1ffa23cf86e.png).
    The value of the discount rate is always between 0 and 1\. A large value of ![](img/7e86090e-61af-42bb-993e-e105248db956.png) means
    a smaller discount, which make the agent care more about the long-term rewards,
    whereas, for smaller values of ![](img/058b9431-3af7-4648-b9ea-184921bdc7e9.png),
    the agent cares more about the short-term rewards. With the discount rate term,
    we can now define our cumulative reward as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1e708f38-37af-4060-8246-3ba7c185487e.png)'
  prefs: []
  type: TYPE_IMG
- en: State of the environment and the agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we saw earlier, the agent interacts with the environment at intervals of
    time *t = 0, 1, 2, 3, ..,* and, at each time instance, the agent gets the state
    of the environment *S[t]* based on which it takes an action *A[t]* and gets a
    reward *R[t+1]*. This sequence of state, action and rewards over time is known
    as the **history of the agen****t** and is given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9ca92514-748f-4bf9-8d99-bad282d7763d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Ideally, we would like the action taken by the agent to be based on its total
    history, but it is generally unfeasible because the history of the agent can be
    huge. Therefore, we define the state in a way such that it is a summary of all
    the history of the agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6698b496-3ddc-40bd-ac20-19db2f9c5c4b.png)'
  prefs: []
  type: TYPE_IMG
- en: Basically, we are defining the state to be a function of the history of the
    agent. It's important to notice that the environment state is the state that the
    environment uses to determine its next state, based on the action, and give out
    rewards. Also, it is private to the environment.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the agent state is the state that the agent uses to decide
    on the next action. The agent state is its internal representation, and can be
    any function of the history as mentioned before.
  prefs: []
  type: TYPE_NORMAL
- en: We use a Markov state to represent an agent's state, which basically means that
    the current state of the agent is able to summarize all the history, and the next
    action of the agent will depend only on the current state of the agent. Hence,
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9eed4f10-981c-4cee-b55f-64575586d4b8.png)'
  prefs: []
  type: TYPE_IMG
- en: In this chapter, we will only be considering the case when the agent is directly
    able to observe the environment's state. This results in the observation from
    the environment to be both the current state of the agent as well as the environment.
    This special case is also commonly known as **MDP**.
  prefs: []
  type: TYPE_NORMAL
- en: Components of an agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will formally define the different kinds of components that
    an agent can have.
  prefs: []
  type: TYPE_NORMAL
- en: '**Policy**: It is a conditional over the action given the state. Based on this
    conditional distribution, the agent chooses its action at any given state. It
    is possible for the policy to be deterministic: *a = π(s)* or stochastic: ![](img/26fa3ff6-449d-4502-ba5a-58f7b5ffbb1a.png).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Value function**: The value function tries to predict the reward to expect
    on taking a given action in a given state. It is given as:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/9cd0fb06-8243-45fc-8474-230d7d871484.png).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: where *E* is the expectation and ![](img/06b33013-eeab-4b47-92e5-64ae385ff31e.png) is
    the discount factor.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Model**: The model is the agent''s representation of the environment. It
    is defined using a transition function *P* which predicts the next state of the
    environment:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/55f70d9a-aaef-4be0-beee-f6f8cdc0a70a.png).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: and a reward function which predicts the reward associated with any given action
    at a given state: ![](img/87664401-62c1-42c9-a764-6582d6bfa877.png).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Based on these components, agents can be classified into the following five
    categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Value-based agents**: Have an implicit policy and the agent takes decisions
    for actions based on the value function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Policy-based agents**: Have an explicit policy and the agent searches for
    the most optimal action-value function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Actor-critic agents**: Combination of both value-based and policy-based agents'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model-based agents**: Try to build a model based on the environment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model-free agents**: Don''t try to learn the environment, rather they try
    to learn the policy and value function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Markov reward process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we gave an introduction to MDP. In this section, we
    will define the problem statement formally and see the algorithms for solving
    it.
  prefs: []
  type: TYPE_NORMAL
- en: An MDP is used to define the environment in reinforcement learning and almost
    all reinforcement learning problems can be defined using an MDP.
  prefs: []
  type: TYPE_NORMAL
- en: 'For understanding MDPs we need to use the concept of the **Markov reward process**
    (**MRP**). An MRP is a stochastic process which extends a Markov chain by adding
    a reward rate to each state. We can also define an additional variable to keep
    a track of the accumulated reward over time. Formally, an MRP is defined by ![](img/7488784d-0670-480e-a673-8320989866c4.png) where
    *S* is a finite state space, *P* is the state transition probability function,
    *R* is a reward function, and ![](img/b829afec-9f1c-4b23-8a78-f77547dbc951.png) is
    the discount rate:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a0b927f4-6354-4751-ac3a-13c348781d1a.png)'
  prefs: []
  type: TYPE_IMG
- en: where ![](img/5ff856d1-0e22-4ad3-ac30-6867098e1371.png) denotes the expectation.
    And the term *R[s]* here denotes the expected reward at the state *s*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of MRPs, we can define the expected return when starting from a
    state *s* as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2f5e4c4d-cff7-45a4-86bb-8ca971bd66c3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where *G[t]* is the cumulative gain as we had defined in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/53d2fd3d-43c8-45ec-8f65-bda4e3592f97.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, for maximizing the cumulative reward, the agent will try to get the most
    expected sum of rewards from every state it goes into. To do that we need to find
    the optimal value function. We will see the algorithm for doing that in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Bellman equation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using the Bellman equation, we decompose our value function into two separate parts,
    one representing the immediate reward and the other term representing the future
    rewards. From our previous definitions, the immediate reward is represented as
    *R[t+1]* and the future rewards by ![](img/d9847e5a-6360-4875-8b40-3f4296a066c1.png) where:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fbee6c17-72b3-4084-bb61-b9d622a90e7b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s now unroll *G[t]* and substitute *G[t+1]* in it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a8d00aeb-1527-47c2-88e6-8960ec6a5d58.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, since we know that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c34fe84e-b0f3-4d8c-b9c9-cd2386ff72b1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Using this identity we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e81848b6-cc68-4947-96d1-58d008685f5a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And this gives us the Bellman equation for MRPs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5ddd6fb3-0ebf-44d8-b385-b49dd34cbe0b.png)'
  prefs: []
  type: TYPE_IMG
- en: MDP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have a basic understanding of MRPs, we can move on to MDPs. An
    MDP is an MRP which also involved decisions. All the states in the environment
    are also Markov, hence the next state is only dependent on the current state.
    Formally, an MDP can be represented using ![](img/57989e16-1d7b-45a9-b644-35221cfb5099.png) where
    *S* is the state space, *A* is the action set, *P* is the state transition probability
    function, *R* is the reward function, and ![](img/ada7f4d2-3aba-46a2-9172-6b188b3457dd.png) is
    the discount rate. The state transition probability function *P* and the reward
    function *R* are formally defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f4c103f7-991b-427d-bcc1-e686de7e028d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also formally define a policy *π* as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dfa1a5a9-553b-4755-8c4a-6add32b46623.png)'
  prefs: []
  type: TYPE_IMG
- en: Since the states in the MDP are considered to be Markov, the MDP policies depend
    only on the current state, which means that the policies are stationary that is, ![](img/04f513a8-31a0-4304-8c53-5d2ad9d73d74.png). This
    means that whenever the agent falls into the same state, it will take the decision
    based on the same policy it had decided before. The decision function can be made
    stochastic so that the agent doesn't keep on taking the same decisions and hence
    is able to explore the environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, since we want to use the Bellman Equation in the case of MDP, we will
    recover an MRP from the MDPs. Given an MDP, ![](img/b35de36f-a812-4f4d-a6a8-56112f1f568f.png) and
    a policy ![](img/f40666b4-eb4f-4fff-a06c-76f3b1f83173.png) the state sequence
    *S[1], S[2], ...* is a Markov Process *(S,P)* on the policy *π*. The state and
    reward sequence *S1, R1, S2, R2, ... *is also an MRP given by ![](img/d82e40a8-d340-4e3c-b9d7-41eb27928ae2.png) where:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f901d745-4aeb-4c7c-a33d-26d0e9786154.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can similarly formulate our reward function as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a46a80a3-643f-4fb5-8c7f-91ea46ee8a84.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And, since we know that the state-value function *Vπ(s)* of an MDP is the expected
    return starting from state *S* and then following the policy *π*, the value function
    is given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/15bcda57-42d2-41de-8726-9027a70ffe97.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Also, the action-value function can be given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7c0a8568-b3be-4d46-9928-0d7ed49b47f6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Having these values, we can again derive the Bellman expectation equation in
    the case of MDPs. We again start by decomposing the state-value function into
    immediate reward and future rewards:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/92216e16-9342-4643-9bc0-4b636cedfcc7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And similar to the case of MRPs, the action-value function can also be decomposed
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7c5528ee-2aa8-4ef0-a8ff-8f8a7e10f534.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And as we have multiple actions from each state *S* and the policy defines
    the probability distribution over the actions, we will need to average over it
    to get the Bellman expectation equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c407fadd-243a-482f-87bf-3e00d4e38a55.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also average over all the possible action-values to know how good being
    in a given state *S* is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a4aa3925-892e-4d08-a5f8-09c0fcc04420.png)'
  prefs: []
  type: TYPE_IMG
- en: Code example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following code example we implement a simple MDP:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Using this MDP, we can now code up a simple betting game:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we started with a short introduction to Reinforcement Learning.
    We talked about agents, rewards and our learning goals in reinforcement learning.
    In the next section, we introduced MRP which is one of the main concepts underlying
    MDP. Having an understanding of MRP we next introduce the concepts of MDP along
    with a code example.
  prefs: []
  type: TYPE_NORMAL
