<html><head></head><body>
        <section id="8NDKQ1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Transfer Learning</h1>
                
            
            <article>
                
<p class="calibre2">In this chapter, we will discuss the concept of Transfer Learning. The following are the topics that will be covered:</p>
<ul class="calibre12">
<li class="calibre13">Illustrating the use of a pretrained model</li>
<li class="calibre13">Setting up the Transfer Learning model</li>
<li class="calibre13">Building an image classification model</li>
<li class="calibre13">Training a deep learning model on a GPU</li>
<li class="calibre13">Comparing performance using CPU and GPU</li>
</ul>


            </article>

            
        </section>
    

        <section id="8OC5C1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Introduction</h1>
                
            
            <article>
                
<p class="calibre2">A lot of development has happened within the deep learning domain in recent years, to enhance algorithmic efficacy and computational efficiency across different domains such as text, images, audio, and video. However, when it comes to training on new datasets, machine learning usually rebuilds the model from scratch, as is done in traditional data science problem solving. This becomes challenging when a new big dataset need to be trained as it will require very high computation power a lot of and time to reach the desired model efficacy.</p>
<p class="calibre2">Transfer Learning is a mechanism to learn new scenarios from existing models. This approach is very useful to train on big datasets, not necessarily from a similar domain or problem statement. For example, researchers have shown examples of Transfer Learning where they have trained Transfer Learning for completely different problem scenarios, such as when a model built using classifications of cat and dog is used for classifying objects such as aeroplane vs automobile.</p>
<p class="calibre2">In terms of analogy, it's more about passing the learned relationship to new architecture in order to fine-tune weights. An example of how Transfer Learning is used is shown in the following figure:</p>
<div class="cdpaligncenter"><img class="image-border107" src="../images/00020.jpeg"/></div>
<div class="packt_figref">Illustration of Transfer Learning flow</div>
<p class="calibre2">The figure shows the steps of Transfer Learning, where the weights/architectures from a predeveloped deep learning model are reused to predict a new problem statement. Transfer Learning helps provide a good starting point for deep learning architectures. There are different open source projects going on in different domains, which facilitate Transfer Learning, for example, ImageNet (<a href="http://image-net.org/index" class="calibre4">http://image-net.org/index</a>) is an open source project for image classification where a lot of different architectures such as Alexnet, VGG16, and VGG19 have been developed. Similarly, in text mining, there is a Word2Vec representation of Google News trained using three billion running words.</p>
<div class="packt_tip">Details on word2vec can be found at <a href="https://code.google.com/archive/p/word2vec/" class="calibre25">https://code.google.com/archive/p/word2vec/.</a></div>


            </article>

            
        </section>
    

        <section id="8PALU1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Illustrating the use of a pretrained model</h1>
                
            
            <article>
                
<p class="calibre2">The current recipe will cover the set-up for using a pretrained model. We will use TensorFlow to demonstrate the recipe. The current recipe will use VGG16 architecture built using the ImageNet as dataset. The ImageNet is an open source image repository of images used for building image recognition algorithms. The database has more than 10 millions tagged images and more than 1 million images have bounding box to capture objects.</p>
<p class="calibre2">Lot of different deep learning architectures are developed using ImageNet dataset. Once of the popular one is VGG networks are convolution neural networks proposed by Zisserman and Simonyan (2014) and trained over ImageNet data with 1,000 classes. The current recipe will consider VGG16 variant of VGG architecture which is known for it's simplicity. The network uses input of 224 x 224 RGB image. The network utilizes 13 convolution layers with different width x height x depth. The maximum pooling layer is used to reduce volume size. The network uses 5 maxpooling layer. The output from convolution layer is passed through 3 fully connected layer. Outcome from fully connected layer go through softmax function to evaluate probability of 1000 classes.</p>
<p class="calibre2">The detailed architecture of VGG16 is shown in the following figure:</p>
<div class="cdpaligncenter"><img class="image-border108" src="../images/00122.jpeg"/></div>
<div class="packt_figref">VGG16 architecture</div>


            </article>

            
        </section>
    

        <section id="8Q96G1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Getting ready</h1>
                
            
            <article>
                
<p class="calibre2">The section covers required to use VGG16 pretrained model for classification.</p>
<ol class="calibre15">
<li value="1" class="calibre13">Download VGG16 weights from <a href="http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz" class="calibre4">http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz</a>. The file can be downloaded using the following script:</li>
</ol>
<pre class="calibre23">
require(RCurl) 
URL &lt;- 'http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz' 
download.file(URL,destfile="vgg_16_2016_08_28.tar.gz",method="libcurl") 
</pre>
<ol start="2" class="calibre15">
<li value="2" class="calibre13">Install TensorFlow in Python.</li>
<li value="3" class="calibre13">Install R and the <kbd class="calibre10">tensorflow</kbd> package in R.</li>
<li value="4" class="calibre13">Download a sample image from <a href="http://image-net.org/download-imageurls" class="calibre4">http://image-net.org/download-imageurls</a>.</li>
</ol>


            </article>

            
        </section>
    

        <section id="8R7N21-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<p class="calibre2">The current section provides steps to use pretrained models:</p>
<ol class="calibre15">
<li value="1" class="calibre13">Load <kbd class="calibre10">tensorflow</kbd> in R:</li>
</ol>
<pre class="calibre23">
require(tensorflow) 
 
</pre>
<ol start="2" class="calibre15">
<li value="2" class="calibre13">Assign the <kbd class="calibre10">slim</kbd> library from TensorFlow:</li>
</ol>
<pre class="calibre23">
slimobj = tf$contrib$slim 
</pre>
<p class="calibre24">The <kbd class="calibre10">slim</kbd> library in TensorFlow is used to maintain complex neural network models in terms of definition, training, and evaluation.</p>
<ol start="3" class="calibre15">
<li value="3" class="calibre13">Reset graph in TensorFlow:</li>
</ol>
<pre class="calibre23">
tf$reset_default_graph() 
 
</pre>
<ol start="4" class="calibre15">
<li value="4" class="calibre13">Define input images:</li>
</ol>
<pre class="calibre23">
# Resizing the images 
input.img= tf$placeholder(tf$float32, shape(NULL, NULL, NULL, 3)) 
scaled.img = tf$image$resize_images(input.img, shape(224,224))
</pre>
<ol start="5" class="calibre15">
<li value="5" class="calibre13">Redefine the VGG16 network:</li>
</ol>
<pre class="calibre23">
# Define VGG16 network 
library(magrittr) 
VGG16.model&lt;-function(slim, input.image){ 
  vgg16.network = slim$conv2d(input.image, 64, shape(3,3), scope='vgg_16/conv1/conv1_1') %&gt;%  
    slim$conv2d(64, shape(3,3), scope='vgg_16/conv1/conv1_2')  %&gt;% 
    slim$max_pool2d( shape(2, 2), scope='vgg_16/pool1')  %&gt;% 
     
    slim$conv2d(128, shape(3,3), scope='vgg_16/conv2/conv2_1')  %&gt;% 
    slim$conv2d(128, shape(3,3), scope='vgg_16/conv2/conv2_2')  %&gt;% 
    slim$max_pool2d( shape(2, 2), scope='vgg_16/pool2')  %&gt;% 
     
    slim$conv2d(256, shape(3,3), scope='vgg_16/conv3/conv3_1')  %&gt;% 
    slim$conv2d(256, shape(3,3), scope='vgg_16/conv3/conv3_2')  %&gt;% 
    slim$conv2d(256, shape(3,3), scope='vgg_16/conv3/conv3_3')  %&gt;% 
    slim$max_pool2d(shape(2, 2), scope='vgg_16/pool3')  %&gt;% 
     
    slim$conv2d(512, shape(3,3), scope='vgg_16/conv4/conv4_1')  %&gt;% 
    slim$conv2d(512, shape(3,3), scope='vgg_16/conv4/conv4_2')  %&gt;% 
    slim$conv2d(512, shape(3,3), scope='vgg_16/conv4/conv4_3')  %&gt;% 
    slim$max_pool2d(shape(2, 2), scope='vgg_16/pool4')  %&gt;% 
     
    slim$conv2d(512, shape(3,3), scope='vgg_16/conv5/conv5_1')  %&gt;% 
    slim$conv2d(512, shape(3,3), scope='vgg_16/conv5/conv5_2')  %&gt;% 
    slim$conv2d(512, shape(3,3), scope='vgg_16/conv5/conv5_3')  %&gt;% 
    slim$max_pool2d(shape(2, 2), scope='vgg_16/pool5')  %&gt;% 
     
    slim$conv2d(4096, shape(7, 7), padding='VALID', scope='vgg_16/fc6')  %&gt;% 
    slim$conv2d(4096, shape(1, 1), scope='vgg_16/fc7') %&gt;%  
     
    slim$conv2d(1000, shape(1, 1), scope='vgg_16/fc8')  %&gt;% 
    tf$squeeze(shape(1, 2), name='vgg_16/fc8/squeezed') 
  return(vgg16.network) 
} 
 
</pre>
<ol start="6" class="calibre15">
<li value="6" class="calibre13">The preceding function defines the network architecture used for the VGG16 network. The network can be assigned using the following script:</li>
</ol>
<pre class="calibre23">
vgg16.network&lt;-VGG16.model(slim, input.image = scaled.img) 
</pre>
<ol start="7" class="calibre15">
<li value="7" class="calibre13">Load the VGG16 weights <kbd class="calibre10">vgg_16_2016_08_28.tar.gz</kbd> downloaded in the <em class="calibre9">Getting started</em> section:</li>
</ol>
<pre class="calibre23">
# Restore the weights 
restorer = tf$train$Saver() 
sess = tf$Session() 
restorer$restore(sess, 'vgg_16.ckpt')
</pre>
<ol start="8" class="calibre15">
<li value="8" class="calibre13">Download a sample test image. Let's download an example image from a <kbd class="calibre10">testImgURL</kbd> location as shown in following script:</li>
</ol>
<pre class="calibre23">
# Evaluating using VGG16 network 
testImgURL&lt;-"http://farm4.static.flickr.com/3155/2591264041_273abea408.jpg" 
img.test&lt;-tempfile() 
download.file(testImgURL,img.test, mode="wb") 
read.image &lt;- readJPEG(img.test) 
# Clean-up the temp file 
file.remove(img.test)  
</pre>
<p class="calibre24">The preceding script downloads the following image from URL mention in variable <kbd class="calibre10">testImgURL</kbd>. The following is the downloaded image:</p>
<div class="cdpaligncenter"><img class="image-border109" src="../images/00128.jpeg"/></div>
<div class="packt_figref">Sample image used to evaluate imagenet</div>
<ol start="9" class="calibre15">
<li value="9" class="calibre13">Determine the class using the VGG16 pretrained model:</li>
</ol>
<pre class="calibre23">
## Evaluate  
size = dim(read.image) 
imgs = array(255*read.image, dim = c(1, size[1], size[2], size[3])) 
VGG16_eval = sess$run(vgg16.network, dict(images = imgs)) 
probs = exp(VGG16_eval)/sum(exp(VGG16_eval))  
 
</pre>
<p class="calibre2">The maximum probability achieved is 0.62 for class 672, which refers to the category--mountain bike, all-terrain bike, off-roader--in the VGG16 trained dataset.</p>


            </article>

            
        </section>
    

        <section id="8S67K1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Setting up the Transfer Learning model</h1>
                
            
            <article>
                
<p class="calibre2">The current recipe will cover Transfer Learning using the CIFAR-10 dataset. The previous recipe presented how to use a pretrained model. The current recipe will demonstrate how to use a pretrained model for different problem statements.</p>
<p class="calibre2">We will use another very good deep learning package, MXNET, to demonstrate the concept with another architecture, Inception. To simplify the computation, we will reduce the problem complexity from 10 classes to two classes: aeroplane and automobile. The recipe focuses on data preparation for Transfer Learning using Inception-BN.</p>


            </article>

            
        </section>
    

        <section id="8T4O61-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Getting ready</h1>
                
            
            <article>
                
<p class="calibre2">The section prepares for the upcoming section for setting-up Transfer Learning model.</p>
<ol class="calibre15">
<li value="1" class="calibre13">Download the CIFAR-10 dataset from <a href="http://www.cs.toronto.edu/~kriz/cifar.html" class="calibre4">http://www.cs.toronto.edu/~kriz/cifar.html</a><span>. The</span> <kbd class="calibre10">download.cifar.data</kbd> <span>function from <a href="part0093.html#2OM4A1-a0a93989f17f4d6cb68b8cfd331bc5ab" target="_blank" class="calibre4">Chapter 3</a>, <em class="calibre9">Convolution Neural Networks,</em> can be used to download the dataset.</span></li>
<li value="2" class="calibre13"><span>Install the <kbd class="calibre10">imager</kbd> package:</span></li>
</ol>
<pre class="calibre23">
install.packages("imager")
</pre>


            </article>

            
        </section>
    

        <section id="8U38O1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<p class="calibre2">The current part of the recipe will provide a step-by-step guide to prepare the dataset for the Inception-BN pretrained model.</p>
<ol class="calibre15">
<li value="1" class="calibre13">Load the dependent packages:</li>
</ol>
<pre class="calibre23">
# Load packages 
require(imager) 
source("download_cifar_data.R") 
The download_cifar_data consists of function to download and read CIFAR10 dataset. 
</pre>
<ol start="2" class="calibre15">
<li value="2" class="calibre13">Read the downloaded CIFAR-10 dataset:</li>
</ol>
<pre class="calibre23">
# Read Dataset and labels  
DATA_PATH&lt;-paste(SOURCE_PATH, "/Chapter 4/data/cifar-10-batches-bin/", sep="") 
labels &lt;- read.table(paste(DATA_PATH, "batches.meta.txt", sep="")) 
cifar_train &lt;- read.cifar.data(filenames = c("data_batch_1.bin","data_batch_2.bin","data_batch_3.bin","data_batch_4.bin")) 
</pre>
<ol start="3" class="calibre15">
<li value="3" class="calibre13">Filter the dataset for aeroplane and automobile. This is an optional step and is done to reduce complexity later:</li>
</ol>
<pre class="calibre23">
# Filter data for Aeroplane and Automobile with label 1 and 2, respectively 
Classes = c(1, 2)  
images.rgb.train &lt;- cifar_train$images.rgb 
images.lab.train &lt;- cifar_train$images.lab 
ix&lt;-images.lab.train%in%Classes 
images.rgb.train&lt;-images.rgb.train[ix] 
images.lab.train&lt;-images.lab.train[ix] 
rm(cifar_train)   
</pre>
<ol start="4" class="calibre15">
<li value="4" class="calibre13">Transform to image. This step is required as the CIFAR-10 dataset is a 32 x 32 x 3 image, which is flattened to a 1024 x 3 format:</li>
</ol>
<pre class="calibre23">
# Function to transform to image 
transform.Image &lt;- function(index, images.rgb) { 
  # Convert each color layer into a matrix,  
  # combine into an rgb object, and display as a plot 
  img &lt;- images.rgb[[index]] 
  img.r.mat &lt;- as.cimg(matrix(img$r, ncol=32, byrow = FALSE)) 
  img.g.mat &lt;- as.cimg(matrix(img$g, ncol=32, byrow = FALSE)) 
  img.b.mat &lt;- as.cimg(matrix(img$b, ncol=32, byrow = FALSE)) 
 
  # Bind the three channels into one image 
  img.col.mat &lt;- imappend(list(img.r.mat,img.g.mat,img.b.mat),"c")  
  return(img.col.mat) 
} 
</pre>
<ol start="5" class="calibre15">
<li value="5" class="calibre13">The next step involve padding images with zeros:</li>
</ol>
<pre class="calibre23">
  # Function to pad image 
  image.padding &lt;- function(x) { 
  img_width &lt;- max(dim(x)[1:2]) 
  img_height &lt;- min(dim(x)[1:2]) 
  pad.img &lt;- pad(x, nPix = img_width - img_height, 
                 axes = ifelse(dim(x)[1] &lt; dim(x)[2], "x", "y")) 
  return(pad.img) 
}
</pre>
<ol start="6" class="calibre15">
<li value="6" class="calibre13">Save the image to a specified folder:</li>
</ol>
<pre class="calibre23">
# Save train images 
MAX_IMAGE&lt;-length(images.rgb.train) 
 
# Write Aeroplane images to aero folder 
sapply(1:MAX_IMAGE, FUN=function(x, images.rgb.train, images.lab.train){ 
  if(images.lab.train[[x]]==1){ 
    img&lt;-transform.Image(x, images.rgb.train)   
    pad_img &lt;- image.padding(img) 
    res_img &lt;- resize(pad_img, size_x = 224, size_y = 224) 
    imager::save.image(res_img, paste("train/aero/aero", x, ".jpeg", sep=""))     
  } 
}, images.rgb.train=images.rgb.train, images.lab.train=images.lab.train) 
 
 
# Write Automobile images to auto folder 
sapply(1:MAX_IMAGE, FUN=function(x, images.rgb.train, images.lab.train){ 
  if(images.lab.train[[x]]==2){ 
    img&lt;-transform.Image(x, images.rgb.train)   
    pad_img &lt;- image.padding(img) 
    res_img &lt;- resize(pad_img, size_x = 224, size_y = 224) 
    imager::save.image(res_img, paste("train/auto/auto", x, ".jpeg", sep=""))     
  } 
}, images.rgb.train=images.rgb.train, images.lab.train=images.lab.train) 
</pre>
<p class="calibre24">The preceding script saves the aeroplane images into the <kbd class="calibre10">aero</kbd> folder and the automobile images in the <kbd class="calibre10">auto</kbd> folder.</p>
<ol start="7" class="calibre15">
<li value="7" class="calibre13">Convert to the recording format <kbd class="calibre10">.rec</kbd> supported by MXNet. This conversion requires <kbd class="calibre10">im2rec.py</kbd> MXnet module from Python as conversion is not supported in R. However, it can be called from R once MXNet is installed in Python using the system command. The splitting of the dataset into train and test can be obtained using the following file:</li>
</ol>
<pre class="calibre23">
System("python ~/mxnet/tools/im2rec.py --list True --recursive True --train-ratio 0.90 cifar_224/pks.lst cifar_224/trainf/")
</pre>
<p class="calibre24">The preceding script will generate two list files: <kbd class="calibre10">pks.lst_train.lst</kbd> and <kbd class="calibre10">pks.lst_train.lst</kbd>. The splitting of train and validation is controlled by the <kbd class="calibre10">-train-ratio</kbd> parameter in the preceding script. The number of classes is based on the number of folders in the <kbd class="calibre10">trainf</kbd> directory. In this scenario, two classes are picked: automotive and aeroplane.</p>
<ol start="8" class="calibre15">
<li value="8" class="calibre13">Convert the <kbd class="calibre10">*.rec</kbd> file for training and validation dataset:</li>
</ol>
<pre class="calibre23">
# Creating .rec file from training sample list 
System("python ~/mxnet/tools/im2rec.py --num-thread=4 --pass-through=1 /home/prakash/deep\ learning/cifar_224/pks.lst_train.lst /home/prakash/deep\ learning/cifar_224/trainf/")   
 
# Creating .rec file from validation sample list 
System("python ~/mxnet/tools/im2rec.py --num-thread=4 --pass-through=1 /home/prakash/deep\ learning/cifar_224/pks.lst_val.lst /home/prakash/deep\ learning/cifar_224/trainf/") 
</pre>
<p class="calibre2">The preceding script will create the <kbd class="calibre10">pks.lst_train.rec</kbd> and <kbd class="calibre10">pks.lst_val.rec</kbd> files to be used in the next recipe to train the model using a pretrained model.</p>


            </article>

            
        </section>
    

        <section id="8V1PA1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Building an image classification model</h1>
                
            
            <article>
                
<p class="calibre2">The recipe focuses on building an image classification model using Transfer Learning. It will utilize the dataset prepared in the previous recipes and use the Inception-BN architecture. The BN in Inception-BN stands for <strong class="calibre1">batch normalization</strong>. Details of the Inception model in computer vision can be found in Szegedy et al. (2015).</p>


            </article>

            
        </section>
    

        <section id="9009S1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Getting ready</h1>
                
            
            <article>
                
<p class="calibre2">The section cover's the prerequisite to set-up a classification model using INCEPTION-BN pretrained model.</p>
<ol class="calibre15">
<li value="1" class="calibre13">Convert images into <kbd class="calibre10">.rec</kbd> file for train and validation.</li>
<li value="2" class="calibre13">Download the Inception-BN architecture from <a href="http://data.dmlc.ml/models/imagenet/inception-bn/" class="calibre4">http://data.dmlc.ml/models/imagenet/inception-bn/.</a></li>
<li value="3" class="calibre13">Install R and the <kbd class="calibre10">mxnet</kbd> package in R.</li>
</ol>


            </article>

            
        </section>
    

        <section id="90UQE1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<ol class="calibre15">
<li value="1" class="calibre13">Load the <kbd class="calibre10">.rec</kbd> file as iterators. The following is the function to load the <kbd class="calibre10">.rec</kbd> data as iterators:</li>
</ol>
<pre class="calibre23">
# Function to load data as iterators 
data.iterator &lt;- function(data.shape, train.data, val.data, BATCHSIZE = 128) { 
   
  # Load training data as iterator 
  train &lt;- mx.io.ImageRecordIter( 
    path.imgrec = train.data, 
    batch.size  = BATCHSIZE, 
    data.shape  = data.shape, 
    rand.crop   = TRUE, 
    rand.mirror = TRUE) 
   
  # Load validation data as iterator 
  val &lt;- mx.io.ImageRecordIter( 
    path.imgrec = val.data, 
    batch.size  = BATCHSIZE, 
    data.shape  = data.shape, 
    rand.crop   = FALSE, 
    rand.mirror = FALSE 
  ) 
   
  return(list(train = train, val = val)) 
} 
</pre>
<p class="calibre24">In the preceding function, <kbd class="calibre10">mx.io.ImageRecordIter</kbd> reads batches of images from the <kbd class="calibre10">RecordIO</kbd> (<kbd class="calibre10">.rec</kbd>) files<em class="calibre9">.</em></p>
<ol start="2" class="calibre15">
<li value="2" class="calibre13">Load data using the <kbd class="calibre10">data.iterator</kbd> function:</li>
</ol>
<pre class="calibre23">
# Load dataset 
data  &lt;- data.iterator(data.shape = c(224, 224, 3), 
                      train.data = "pks.lst_train.rec", 
                      val.data  = "pks.lst_val.rec", 
                      BATCHSIZE = 8) 
train &lt;- data$train 
val   &lt;- data$val 
</pre>
<ol start="3" class="calibre15">
<li value="3" class="calibre13">Load the Inception-BN pretrained model from the <kbd class="calibre10">Inception-BN</kbd> folder:</li>
</ol>
<pre class="calibre23">
# Load Inception-BN model 
inception_bn &lt;- mx.model.load("Inception-BN", iteration = 126) 
symbol &lt;- inception_bn$symbol 
The different layers of the model can be viewed using function symbol$arguments 
</pre>
<ol start="4" class="calibre15">
<li value="4" class="calibre13">Get the layers of the Inception-BN model:</li>
</ol>
<pre class="calibre23">
# Load model information 
internals &lt;- symbol$get.internals() 
outputs &lt;- internals$outputs 
flatten &lt;- internals$get.output(which(outputs == "flatten_output")) 
</pre>
<ol start="5" class="calibre15">
<li value="5" class="calibre13">Define a new layer to replace the <kbd class="calibre10">flatten_output</kbd> layer:</li>
</ol>
<pre class="calibre23">
# Define new layer 
new_fc &lt;- mx.symbol.FullyConnected(data = flatten,  
                                   num_hidden = 2,  
                                   name = "fc1")  
new_soft &lt;- mx.symbol.SoftmaxOutput(data = new_fc,  
                                    name = "softmax") 
</pre>
<ol start="6" class="calibre15">
<li value="6" class="calibre13">Initialize weights for the newly defined layer. To retrain the last layer, weight initialization is performed using the following script:</li>
</ol>
<pre class="calibre23">
# Re-initialize the weights for new layer 
arg_params_new &lt;- mxnet:::mx.model.init.params( 
  symbol = new_soft,  
  input.shape = c(224, 224, 3, 8),  
  output.shape = NULL,  
  initializer = mxnet:::mx.init.uniform(0.2),  
  ctx = mx.cpu(0) 
)$arg.params 
fc1_weights_new &lt;- arg_params_new[["fc1_weight"]] 
fc1_bias_new &lt;- arg_params_new[["fc1_bias"]] 
</pre>
<p class="calibre24">In the aforementioned layer, weights are assigned using uniform distribution between [<em class="calibre9">-0.2</em>, <em class="calibre9">0.2</em>]. The <kbd class="calibre10">ctx</kbd> define the device on which the execution is to be performed.</p>
<ol start="7" class="calibre15">
<li value="7" class="calibre13">Retrain the model:</li>
</ol>
<pre class="calibre23">
# Mode re-train 
model &lt;- mx.model.FeedForward.create( 
  symbol             = new_soft, 
  X                  = train, 
  eval.data          = val, 
  ctx                = mx.cpu(0), 
  eval.metric        = mx.metric.accuracy, 
  num.round          = 5, 
  learning.rate      = 0.05, 
  momentum           = 0.85, 
  wd                 = 0.00001, 
  kvstore            = "local", 
  array.batch.size   = 128, 
  epoch.end.callback = mx.callback.save.checkpoint("inception_bn"), 
  batch.end.callback = mx.callback.log.train.metric(150), 
  initializer        = mx.init.Xavier(factor_type = "in", magnitude = 2.34), 
  optimizer          = "sgd", 
  arg.params         = arg_params_new, 
  aux.params         = inception_bn$aux.params 
)  
</pre>
<p class="calibre2">The preceding model is set to run on CPU with five rounds, using accuracy as the evaluation metric. The following screenshot shows the execution of the described model:</p>
<div class="cdpaligncenter"><img class="image-border56" src="../images/00134.jpeg"/></div>
<div class="packt_figref">Output from Inception-BN model, trained using CIFAR-10 dataset</div>
<p class="calibre2">The trained model has produced a training accuracy of 0.97 and a validation accuracy of 0.95.</p>


            </article>

            
        </section>
    

        <section id="91TB01-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Training a deep learning model on a GPU</h1>
                
            
            <article>
                
<p class="calibre2">The <strong class="calibre1">Graphical processing unit</strong> (<strong class="calibre1">GPU</strong>) is hardware used for rendering images using a lot of cores. Pascal is the latest GPU micro architecture released by NVIDIA. The presence of hundreds of cores in GPU helps enhance the computation. This section provides the recipe for running a deep learning model using GPU.</p>


            </article>

            
        </section>
    

        <section id="92RRI1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Getting ready</h1>
                
            
            <article>
                
<p class="calibre2">This section provides dependencies required to run GPU and CPU:</p>
<ol class="calibre15">
<li value="1" class="calibre13">The experiment performed in this recipe uses GPU hardware such as GTX 1070.</li>
<li value="2" class="calibre13">Install <kbd class="calibre10">mxnet</kbd> for GPU. To install <kbd class="calibre10">mxnet</kbd> for GPU for a specified machine, follow the installation instruction from <kbd class="calibre10">mxnet.io</kbd>. Select the requirement as shown in the screenshot, and follow the instructions:</li>
</ol>
<div class="cdpaligncenter"><img class="image-border56" src="../images/00097.jpeg"/></div>
<div class="packt_figref">Steps to get installation instruction for MXNet</div>


            </article>

            
        </section>
    

        <section id="93QC41-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<p class="calibre2">Here is how you train a deep learning model on a GPU:</p>
<ol class="calibre15">
<li value="1" class="calibre13">The Inception-BN-transferred learning recipe discussed in the previous section can be made to run on the GPU installed and the configured machine by changing the device settings as shown in the following script:</li>
</ol>
<pre class="calibre23">
# Mode re-train 
model &lt;- mx.model.FeedForward.create( 
  symbol             = new_soft, 
  X                  = train, 
  eval.data          = val, 
  ctx                = mx.gpu(0), 
  eval.metric        = mx.metric.accuracy, 
  num.round          = 5, 
  learning.rate      = 0.05, 
  momentum           = 0.85, 
  wd                 = 0.00001, 
  kvstore            = "local", 
  array.batch.size   = 128, 
  epoch.end.callback = mx.callback.save.checkpoint("inception_bn"), 
  batch.end.callback = mx.callback.log.train.metric(150), 
  initializer        = mx.init.Xavier(factor_type = "in", magnitude = 2.34), 
  optimizer          = "sgd", 
  arg.params         = arg_params_new, 
  aux.params         = inception_bn$aux.params 
)  
</pre>
<p class="calibre2">In the aforementioned model, the device setting is changed from <kbd class="calibre10">mx.cpu</kbd> to <kbd class="calibre10">mx.gpu</kbd>. The CPU for five iteration tools takes ~2 hours of computational effort, whereas the same iteration is completed in ~15 min with GPU.</p>


            </article>

            
        </section>
    

        <section id="94OSM1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Comparing performance using CPU and GPU</h1>
                
            
            <article>
                
<p class="calibre2">One of the questions with device change is why so much improvement is observed when the device is switched from CPU to GPU. As the deep learning architecture involves a lot of matrix computations, GPUs help expedite these computations using a lot of parallel cores, which are usually used for image rendering.</p>
<p class="calibre2">The power of GPU has been utilized by a lot of algorithms to accelerate the execution. The following recipe provides some benchmarks of matrix computation using the <kbd class="calibre10">gpuR</kbd> package. The <kbd class="calibre10">gpuR</kbd> package is a general-purpose package for GPU computing in R.</p>


            </article>

            
        </section>
    

        <section id="95ND81-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Getting ready</h1>
                
            
            <article>
                
<p class="calibre2">The section covers requirement to set-up a comparison between GPU Vs CPU.</p>
<ol class="calibre15">
<li value="1" class="calibre13">Use GPU hardware installed such as GTX 1070.</li>
<li value="2" class="calibre13">CUDA toolkit installation using URL <a href="https://developer.nvidia.com/cuda-downloads" class="calibre4">https://developer.nvidia.com/cuda-downloads.</a></li>
<li value="3" class="calibre13">Install the <kbd class="calibre10">gpuR</kbd> package:</li>
</ol>
<pre class="calibre23">
install.packages("gpuR") 
</pre>
<ol start="4" class="calibre15">
<li value="4" class="calibre13">Test <kbd class="calibre10">gpuR</kbd>:</li>
</ol>
<pre class="calibre23">
library(gpuR) 
# verify you have valid GPUs 
detectGPUs()  
</pre>


            </article>

            
        </section>
    

        <section id="96LTQ1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
                
            
            <article>
                
<p class="calibre2">Let's get started by loading the packages:</p>
<ol class="calibre15">
<li value="1" class="calibre13">Load the package, and set the precision to <kbd class="calibre10">float</kbd> (by default, the GPU precision is set to a single digit):</li>
</ol>
<pre class="calibre23">
library("gpuR") 
options(gpuR.default.type = "float") 
 
</pre>
<ol start="2" class="calibre15">
<li value="2" class="calibre13">Matrix assignment to GPU:</li>
</ol>
<pre class="calibre23">
# Assigning a matrix to GPU 
A&lt;-matrix(rnorm(1000), nrow=10) 
<br class="title-page-tagline"/>vcl1 = vclMatrix(A)
</pre>
<p class="calibre24">The output of the preceding command will contain details of the object. An illustration is shown in the following script:</p>
<pre class="calibre23">
&gt; vcl1 
An object of class "fvclMatrix" 
Slot "address": 
&lt;pointer: 0x000000001822e180&gt; 
 
Slot ".context_index": 
[1] 1 
 
Slot ".platform_index": 
[1] 1 
 
Slot ".platform": 
[1] "Intel(R) OpenCL" 
 
Slot ".device_index": 
[1] 1 
 
Slot ".device": 
[1] "Intel(R) HD Graphics 530" 
</pre>
<ol start="3" class="calibre15">
<li value="3" class="calibre13">Let's consider the evaluation of CPU vs GPU<em class="calibre9">.</em> As most of the deep learning will be using GPUs for matrix computation, the performance is evaluated by matrix multiplication using the following script:</li>
</ol>
<pre class="calibre23">
# CPU vs GPU performance 
DF &lt;- data.frame() 
evalSeq&lt;-seq(1,2501,500) 
for (dimpower in evalSeq){ 
  print(dimpower) 
  Mat1 = matrix(rnorm(dimpower^2), nrow=dimpower) 
  Mat2 = matrix(rnorm(dimpower^2), nrow=dimpower) 
   
  now &lt;- Sys.time() 
  Matfin = Mat1%*%Mat2 
  cpu &lt;- Sys.time()-now 
   
  now &lt;- Sys.time() 
  vcl1 = vclMatrix(Mat1) 
  vcl2 = vclMatrix(Mat2) 
  vclC = vcl1 %*% vcl2 
  gpu &lt;- Sys.time()-now 
   
  DF &lt;- rbind(DF,c(nrow(Mat1), cpu, gpu))  
} 
DF&lt;-data.frame(DF) 
colnames(DF) &lt;- c("nrow", "CPU_time", "gpu_time")   
</pre>
<p class="calibre2">The preceding script computes the matrix multiplication using CPU and GPU; the time is stored for different dimensions of the matrix. The output from the preceding script is shown in the following diagram:</p>
<div class="cdpaligncenter"><img class="image-border110" src="../images/00043.jpeg"/></div>
<div class="packt_figref">Comparison between CPU and GPU</div>
<p class="calibre2">The graph shows that the computation effort required by CPUs increases exponentially with the CPU. Thus, GPUs help expedite it drastically.</p>


            </article>

            
        </section>
    

        <section id="97KEC1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">There's more...</h1>
                
            
            <article>
                
<p class="calibre2">The GPUs are new arena in machine learning computing, and a lot of packages have been developed in R to access GPUs while keeping you in a familiar R environment such as <kbd class="calibre10">gputools</kbd> , <span><kbd class="calibre10">gmatrix</kbd> , and <kbd class="calibre10">gpuR</kbd></span>. The other algorithms are also developed and implemented while accessing GPUs to enhance their computational power such as <kbd class="calibre10">RPUSVM</kbd>, which implements SVM using GPUs. Thus, the topic requires a lot of creativity with some exploration to deploy algorithms while utilizes full capability of hardware.</p>


            </article>

            
        </section>
    

        <section id="98IUU1-a0a93989f17f4d6cb68b8cfd331bc5ab">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">See also</h1>
                
            
            <article>
                
<p class="calibre2">To learn more on parallel computing using R, go through <em class="calibre9">Mastering Parallel Programming with R</em> by Simon R. Chapple et al<em class="calibre9">.</em> (2016).</p>


            </article>

            
        </section>
    
</body></html>