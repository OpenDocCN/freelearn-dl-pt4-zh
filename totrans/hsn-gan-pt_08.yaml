- en: Image-to-Image Translation and Its Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will push the label-based image generation to the next
    level: we will use pixel-wise labeling to perform image-to-image translation and
    transfer image styles.'
  prefs: []
  type: TYPE_NORMAL
- en: You will learn how to use pixel-wise label information to perform image-to-image
    translation with pix2pix and translate high-resolution images with pix2pixHD.
    Following this, you will learn how to perform style transfer between unpaired
    image collections with CycleGAN.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, combined with the knowledge from the previous chapter,
    you will have grasped the core methodology of using image-wise and pixel-wise
    label information to improve the quality, or manipulate the attributes, of generated
    images. You will also know how to flexibly design model architectures to accomplish
    your goals, including generating larger images or transferring textures between
    different styles of images.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Using pixel-wise labels to translate images with pix2pix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pix2pixHD – high-resolution image translation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CycleGAN – image-to-image translation from unpaired collections
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using pixel-wise labels to translate images with pix2pix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned how to use auxiliary information such as
    labels and attributes to improve the quality of images that are generated by GANs.
    The labels we used in the previous chapter were image-wise, which means that each
    image has only one or several labels. Labels can be assigned to specific pixels,
    which are known as pixel-wise labels. Pixel-wise labels are playing an increasingly
    important role in the realm of deep learning. For example, one of the most famous
    online image classification contests, the **ImageNet Large Scale Visual Recognition
    Challenge** (**ILSVRC**, [http://www.image-net.org/challenges/LSVRC](http://www.image-net.org/challenges/LSVRC/)),
    is no longer being hosted since its last event in 2017, whereas object detection
    and segmentation challenges such as COCO ([http://cocodataset.org](http://cocodataset.org))
    are receiving more attention.
  prefs: []
  type: TYPE_NORMAL
- en: An iconic application of pixel-wise labeling is semantic segmentation. **Semantic
    segmentation** (or image/object segmentation) is a task in which every pixel in
    the image must belong to one object. The most promising application of semantic
    segmentation is autonomous cars (or self-driving cars). If each and every pixel
    that's captured by the camera that's mounted on the self-driving car is correctly
    classified, all of the objects in the image will be easily recognized, which makes
    it much easier for the vehicle to properly analyze the current environment and
    make the right decision upon whether it should, for example, turn or slow down
    to avoid other vehicles and pedestrians. To understand more about semantic segmentation,
    please refer to the following link: [https://devblogs.nvidia.com/image-segmentation-using-digits-5](https://devblogs.nvidia.com/image-segmentation-using-digits-5).
  prefs: []
  type: TYPE_NORMAL
- en: Transforming the original color image into a segmentation map (as shown in the
    following diagram) can be considered as an image-to-image translation problem,
    which is a much larger field and includes style transfer, image colorization,
    and more. Image **style transfer** is about moving the iconic textures and colors
    from one image to another, such as combining your photo with a Vincent van Gogh
    painting to create a unique artistic portrait of you. **Image colorization** is
    a task where we feed a 1-channel grayscale image to the model and let it predict
    the color information for each pixel, which leads to a 3-channel color image.
  prefs: []
  type: TYPE_NORMAL
- en: GANs can be used in image-to-image translation as well. In this section, we
    will use a classic image-to-image translation model, pix2pix, to transform images
    from one domain to another. Pix2pix was proposed by Phillip Isola, Jun-Yan Zhu,
    and Tinghui Zhou, et. al. in their paper *Image-to-Image Translation with Conditional
    Adversarial Networks*. Pix2pix was designed to learn of the connections between
    paired collections of images, for example, transforming an aerial photo taken
    by a satellite into a regular map, or a sketch image into a color image, and vice
    versa.
  prefs: []
  type: TYPE_NORMAL
- en: The authors of the paper have kindly provided the full source code for their
    work, which runs perfectly on PyTorch 1.3. The source code is also well organized.
    Therefore, we will use their code directly in order to train and evaluate the
    pix2pix model and learn how to organize our models in a different way.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, open a Terminal and download the code for this section using the following
    command. This is also available under the `pix2pix` directory in this chapter''s
    code repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, `install` the prerequisites to be able to visualize the results during
    training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Generator architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The architecture of the generator network of pix2pix is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6b0e72d5-fb06-4e4f-9642-65a97cfe2f52.png)'
  prefs: []
  type: TYPE_IMG
- en: Generator architecture of pix2pix
  prefs: []
  type: TYPE_NORMAL
- en: Here, we assume that both the input and output data are 3-channel 256x256 images. In
    order to illustrate the generator structure of pix2pix, feature maps are represented
    by colored blocks and convolution operations are represented by gray and blue
    arrows, in which gray arrows are convolution layers for reducing the feature map
    sizes and blue arrows are for doubling the feature map sizes. Identity mapping
    (including skip connections) is represented by black arrows.
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the first half layers of this network gradually transform the
    input image into 1x1 feature maps (with wider channels) and the last half layers
    transform these very small feature maps into an output image with the same size
    of the input image. It compresses the input data into much lower dimensions and
    changes them back to their original dimensions. Therefore, this U-shaped kind
    of network structure is often known as U-Net. There are also many skip connections
    in the U-Net that connect the mirrored layers in order to help information (including
    details coming from previous layers in the forward pass and gradients coming from
    the latter layers in the backward pass) flow through the network. Without these
    skip connections, the network is also known as an encoder-decoder model, meaning
    that we stack a decoder at the end of an encoder.
  prefs: []
  type: TYPE_NORMAL
- en: The pix2pix model is defined in the `models.pix2pix_model.Pix2PixModel` class,
    which is derived from an **abstract base class** (**ABC**) known as `models.base_model.BaseModel`.
  prefs: []
  type: TYPE_NORMAL
- en: An **abstract base class** in Python is a class containing at least one **abstract
    method** (that's declared and not implemented). It cannot be instantiated. You
    can only create objects with its subclasses after providing the implementations
    for all the abstract methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'The generator network, `netG`, is created by the `models.networks.define_G`
    method. By default, it takes `''unet_256''` as the `netG` argument value (which
    is specified at line 32 in `models/pix2pix_model.py` and overrides the initialized
    value, that is, `"resnet_9blocks"`, at line 34 in `options/base_options.py`).
    Therefore, `models.networks.UnetGenerator` is used to create the U-Net. In order
    to show how the U-Net is created in a recursive manner, we replace the arguments
    with their actual values, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'At the fourth line in the preceding code snippet, the innermost block is defined,
    which creates the layers in the middle of the U-Net. The innermost block is defined
    as follows. Note that the following code should be treated as pseudocode since
    it''s simply to show you how different blocks are designed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The `nn.Conv2d` layer in `down` transforms 2x2 input feature maps into 1x1 ones
    (because `kernel_size`=4 and `padding`=1), and the `nn.ConvTranspose2d` layer
    transforms them back so that they're 2x2 in size.
  prefs: []
  type: TYPE_NORMAL
- en: Remember the calculation formula of the output size for `nn.Conv2d` and `nn.ConvTranspose2d`?
    The output size of the convolution is ![](img/1539e7fa-afb0-43b4-8182-da522a169eb1.png),
    while the output size of the transposed convolution is ![](img/9050dcbb-74e6-46ec-b1e7-764b92864e6b.png).
  prefs: []
  type: TYPE_NORMAL
- en: In the forward pass, it concatenates the output with a skip connection (that
    is, the input *x* itself) along the depth channel, which doubles the number of
    channels (and leads to the first 1,024-channel feature maps in the preceding diagram).
  prefs: []
  type: TYPE_NORMAL
- en: When designing complex networks, it's been observed that the concatenation of
    the feature maps from two branches is better than their sum because the concatenation
    reserves more information. Of course, this concatenation costs a little more memory
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, the rest of the layers are built recursively, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Although in `models.networks.UnetGenerator`, the `unet_block` object is recursively
    passed as a `submodule` to a new `unet_block`, thanks to the compact design to
    the implementation of tensors, the actual modules will be created and saved on
    memory properly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the first and last layers (which can be seen in the outermost block)
    are defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: All the convolution kernels in the generator network are initialized based on
    a normal distribution with a mean of 0 and a standard deviation of 0.02\. The
    scale factors in all the batch normalization layers are initialized based on the normal
    distribution with a mean of 1 and a standard deviation of 0.02.
  prefs: []
  type: TYPE_NORMAL
- en: Discriminator architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The architecture of the discriminator network of pix2pix is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/496267dd-f9f8-4c85-a1ad-de0fb427a0ed.png)'
  prefs: []
  type: TYPE_IMG
- en: Discriminator architecture of pix2pix
  prefs: []
  type: TYPE_NORMAL
- en: A pair of samples (one from each collection) are concatenated along the depth
    channel, and this 6-channel image is treated as the actual input of the discriminator
    network. The discriminator network maps the 6-channel 256x256 image to a 1-channel
    30x30 image, which is used to calculate the discriminator loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'The discriminator network, `netD`, is created by the `models.networks.define_G`
    method. By default, it takes `"basic"` as the argument value of `netD`, which
    is defined at line 33 in `options/base_options.py`. The `models.networks.NLayerDiscriminator`
    module, which has `n_layer=3`, is initialized so that it can serve as the discriminator
    network. Again, we''ve simplified the code so that it''s easy to read. You may
    refer to the full code in the `models/networks.py` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we provide a short snippet so that we can print the sizes of all the
    feature maps if the model is created, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'You can replace the line `return self.model(input)` with the following code
    to check the feature map sizes in all the layers (including the normalization
    and activation function layers):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Alternatively, you can always use TensorBoard or other tools, which we will
    introduce in the last chapter of this book, so that you can easily examine the
    architectures of your models.
  prefs: []
  type: TYPE_NORMAL
- en: The discriminator network creates a 30x30 feature map to represent the loss.
    This kind of architecture is called **PatchGAN**, which means that every small
    image patch in the original image is mapped to a pixel in the final loss map.
    A big advantage of PatchGAN is that it can handle the arbitrary sizes of input
    images as long as the labels have been transformed so that they're the same size
    as the loss map. It also evaluates the quality of the input image according to
    the quality of the local patches, rather than their global property. Here, we
    will show you how the size of the image patch (that is, 70) is calculated.
  prefs: []
  type: TYPE_NORMAL
- en: First, let's consider a single convolution layer with a kernel size of `k` and
    a stride size of `s`. For each pixel in the output feature map, its value is only
    determined by a small patch of pixels in the input image, whose size is the same
    as the convolution kernel.
  prefs: []
  type: TYPE_NORMAL
- en: 'When there are more than two convolution layers, the size of the input patch
    is calculated with the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ed0ae0fe-f0b9-4825-999a-41e71a2e3bf7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, the size of the input patch corresponding to a single pixel in the
    output feature map in each layer of the discriminator network can be obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '5th layer (k=4, s=1): Input patch size is 4 (which is the size of the kernel)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '4th layer (k=4, s=1): Input patch size is 4+1*(4-1)=7'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '3rd layer (k=4, s=2): Input patch size is 4+2*(7-1)=16'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '2nd layer (k=4, s=2): Input patch size is 4+2*(16-1)=34'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '1st layer (k=4, s=2): Input patch size is 4+2*(34-1)=70'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This means that all of these 70x70 overlapping image patches are transformed
    by convolution layers into individual pixels in the 30x30 loss map. Any pixel
    outside this 70x70 image patch has no influence over the corresponding pixel in
    the loss map.
  prefs: []
  type: TYPE_NORMAL
- en: Training and evaluation of pix2pix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The training of pix2pix is very similar to conditional GANs, which we introduced
    in the previous chapter. When training the discriminator network, a pair of real
    data and a label should be mapped to 1, whereas a pair of generated data and a
    label (that fake data is generated from) is mapped to 0\. When training the generator
    network, the gradients are passed through both of the discriminator and generator
    networks when the parameters in the generator network are updated. This generated
    data and the label should be mapped to 1 by the discriminator network. The major
    difference is that the labels are image-wise in CGAN and are pixel-wise in pix2pix.
    This process is described in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7ccb49b6-c6c5-415b-a0a2-65129f267e48.png)'
  prefs: []
  type: TYPE_IMG
- en: Basic training process of image-wise and pixel-wise labeled GANs. A* and B*
    denote real samples. Networks in red boxes are actually updated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that, when training pix2pix, in order to let the generated samples be
    as similar to the real ones as possible, an additional term is added to the loss
    function when training the generator network, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cd1117ed-2fbc-4293-add1-c65349fd6b54.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/bd348fa5-fff8-4911-bc8d-58bb0795875f.png) represents the L1-loss
    between the generated samples and the real ones from the paired collection. The
    purpose of the L1-loss is to reserve the low-frequency information in the images
    for better image quality.
  prefs: []
  type: TYPE_NORMAL
- en: It is worth mentioning that using L1-norm or L2-norm alone will generate blurry
    or blocky images. A short explanation of this can be found here: [https://wiseodd.github.io/techblog/2017/02/09/why-l2-blurry](https://wiseodd.github.io/techblog/2017/02/09/why-l2-blurry).
    It is also common to use them as regularization terms in the traditional image
    restoration methods, in which the gradients of the restored images control the
    sharpness. If you are interested in the roles of L1-loss and L2-loss in the field
    of image processing, feel free to check out the famous paper, *Total variation
    blind deconvolution* by Tony F. Chan and C.K. Wong in 1998.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can define the training procedure of pix2pix, as follows (pseudocode):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The `Pix2PixModel` class serves a similar purpose to the `Model` class in `build_gan.py`
    from the previous chapter, which creates the generator and discriminator networks,
    defines their optimizers, and controls the training procedures of the networks.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's download some images and train the pix2pix model to perform image-to-image
    translation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the `datasets/download_pix2pix_dataset.sh` script to download the dataset
    files, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Alternatively, you can go to [http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/](http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/)
    to download the dataset files manually and extract them to any location you like
    (for example, an external hard drive such as `/media/john/HouseOfData/image_transfer/maps`).
    The maps dataset file is approximately 239 MB in size and contains a few more
    than 1,000 images in the collections of the train, validation, and test sets.
  prefs: []
  type: TYPE_NORMAL
- en: Note that collection A in the maps dataset contains satellite photos and that
    collection B contains map images, which is opposite to what was shown in the diagrams
    in the previous subsections.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, open a Terminal and run the following script to start training. Make
    sure you modify the `dataroot` argument so that it specifies your own location.
    You may also try other datasets and change `direction` from `BtoA` to `AtoB` to
    change the translation direction between two image collections:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: For the first time of training, you may encounter an error stating `Could not
    connect to Visdom server`. This is because the training script calls the `Visdom`
    module to dynamically update the generated results so that we can monitor the
    training process via web browsers. You can manually open the `checkpoints/maps_pix2pix/web/index.html`
    file with your favorite browser to keep an eye on the generated images as the
    model is being trained. Note that there is a chance that closing the `index.html`
    page in the web browser could cause the training process to freeze.
  prefs: []
  type: TYPE_NORMAL
- en: It takes about 6.7 hours to finish 200 epochs of training and costs about 1,519
    MB GPU memory on a GTX 1080Ti graphics card.
  prefs: []
  type: TYPE_NORMAL
- en: 'The results are also saved in the `checkpoints/maps_pix2pix/web/images` directory.
    The images that are generated by doing this are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a9938fd6-2b4d-4da7-b15e-33470c8bf30c.png)'
  prefs: []
  type: TYPE_IMG
- en: Generated images by pix2pix
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the generated satellite photos look pretty convincing on their
    own. Compared to real satellite photos, they do a good job of organizing the trees
    along the trails in the park.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we managed to translate and generate 256 x 256 images. In
    the next section, we will learn how to generate high-resolution images with an
    upgraded version of pix2pix: pix2pixHD.'
  prefs: []
  type: TYPE_NORMAL
- en: Pix2pixHD – high-resolution image translation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pix2pixHD was proposed by Ting-Chun Wang, Ming-Yu Liu, and Jun-Yan Zhu, et.
    al. in their paper, *High-Resolution Image Synthesis and Semantic Manipulation
    with Conditional GANs*, which was an upgraded version of the pix2pix model. The
    biggest improvement of pix2pixHD over pix2pix is that it supports image-to-image
    translation at 2,048x1,024 resolution and with high quality.
  prefs: []
  type: TYPE_NORMAL
- en: Model architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To make this happen, they designed a two-stage approach to gradually train
    and refine the networks, as shown in the following diagram. First, a lower resolution
    image of 1,024x512 is generated by a generator network, ![](img/aaed65e0-5db0-43b6-9ce6-4301c6984bf0.png),
    called the **global generator** (the red box). Second, the image is enlarged by
    a generator network, ![](img/1d7444a7-949b-4f66-871d-8ecd2f72d759.png), called
    the **local enhancer network** so that it''s around 2,048x1,024 in size (the black
    box). It is also viable to put another local enhancer network at the end to generate
    4,096x2,048 images. Note that the last feature map in ![](img/aaed65e0-5db0-43b6-9ce6-4301c6984bf0.png)
    is also inserted into ![](img/1d7444a7-949b-4f66-871d-8ecd2f72d759.png) (before
    the residual blocks) via an element-wise sum to introduce more global information
    into higher resolution images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/084d449a-d919-449f-9768-40daf14dfb27.png)'
  prefs: []
  type: TYPE_IMG
- en: Architecture of the generator model in pix2pixHD (image retrieved from the paper
    by T. C. Wang, et. al., 2018)
  prefs: []
  type: TYPE_NORMAL
- en: 'The discriminator network in pix2pixHD is also designed in a multi-scale fashion.
    Three identical discriminator networks work on different image scales (original
    size, 1/2 size, and 1/4 size) and their loss values are added together. It is
    reported by the authors that, without multi-scale design in the discriminator,
    repeated patterns are often observed in the generated images. Also, an additional
    term, called the **feature matching loss**, is added to the final discriminator
    loss, as shown the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fecc2df3-fec9-4df6-bf1a-0cc22388f685.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/8df2c128-fd0c-4a48-b0eb-e61089bf1d48.png) measures the L1-loss
    between the feature maps of the generated and real images at multiple layers in
    the discriminator networks. It forces the generator to approximate the real data
    at different scales, thereby generating more realistic images.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, several objects with the same label may find their way together,
    which makes it difficult for the generator to correctly distinguish these objects.
    It would help if the generator knew which pixels belong to which object compared
    to which class they belong to. Therefore, in pix2pixHD, an **instance boundary
    map** (which is a binary map denoting the boundaries of all the objects) is channel-wise concatenated
    to the semantic label map before it's fed into the generator. Similarly, the instance
    boundary map is also concatenated to the semantic label map and the image (the
    generated one or the real one), before being fed into the discriminator.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, in order to make it easier to manipulate the attributes of the
    generated images, pix2pixHD uses an additional **encoder** to extract features
    from the real images and performs instance-wise average pooling (averages all
    the pixels in one object and then broadcasts back to these pixels) on the features.
    These features are also part of the input to the generator. K-means clustering
    is performed on the features of all the objects in each class, and several available
    textures or colors can be chosen for the objects during inference.
  prefs: []
  type: TYPE_NORMAL
- en: We will not dive deep into the specific architecture designs of pix2pixHD since
    the main structure of its source code is similar to pix2pix. You can check out
    the source code if you're interested.
  prefs: []
  type: TYPE_NORMAL
- en: Model training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The training of pix2pixHD is both time- and memory-consuming. It requires about
    24 GB GPU memory to train 2,048x1,024 images. Therefore, we will only train on
    a 1,024x512 resolution in order to fit this on a single graphic card.
  prefs: []
  type: TYPE_NORMAL
- en: 'NVIDIA has already open-sourced the full source code of pix2pixHD for PyTorch.
    All we need to do is download the source code and dataset to produce our own high
    resolution synthesized images. Let''s do this now:'
  prefs: []
  type: TYPE_NORMAL
- en: Install the prerequisites (dominate and apex). We previously installed the `dominate` library. **Apex** is
    a mixed precision and distributed training library that's developed by NVIDIA.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use **Automatic Mixed Precision** (**AMP**) to reduce the GPU memory consumption
    (or even the training time) during training by replacing the standard floating-point
    values with lower bit floats.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open a Terminal in Ubuntu and type in the following scripts to install `apex`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Download the source code of pix2pixHD (also available under the code repository
    for this chapter):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Use the **Cityscapes** dataset to train the pix2pixHD model. It is available
    at [https://www.cityscapes-dataset.com](https://www.cityscapes-dataset.com) and
    you'll need to register first before being granted access to the download links.
    We need to download the `gtFine_trainvaltest.zip` *(*241 MB*)* and `leftImg8bit_trainvaltest.zip` *(*11
    GB*)* files for this experiment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When the download is finished, we need to reorganize the images so that the
    training script can pick up the images correctly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Put all the image files in the `gtFine/train/*` folders that end with `_gtFine_instanceIds.png` into
    the `datasets/cityscapes/train_inst` directory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Put all the image files in the `gtFine/train/*` folders that end with `_gtFine_labelIds.png` into
    the `datasets/cityscapes/train_label` directory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Put all the image files in the `leftImg8bit/train/*` folders that end with `_leftImg8bit.png` into
    the `datasets/cityscapes/train_img` directory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The test and validation sets can be ignored since we only need the training
    images. There should be 2,975 images in each of the training folders.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run `scripts/train_512p.sh` to start the training process or simply type the
    following in the Terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: All the intermediate results (arguments taken, generated images, logging information,
    and model files) will be saved in the `checkpoints/label2city_512p` folder. You
    can always check the `checkpoints/label2city_512p/web/index.html` file in your
    favorite browser or directly check out the images in the `checkpoints/label2city_512p/web/images`
    folder to monitor the training process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the results after 35 epochs of training (about 20 hours):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/655ba229-1df2-4a56-92a5-579539a25957.png)'
  prefs: []
  type: TYPE_IMG
- en: Generated image after 35 epochs of training by pix2pixHD
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that the model has already figured out where to put vehicles,
    trees, buildings, and pedestrians based on the label information from the instance
    map, although the objects themselves still have much to improve on in terms of
    appearance. It is interesting to observe that the model is trying to put road
    lines in the correct positions and that the badge of the car that the images have
    been captured from has an almost perfect reflection on the front hood (which makes
    sense since the badge and the hood appear in every image).
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are willing to wait long enough (approximately 110 hours), the results
    are pretty impressive:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fd1df132-f082-48aa-9526-74cc7e3e8450.png)'
  prefs: []
  type: TYPE_IMG
- en: Generated image by pix2pixHD (images retrieved from https://github.com/NVIDIA/pix2pixHD)
  prefs: []
  type: TYPE_NORMAL
- en: It costs about 8,077 MB GPU memory to train on a 1,024x512 resolution. When
    AMP is enabled (trained with `--fp16`), the GPU memory consumption starts with
    7,379 MB at first and gradually increases to 7,829 MB after a few epochs, which
    is indeed lower than before. However, the training time is almost half as long
    than it is without AMP. Therefore, you should go without AMP for now, until its
    performance is improved in the future.
  prefs: []
  type: TYPE_NORMAL
- en: CycleGAN – image-to-image translation from unpaired collections
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You may have noticed that, when training pix2pix, we need to determine a direction
    (`AtoB` or `BtoA`) that the images are translated to. Does this mean that, if
    we want to freely translate from image set A to image set B and vice versa, we
    need to train two models separately? Not with CycleGAN, we say!
  prefs: []
  type: TYPE_NORMAL
- en: CycleGAN was proposed by Jun-Yan Zhu, Taesung Park, and Phillip Isola, et. al.
    in their paper, *Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial
    Networks*. It is a bidirectional generative model based on unpaired image collections.
    The core idea of CycleGAN is built on the assumption of cycle consistency, which
    means that if we have two generative models, G and F, that translate between two
    sets of images, X and Y, in which Y=G(X) and X=F(Y), we can naturally assume that
    F(G(X)) should be very similar to X and G(F(Y)) should be very similar to Y. This
    means that we can train two sets of generative models at the same time that can
    freely translate between two sets of images.
  prefs: []
  type: TYPE_NORMAL
- en: CycleGAN is specifically designed for unpaired image collections, which means
    that the training samples are not necessarily strictly paired like they were in
    the previous sections when we looked at pix2pix and pix2pixHD (for example, semantic
    segmentation maps versus street views from the same perspective, or regular maps
    versus satellite photos of the same location). This makes CycleGAN more than just
    an image-to-image translation tool. It unlocks the potential to **transfer style**
    from any kind of images to your own images, for example, turning apples into oranges,
    horses into zebras, photos into oil paintings, and vice versa. Here, we'll perform
    image-to-image translation on landscape photos and Vincent van Gogh's paintings
    as an example to show you how CycleGAN is designed and trained.
  prefs: []
  type: TYPE_NORMAL
- en: Note that, in this section, the code layout is similar to CGAN in the previous
    chapter. The full source code is available under the code repository for this
    chapter. The models are defined in `cyclegan.py`, the training process is defined
    in `build_gan.py`, and the main entry is located at `main.py`. The source code
    is based on the implementation provided by [https://github.com/eriklindernoren/PyTorch-GAN](https://github.com/eriklindernoren/PyTorch-GAN). It
    is worth mentioning that our implementation trains 1.2x faster and costs 28% less
    GPU memory than that implementation. Also, in the source code of pix2pix, which
    can be found in the first section of this chapter, an implementation of CycleGAN
    was provided. You may choose whichever implementation you like since there isn't
    much of a difference between the two.
  prefs: []
  type: TYPE_NORMAL
- en: Cycle consistency-based model design
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Two pairs of generator and discriminator networks are used, with each being
    responsible for a translation direction. In order to understand why CycleGAN is
    designed as such, we need to understand how the cycle consistency is constructed.
  prefs: []
  type: TYPE_NORMAL
- en: In the following diagram, the generator, ![](img/f940b35c-4e1c-481b-94a6-ca18e1505cea.png)maps
    sample A to sample B and its performance is measured by the discriminator, ![](img/8cb29fbf-9cbb-4552-8a33-f05dd7f5090e.png).
    At the same time, another generator, ![](img/844e96ce-5649-43b2-a58c-d543f0629aa1.png), is
    trained to map sample B back to sample A, whose performance is measured by the
    discriminator, ![](img/999a4fc7-a3ca-4056-a923-716408abd75c.png). In this process,
    the distance between a generated sample, ![](img/ec2bfd57-c769-41de-aba9-61400cf32047.png), and
    the corresponding original real sample, ![](img/ecc613f1-2253-46d4-b76c-041603a3d2b0.png), tells
    us whether a cycle consistency exists in our model, as shown in the dotted box
    in the following diagram. The distance between ![](img/deac1805-46a7-4b5c-a2df-42904c879e9a.png) and ![](img/ecc613f1-2253-46d4-b76c-041603a3d2b0.png) is
    measured by the **cycle consistency loss**, which takes the form of the L1-norm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides the traditional **adversarial loss** (distance between ![](img/b1979e0b-0650-450b-b9ec-aea0e7b059dc.png) and
    1), the **identity loss** (which means that ![](img/58282974-6e86-4511-a954-dddbfcfaec32.png) should
    be very close to ![](img/ecc613f1-2253-46d4-b76c-041603a3d2b0.png) itself) is
    also added to help maintain the color style of the images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dc7bfabe-2c11-425e-9ebb-793ad0ad6e00.png)'
  prefs: []
  type: TYPE_IMG
- en: The calculation of loss in CycleGAN. A* and B* denote real samples. Networks
    denoted by red boxes are updated while training the generators.
  prefs: []
  type: TYPE_NORMAL
- en: 'The two generator networks, ![](img/f940b35c-4e1c-481b-94a6-ca18e1505cea.png)and, are
    identical. The architecture of the generator network can be seen in the following
    diagram. The 256x256 input image is downsampled by multiple convolution layers
    to 64x64, processed by nine successive residual blocks, and finally upsampled
    by convolutions back to 256x256:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/53aa9cf0-ebf0-43f4-a5b4-c1ddd3a6b58d.png)'
  prefs: []
  type: TYPE_IMG
- en: Generator architecture in CycleGAN
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll start the code with a blank file named `cyclegan.py`, as we mentioned
    previously. Let''s start with the imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll create the code for the definition of the residual block, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can define the generator network, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: As you may have noticed, here, we used `torch.nn.InstanceNorm2d` instead of `torch.nn.BatchNorm2d`.
    The former normalization layer is more suitable for style transfer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, two identical discriminator networks are used in CycleGAN and their
    relationship can be seen in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eb67f62f-54fd-4654-916a-6ee30cc817ab.png)'
  prefs: []
  type: TYPE_IMG
- en: Relationship between two discriminator networks in CycleGAN. Networks denoted
    by red boxes are updated during training.
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of the discriminator network is almost the same as it is in
    pix2pix (which is called PatchGAN), except that the input image has a depth channel
    of 3, instead of 6, and `torch.nn.BatchNorm2d` is replaced with `torch.nn.InstanceNorm2d`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for the definition of the discriminator network is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's learn how the model can be trained and evaluated.
  prefs: []
  type: TYPE_NORMAL
- en: Model training and evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we''ll create the `build_gan.py` file. As usual, we''ll begin with the
    imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll need a function to initialize the weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will create the `Model` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The training processes for the generator and discriminator networks were shown
    previously. Here, we will dive into the implementation of `build_gan.train()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to train the generator networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we need to train the discriminator networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The last variable, `d_loss`, is simply for logging and has been omitted here.
    You can refer to the source code file for this chapter if you want to find out
    more about logging printing and image exporting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, as suggested in the paper, we update the discriminators by randomly picking
    an image from the history of generated images, rather than the fake samples in
    real-time. The history of generated images is maintained by the `ImageBuffer`
    class, which is defined as follows. Copy the `utils.py` file from the previous
    chapter and add the `ImageBuffer` class to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We also need to write a custom dataset reader that picks up unpaired images
    from separate folders. Place the following content into a new file called `datasets.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The shapes of the paintings and photos are not always square. Therefore, we
    need to crop 256x256 patches from the original images. We preprocess the data
    (**data augmentation**) in `main.py`. Here, we''re only showing a part of the
    code. You can find the rest of the code in the `main.py` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Don''t forget to adjust the argument parsing for CycleGAN. Remember, you should
    change the `--data_dir` default so that it matches your own setup, so be sure
    to include the following on the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Now, it's time to download the datasets and start having fun! Go to [https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets](https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets) to
    manually download the dataset files. Alternatively, you can use the `datasets/download_cyclegan_dataset.sh`
    script that's located in the source code of pix2pix to download the `vangogh2photo.zip`
    file, which is about 292 MB in size and contains 400 Van Gogh paintings and 7,038
    photos (6,287 in train and 751 in test). When the download is finished, extract
    the images to a folder (for example, an external hard drive such as `/media/john/HouseOfData/image_transfer`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a Terminal and type the following script to start training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'It takes about 10 hours to train CycleGAN for 20 epochs and costs about 4,031
    MB GPU memory on a GTX 1080Ti graphics card. Some of the results can be seen in
    the following image. Here, we can see that the style transfer capability of CycleGAN
    is pretty amazing. You can also check out this site to learn about more applications
    of CycleGAN: [https://junyanz.github.io/CycleGAN](https://junyanz.github.io/CycleGAN):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/74b56dc8-0c58-4e0d-81cb-ed1689b9868e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Generated images by CycleGAN. Top two rows: Painting to photo; Bottom two rows:
    Photo to painting.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have been getting familiar with image generation for several chapters now.
    Although it is always challenging and fulfilling to successfully train GANs to
    generate amazing images, we should recognize that GANs can also be used to fix
    things and restore images.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore the generative power of GANs to address
    some of the challenging problems in image restoration.
  prefs: []
  type: TYPE_NORMAL
- en: Furthering reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Le J. (May 3, 2018) *How to do Semantic Segmentation using Deep learning*. Retrieved
    from [https://medium.com/nanonets/how-to-do-image-segmentation-using-deep-learning-c673cc5862ef](https://medium.com/nanonets/how-to-do-image-segmentation-using-deep-learning-c673cc5862ef).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rainy J. (Feb 12, 2018) *Stabilizing neural style-transfer for video*. Retrieved
    from [https://medium.com/element-ai-research-lab/stabilizing-neural-style-transfer-for-video-62675e203e42](https://medium.com/element-ai-research-lab/stabilizing-neural-style-transfer-for-video-62675e203e42).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Isola P, Zhu JY, Zhou T, Efros A. (2017) *Image-to-Image Translation with Conditional
    Adversarial Networks*. CVPR.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Agustinus K. (Feb 9, 2017) *Why does L2 reconstruction loss yield blurry images?* Retrieved
    from [https://wiseodd.github.io/techblog/2017/02/09/why-l2-blurry](https://wiseodd.github.io/techblog/2017/02/09/why-l2-blurry).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Chan T F, Wong C K. (1998) *Total Variation Blind Deconvolution. IEEE Transactions
    on Image Processing*. 7(3): 370-375.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wang T C, Liu M Y, Zhu J Y, et. al. (2018) *High-Resolution Image Synthesis
    and Semantic Manipulation with Conditional GANs*. CVPR.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Zhu J Y, Park T, Isola P, et. al. (2017) *Unpaired Image-to-Image Translation
    using Cycle-Consistent Adversarial Networks*. ICCV.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
