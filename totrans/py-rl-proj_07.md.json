["```py\nimport pickle\nimport codecs\nimport re\nimport os\nimport time\nimport numpy as np\n```", "```py\ndef preProBuildWordVocab(word_count_threshold=5, all_words_path='data/all_words.txt'):\n    # borrowed this function from NeuralTalk\n\n    if not os.path.exists(all_words_path):\n        parse_all_words(all_words_path)\n\n    corpus = open(all_words_path, 'r').read().split('\\n')[:-1]\n    captions = np.asarray(corpus, dtype=np.object)\n\n    captions = map(lambda x: x.replace('.', ''), captions)\n    captions = map(lambda x: x.replace(',', ''), captions)\n    captions = map(lambda x: x.replace('\"', ''), captions)\n    captions = map(lambda x: x.replace('\\n', ''), captions)\n    captions = map(lambda x: x.replace('?', ''), captions)\n    captions = map(lambda x: x.replace('!', ''), captions)\n    captions = map(lambda x: x.replace('\\\\', ''), captions)\n    captions = map(lambda x: x.replace('/', ''), captions)\n```", "```py\n\n    print('preprocessing word counts and creating vocab based on word count threshold %d' % (word_count_threshold))\n    word_counts = {}\n    nsents = 0\n    for sent in captions:\n        nsents += 1\n        for w in sent.lower().split(' '):\n\n            word_counts[w] = word_counts.get(w, 0) + 1\n    vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n    print('filtered words from %d to %d' % (len(word_counts), len(vocab)))\n\n    ixtoword = {}\n    ixtoword[0] = '<pad>'\n    ixtoword[1] = '<bos>'\n    ixtoword[2] = '<eos>'\n    ixtoword[3] = '<unk>'\n\n    wordtoix = {}\n    wordtoix['<pad>'] = 0\n    wordtoix['<bos>'] = 1\n    wordtoix['<eos>'] = 2\n    wordtoix['<unk>'] = 3\n\n    for idx, w in enumerate(vocab):\n        wordtoix[w] = idx+4\n        ixtoword[idx+4] = w\n\n    word_counts['<pad>'] = nsents\n    word_counts['<bos>'] = nsents\n    word_counts['<eos>'] = nsents\n    word_counts['<unk>'] = nsents\n\n    bias_init_vector = np.array([1.0 * word_counts[ixtoword[i]] for i in ixtoword])\n    bias_init_vector /= np.sum(bias_init_vector) # normalize to frequencies\n    bias_init_vector = np.log(bias_init_vector)\n    bias_init_vector -= np.max(bias_init_vector) # shift to nice numeric range\n\n    return wordtoix, ixtoword, bias_init_vector\n```", "```py\n\ndef parse_all_words(all_words_path):\n    raw_movie_lines = open('data/movie_lines.txt', 'r', encoding='utf-8', errors='ignore').read().split('\\n')[:-1]\n\n    with codecs.open(all_words_path, \"w\", encoding='utf-8', errors='ignore') as f:\n        for line in raw_movie_lines:\n            line = line.split(' +++$+++ ')\n            utterance = line[-1]\n            f.write(utterance + '\\n')\n```", "```py\n\ndef refine(data):\n    words = re.findall(\"[a-zA-Z'-]+\", data)\n    words = [\"\".join(word.split(\"'\")) for word in words]\n    # words = [\"\".join(word.split(\"-\")) for word in words]\n    data = ' '.join(words)\n    return data\n```", "```py\n\nif __name__ == '__main__':\n    parse_all_words('data/all_words.txt')\n\n    raw_movie_lines = open('data/movie_lines.txt', 'r', encoding='utf-8', errors='ignore').read().split('\\n')[:-1]\n\n    utterance_dict = {}\n    with codecs.open('data/tokenized_all_words.txt', \"w\", encoding='utf-8', errors='ignore') as f:\n        for line in raw_movie_lines:\n            line = line.split(' +++$+++ ')\n            line_ID = line[0]\n            utterance = line[-1]\n            utterance_dict[line_ID] = utterance\n            utterance = \" \".join([refine(w) for w in utterance.lower().split()])\n            f.write(utterance + '\\n')\n    pickle.dump(utterance_dict, open('data/utterance_dict', 'wb'), True)\n```", "```py\nimport pickle\nimport random\n```", "```py\nclass Data_Reader:\n    def __init__(self, cur_train_index=0, load_list=False):\n        self.training_data = pickle.load(open('data/conversations_lenmax22_formersents2_with_former', 'rb'))\n        self.data_size = len(self.training_data)\n        if load_list:\n            self.shuffle_list = pickle.load(open('data/shuffle_index_list', 'rb'))\n        else:    \n            self.shuffle_list = self.shuffle_index()\n        self.train_index = cur_train_index\n```", "```py\n    def get_batch_num(self, batch_size):\n        return self.data_size // batch_size\n```", "```py\n    def shuffle_index(self):\n        shuffle_index_list = random.sample(range(self.data_size), self.data_size)\n        pickle.dump(shuffle_index_list, open('data/shuffle_index_list', 'wb'), True)\n        return shuffle_index_list\n```", "```py\n    def generate_batch_index(self, batch_size):\n        if self.train_index + batch_size > self.data_size:\n            batch_index = self.shuffle_list[self.train_index:self.data_size]\n            self.shuffle_list = self.shuffle_index()\n            remain_size = batch_size - (self.data_size - self.train_index)\n            batch_index += self.shuffle_list[:remain_size]\n            self.train_index = remain_size\n        else:\n            batch_index = self.shuffle_list[self.train_index:self.train_index+batch_size]\n            self.train_index += batch_size\n\n        return batch_index\n```", "```py\n\n    def generate_training_batch(self, batch_size):\n        batch_index = self.generate_batch_index(batch_size)\n        batch_X = [self.training_data[i][0] for i in batch_index]   # batch_size of conv_a\n        batch_Y = [self.training_data[i][1] for i in batch_index]   # batch_size of conv_b\n\n        return batch_X, batch_Y\n```", "```py\n\n    def generate_training_batch_with_former(self, batch_size):\n        batch_index = self.generate_batch_index(batch_size)\n        batch_X = [self.training_data[i][0] for i in batch_index]   # batch_size of conv_a\n        batch_Y = [self.training_data[i][1] for i in batch_index]   # batch_size of conv_b\n        former = [self.training_data[i][2] for i in batch_index]    # batch_size of former utterance\n\n        return batch_X, batch_Y, former\n```", "```py\n\n    def generate_testing_batch(self, batch_size):\n        batch_index = self.generate_batch_index(batch_size)\n        batch_X = [self.training_data[i][0] for i in batch_index]   # batch_size of conv_a\n\n        return batch_X\n```", "```py\nimport tensorflow as tf\nimport numpy as np\nimport re\n```", "```py\ndef model_inputs(embed_dim, reinforcement= False):    \n    word_vectors = tf.placeholder(tf.float32, [None, None, embed_dim], name = \"word_vectors\")\n    reward = tf.placeholder(tf.float32, shape = (), name = \"rewards\")\n    caption = tf.placeholder(tf.int32, [None, None], name = \"captions\")\n    caption_mask = tf.placeholder(tf.float32, [None, None], name = \"caption_masks\")\n    if reinforcement: #Normal training returns only the word_vectors, caption and caption_mask placeholders, \n        #With reinforcement learning, there is an extra placeholder for rewards\n        return word_vectors, caption, caption_mask, reward\n    else:\n        return word_vectors, caption, caption_mask\n```", "```py\n\ndef encoding_layer(word_vectors, lstm_size, num_layers, keep_prob, \n                   vocab_size):\n\n    cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.LSTMCell(lstm_size), keep_prob) for _ in range(num_layers)])\n\n    outputs, state = tf.nn.dynamic_rnn(cells, \n                                       word_vectors, \n                                       dtype=tf.float32)\n    return outputs, state\n```", "```py\ndef decode_train(enc_state, dec_cell, dec_input, \n                         target_sequence_length,output_sequence_length,\n                         output_layer, keep_prob):\n    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell,                #Apply dropout to the LSTM cell\n                                             output_keep_prob=keep_prob)\n\n    helper = tf.contrib.seq2seq.TrainingHelper(dec_input,             #Training helper for decoder \n                                               target_sequence_length)\n\n    decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, \n                                              helper, \n                                              enc_state, \n                                              output_layer)\n\n    # unrolling the decoder layer\n    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, \n                                                      impute_finished=True,\n                                                     maximum_iterations=output_sequence_length)\n    return outputs\n```", "```py\ndef decode_generate(encoder_state, dec_cell, dec_embeddings,\n                         target_sequence_length,output_sequence_length,\n                         vocab_size, output_layer, batch_size, keep_prob):\n    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, \n                                             output_keep_prob=keep_prob)\n\n    helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embeddings, \n                                                      tf.fill([batch_size], 1),  #Decoder helper for inference\n                                                      2)\n\n    decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, \n                                              helper, \n                                              encoder_state, \n                                              output_layer)\n\n    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, \n                                                      impute_finished=True,\n                                                     maximum_iterations=output_sequence_length)\n    return outputs\n```", "```py\ndef decoding_layer(dec_input, enc_state,\n                   target_sequence_length,output_sequence_length,\n                   lstm_size,\n                   num_layers,n_words,\n                   batch_size, keep_prob,embedding_size, Train = True):\n    target_vocab_size = n_words\n    with tf.device(\"/cpu:0\"):\n        dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size,embedding_size], -0.1, 0.1), name='Wemb')\n    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n\n    cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.LSTMCell(lstm_size) for _ in range(num_layers)])\n\n    with tf.variable_scope(\"decode\"):\n        output_layer = tf.layers.Dense(target_vocab_size)\n\n    if Train:\n        with tf.variable_scope(\"decode\"):\n            train_output = decode_train(enc_state, \n                                                cells, \n                                                dec_embed_input, \n                                                target_sequence_length, output_sequence_length,\n                                                output_layer, \n                                                keep_prob)\n\n    with tf.variable_scope(\"decode\", reuse=tf.AUTO_REUSE):\n        infer_output = decode_generate(enc_state, \n                                            cells, \n                                            dec_embeddings, target_sequence_length,\n                                           output_sequence_length,\n                                            target_vocab_size, \n                                            output_layer,\n                                            batch_size,\n                                            keep_prob)\n    if Train:\n        return train_output, infer_output\n    return infer_output\n```", "```py\ndef bos_inclusion(caption,batch_size):\n\n    sliced_target = tf.strided_slice(caption, [0,0], [batch_size, -1], [1,1])\n    concat = tf.concat([tf.fill([batch_size, 1],1), sliced_target],1)\n    return concat\n```", "```py\ndef pad_sequences(questions, sequence_length =22):\n    lengths = [len(x) for x in questions]\n    num_samples = len(questions)\n    x = np.zeros((num_samples, sequence_length)).astype(int)\n    for idx, sequence in enumerate(questions):\n        if not len(sequence):\n            continue  # empty list/array was found\n        truncated  = sequence[-sequence_length:]\n\n        truncated = np.asarray(truncated, dtype=int)\n\n        x[idx, :len(truncated)] = truncated\n\n    return x\n```", "```py\ndef refine(data):\n    words = re.findall(\"[a-zA-Z'-]+\", data)\n    words = [\"\".join(word.split(\"'\")) for word in words]\n    data = ' '.join(words)\n    return data\n```", "```py\ndef make_batch_input(batch_input, input_sequence_length, embed_dims, word2vec):\n\n    for i in range(len(batch_input)):\n\n        batch_input[i] = [word2vec[w] if w in word2vec else np.zeros(embed_dims) for w in batch_input[i]]\n        if len(batch_input[i]) >input_sequence_length:\n            batch_input[i] = batch_input[i][:input_sequence_length]\n        else:\n            for _ in range(input_sequence_length - len(batch_input[i])):\n                batch_input[i].append(np.zeros(embed_dims))\n\n    return np.array(batch_input)\n\ndef replace(target,symbols):  #Remove symbols from sequence\n    for symbol in symbols:\n        target = list(map(lambda x: x.replace(symbol,''),target))\n    return target\n\ndef make_batch_target(batch_target, word_to_index, target_sequence_length):\n    target = batch_target\n    target = list(map(lambda x: '<bos> ' + x, target))\n    symbols = ['.', ',', '\"', '\\n','?','!','\\\\','/']\n    target = replace(target, symbols)\n\n    for idx, each_cap in enumerate(target):\n        word = each_cap.lower().split(' ')\n        if len(word) < target_sequence_length:\n            target[idx] = target[idx] + ' <eos>'  #Append the end of symbol symbol \n        else:\n            new_word = ''\n            for i in range(target_sequence_length-1):\n                new_word = new_word + word[i] + ' '\n            target[idx] = new_word + '<eos>'\n\n    target_index = [[word_to_index[word] if word in word_to_index else word_to_index['<unk>'] for word in \n                          sequence.lower().split(' ')] for sequence in target]\n    #print(target_index[0])\n\n    caption_matrix = pad_sequences(target_index,target_sequence_length)\n    caption_matrix = np.hstack([caption_matrix, np.zeros([len(caption_matrix), 1])]).astype(int)\n    caption_masks = np.zeros((caption_matrix.shape[0], caption_matrix.shape[1]))\n    nonzeros = np.array(list(map(lambda x: (x != 0).sum(), caption_matrix)))\n    #print(nonzeros)\n    #print(caption_matrix[1])\n\n    for ind, row in enumerate(caption_masks): #Set the masks as an array of ones where actual words exist and zeros otherwise\n        row[:nonzeros[ind]] = 1                 \n        #print(row)\n    print(caption_masks[0])\n    print(caption_matrix[0])\n    return caption_matrix,caption_masks   \n\ndef generic_batch(generic_responses, batch_size, word_to_index, target_sequence_length):\n    size = len(generic_responses) \n    if size > batch_size:\n        generic_responses = generic_responses[:batch_size]\n\n    else:\n        for j in range(batch_size - size):\n            generic_responses.append('')\n\n    return make_batch_Y(generic_responses, word_to_index, target_sequence_length)\n\n```", "```py\ndef index2sentence(generated_word_index, prob_logit, ixtoword):\n    generated_word_index = list(generated_word_index)\n    for i in range(len(generated_word_index)):\n        if generated_word_index[i] == 3 or generated_word_index[i] == 0:\n            sort_prob_logit = sorted(prob_logit[i])\n            curindex = np.where(prob_logit[i] == sort_prob_logit[-2])[0][0]\n            count = 1\n            while curindex <= 3:\n                curindex = np.where(prob_logit[i] == sort_prob_logit[(-2)-count])[0][0]\n                count += 1\n\n            generated_word_index[i] = curindex\n\n    generated_words = []\n    for ind in generated_word_index:\n        generated_words.append(ixtoword[ind])    \n    generated_sentence = ' '.join(generated_words)\n    generated_sentence = generated_sentence.replace('<bos> ', '')  #Replace the beginning of sentence tag\n    generated_sentence = generated_sentence.replace('<eos>', '')   #Replace the end of sentence tag\n    generated_sentence = generated_sentence.replace('--', '')      #Replace the other symbols predicted\n    generated_sentence = generated_sentence.split('  ')\n    for i in range(len(generated_sentence)):       #Begin sentences with Upper case \n        generated_sentence[i] = generated_sentence[i].strip()\n        if len(generated_sentence[i]) > 1:\n            generated_sentence[i] = generated_sentence[i][0].upper() + generated_sentence[i][1:] + '.'\n        else:\n            generated_sentence[i] = generated_sentence[i].upper()\n    generated_sentence = ' '.join(generated_sentence)\n    generated_sentence = generated_sentence.replace(' i ', ' I ')\n    generated_sentence = generated_sentence.replace(\"i'm\", \"I'm\")\n    generated_sentence = generated_sentence.replace(\"i'd\", \"I'd\")\n\n    return generated_sentence\n```", "```py\nimport tensorflow as tf\nimport numpy as np\nimport helper as h\n```", "```py\nclass Chatbot():\n    def __init__(self, embed_dim, vocab_size, lstm_size, batch_size, input_sequence_length, target_sequence_length, learning_rate =0.0001, keep_prob = 0.5, num_layers = 1, policy_gradients = False, Training = True):\n        self.embed_dim = embed_dim\n        self.lstm_size = lstm_size\n        self.batch_size = batch_size\n        self.vocab_size = vocab_size\n        self.input_sequence_length = tf.fill([self.batch_size],input_sequence_length+1)\n        self.target_sequence_length = tf.fill([self.batch_size],target_sequence_length+1)\n        self.output_sequence_length = target_sequence_length +1\n        self.learning_rate = learning_rate\n        self.keep_prob = keep_prob\n        self.num_layers = num_layers\n        self.policy_gradients = policy_gradients\n        self.Training = Training\n\n```", "```py\n    def build_model(self):\n        if self.policy_gradients:\n            word_vectors, caption, caption_mask, rewards = h.model_inputs(self.embed_dim, True)\n            place_holders = {'word_vectors': word_vectors,\n                'caption': caption,\n                'caption_mask': caption_mask, \"rewards\": rewards\n                             }\n        else:\n            word_vectors, caption, caption_mask = h.model_inputs(self.embed_dim)\n\n            place_holders = {'word_vectors': word_vectors,\n                'caption': caption,\n                'caption_mask': caption_mask}\n        enc_output, enc_state = h.encoding_layer(word_vectors, self.lstm_size, self.num_layers,\n                                         self.keep_prob, self.vocab_size)\n        #dec_inp = h.bos_inclusion(caption, self.batch_size)\n        dec_inp = caption\n\n```", "```py\n        if not self.Training:\n            print(\"Test mode\")\n            inference_out = h.decoding_layer(dec_inp, enc_state,self.target_sequence_length, \n                                                    self.output_sequence_length,\n                                                    self.lstm_size, self.num_layers,\n                                                    self.vocab_size, self.batch_size,\n                                                  self.keep_prob, self.embed_dim, False)\n            logits = tf.identity(inference_out.rnn_output, name = \"train_logits\")\n            predictions = tf.identity(inference_out.sample_id, name = \"predictions\")\n            return place_holders, predictions, logits\n\n```", "```py\n        train_out, inference_out = h.decoding_layer(dec_inp, enc_state,self.target_sequence_length, \n                                                    self.output_sequence_length,\n                                                    self.lstm_size, self.num_layers,\n                                                    self.vocab_size, self.batch_size,\n                                                  self.keep_prob, self.embed_dim)\n\n        training_logits = tf.identity(train_out.rnn_output, name = \"train_logits\")\n        prediction_logits = tf.identity(inference_out.sample_id, name = \"predictions\")\n        cross_entropy = tf.contrib.seq2seq.sequence_loss(training_logits, caption, caption_mask)\n        losses = {\"entropy\": cross_entropy}\n\n```", "```py\n        if self.policy_gradients:\n            pg_loss = tf.contrib.seq2seq.sequence_loss(training_logits, caption, caption_mask*rewards)\n            with tf.variable_scope(tf.get_variable_scope(), reuse=False):\n                optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(pg_loss)\n            losses.update({\"pg\":pg_loss}) \n        else:\n            with tf.variable_scope(tf.get_variable_scope(), reuse=False):\n                optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(cross_entropy)\n\n        return optimizer, place_holders,prediction_logits,training_logits, losses\n```", "```py\nfrom data_reader import Data_Reader\nimport data_parser\nfrom gensim.models import KeyedVectors\nimport helper as h\nfrom seq_model import Chatbot\nimport tensorflow as tf\nimport numpy as np\n```", "```py\ngeneric_responses = [\n    \"I don't know what you're talking about.\", \n    \"I don't know.\", \n    \"You don't know.\",\n    \"You know what I mean.\", \n    \"I know what you mean.\", \n    \"You know what I'm saying.\",\n    \"You don't know anything.\"\n]\n```", "```py\ncheckpoint = True\nforward_model_path = 'model/forward'\nreversed_model_path = 'model/reversed'\nrl_model_path = \"model/rl\"\nmodel_name = 'seq2seq'\nword_count_threshold = 20\nreversed_word_count_threshold = 6\ndim_wordvec = 300\ndim_hidden = 1000\ninput_sequence_length = 22\noutput_sequence_length = 22\nlearning_rate = 0.0001\nepochs = 1\nbatch_size = 200\nforward_ = \"forward\"\nreverse_ = \"reverse\"\nforward_epochs = 50\nreverse_epochs = 50\ndisplay_interval = 100\n\n```", "```py\ndef train(type_, epochs=epochs, checkpoint=False):\n    tf.reset_default_graph()\n    if type_ == \"forward\":\n        path = \"model/forward/seq2seq\"\n        dr = Data_Reader(reverse=False)\n    else:\n        dr = Data_Reader(reverse=True)\n        path = \"model/reverse/seq2seq\"\n\n```", "```py\n    word_to_index, index_to_word, _ = data_parser.preProBuildWordVocab(word_count_threshold=word_count_threshold)\n```", "```py\npreprocessing word counts and creating vocab based on word count threshold 20\nfiltered words from 76029 to 6847\n```", "```py\n{'': 4,\n'deposition': 1769,\n'next': 3397,\n'dates': 1768,\n'chance': 2597,\n'slipped': 4340,...\n```", "```py\n5: 'tastes',\n6: 'shower',\n7: 'agent',\n8: 'lack',\n```", "```py\n    word_vector = KeyedVectors.load_word2vec_format('model/word_vector.bin', binary=True)\n```", "```py\n    model = Chatbot(dim_wordvec, len(word_to_index), dim_hidden, batch_size,\n                    input_sequence_length, output_sequence_length, learning_rate)\n    optimizer, place_holders, predictions, logits, losses = model.build_model()\n    saver = tf.train.Saver()\n    sess = tf.InteractiveSession()\n    if checkpoint:\n        saver.restore(sess, path)\n        print(\"checkpoint restored at path: {}\".format(path))\n    else:\n        tf.global_variables_initializer().run()\n```", "```py\n    for epoch in range(epochs):\n        n_batch = dr.get_batch_num(batch_size=batch_size)\n        for batch in range(n_batch):\n\n            batch_input, batch_target = dr.generate_training_batch(batch_size)\n```", "```py\n            inputs_ = h.make_batch_input(batch_input, input_sequence_length, dim_wordvec, word_vector)\n\n            targets, masks = h.make_batch_target(batch_target, word_to_index, output_sequence_length)\n            feed_dict = {\n                place_holders['word_vectors']: inputs_,\n                place_holders['caption']: targets,\n                place_holders['caption_mask']: masks\n            }\n```", "```py\n            _, loss_val, preds = sess.run([optimizer, losses[\"entropy\"], predictions],\n                                          feed_dict=feed_dict)\n\n            if batch % display_interval == 0:\n                print(preds.shape)\n                print(\"Epoch: {}, batch: {}, loss: {}\".format(epoch, batch, loss_val))\n                print(\"===========================================================\")\n\n        saver.save(sess, path)\n\n        print(\"Model saved at {}\".format(path))\n    print(\"Training done\")\n\n    sess.close()\n```", "```py\n(200, 23)\nEpoch: 0, batch: 0, loss: 8.831538200378418\n===========================================================\n```", "```py\ndef pg_train(epochs=epochs, checkpoint=False):\n    tf.reset_default_graph()\n    path = \"model/reinforcement/seq2seq\"\n    word_to_index, index_to_word, _ = data_parser.preProBuildWordVocab(word_count_threshold=word_count_threshold)\n    word_vector = KeyedVectors.load_word2vec_format('model/word_vector.bin', binary=True)\n    generic_caption, generic_mask = h.generic_batch(generic_responses, batch_size, word_to_index,\n                                                    output_sequence_length)\n\n    dr = Data_Reader()\n    forward_graph = tf.Graph()\n    reverse_graph = tf.Graph()\n    default_graph = tf.get_default_graph()\n```", "```py\n    with forward_graph.as_default():\n        pg_model = Chatbot(dim_wordvec, len(word_to_index), dim_hidden, batch_size,\n                           input_sequence_length, output_sequence_length, learning_rate, policy_gradients=True)\n        optimizer, place_holders, predictions, logits, losses = pg_model.build_model()\n\n        sess = tf.InteractiveSession()\n        saver = tf.train.Saver()\n        if checkpoint:\n            saver.restore(sess, path)\n            print(\"checkpoint restored at path: {}\".format(path))\n        else:\n            tf.global_variables_initializer().run()\n            saver.restore(sess, 'model/forward/seq2seq')\n    # tf.global_variables_initializer().run()\n    with reverse_graph.as_default():\n        model = Chatbot(dim_wordvec, len(word_to_index), dim_hidden, batch_size,\n                        input_sequence_length, output_sequence_length, learning_rate)\n        _, rev_place_holders, _, _, reverse_loss = model.build_model()\n        sess2 = tf.InteractiveSession()\n        saver2 = tf.train.Saver()\n\n        saver2.restore(sess2, \"model/reverse/seq2seq\")\n        print(\"reverse model restored\")\n\n    dr = Data_Reader(load_list=True)\n```", "```py\n    for epoch in range(epochs):\n        n_batch = dr.get_batch_num(batch_size=batch_size)\n        for batch in range(n_batch):\n\n            batch_input, batch_caption, prev_utterance = dr.generate_training_batch_with_former(batch_size)\n            targets, masks = h.make_batch_target(batch_caption, word_to_index, output_sequence_length)\n            inputs_ = h.make_batch_input(batch_input, input_sequence_length, dim_wordvec, word_vector)\n\n            word_indices, probabilities = sess.run([predictions, logits],\n                                                   feed_dict={place_holders['word_vectors']: inputs_\n\n                                                       , place_holders[\"caption\"]: targets})\n\n            sentence = [h.index2sentence(generated_word, probability, index_to_word) for\n                        generated_word, probability in zip(word_indices, probabilities)]\n\n            word_list = [word.split() for word in sentence]\n\n            generic_test_input = h.make_batch_input(word_list, input_sequence_length, dim_wordvec, word_vector)\n\n            forward_coherence_target, forward_coherence_masks = h.make_batch_target(sentence,\n                                                                                    word_to_index,\n                                                                                    output_sequence_length)\n\n            generic_loss = 0.0\n```", "```py\n            for response in generic_test_input:\n                sentence_input = np.array([response] * batch_size)\n                feed_dict = {place_holders['word_vectors']: sentence_input,\n                             place_holders['caption']: generic_caption,\n                             place_holders['caption_mask']: generic_mask,\n                             }\n                generic_loss_i = sess.run(losses[\"entropy\"], feed_dict=feed_dict)\n                generic_loss -= generic_loss_i / batch_size\n\n            # print(\"generic loss work: {}\".format(generic_loss))\n\n            feed_dict = {place_holders['word_vectors']: inputs_,\n                         place_holders['caption']: forward_coherence_target,\n                         place_holders['caption_mask']: forward_coherence_masks,\n                         }\n\n            forward_entropy = sess.run(losses[\"entropy\"], feed_dict=feed_dict)\n\n            previous_utterance, previous_mask = h.make_batch_target(prev_utterance,\n                                                                    word_to_index, output_sequence_length)\n\n            feed_dict = {rev_place_holders['word_vectors']: generic_test_input,\n                         rev_place_holders['caption']: previous_utterance,\n                         rev_place_holders['caption_mask']: previous_mask,\n                         }\n            reverse_entropy = sess2.run(reverse_loss[\"entropy\"], feed_dict=feed_dict)\n\n            rewards = 1 / (1 + np.exp(-reverse_entropy - forward_entropy - generic_loss))\n\n            feed_dict = {place_holders['word_vectors']: inputs_,\n                         place_holders['caption']: targets,\n                         place_holders['caption_mask']: masks,\n                         place_holders['rewards']: rewards\n                         }\n\n            _, loss_pg, loss_ent = sess.run([optimizer, losses[\"pg\"], losses[\"entropy\"]], feed_dict=feed_dict)\n\n            if batch % display_interval == 0:\n                print(\"Epoch: {}, batch: {}, Entropy loss: {}, Policy gradient loss: {}\".format(epoch, batch, loss_ent,\n                                                                                                loss_pg))\n\n                print(\"rewards: {}\".format(rewards))\n                print(\"===========================================================\")\n        saver.save(sess, path)\n        print(\"Model saved at {}\".format(path))\n    print(\"Training done\")\n\n```", "```py\ntrain(forward_, forward_epochs, False)\ntrain(reverse_, reverse_epochs, False)\npg_train(100, False)\n```", "```py\nimport data_parser\nfrom gensim.models import KeyedVectors\nfrom seq_model import Chatbot\nimport tensorflow as tf\nimport numpy as np\nimport helper as h\n```", "```py\nreinforcement_model_path = \"model/reinforcement/seq2seq\"\nforward_model_path = \"model/forward/seq2seq\"\nreverse_model_path = \"model/reverse/seq2seq\"\n```", "```py\npath_to_questions = 'results/sample_input.txt'\nresponses_path = 'results/sample_output_RL.txt'\n```", "```py\nword_count_threshold = 20\ndim_wordvec = 300\ndim_hidden = 1000\n\ninput_sequence_length = 25\ntarget_sequence_length = 22\n\nbatch_size = 2\n\n```", "```py\ndef test(model_path=forward_model_path):\n    testing_data = open(path_to_questions, 'r').read().split('\\n')\n    word_vector = KeyedVectors.load_word2vec_format('model/word_vector.bin', binary=True)\n\n    _, index_to_word, _ = data_parser.preProBuildWordVocab(word_count_threshold=word_count_threshold)\n\n    model = Chatbot(dim_wordvec, len(index_to_word), dim_hidden, batch_size,\n                            input_sequence_length, target_sequence_length, Training=False)\n\n    place_holders, predictions, logits = model.build_model()\n\n    sess = tf.InteractiveSession()\n\n    saver = tf.train.Saver()\n\n    saver.restore(sess, model_path)\n```", "```py\n    with open(responses_path, 'w') as out:\n\n        for idx, question in enumerate(testing_data):\n            print('question =>', question)\n\n            question = [h.refine(w) for w in question.lower().split()]\n            question = [word_vector[w] if w in word_vector else np.zeros(dim_wordvec) for w in question]\n            question.insert(0, np.random.normal(size=(dim_wordvec,)))  # insert random normal at the first step\n\n            if len(question) > input_sequence_length:\n                question = question[:input_sequence_length]\n            else:\n                for _ in range(input_sequence_length - len(question)):\n                    question.append(np.zeros(dim_wordvec))\n\n            question = np.array([question])\n\n            feed_dict = {place_holders[\"word_vectors\"]: np.concatenate([question] * 2, 0),\n                         }\n\n            word_indices, prob_logit = sess.run([predictions, logits], feed_dict=feed_dict)\n\n            # print(word_indices[0].shape)\n            generated_sentence = h.index2sentence(word_indices[0], prob_logit[0], index_to_word)\n\n            print('generated_sentence =>', generated_sentence)\n            out.write(generated_sentence + '\\n')\n\ntest(reinforcement_model_path)\n```"]