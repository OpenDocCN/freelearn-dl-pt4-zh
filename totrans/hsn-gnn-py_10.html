<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer461">
<h1 class="chapter-number" id="_idParaDest-112"><a id="_idTextAnchor116"/>10</h1>
<h1 id="_idParaDest-113"><a id="_idTextAnchor117"/>Predicting Links with Graph Neural Networks</h1>
<p><strong class="bold">Link prediction</strong> is <a id="_idIndexMarker556"/>one of the most popular tasks performed with graphs. It is defined as the problem of predicting the existence of a link between two nodes. This ability is at the core of social networks and recommender systems. A good example is how social media networks display friends and followers you have in common with others. Intuitively, if this number is high, you are more likely to connect with these people. This likelihood is precisely what link prediction tries <span class="No-Break">to estimate.</span></p>
<p>In this chapter, we will first see how to perform link prediction without any machine learning. These traditional techniques are essential to understanding what GNNs learn. We will then refer to previous chapters about <strong class="source-inline">DeepWalk</strong> and <strong class="source-inline">Node2Vec</strong> to link prediction <a id="_idIndexMarker557"/>through <strong class="bold">matrix factorization</strong>. Unfortunately, these techniques have significant limitations, which is why we will transition to <span class="No-Break">GNN-based methods.</span></p>
<p>We will explore three methods from two different families. The first family is based on node embeddings and performs a GNN-based matrix factorization. The second method focuses on subgraph representation. The neighborhood around each link (fake or real) is considered an input to predict the link probability. Finally, we will implement a model of each family in <span class="No-Break">PyTorch Geometric.</span></p>
<p>By the end of this chapter, you will be able to implement various link prediction techniques. Given a link prediction problem, you will know which technique is the best suited to address it – heuristics, matrix factorization, GNN-based embeddings, or <span class="No-Break">subgraph-based techniques.</span></p>
<p>In this chapter, we will cover the following <span class="No-Break">main topics:</span></p>
<ul>
<li>Predicting links with <span class="No-Break">traditional methods</span></li>
<li>Predicting links with <span class="No-Break">node embeddings</span></li>
<li>Predicting links <span class="No-Break">with SEAL</span></li>
</ul>
<h1 id="_idParaDest-114"><a id="_idTextAnchor118"/>Technical requirements</h1>
<p>All the code examples from this chapter can be found on GitHub <span class="No-Break">at </span><span class="No-Break">https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter10</span><span class="No-Break">.</span></p>
<p>Installation steps required to run the code on your local machine can be found in the <em class="italic">Preface</em> section of <span class="No-Break">this book.</span></p>
<h1 id="_idParaDest-115"><a id="_idTextAnchor119"/>Predicting links with traditional methods</h1>
<p>The link prediction problem has been around for a long time, which is why numerous techniques have been proposed to solve it. First, this section will describe popular heuristics based on local and global neighborhoods. Then, we will introduce matrix factorization and its connection to DeepWalk <span class="No-Break">and Node2Vec.</span></p>
<h2 id="_idParaDest-116"><a id="_idTextAnchor120"/>Heuristic techniques</h2>
<p>Heuristic techniques <a id="_idIndexMarker558"/>are<a id="_idIndexMarker559"/> a simple and practical way to predict links between nodes. They are easy to implement and offer strong baselines for this task. We can classify them based on the number of hops they perform (see <span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.1</em>). Some of them only require 1-hop neighbors that are adjacent to the target nodes. More complex techniques also consider 2-hop neighbors or an entire graph. In this section, we will divide them into two categories – <em class="italic">local</em> (1-hop and 2-hop) and <span class="No-Break"><em class="italic">global</em></span><span class="No-Break"> heuristics.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer384">
<img alt="Figure 10.1 – Graph with 1-hop, 2-hop, and 3-hop neighbors" height="882" src="image/B19153_10_001.jpg" width="916"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.1 – Graph with 1-hop, 2-hop, and 3-hop neighbors</p>
<p>Local heuristics <a id="_idIndexMarker560"/>measure the<a id="_idIndexMarker561"/> similarity between two nodes by<a id="_idIndexMarker562"/> considering their local neighborhoods. We use <img alt="" height="41" src="image/Formula_B19153_10_001.png" width="97"/> to denote the neighbors of node <img alt="" height="24" src="image/Formula_B19153_10_002.png" width="26"/>. Here are three examples of popular <span class="No-Break">local heuristics:</span></p>
<ul>
<li><strong class="bold">Common neighbors</strong> simply <a id="_idIndexMarker563"/>counts the number of <a id="_idIndexMarker564"/>neighbors two nodes have in common (1-hop neighbors). The idea is similar to our previous example with social networks – the more neighbors you have in common, the more likely you are to <span class="No-Break">be connected:</span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer387">
<img alt="" height="54" src="image/Formula_B19153_10_003.jpg" width="471"/>
</div>
</div>
<ul>
<li><strong class="bold">Jaccard’s coefficient</strong> measures<a id="_idIndexMarker565"/> the proportion<a id="_idIndexMarker566"/> of neighbors shared by two nodes (1-hop neighbors). It relies on the same idea as common neighbors but normalizes the result by the total number of neighbors. This rewards nodes with few interconnected neighbors instead of nodes with <span class="No-Break">high degrees:</span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer388">
<img alt="" height="108" src="image/Formula_B19153_10_004.jpg" width="480"/>
</div>
</div>
<ul>
<li>The <strong class="bold">Adamic–Adar index</strong> sums<a id="_idIndexMarker567"/> the <a id="_idIndexMarker568"/>inverse logarithmic degree of neighbors shared by the two<a id="_idIndexMarker569"/> target nodes (2-hop neighbors). The idea is that common neighbors with large neighborhoods are less significant than those with small neighborhoods. This is why they should have less importance in the <span class="No-Break">final score:</span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer389">
<img alt="" height="138" src="image/Formula_B19153_10_005.jpg" width="639"/>
</div>
</div>
<p>All these techniques rely on neighbors’ node degrees, whether they are direct (common neighbors or Jaccard’s coefficient) or indirect (the Adamic–Adar index). This is beneficial for speed and explainability but also limits the complexity of the relationships they <span class="No-Break">can capture.</span></p>
<p>Global heuristics <a id="_idIndexMarker570"/>offer a solution to this problem by <a id="_idIndexMarker571"/>considering an entire network instead of a local neighborhood. Here are two <span class="No-Break">well-known examples:</span></p>
<ul>
<li>The <strong class="bold">Katz index</strong><strong class="bold"> </strong>computes <a id="_idIndexMarker572"/>the weighted sum of every <a id="_idIndexMarker573"/>possible path between two nodes. Weights correspond to a discount factor, <img alt="" height="48" src="image/Formula_B19153_10_006.png" width="147"/> (usually between 0.8 and 0.9), to penalize longer paths. With this definition, two nodes are more likely to be connected if there are many (preferably short) paths between them. Paths of any length can be calculated using adjacency matrix powers, <img alt="" height="35" src="image/Formula_B19153_10_007.png" width="52"/>, which is why the Katz index is defined <span class="No-Break">as follows:</span><div class="IMG---Figure" id="_idContainer392"><img alt="" height="152" src="image/Formula_B19153_10_008.jpg" width="382"/></div></li>
</ul>
<ul>
<li><strong class="bold">Random walk with restart</strong> [1] performs<a id="_idIndexMarker574"/> random walks, starting from a target node. After <a id="_idIndexMarker575"/>each walk, it increases the visit count of the current node. With an <img alt="" height="25" src="image/Formula_B19153_10_009.png" width="26"/> probability, it restarts the walk at the target node. Otherwise, it continues its random walk. After a predefined number of iterations, we stop the algorithm, and we can suggest links between the target node and nodes with the highest visit counts. This idea is also essential in <strong class="source-inline">DeepWalk</strong> and <span class="No-Break"><strong class="source-inline">Node2Vec</strong></span><span class="No-Break"> algorithms.</span></li>
</ul>
<p>Global heuristics<a id="_idIndexMarker576"/> are usually more accurate but require knowing the entirety of a graph. However, they are not the only way to predict links with <span class="No-Break">this knowledge.</span></p>
<h2 id="_idParaDest-117"><a id="_idTextAnchor121"/>Matrix factorization</h2>
<p>Matrix factorization<a id="_idIndexMarker577"/> for link prediction<a id="_idIndexMarker578"/> is inspired by the previous work on recommender systems [2]. With this technique, we indirectly predict links by predicting the entire adjacency matrix, <img alt="" height="46" src="image/Formula_B19153_10_010.png" width="33"/>. This is performed using node embeddings – similar nodes, <img alt="" height="28" src="image/Formula_B19153_10_011.png" width="31"/> and <img alt="" height="28" src="image/Formula_B19153_10_012.png" width="27"/>, should have similar embeddings, <img alt="" height="34" src="image/Formula_B19153_10_013.png" width="43"/> and <img alt="" height="34" src="image/Formula_B19153_10_014.png" width="39"/> respectively. Using the dot product, we can write it <span class="No-Break">as follows:</span></p>
<ul>
<li>If these nodes are similar, <img alt="" height="56" src="image/Formula_B19153_10_015.png" width="90"/> should <span class="No-Break">be maximal</span></li>
<li>If these nodes are different, <img alt="" height="51" src="image/Formula_B19153_10_016.png" width="88"/> should <span class="No-Break">be minimal</span></li>
</ul>
<p>So far, we have assumed that similar nodes should be connected. This is why we can use this dot product to approximate each element (link) of the adjacency <span class="No-Break">matrix, <img alt="" height="32" src="image/Formula_B19153_10_017.png" width="29"/>:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer402">
<img alt="" height="63" src="image/Formula_B19153_10_018.jpg" width="224"/>
</div>
</div>
<p>In terms of matrix multiplication, we have <span class="No-Break">the following:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer403">
<img alt="" height="55" src="image/Formula_B19153_10_019.jpg" width="193"/>
</div>
</div>
<p>Here, <img alt="" height="34" src="image/Formula_B19153_10_020.png" width="31"/> is the node embedding matrix. The following figure shows a visual explanation of how matrix <span class="No-Break">factorization works:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer405">
<img alt="Figure 10.2 – Matrix multiplication with node embeddings" height="489" src="image/B19153_10_002.jpg" width="1377"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.2 – Matrix multiplication with node embeddings</p>
<p>This<a id="_idIndexMarker579"/> technique is called <a id="_idIndexMarker580"/>matrix factorization because the adjacency matrix, <img alt="" height="34" src="image/Formula_B19153_10_021.png" width="29"/>, is decomposed into a product of two matrices. The goal is to learn relevant node embeddings that minimize the L2 norm between true and predicted elements, <img alt="" height="48" src="image/Formula_B19153_10_022.png" width="81"/>, for the <span class="No-Break">graph, <img alt="" height="45" src="image/Formula_B19153_10_023.png" width="179"/></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer409">
<img alt="" height="146" src="image/Formula_B19153_10_024.jpg" width="682"/>
</div>
</div>
<p>There are more advanced variants of matrix factorization that include the Laplacian matrix and powers of <img alt="" height="33" src="image/Formula_B19153_10_025.png" width="29"/>. Another solution consists of using models such as <strong class="source-inline">DeepWalk</strong> and <strong class="source-inline">Node2Vec</strong>. They produce node embeddings that can be paired to create link representations. According to Qiu, et al. [3], these algorithms implicitly approximate and factorize complex matrices. For example, here is the matrix computed <span class="No-Break">by </span><span class="No-Break"><strong class="source-inline">DeepWalk</strong></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer411">
<img alt="" height="183" src="image/Formula_B19153_10_026.jpg" width="919"/>
</div>
</div>
<p>Here, <img alt="" height="40" src="image/Formula_B19153_10_027.png" width="25"/> is<a id="_idIndexMarker581"/> the parameter for <a id="_idIndexMarker582"/>negative sampling. The same can be said for similar algorithms, such as LINE and PTE. Although they can capture more complex relationships, they suffer from the same limitations that we saw in <em class="italic">Chapters 3 </em><span class="No-Break">and </span><span class="No-Break"><em class="italic">4</em></span><span class="No-Break">:</span></p>
<ul>
<li><strong class="bold">They cannot use node features</strong>: They only use topological information to <span class="No-Break">create embeddings</span></li>
<li><strong class="bold">They have no inductive capabilities</strong>: They cannot generalize to nodes that were not in the <span class="No-Break">training set</span></li>
<li><strong class="bold">They cannot capture structural similarity</strong>: Structurally similar nodes in the graph can obtain vastly <span class="No-Break">different embeddings</span></li>
</ul>
<p>These limitations motivate the need for GNN-based techniques, as we will see in the <span class="No-Break">next sections.</span></p>
<h1 id="_idParaDest-118"><a id="_idTextAnchor122"/>Predicting links with node embeddings</h1>
<p>In the <a id="_idIndexMarker583"/>previous chapters, we saw how to use GNNs to produce node embeddings. A popular link prediction technique consists of using these embeddings to perform matrix factorization. This section will discuss two GNN architectures for link prediction – the <strong class="bold">Graph Autoencoder</strong> (<strong class="bold">GAE</strong>) and the <strong class="bold">Variational Graph </strong><span class="No-Break"><strong class="bold">Autoencoder</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">VGAE</strong></span><span class="No-Break">).</span></p>
<h2 id="_idParaDest-119"><a id="_idTextAnchor123"/>Introducing Graph Autoencoders</h2>
<p>Both architectures <a id="_idIndexMarker584"/>were <a id="_idIndexMarker585"/>introduced by Kipf and Welling in 2016 [5] in a three-page paper. They represent the GNN counterparts of two popular neural network architectures – the autoencoder and the variational autoencoder. Prior knowledge about these architectures is helpful but not necessary. For ease of understanding, we will first focus on <span class="No-Break">the GAE.</span></p>
<p>The GAE is composed of <span class="No-Break">two modules:</span></p>
<ul>
<li>The <strong class="bold">encoder</strong> is a <a id="_idIndexMarker586"/>classic two-layer GCN that computes node embeddings <span class="No-Break">as follows:</span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer413">
<img alt="" height="43" src="image/Formula_B19153_10_028.jpg" width="249"/>
</div>
</div>
<ul>
<li>The <strong class="bold">decoder</strong> approximates<a id="_idIndexMarker587"/> the adjacency matrix, <img alt="" height="40" src="image/Formula_B19153_10_029.png" width="28"/>, using matrix factorization and a sigmoid function, <img alt="" height="25" src="image/Formula_B19153_10_030.png" width="27"/>, to <span class="No-Break">output probabilities:</span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer416">
<img alt="" height="53" src="image/Formula_B19153_10_031.jpg" width="221"/>
</div>
</div>
<p>Note that we are not trying to classify nodes or graphs. The goal is to predict a probability (between 0 and 1) for each element of the adjacency matrix, <img alt="" height="43" src="image/Formula_B19153_10_032.png" width="29"/>. This is why the GAE is <a id="_idIndexMarker588"/>trained using the binary cross-entropy loss (negative log-likelihood) between the elements of both <span class="No-Break">adjacency matrices:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer418">
<img alt="" height="121" src="image/Formula_B19153_10_033.jpg" width="957"/>
</div>
</div>
<p>However, adjacency matrices are often very sparse, which biases the GAE toward predicting zero values. There are two simple techniques to fix this bias. First, we can add a weight to favor <img alt="" height="43" src="image/Formula_B19153_10_034.png" width="122"/> in the previous loss function. Secondly, we can sample fewer zero values during training, making labels more balanced. The latter technique is the one implemented by Kipf <span class="No-Break">and Welling.</span></p>
<p>This architecture is flexible – the encoder can be replaced with another type of GNN (GraphSAGE, for example), and an MLP can take the role of a decoder, for instance. Another possible improvement involves transforming the GAE into a probabilistic variant – the <span class="No-Break">Variational GAE.</span></p>
<h2 id="_idParaDest-120"><a id="_idTextAnchor124"/>Introducing VGAEs</h2>
<p>The<a id="_idIndexMarker589"/> difference<a id="_idIndexMarker590"/> between GAEs and VGAEs is the same as between autoencoders and variational autoencoders. Instead of directly learning node embeddings, VGAEs learn normal distributions that are then sampled to produce embeddings. They are also divided into <span class="No-Break">two modules:</span></p>
<ul>
<li>The <strong class="bold">encoder</strong> is composed<a id="_idIndexMarker591"/> of two GCNs that share their first layer. The objective is to learn the parameters of each latent normal distribution – a mean, <img alt="" height="35" src="image/Formula_B19153_10_035.png" width="36"/> (learned by <img alt="" height="41" src="image/Formula_B19153_10_036.png" width="80"/>), and a variance, <img alt="" height="50" src="image/Formula_B19153_10_037.png" width="43"/> (in practice, <img alt="" height="40" src="image/Formula_B19153_10_038.png" width="82"/> learned <span class="No-Break">by <img alt="" height="36" src="image/Formula_B19153_10_039.png" width="90"/>).</span></li>
<li>The <strong class="bold">decoder</strong> samples<a id="_idIndexMarker592"/> embeddings, <img alt="" height="34" src="image/Formula_B19153_10_040.png" width="33"/>, from the learned distributions, <img alt="" height="50" src="image/Formula_B19153_10_041.png" width="152"/>, using the<a id="_idIndexMarker593"/> reparametrization trick [4]. Then, it uses the same inner product between latent variables to approximate the adjacency <span class="No-Break">matrix, <img alt="" height="43" src="image/Formula_B19153_10_042.png" width="179"/>.</span></li>
</ul>
<p>With VGAEs, it is important to ensure that the encoder’s output follows a normal distribution. This is why <a id="_idIndexMarker594"/>we add a new term to the loss function – the <strong class="bold">Kullback-Leibler</strong> (<strong class="bold">KL</strong>) divergence, which measures the divergence between two distributions. We obtain the following loss, also<a id="_idIndexMarker595"/> called the <strong class="bold">evidence lower </strong><span class="No-Break"><strong class="bold">bound</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">ELBO</strong></span><span class="No-Break">):</span></p>
<div>
<div class="IMG---Figure" id="_idContainer428">
<img alt="" height="69" src="image/Formula_B19153_10_043.jpg" width="817"/>
</div>
</div>
<p>Here, <img alt="" height="47" src="image/Formula_B19153_10_044.png" width="166"/> represents the encoder and <img alt="" height="45" src="image/Formula_B19153_10_045.png" width="87"/> is the prior distribution <span class="No-Break">of <img alt="" height="33" src="image/Formula_B19153_10_046.png" width="29"/>.</span></p>
<p>The model’s <a id="_idIndexMarker596"/>performance is generally evaluated using two metrics – the area under the ROC (<strong class="bold">AUROC</strong>) curve and<a id="_idIndexMarker597"/> the <strong class="bold">average </strong><span class="No-Break"><strong class="bold">precision</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">AP</strong></span><span class="No-Break">).</span></p>
<p>Let’s see how to implement a VGAE using <span class="No-Break">PyTorch Geometric.</span></p>
<h2 id="_idParaDest-121"><a id="_idTextAnchor125"/>Implementing a VGAE</h2>
<p>There are two <a id="_idIndexMarker598"/>main<a id="_idIndexMarker599"/> differences with previous <span class="No-Break">GNN implementations:</span></p>
<ul>
<li>We will preprocess the dataset to remove links to <span class="No-Break">predict randomly.</span></li>
<li>We will create an encoder model that we will feed to a <strong class="source-inline">VGAE</strong> class, instead of directly implementing a VGAE <span class="No-Break">from scratch.</span></li>
</ul>
<p>The following code is inspired by PyTorch Geometric’s <span class="No-Break">VGAE example:</span></p>
<ol>
<li>First, we import the <span class="No-Break">required libraries:</span><pre class="console">
import numpy as np
np.random.seed(0)
import torch
torch.manual_seed(0)
import matplotlib.pyplot as plt
import torch_geometric.transforms as T
from torch_geometric.datasets import Planetoid</pre></li>
<li>We try to use the GPU if <span class="No-Break">it’s available:</span><pre class="console">
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')</pre></li>
<li>We create a <strong class="source-inline">transform</strong> object that normalizes input features, directly performs<a id="_idIndexMarker600"/> tensor device conversion, and randomly splits links. In this example, we have an 85/5/10 split. The <strong class="source-inline">add_negative_train_samples</strong> parameter is set to <strong class="source-inline">False</strong> because the model already performs negative sampling, so it is not <a id="_idIndexMarker601"/>needed in <span class="No-Break">the dataset:</span><pre class="console">
transform = T.Compose([
    T.NormalizeFeatures(),
    T.ToDevice(device),
    T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True, split_labels=True, add_negative_train_samples=False),
])</pre></li>
<li>We load the <strong class="source-inline">Cora</strong> dataset with the previous <span class="No-Break"><strong class="source-inline">transform</strong></span><span class="No-Break"> object:</span><pre class="console">
dataset = Planetoid('.', name='Cora', transform=transform)</pre></li>
<li>The <strong class="source-inline">RandomLinkSplit</strong> produces a train/val/test split by design. We store these splits <span class="No-Break">as follows:</span><pre class="console">
train_data, val_data, test_data = dataset[0]</pre></li>
<li>Now, let’s implement the encoder. First, we need to import <strong class="source-inline">GCNConv</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">VGAE</strong></span><span class="No-Break">:</span><pre class="console">
from torch_geometric.nn import GCNConv, VGAE</pre></li>
<li>We declare a new class. In this class, we want three GCN layers – a shared layer, a second layer to approximate mean values, <img alt="" height="36" src="image/Formula_B19153_10_047.png" width="40"/>, and a third layer to approximate variance values (in practice, the log standard <span class="No-Break">deviation, <img alt="" height="42" src="image/Formula_B19153_10_048.png" width="91"/>):</span><pre class="console">
class Encoder(torch.nn.Module):
    def __init__(self, dim_in, dim_out):
        super().__init__()
        self.conv1 = GCNConv(dim_in, 2 * dim_out)
        self.conv_mu = GCNConv(2 * dim_out, dim_out)
        self.conv_logstd = GCNConv(2 * dim_out, dim_out)
    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index).relu()
        return self.conv_mu(x, edge_index), self.conv_logstd(x, edge_index)</pre></li>
<li>We <a id="_idIndexMarker602"/>can initialize our<a id="_idIndexMarker603"/> VGAE and give the encoder as input. By default, it will use the inner product as <span class="No-Break">a decoder:</span><pre class="console">
model = VGAE(Encoder(dataset.num_features, 16)).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)</pre></li>
<li>The <strong class="source-inline">train()</strong> function includes two important steps. First, the embedding matrix, <img alt="" height="32" src="image/Formula_B19153_10_049.png" width="29"/>, is computed using <strong class="source-inline">model.encode()</strong>; the name might be counter-intuitive, but <a id="_idIndexMarker604"/>this function does sample embeddings from the learned distributions. Then, the ELBO loss is computed with <strong class="source-inline">model.recon_loss()</strong> (binary cross-entropy loss) and <strong class="source-inline">model.kl_loss()</strong> (KL divergence). The decoder is implicitly called to calculate the <span class="No-Break">cross-entropy loss:</span><pre class="console">
def train():
    model.train()
    optimizer.zero_grad()
    z = model.encode(train_data.x, train_data.edge_index)
    loss = model.recon_loss(z, train_data.pos_edge_label_index) + (1 / train_data.num_nodes) * model.kl_loss()
    loss.backward()
    optimizer.step()
    return float(loss)</pre></li>
<li>The <strong class="source-inline">test()</strong> function <a id="_idIndexMarker605"/>simply calls the VGAE's <span class="No-Break">dedicated method:</span><pre class="console">
@torch.no_grad()
def test(data):
    model.eval()
    z = model.encode(data.x, data.edge_index)
    return model.test(z, data.pos_edge_label_index, data.neg_edge_label_index)</pre></li>
<li>We train this model for 301 epochs and print the two built-in metrics – the AUC and <span class="No-Break">the AP:</span><pre class="console">
for epoch in range(301):
    loss = train()
    val_auc, val_ap = test(val_data)
    if epoch % 50 == 0:
        print(f'Epoch {epoch:&gt;2} | Loss: {loss:.4f} | Val AUC: {val_auc:.4f} | Val AP: {val_ap:.4f}')</pre></li>
<li>We <a id="_idIndexMarker606"/>obtain the <a id="_idIndexMarker607"/><span class="No-Break">following output:</span><pre class="console">
<strong class="bold">Epoch 0 | Loss: 3.4210 | Val AUC: 0.6772 | Val AP: 0.7110</strong>
<strong class="bold">Epoch 50 | Loss: 1.3324 | Val AUC: 0.6593 | Val AP: 0.6922</strong>
<strong class="bold">Epoch 100 | Loss: 1.1675 | Val AUC: 0.7366 | Val AP: 0.7298</strong>
<strong class="bold">Epoch 150 | Loss: 1.1166 | Val AUC: 0.7480 | Val AP: 0.7514</strong>
<strong class="bold">Epoch 200 | Loss: 1.0074 | Val AUC: 0.8390 | Val AP: 0.8395</strong>
<strong class="bold">Epoch 250 | Loss: 0.9541 | Val AUC: 0.8794 | Val AP: 0.8797</strong>
<strong class="bold">Epoch 300 | Loss: 0.9509 | Val AUC: 0.8833 | Val AP: 0.8845</strong></pre></li>
<li>We evaluate our model on the <span class="No-Break">test set:</span><pre class="console">
test_auc, test_ap = test(test_data)
print(f'Test AUC: {test_auc:.4f} | Test AP {test_ap:.4f}')
<strong class="bold">Test AUC: 0.8833 | Test AP 0.8845</strong></pre></li>
<li>Finally, we can manually calculate the approximated adjacency <span class="No-Break">matrix, <img alt="" height="41" src="image/Formula_B19153_10_050.png" width="29"/>:</span><pre class="console">
z = model.encode(test_data.x, test_data.edge_index)
Ahat = torch.sigmoid(z @ z.T)
<strong class="bold">tensor([[0.8846, 0.5068, ..., 0.5160, 0.8309, 0.8378],</strong>
<strong class="bold">        [0.5068, 0.8741, ..., 0.3900, 0.5367, 0.5495],</strong>
<strong class="bold">        [0.7074, 0.7878, ..., 0.4318, 0.7806, 0.7602],</strong>
<strong class="bold">        ...,</strong>
<strong class="bold">        [0.5160, 0.3900, ..., 0.5855, 0.5350, 0.5176],</strong>
<strong class="bold">        [0.8309, 0.5367, ..., 0.5350, 0.8443, 0.8275],</strong>
<strong class="bold">        [0.8378, 0.5495, ..., 0.5176, 0.8275, 0.8200]</strong>
<strong class="bold">  ], device='cuda:0', grad_fn=&lt;SigmoidBackward0&gt;)</strong></pre></li>
</ol>
<p>Training <a id="_idIndexMarker608"/>a VGAE is fast and <a id="_idIndexMarker609"/>outputs results that are easily understandable. However, we saw that the GCN is not the most expressive operator. In order to improve the model’s expressiveness, we need to incorporate <span class="No-Break">better techniques.</span></p>
<h1 id="_idParaDest-122"><a id="_idTextAnchor126"/>Predicting links with SEAL</h1>
<p>The <a id="_idIndexMarker610"/>previous section introduced node-based methods, which learn relevant node embeddings to compute link likelihoods. Another approach consists of looking at the local neighborhood around the target nodes. These techniques are called subgraph-based algorithms and were popularized by <strong class="bold">SEAL</strong> (which could be said to stand for <strong class="bold">Subgraphs, Embeddings, and Attributes for Link prediction</strong> – though not always!). In this section, we will describe the SEAL framework and implement it using <span class="No-Break">PyTorch Geometric.</span></p>
<h2 id="_idParaDest-123"><a id="_idTextAnchor127"/>Introducing the SEAL framework</h2>
<p>Introduced <a id="_idIndexMarker611"/>in 2018 by Zhang and Chen [6], SEAL is a framework that learns graph structure features for link prediction. It defines the subgraph formed by the target nodes <img alt="" height="45" src="image/Formula_B19153_10_051.png" width="92"/> and their <img alt="" height="32" src="image/Formula_B19153_10_052.png" width="24"/>-hop neighbors<a id="_idIndexMarker612"/> as the <strong class="bold">enclosing subgraph</strong>. Each enclosing subgraph is used as input (instead of the entire graph) to predict a link likelihood. Another way to look at it is that SEAL automatically learns a local heuristic for <span class="No-Break">link prediction.</span></p>
<p>The framework involves <span class="No-Break">three steps:</span></p>
<ol>
<li value="1"><strong class="bold">Enclosing subgraph extraction</strong>, which <a id="_idIndexMarker613"/>consists of taking a set of real links and a set of fake links (negative sampling) to form the <span class="No-Break">training data.</span></li>
<li><strong class="bold">Node information matrix construction</strong>, which involves three components – node labels, node<a id="_idIndexMarker614"/> embeddings, and <span class="No-Break">node features.</span></li>
<li><strong class="bold">GNN training</strong>, which<a id="_idIndexMarker615"/> takes the node information matrices as input and outputs <span class="No-Break">link likelihoods.</span></li>
</ol>
<p>These steps are summarized in the <span class="No-Break">following figure:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer438">
<img alt="Figure 10.3 – The SEAL framework" height="398" src="image/B19153_10_003.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.3 – The SEAL framework</p>
<p>The enclosing subgraph extraction is a straightforward process. It consists of listing the target nodes and their <img alt="" height="34" src="image/Formula_B19153_10_053.png" width="27"/>-hop neighbors to extract their edges and features. A high <img alt="" height="38" src="image/Formula_B19153_10_054.png" width="26"/> will improve the quality of the heuristics SEAL can learn, but it also creates larger subgraphs that are more <span class="No-Break">computationally expensive.</span></p>
<p>The<a id="_idIndexMarker616"/> first component of the node information construction is node labeling. This process assigns a specific number to each node. Without it, the GNN would be unable to differentiate between target and contextual nodes (their neighbors). It also embeds distances, which describe nodes’ relative positions and <span class="No-Break">structural importance.</span></p>
<p>In practice, the target nodes, <img alt="" height="27" src="image/Formula_B19153_10_055.png" width="28"/> and <img alt="" height="33" src="image/Formula_B19153_10_056.png" width="27"/>, must share a unique label to identify them as target nodes. For contextual nodes, <img alt="" height="34" src="image/Formula_B19153_10_057.png" width="21"/> and <img alt="" height="38" src="image/Formula_B19153_10_058.png" width="25"/>, they must share the same label if they have the same distance as the target nodes – <img alt="" height="47" src="image/Formula_B19153_10_059.png" width="277"/> and <img alt="" height="51" src="image/Formula_B19153_10_060.png" width="282"/>. We call this distance the double radius, noted <span class="No-Break">as <img alt="" height="47" src="image/Formula_B19153_10_061.png" width="263"/>.</span></p>
<p>Different solutions can be considered, but SEAL’s authors propose the <strong class="bold">Double-Radius Node Labeling</strong> (<strong class="bold">DRNL</strong>) algorithm. It <a id="_idIndexMarker617"/>works <span class="No-Break">as follows:</span></p>
<ol>
<li value="1">First, assign label 1 to <img alt="" height="24" src="image/Formula_B19153_10_062.png" width="27"/> <span class="No-Break">and <img alt="" height="33" src="image/Formula_B19153_10_063.png" width="27"/>.</span></li>
<li>Assign label 1 to nodes with a radius – <img alt="" height="37" src="image/Formula_B19153_10_064.png" width="79"/>.</li>
<li>Assign label 3 to nodes with a radius – <img alt="" height="38" src="image/Formula_B19153_10_065.png" width="80"/> <span class="No-Break">or <img alt="" height="38" src="image/Formula_B19153_10_066.png" width="76"/>.</span></li>
<li>Assign<a id="_idIndexMarker618"/> label 4 to nodes with a radius – <img alt="" height="38" src="image/Formula_B19153_10_067.png" width="76"/>, <img alt="" height="39" src="image/Formula_B19153_10_068.png" width="80"/>, and <span class="No-Break">so on.</span></li>
</ol>
<p>The<a id="_idIndexMarker619"/> DRNL function can be written <span class="No-Break">as follows:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer455">
<img alt="" height="66" src="image/Formula_B19153_10_069.jpg" width="1105"/>
</div>
</div>
<p>Here, <img alt="" height="47" src="image/Formula_B19153_10_070.png" width="345"/>, and <img alt="" height="44" src="image/Formula_B19153_10_071.png" width="97"/> and <img alt="" height="41" src="image/Formula_B19153_10_072.png" width="111"/> are the integer quotient and remainder of <img alt="" height="31" src="image/Formula_B19153_10_073.png" width="26"/> divided by 2 respectively. Finally, these node labels are <span class="No-Break">one-hot encoded.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">The two other components are easier to obtain. The node embeddings are optional but can be calculated using another algorithm, such as <strong class="source-inline">Node2Vec</strong>. Then, they are concatenated with the node features and one-hot encoded labels to build the final node <span class="No-Break">information matrix.</span></p>
<p>Finally, a GNN<a id="_idIndexMarker620"/> is trained to predict links, using enclosing subgraphs’ information and adjacency matrices. For this task, SEAL’s authors chose the <strong class="bold">Deep Graph Convolutional Neural Network</strong> (<strong class="bold">DGCNN</strong>) [7]. This<a id="_idIndexMarker621"/> architecture performs <span class="No-Break">three steps:</span></p>
<ol>
<li value="1">Several GCN layers compute node embeddings that are then concatenated (like <span class="No-Break">a GIN).</span></li>
<li>A global sort pooling layer sorts these embeddings in a consistent order before feeding them into convolutional layers, which are <span class="No-Break">not permutation-invariant.</span></li>
<li>Traditional convolutional and dense layers are applied to the sorted graph representations and output a <span class="No-Break">link probability.</span></li>
</ol>
<p>The DGCNN model is trained using the binary cross-entropy loss and outputs probabilities between <strong class="source-inline">0</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">1</strong></span><span class="No-Break">.</span></p>
<h2 id="_idParaDest-124"><a id="_idTextAnchor128"/>Implementing the SEAL framework</h2>
<p>The SEAL framework <a id="_idIndexMarker622"/>requires extensive preprocessing to extract and label the enclosing subgraphs. Let’s implement it using <span class="No-Break">PyTorch Geometric:</span></p>
<ol>
<li value="1">First, we import all the <span class="No-Break">necessary libraries:</span><pre class="console">
import numpy as np
from sklearn.metrics import roc_auc_score, average_precision_score
from scipy.sparse.csgraph import shortest_path
import torch
import torch.nn.functional as F
from torch.nn import Conv1d, MaxPool1d, Linear, Dropout, BCEWithLogitsLoss
from torch_geometric.datasets import Planetoid
from torch_geometric.transforms import RandomLinkSplit
from torch_geometric.data import Data
from torch_geometric.loader import DataLoader
from torch_geometric.nn import GCNConv, aggr
from torch_geometric.utils import k_hop_subgraph, to_scipy_sparse_matrix</pre></li>
<li>We load <a id="_idIndexMarker623"/>the <strong class="source-inline">Cora</strong> dataset and apply a link-level random split, like in the <span class="No-Break">previous section:</span><pre class="console">
transform = RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True, split_labels=True)
dataset = Planetoid('.', name='Cora', transform=transform)
train_data, val_data, test_data = dataset[0]</pre></li>
<li>The link-level random split creates new fields in the <strong class="source-inline">Data</strong> object to store the labels and index of each positive (real) and negative (<span class="No-Break">fake) edge:</span><pre class="console">
train_data
<strong class="bold">Data(x=[2708, 1433], edge_index=[2, 8976], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], pos_edge_label=[4488], pos_edge_label_index=[2, 4488], neg_edge_label=[4488], neg_edge_label_index=[2, 4488])</strong></pre></li>
<li>We create a function to process each split and obtain enclosing subgraphs with one-hot encoded node labels and node features. We declare a list to store <span class="No-Break">these subgraphs:</span><pre class="console">
def seal_processing(dataset, edge_label_index, y):
    data_list = []</pre></li>
<li>For each (source and destination) pair in the dataset, we extract the k-hop neighbors (<span class="No-Break">here, <img alt="" height="37" src="image/Formula_B19153_10_074.png" width="110"/></span><span class="No-Break">):</span><pre class="console">
    for src, dst in edge_label_index.t().tolist():
        sub_nodes, sub_edge_index, mapping, _ = k_hop_subgraph([src, dst], 2, dataset.edge_index, relabel_nodes=True)
        src, dst = mapping.tolist()</pre></li>
<li>We<a id="_idIndexMarker624"/> calculate the distances using the DRNL function. First, we remove the target nodes from <span class="No-Break">the subgraph:</span><pre class="console">
        mask1 = (sub_edge_index[0] != src) | (sub_edge_index[1] != dst)
        mask2 = (sub_edge_index[0] != dst) | (sub_edge_index[1] != src)
        sub_edge_index = sub_edge_index[:, mask1 &amp; mask2]</pre></li>
<li>We compute the adjacency matrices for source and destination nodes based on the <span class="No-Break">previous subgraph:</span><pre class="console">
        src, dst = (dst, src) if src &gt; dst else (src, dst)
        adj = to_scipy_sparse_matrix(sub_edge_index, num_nodes=sub_nodes.size(0)).tocsr()
        idx = list(range(src)) + list(range(src + 1, adj.shape[0]))
        adj_wo_src = adj[idx, :][:, idx]
        idx = list(range(dst)) + list(range(dst + 1, adj.shape[0]))
        adj_wo_dst = adj[idx, :][:, idx]</pre></li>
<li>We<a id="_idIndexMarker625"/> calculate the distance between every node and the source/destination <span class="No-Break">target node:</span><pre class="console">
        d_src = shortest_path(adj_wo_dst, directed=False, unweighted=True, indices=src)
        d_src = np.insert(d_src, dst, 0, axis=0)
        d_src = torch.from_numpy(d_src)
        d_dst = shortest_path(adj_wo_src, directed=False, unweighted=True, indices=dst-1)
        d_dst = np.insert(d_dst, src, 0, axis=0)
        d_dst = torch.from_numpy(d_dst)</pre></li>
<li>We calculate the node labels, <strong class="source-inline">z</strong>, for every node in <span class="No-Break">the subgraph:</span><pre class="console">
        dist = d_src + d_dst
        z = 1 + torch.min(d_src, d_dst) + dist // 2 * (dist // 2 + dist % 2 - 1)
        z[src], z[dst], z[torch.isnan(z)] = 1., 1., 0.
        z = z.to(torch.long)</pre></li>
<li>In this example, we will not use node embeddings, but we still concatenate features and one-hot-encoded labels to build the node <span class="No-Break">information matrix:</span><pre class="console">
        node_labels = F.one_hot(z, num_classes=200).to(torch.float)
        node_emb = dataset.x[sub_nodes]
        node_x = torch.cat([node_emb, node_labels], dim=1)</pre></li>
<li>We<a id="_idIndexMarker626"/> create a <strong class="source-inline">Data</strong> object and append it to the list, which is the final output of <span class="No-Break">this function:</span><pre class="console">
        data = Data(x=node_x, z=z, edge_index=sub_edge_index, y=y)
        data_list.append(data)
    return data_list</pre></li>
<li>Let’s use it to extract enclosing subgraphs for each dataset. We separate positive and negative examples to get the correct label <span class="No-Break">to predict:</span><pre class="console">
train_pos_data_list = seal_processing(train_data, train_data.pos_edge_label_index, 1)
train_neg_data_list = seal_processing(train_data, train_data.neg_edge_label_index, 0)
val_pos_data_list = seal_processing(val_data, val_data.pos_edge_label_index, 1)
val_neg_data_list = seal_processing(val_data, val_data.neg_edge_label_index, 0)
test_pos_data_list = seal_processing(test_data, test_data.pos_edge_label_index, 1)
test_neg_data_list = seal_processing(test_data, test_data.neg_edge_label_index, 0)</pre></li>
<li>Next, we merge positive and negative data lists to reconstruct the training, validation, and <span class="No-Break">test datasets:</span><pre class="console">
train_dataset = train_pos_data_list + train_neg_data_list
val_dataset = val_pos_data_list + val_neg_data_list
test_dataset = test_pos_data_list + test_neg_data_list</pre></li>
<li>We <a id="_idIndexMarker627"/>create data loaders to train the GNN <span class="No-Break">using batches:</span><pre class="console">
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32)
test_loader = DataLoader(test_dataset, batch_size=32)</pre></li>
<li>We create a new class for the DGCNN model. The <strong class="source-inline">k</strong> parameter represents the number of nodes to hold for <span class="No-Break">each subgraph:</span><pre class="console">
class DGCNN(torch.nn.Module):
    def __init__(self, dim_in, k=30):
        super().__init__()</pre></li>
<li>We create four GCN layers with a fixed hidden dimension <span class="No-Break">of 32:</span><pre class="console">
        self.gcn1 = GCNConv(dim_in, 32)
        self.gcn2 = GCNConv(32, 32)
        self.gcn3 = GCNConv(32, 32)
        self.gcn4 = GCNConv(32, 1)</pre></li>
<li>We instantiate the global sort pooling at the core of the <span class="No-Break">DGCNN architecture:</span><pre class="console">
        self.global_pool = aggr.SortAggregation(k=k)</pre></li>
<li>The node ordering provided by global pooling allows us to use traditional <span class="No-Break">convolutional layers:</span><pre class="console">
        self.conv1 = Conv1d(1, 16, 97, 97)
        self.conv2 = Conv1d(16, 32, 5, 1)
        self.maxpool = MaxPool1d(2, 2)</pre></li>
<li>Finally, the prediction is managed by <span class="No-Break">an MLP:</span><pre class="console">
        self.linear1 = Linear(352, 128)
        self.dropout = Dropout(0.5)
        self.linear2 = Linear(128, 1)</pre></li>
<li>In <a id="_idIndexMarker628"/>the <strong class="source-inline">forward()</strong> function, we calculate node embeddings for each GCN and concatenate <span class="No-Break">the results:</span><pre class="console">
    def forward(self, x, edge_index, batch):
        h1 = self.gcn1(x, edge_index).tanh()
        h2 = self.gcn2(h1, edge_index).tanh()
        h3 = self.gcn3(h2, edge_index).tanh()
        h4 = self.gcn4(h3, edge_index).tanh()
        h = torch.cat([h1, h2, h3, h4], dim=-1)</pre></li>
<li>The global sort pooling, convolutional layers, and dense layers are sequentially applied to <span class="No-Break">this result:</span><pre class="console">
        h = self.global_pool(h, batch)
        h = h.view(h.size(0), 1, h.size(-1))
        h = self.conv1(h).relu()
        h = self.maxpool(h)
        h = self.conv2(h).relu()
        h = h.view(h.size(0), -1)
        h = self.linear1(h).relu()
        h = self.dropout(h)
        h = self.linear2(h).sigmoid()
        return h</pre></li>
<li>We<a id="_idIndexMarker629"/> instantiate the model on a GPU if available, and train it using the <strong class="source-inline">Adam</strong> optimizer and the binary <span class="No-Break">cross-entropy loss:</span><pre class="console">
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = DGCNN(train_dataset[0].num_features).to(device)
optimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)
criterion = BCEWithLogitsLoss()</pre></li>
<li>We create a traditional <strong class="source-inline">train()</strong> function for <span class="No-Break">batch training:</span><pre class="console">
def train():
    model.train()
    total_loss = 0
    for data in train_loader:
        data = data.to(device)
        optimizer.zero_grad()
        out = model(data.x, data.edge_index, data.batch)
        loss = criterion(out.view(-1), data.y.to(torch.float))
        loss.backward()
        optimizer.step()
        total_loss += float(loss) * data.num_graphs
    return total_loss / len(train_dataset)</pre></li>
<li>In the <strong class="source-inline">test()</strong> function, we calculate the ROC AUC score and the average precision <a id="_idIndexMarker630"/>to compare the SEAL performance with the <span class="No-Break">VGAE performance:</span><pre class="console">
@torch.no_grad()
def test(loader):
    model.eval()
    y_pred, y_true = [], []
    for data in loader:
        data = data.to(device)
        out = model(data.x, data.edge_index, data.batch)
        y_pred.append(out.view(-1).cpu())
        y_true.append(data.y.view(-1).cpu().to(torch.float))
    auc = roc_auc_score(torch.cat(y_true), torch.cat(y_pred))
    ap = average_precision_score(torch.cat(y_true), torch.cat(y_pred))
    return auc, ap</pre></li>
<li>We train the DGCNN for <span class="No-Break">31 epochs:</span><pre class="console">
for epoch in range(31):
    loss = train()
    val_auc, val_ap = test(val_loader)
    print(f'Epoch {epoch:&gt;2} | Loss: {loss:.4f} | Val AUC: {val_auc:.4f} | Val AP: {val_ap:.4f}')
<strong class="bold">Epoch 0 | Loss: 0.6925 | Val AUC: 0.8215 | Val AP: 0.8357</strong>
<strong class="bold">Epoch 1 | Loss: 0.6203 | Val AUC: 0.8543 | Val AP: 0.8712</strong>
<strong class="bold">Epoch 2 | Loss: 0.5888 | Val AUC: 0.8783 | Val AP: 0.8877...</strong>
<strong class="bold">Epoch 29 | Loss: 0.5461 | Val AUC: 0.8991 | Val AP: 0.8973</strong>
<strong class="bold">Epoch 30 | Loss: 0.5460 | Val AUC: 0.9005 | Val AP: 0.8992</strong></pre></li>
<li>Finally, we<a id="_idIndexMarker631"/> test it on the <span class="No-Break">test dataset:</span><pre class="console">
test_auc, test_ap = test(test_loader)
print(f'Test AUC: {test_auc:.4f} | Test AP {test_ap:.4f}')
<strong class="bold">Test AUC: 0.8808 | Test AP 0.8863</strong></pre></li>
</ol>
<p>We obtain results that are similar to those observed using the VGAE (test AUC – <strong class="source-inline">0.8833</strong> and test AP – <strong class="source-inline">0.8845</strong>). In theory, subgraph-based methods such as SEAL are more expressive than node-based methods such as VGAEs. They capture more information by explicitly considering the entire neighborhood around target nodes. SEAL’s accuracy can also be improved by increasing the number of neighbors taken into account with the <span class="No-Break"><strong class="source-inline">k</strong></span><span class="No-Break"> parameter.</span></p>
<h1 id="_idParaDest-125"><a id="_idTextAnchor129"/>Summary</h1>
<p>In this chapter, we explored a new task with link prediction. We gave an overview of this field by presenting heuristic and matrix factorization techniques. Heuristics can be classified according to the k-hop neighbors they consider – from local with 1-hop neighbors to global with the knowledge of the entire graph. Conversely, matrix factorization approximates the adjacency matrix using node embeddings. We also explained how this technique was connected to algorithms described in previous chapters (<strong class="source-inline">DeepWalk</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">Node2Vec</strong></span><span class="No-Break">).</span></p>
<p>After this introduction to link prediction, we saw how to implement it using GNNs. We outlined two kinds of techniques, based on node embeddings (GAE and VGAE) and subgraph representations (SEAL). Finally, we implemented a VGAE and SEAL on the <strong class="source-inline">Cora</strong> dataset with an edge-level random split and negative sampling. Both models obtained comparable performance, although SEAL is strictly <span class="No-Break">more expressive.</span></p>
<p>In <a href="B19153_11.xhtml#_idTextAnchor131"><span class="No-Break"><em class="italic">Chapter 11</em></span></a><em class="italic">, Generating Graphs with Graph Neural Networks</em>, we will see different strategies to produce realistic graphs. First, we will describe traditional techniques with the popular Erdős–Rényi model. Then, we will see how deep generative methods work by reusing the GVAE and introducing a new architecture – the <strong class="bold">Graph Recurrent Neural </strong><span class="No-Break"><strong class="bold">Network</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">GraphRNN</strong></span><span class="No-Break">).</span></p>
<h1 id="_idParaDest-126"><a id="_idTextAnchor130"/>Further reading</h1>
<ul>
<li>[1] H. Tong, C. Faloutsos and J. -y. Pan. “Fast Random Walk with Restart and Its Applications” in <em class="italic">Sixth International Conference on Data Mining (ICDM’06)</em>, 2006, pp. 613-622, <span class="No-Break">doi: 10.1109/ICDM.2006.70.</span></li>
<li>[2] Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. <em class="italic">Matrix Factorization Techniques for Recommender Systems.</em> Computer 42, 8 (August 2009), <span class="No-Break">30–37. https://doi.org/10.1109/MC.2009.263.</span></li>
<li>[3] J. Qiu, Y. Dong, H. Ma, J. Li, K. Wang, and J. Tang. <em class="italic">Network Embedding as Matrix Factorization</em>. Feb. 2018. <span class="No-Break">doi: 10.1145/3159652.3159706.</span></li>
<li>[4] D. P. Kingma and M. Welling. <em class="italic">Auto-Encoding Variational Bayes.</em> arXiv, 2013. <span class="No-Break">doi: 10.48550/ARXIV.1312.6114.</span></li>
<li>[5] T. N. Kipf and M. Welling. <em class="italic">Variational Graph Auto-Encoders</em>. arXiv, 2016. <span class="No-Break">doi: 10.48550/ARXIV.1611.07308.</span></li>
<li>[6] M. Zhang and Y. Chen. <em class="italic">Link Prediction Based on Graph Neural Networks</em>. arXiv, 2018. <span class="No-Break">doi: 10.48550/ARXIV.1802.09691.</span></li>
<li>[7] Muhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen. 2018. <em class="italic">An end-to-end deep learning architecture for graph classification</em>. In <em class="italic">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</em> and <em class="italic">Thirtieth Innovative Applications of Artificial Intelligence Conference</em> and <em class="italic">Eighth AAAI Symposium on Educational Advances in Artificial Intelligence</em> (AAAI’18/IAAI’18/EAAI’18). AAAI Press, Article <span class="No-Break">544, 4438–4445.</span></li>
</ul>
</div>
</div></body></html>