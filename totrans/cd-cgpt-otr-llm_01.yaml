- en: '1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is ChatGPT and What are LLMs?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The world has been strongly influenced by the recent advancements in AI, especially
    **large language models** ( **LLMs** ) such as ChatGPT and Gemini (formerly Bard).
    We’ve witnessed stories such as OpenAI reaching one million users in five days,
    huge tech company lay-offs, history-revising image scandals, more tech companies
    getting multi-trillion dollar valuations (Microsoft and NVIDIA), a call for funding
    of $5–7 trillion for the next stage of technology, and talks of revolutions in
    how *everything* is done!
  prefs: []
  type: TYPE_NORMAL
- en: Yes, these are all because of new AI technologies, especially LLM tech.
  prefs: []
  type: TYPE_NORMAL
- en: 'LLMs are large in multiple ways: not just large training sets and large training
    costs but also large impacts on the world!'
  prefs: []
  type: TYPE_NORMAL
- en: This book is about harnessing that power effectively, for your benefit, if you
    are a coder.
  prefs: []
  type: TYPE_NORMAL
- en: Coding has changed, and we must all keep up or else our skills will become redundant
    or outdated. In this book are tools needed by coders to quickly generate code
    and do it well, to comment, debug, document, and stay ethical and on the right
    side of the law.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re a programmer or coder, this is for you. Software, especially AI/machine
    learning, is changing everything at ever-accelerating rates, so you’ll have to
    learn this stuff quickly, and then use it to create and understand future technologies.
  prefs: []
  type: TYPE_NORMAL
- en: I don’t want to delay you any longer, so let’s get into the first chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll cover some basics of ChatGPT, Gemini, and other LLMs,
    where they come from, who develops them, and what the architectures entail. We’ll
    introduce some organizations that use LLMs and their services. We’ll also briefly
    touch on some mathematics that go into LLMs. Lastly, we’ll check out some of the
    competition and applications of LLMs in the field.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Origins of LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Early LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring modern LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How transformers work
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications of LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ChatGPT is an LLM. LLMs can be used to answer questions and generate emails,
    marketing materials, blogs, video scripts, code, and even books that look a lot
    like they’ve been written by humans. However, you probably want to know about
    the technology.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with what an LLM is.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are deep learning models, specifically, transformer networks or just “
    *transformers* .” Transformers certainly have transformed our culture!
  prefs: []
  type: TYPE_NORMAL
- en: An LLM is trained on huge amounts of text data, petabytes (thousands of terabytes)
    of data, and predicts the next word or words. Due to the way LLMs operate, they
    are not perfect at outputting text; they can give alternative facts, facts that
    are “hallucinated.”
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT is, as of the time of writing, the most popular and famous LLM, created
    and managed by OpenAI. OpenAI is a charity and a capped-profit organization based
    in San Francisco [ *OpenAI_LP* , *OpenAIStructure* ].
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT is now widely used for multiple purposes by a huge number of people
    around the world. Of course, there’s GPT-4 and now GPT-4 Turbo, which are paid,
    more powerful, and do more things, as well as taking more text in prompts.
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s called ChatGPT: *Chat* because that’s what you do with it, it’s a chatbot,
    and **GPT** is the technology and stands for **generative pre-trained transformer**
    . We will get more into that in the *GPT* *lineage* subsection.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A transformer is a type of neural network architecture, and a transformer is
    the basis of the most successful LLMs today (2024). GPT is a Generative Pre-trained
    Transformer. Gemini is a transformer [ *ChatGPT* , *Gemini* , *Menon* , *HuggingFace*
    ]. OpenAI’s GPT-4 is a remarkable advancement in the field of AI. This model,
    which is the fourth iteration of the GPT series, has introduced a new feature:
    the ability to generate images alongside text. This is a significant leap from
    its predecessors, which were primarily text-based models.'
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI also has an image generation AI, DALL-E, and an AI that can connect images
    and text and does image recognition, called CLIP ( *OpenAI_CLIP* ). The image
    generation capability of DALL-E is achieved by training the transformer model
    on image data. This means that the model has been exposed to a vast array of images
    during its training phase, enabling it to understand and generate visual content
    [ *OpenAI_DALL.E* ] .
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, since images can be sequenced to form videos, DALL.E can also be
    considered a video generator. This opens up a plethora of possibilities for content
    creation, ranging from static images to dynamic videos. It’s a testament to the
    versatility and power of transformer models, and a glimpse into the future of
    AI capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: In essence, tools from OpenAI are not just text generators but a comprehensive
    suite of content generators, capable of producing a diverse range of outputs.
    It’s called being **multi-modal** . This makes these tools invaluable in numerous
    applications, from content creation and graphic design to research and development.
    The evolution from GPT-3 to GPT-4 signifies a major milestone in AI development,
    pushing the boundaries of what AI models can achieve.
  prefs: []
  type: TYPE_NORMAL
- en: Origins of LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Earlier neural networks with their ability to read sentences and predict the
    next word could only read one word at a time and were called **recurrent neural
    networks** , ( **RNNs** ). RNNs attempted to mimic human-like sequential processing
    of words and sentences but faced challenges in handling long-term dependencies
    between words and sentences due to very limited memory capacity.
  prefs: []
  type: TYPE_NORMAL
- en: In 1925, the groundwork was laid by Wilhelm Lenz and Ernst Ising with their
    non-learning Ising model, considered an early RNN architecture [ *Brush* , *Gemini*
    ].
  prefs: []
  type: TYPE_NORMAL
- en: In 1972, Shun’ichi Amari made this architecture adaptive, paving the way for
    learning RNNs. This work was later popularized by John Hopfield in 1982 [ *Amari*
    , *Gemini* ].
  prefs: []
  type: TYPE_NORMAL
- en: Due to this, there has been a fair amount of research to find ways to stretch
    this memory to include more text to get more context. RNNs are transformers. There
    are other transformers, including **LSTMs** , which are **long short-term memory**
    neural networks that are based on a more advanced version of RNNs, but we won’t
    go into that here [ *Brownlee_LLMs* , *Gemini* ]. LSTMs were invented by Hochreiter
    and Schmidhuber in 1997 [ *Wiki_LSTM* , *Hochreiter1997* ].
  prefs: []
  type: TYPE_NORMAL
- en: 'There is another network called the **convolutional neural network** ( **CNN**
    ). Without going into much detail, CNNs are very good at images and lead the world
    in image recognition and similar jobs. CNNs (or ConvNets) were invented in 1980
    by Kunihiko Fukushima and developed by Yann LeCun, but they only really became
    popular in the 2000s, when GPUs became available. Chellapilla *et al* . tested
    the speeds of training CNNs on CPUs and GPUs and found the network trained on
    GPUs 4.1 times faster [ *Fukushima1980* , *LeCun1989* , *Chellapilla2006* ]. Sometimes,
    your inventions take time to bear fruit, but keep inventing! CNNs use many layers
    or stages to do many different mathematical things to their inputs and try to
    look at them in different ways: different angles, with detail taken out (dropout
    layers), pooling nearby regions of each image, zeroing negative numbers, and other
    tricks.'
  prefs: []
  type: TYPE_NORMAL
- en: What was needed was a model with some form of memory to remember and also generate
    sentences and longer pieces of writing.
  prefs: []
  type: TYPE_NORMAL
- en: In 2017, Ashish Vaswani and others published a paper called *Attention Is All
    You Need* , [ *Vaswani* , *2017* ]. In this important paper, the transformer architecture
    was proposed based on attention mechanisms. In other words, this model didn’t
    use recurrence and convolutions, such as RNNs and CNNs. These methods have been
    very successful and popular AI architectures in their own right.
  prefs: []
  type: TYPE_NORMAL
- en: Compared to RNNs and CNNs, Vaswani’s Transformer performed faster training and
    allowed for higher parallelizability.
  prefs: []
  type: TYPE_NORMAL
- en: The Transformer was the benchmark for English-to-German translation and established
    a new state-of-the-art single model in the WMT 2014 English-to-French translation
    task. It also performed this feat after being trained for a small fraction of
    the training times of the next best existing models. Indeed, Transformers were
    a groundbreaking advancement in natural language processing [ *Vaswani* , *2017*
    ].
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have covered the origins of LLMs, we will check out some of the
    earliest LLMs that were created.
  prefs: []
  type: TYPE_NORMAL
- en: Early LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are many LLMs today and they can be put into a family tree; see *Figure
    1* *.1* . The figure shows the evolution from word2vec to the most advanced LLMs
    in 2023: GPT-4 and Gemini [ *Bard* ].'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.1: Family tree of LLMs from word2vec to GPT-4 and Bard, from Yang2023
    with permission](img/B21009_01_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.1: Family tree of LLMs from word2vec to GPT-4 and Bard, from Yang2023
    with permission'
  prefs: []
  type: TYPE_NORMAL
- en: So, that’s all of them but, for now, we’ll look at the earlier LLMs that lead
    to the most advanced technologies today. We’ll start with GPT.
  prefs: []
  type: TYPE_NORMAL
- en: GPT lineage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The development of GPT is a constantly changing and iterative process, with
    each new model building upon the strengths and weaknesses of its ancestors. The
    GPT series, initiated by OpenAI, has undergone a great deal of evolution, leading
    to advancements in **natural language processing** ( **NLP** ) and understanding.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3, the third iteration, brought a significant leap in terms of size and
    complexity, with an impressive 175 billion parameters. This allowed it to generate
    pretty human-like text across a wide range of topics and subjects [ *Wiki_GPT3*
    , *ProjectPro* ].
  prefs: []
  type: TYPE_NORMAL
- en: As the GPT series progressed, OpenAI continued to refine and enhance the architecture.
    In subsequent iterations, GPT-4 and GPT-4 Turbo have further pushed back the boundaries
    of what these LLMs can achieve. The iterative development process focuses on increasing
    model size and improving fine-tuning capabilities, enabling more nuanced and contextually
    relevant outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Further to this, there are more modalities, such as GPT-4 with vision and text-to-speech.
  prefs: []
  type: TYPE_NORMAL
- en: GPT model iteration is not solely about scaling up the number of parameters;
    it also involves addressing the limitations observed in earlier versions. Feedback
    from user interactions, research findings, and technological advancements contribute
    to the iterative nature of the GPT series. OpenAI is constantly working to reduce
    the amount of inaccurate information and incoherent outputs (hallucinations) that
    its chatbots produce. Also, each iteration of the chatbot takes on board the lessons
    learned from real-world applications and user feedback.
  prefs: []
  type: TYPE_NORMAL
- en: GPT models are trained and fine-tuned on very large, diverse datasets to make
    sure the chatbots can adapt to many different contexts, industries, and user requirements.
    The iterative development approach ensures that later GPT models are better equipped
    to understand and generate human-like text, making them extremely valuable tools
    for a huge number of applications, including content creation such as blogs, scripts
    for videos, and copywriting (writing the text in adverts) as well as conversational
    agents (chatbots and AI assistants).
  prefs: []
  type: TYPE_NORMAL
- en: The way GPT models are developed iteratively shows OpenAI’s commitment to continuous
    improvement and innovation in the field of LLMs, allowing even more sophisticated
    and capable models to be built from these models in the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the dates for when the different versions of GPT were launched:'
  prefs: []
  type: TYPE_NORMAL
- en: GPT was first launched in June 2018
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPT-2 was released in February 2019
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPT-3 in 2020
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPT-3.5 in 2022/ChatGPT in November 2022
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There will be more on the GPT family later, in the *GPT-4 /GPT-4* *Turbo* section.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we will detail the architecture of LLMs and how they operate.
  prefs: []
  type: TYPE_NORMAL
- en: BERT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To comprehend the roots and development of **Bidirectional Encoder Representations
    from Transformers** ( **BERT** ), we must know more about the intricate and fast-moving
    landscape of neural networks. Without hyperbole, BERT was a seriously important
    innovation in NLP, part of the ongoing evolution of AI. BERT was the state of
    the art for a wide range of NLP tasks in October 2018, when it was released [
    *Gemini* ]. This included question answering, sentiment analysis, and text summarization.
  prefs: []
  type: TYPE_NORMAL
- en: BERT also paved the way for later R&D of LLMs; it played a pivotal role in LLM
    development. BERT, being open source, helped to speed up LLM advancement.
  prefs: []
  type: TYPE_NORMAL
- en: BERT takes some of its DNA from RNNs (mentioned in the *Origins of LLMs* section),
    the neural nets that loop back on themselves to create a kind of memory, although
    rather limited memory.
  prefs: []
  type: TYPE_NORMAL
- en: The invention of the first transformer architecture was key to the origin of
    BERT. The creation of BERT as a bidirectional encoder (these go backward and forward
    along a sentence) drew inspiration from the transformer’s attention-based mechanism,
    allowing it to capture contextual relationships between words in both directions
    within a sentence.
  prefs: []
  type: TYPE_NORMAL
- en: So, BERT’s attention is bidirectional (left-to-right and right-to-left context).
    At its creation, this was unique, and it enabled BERT to gain a more comprehensive
    understanding of nuanced language semantics.
  prefs: []
  type: TYPE_NORMAL
- en: While BERT’s foundations are in transformer architecture, its characteristics
    have evolved with further research and development, though it is not currently
    in development. Each iteration of BERT refined and expanded its capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: The BERT LLM was a stage of the ongoing innovation in AI. BERT’s ability to
    understand language bidirectionally, drawing insights from both preceding and
    succeeding words, is part of the endeavors taken to achieve the creation of an
    AI with a sufficiently deep awareness of the intricacies of natural language.
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 1.2: Architecture of BERT, a bidirectional encoder (reprodu\uFEFF\
    ced from GeekCultureBERT)](img/B21009_01_2.jpg)"
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.2: Architecture of BERT, a bidirectional encoder (reprodu ced from
    GeekCultureBERT)'
  prefs: []
  type: TYPE_NORMAL
- en: LaMDA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Understanding the ancestry of **Language Model for Dialogue Applications** (
    **LaMDA** ) involves tracing the roots of its architectural design and the evolutionary
    path it followed in the landscape of NLP. LaMDA, like its counterparts, emerges
    from a family of models that have collectively revolutionized how machines comprehend
    and generate human-like text.
  prefs: []
  type: TYPE_NORMAL
- en: RNNs, mentioned in this chapter’s first section, play a pivotal role in LaMDA’s
    family tree.
  prefs: []
  type: TYPE_NORMAL
- en: The breakthrough came with the invention of transformer architectures, and LaMDA
    owes a significant debt to the transformative *Attention Is All You Need* paper
    [ *Vaswani* *2017* , *2023* ]. This paper laid the groundwork for a novel approach,
    moving away from sequential processing to a more parallelized and attention-based
    mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: The LaMDA LLM inherits its core architecture from the transformer family and
    was developed by Google. These models learn very well how words in a sentence
    relate to each other. This allows a transformer to have a richer understanding
    of language. This change from using traditional processing in sequence was a paradigm
    shift in NLP, enabling LaMDA to more effectively grasp nuanced interactions and
    dependencies within texts.
  prefs: []
  type: TYPE_NORMAL
- en: While the origins lie in the transformer architecture, LaMDA’s unique characteristics
    may have been fine-tuned and evolved through subsequent research and development
    efforts. LaMDA’s lineage is not just a linear progression but a family tree, a
    branching exploration of many possibilities, with each iteration refining and
    expanding its capabilities. In *Figure 1* *.1* , LaMDA is near ERNIE 3.0, Gopher,
    and PaLM on the right of the main, vertical blue branch.
  prefs: []
  type: TYPE_NORMAL
- en: Simply put, LaMDA is a product of ongoing innovation and refinement in the field
    of AI, standing on the shoulders of earlier models and research breakthroughs.
    Its ability to comprehend and generate language is deeply rooted in an evolutionary
    process of learning from vast amounts of text data, mimicking the way humans process
    and understand language on a grand, digital scale.
  prefs: []
  type: TYPE_NORMAL
- en: LaM DA was launched in May 2021.
  prefs: []
  type: TYPE_NORMAL
- en: LLaMA‘s family tree
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLaMA is the AI brainchild of Meta AI. It might not be one you’ve heard the
    most about but its lineage holds stories of innovation and evolution, tracing
    a fascinating path through the history of AI communication.
  prefs: []
  type: TYPE_NORMAL
- en: Like the other chatbot LLMs, LLaMA’s roots are also in transformer architectures.
    These models rely on intricate attention mechanisms, allowing them to analyze
    relationships between words, not just their sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Trained on massive datasets of text and code, LLaMA learned to generate basic
    responses, translate languages, and even write different kinds of creative text
    formats.
  prefs: []
  type: TYPE_NORMAL
- en: However, like a newborn foal, their capabilities were limited. They stumbled
    with complex contexts, lacked common sense reasoning, and sometimes sputtered
    out nonsensical strings.
  prefs: []
  type: TYPE_NORMAL
- en: Yet their potential was undeniable. The ability to learn and adapt from data
    made them valuable tools for researchers. Meta AI nurtured these nascent models,
    carefully tweaking their architecture and feeding them richer datasets. They delved
    deeper into the understanding of human language, acquiring skills such as factual
    grounding, reasoning, and the ability to engage in multi-turn conversations (Wiki_llama).
  prefs: []
  type: TYPE_NORMAL
- en: 'The Llama family tree is not a linear progression but, rather, a family of
    multiple branches of exploration. Different versions explored specific avenues:
    Code Llama focused on code generation, while Megatron-Turing NLG 530 B was trained
    on filling in missing words, reading comprehension, and common-sense reasoning,
    among other things ( *CodeLlama 2023* , *Megatron-Turing 2022* ).'
  prefs: []
  type: TYPE_NORMAL
- en: For an idea of how LLaMA fits into the evolutionary tree, see *Figure 1* *.1*
    at the top left of the vertical blue branch, near Bard ( *Gemini* ).
  prefs: []
  type: TYPE_NORMAL
- en: Each experiment, each successful leap forward, contributed valuable DNA to future
    generations.
  prefs: []
  type: TYPE_NORMAL
- en: Why the name *Megatron-Turing NLG 530 B* ? *Megatron* because it represents
    a powerful hardware and software framework. *Turing* to honor Alan Turing, the
    first AI researcher, and the originator of AI and ML. **NLG** stands for **natural
    language generation** , and it has 530 billion parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Meta AI continues to shepherd the Llama family, and the future promises more
    exciting developments.
  prefs: []
  type: TYPE_NORMAL
- en: Llama LLM was launched in February 2023, while Megatron-Turing NLG 530 B was
    released in January 2022.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have covered the origins and explored the early stages of LLMs,
    let us fast-forward and talk about modern LLMs in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring modern LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After the explosive take-off of ChatGPT in late 2022, with 1 million active
    users in 5 days and 100 million active users in January 2023 (about 2 months),
    2023 was a pretty hot year for LLMs, AI research, and the use of AI in general.
  prefs: []
  type: TYPE_NORMAL
- en: Most tech companies have worked on their own LLMs or transformer models to use
    and make publicly available. Many companies, organizations, and individuals (students
    included) have used LLMs for a multitude of tasks. OpenAI keeps updating its GPT
    family and Google keeps updating its Bard version. Bard became Gemini in February
    2024, so all references to Bard have changed to Gemini. Many companies use ChatGPT
    or GPT-4 as the core of their offering, just creating a wrapper and selling it.
  prefs: []
  type: TYPE_NORMAL
- en: This might change as OpenAI keeps adding modalities (speech, image, etc.) to
    the GPTs and even a new marketplace platform where users can create and sell their
    own GPT agents right on OpenAI servers. This was launched in early January 2024
    to paid users ($20/month before VAT). We’ll cover some of the latest LLMs that
    companies have wor ked on in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GPT-4 Turbo, OpenAI’s latest hot chatbot, is another big upgrade. It’s the GPT-4
    you know, but on steroids, with 10 times more memory and a newfound understanding
    of images.
  prefs: []
  type: TYPE_NORMAL
- en: If GPT-4 was a gifted writer, GPT-4 Turbo is a multimedia polymath. It can not
    only spin captivating stories and poems but also decipher images, paint vivid
    digital landscapes, and even caption photos with witty remarks. Forget outdated
    information – Turbo’s knowledge base refreshes constantly, keeping it as sharp
    as a tack on current events.
  prefs: []
  type: TYPE_NORMAL
- en: But it’s not just about flashy tricks. Turbo is a stickler for facts. It taps
    into external knowledge bases and employs sophisticated reasoning, ensuring its
    responses are accurate and reliable. Gone are the days of biased or misleading
    outputs – Turbo strives for truth and clarity, making it a trustworthy companion
    for learning and exploration.
  prefs: []
  type: TYPE_NORMAL
- en: The best part? OpenAI isn’t keeping this powerhouse locked away. They’ve crafted
    an API and developer tools, inviting programmers and innovators to customize Turbo
    for specific tasks and domains. This democratization of advanced language processing
    opens doors to a future where everyone, from artists to scientists, can harness
    the power of language models to create, analyze, and understand the world around
    them.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4 Turbo is probably widely considered the pinnacle of technology at the
    moment, showing us the breathtaking potential of LLMs. It’s not just a language
    model; it’s a glimpse into a future where machines understand and interact with
    us like never before. So, buckle up! The future of language is here, and it’s
    powered by GPT-4 Turbo.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4 was launched in March 2023 and GPT-4 Turbo in November 2023 ( *Wiki_GPT4*
    , *OpenAI_GPT4Turbo* , *Gemini* ).
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4o or GPT-4 omni was released in May 2024, and it can understand multiple
    formats of data. Omni is faster than previous models and can respond to speech
    in 0.32 seconds on average, similar to human response times, while Turbo takes
    about 5.4 seconds to respond in Voice Mode.
  prefs: []
  type: TYPE_NORMAL
- en: This is partially because, while Turbo takes in text, transcribed from the audio
    by a simple model, and a third model converts the text back into audio response,
    omni is a single model that understands audio, video, and text. The three models
    for Turbo are slower than omni and a lot of information is lost to GPT-4 Turbo
    due to transcription.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4o is much better than GPT-4 Turbo in non-English human languages.
  prefs: []
  type: TYPE_NORMAL
- en: The Omni API is also half the cost of Turbo ( *OpenAI-GPT-4o* )!
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4o does very well on code generation versus Claude 3 Opus and Gemini 1.5
    Pro. Claude is moderate, Gemini is judged to be very good, and GPT-4o is excellent
    [ *encord* ].
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4 architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: OpenAI has not released details of the architecture and full details of GPT-4,
    proprietary information for now, but we can piece together elements from similar
    work.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4 has 1.75 trillion parameters (1.75 million million) ( *MotiveX_Gemini*
    ).
  prefs: []
  type: TYPE_NORMAL
- en: 'The vision transformer will likely involve some encoder-decoder architecture:
    image and video inputs for the encoder, then the decoder will generate output
    such as text descriptions or captions as well as images ( *Gemini* ).'
  prefs: []
  type: TYPE_NORMAL
- en: It will have an attention mechanism because “attention is all you need.”
  prefs: []
  type: TYPE_NORMAL
- en: The vision components will probably multi-head to process various aspects of
    the input simultaneously. There should also be positional encoding, image pro-processing
    layers, and modality fusion.
  prefs: []
  type: TYPE_NORMAL
- en: Modality fusion is where the vision capabilities are combined with the faculties
    to process text. From this, it would need to generate a unified understanding
    of the inputs or the scene given to it.
  prefs: []
  type: TYPE_NORMAL
- en: So, GPT-4 can understand images, and it’s believed that it uses a combination
    of **Vision Transformer** ( **ViT** ) and Flamingo visual language models.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 1* *.3* shows the architecture of ViT (reproduced from Wagh).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.3: This is what the internal workings of ViT involve (reproduced
    from Wagh)](img/B21009_01_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.3: This is what the internal workings of ViT involve (reproduced from
    Wagh)'
  prefs: []
  type: TYPE_NORMAL
- en: So, the inner workings of GPT-4 that handle vision processing likely involve
    visual transformers as shown in the preceding figure, along with the text processors
    in the *How an LLM processes a* *sentence* subsection.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find out more about ViT here: [https://github.com/lucidrains/vit-pytorch](https://github.com/lucidrains/vit-pytorch)
    .'
  prefs: []
  type: TYPE_NORMAL
- en: LLaMA-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The latest official LLaMA, LLaMA-2, is capable of holding complicated conversations,
    generating various creative text formats, and even adapting its responses to specific
    user personalities.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenLLaMA is an open source version of LLaMA released by Open LM Research (
    *Watson 2023* , *OpenLMR* , *Gemini* ). OpenLLaMA has several versions, each trained
    on different datasets but the training process was very similar to the original
    LLaMA. Model weights can be found on the HuggingFace Hub and accessed without
    the need for any additional permission. The HuggingFace page for Open LLaMA is
    here: [https://huggingface.co/docs/transformers/en/model_doc/open-llama](https://huggingface.co/docs/transformers/en/model_doc/open-llama)
    .'
  prefs: []
  type: TYPE_NORMAL
- en: OpenLLaMA models serve as benchmarks for LLM research. Their open source nature
    makes it possible to compare with other models. This is made easier because there
    are PyTorch and TensorFlow formats available.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLaMA-2 was released in April 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenLLaMA was released in June 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In early 2024, the rumors are that LLaMA-3 will be released this year.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gemini (formerly Bard)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Google’s Gemini is a chatbot LLM with access to the internet and just requires
    a Google login. Technically, Gemini is the face and the brain is whatever Google
    slots in.
  prefs: []
  type: TYPE_NORMAL
- en: Previously, Gemini was powered by PaLM 2.
  prefs: []
  type: TYPE_NORMAL
- en: 'As of writing (early February 2024), Bard was earlier powered by Gemini. There
    are three versions of Gemini: Nano, Pro, and Ultra. Nano is for mobile devices.
    As Bard is powered by Gemini Pro, the name changed to Gemini. There may soon be
    a paid version.'
  prefs: []
  type: TYPE_NORMAL
- en: Gemini was released in March 2023 ( *Wiki_Gemini* ).
  prefs: []
  type: TYPE_NORMAL
- en: Gemini has 142.4 million u sers, 62.6% of which are in the USA ( *AnswerIQ*
    ).
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of Gemini
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Gemini is one of the LLMs and AIs developed and used by Google/Alphabet. Let’s
    take a peek under the hood to understand what makes Gemini tick!
  prefs: []
  type: TYPE_NORMAL
- en: Gemini is trained on a vast library of the world’s books, articles, and internet
    chatter. 1.56 trillion words are in the Infiniset dataset of Google Gemini; that’s
    750 GB of data. Gemini has 137 billion parameters, which are the neural network
    weights (ChatGPT has 175 billion parameters/weights) ( *ProjectPro* ).
  prefs: []
  type: TYPE_NORMAL
- en: In November 2023, Bard got an upgrade and started to be powered by Gemini, a
    new AI system ( *SkillLeapAI* ). Previously, Gemini was powered by LaMDA from
    March 2023, then PaLM 2 from May 2023.
  prefs: []
  type: TYPE_NORMAL
- en: There are three models, Gemini Nano, Gemini Pro, and Gemini Ultra. As of 19th
    January 2024, Gemini is powered by Gemini Ultra, which was launched in December
    2023.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 1* *.4* shows the architecture of Gemini ( *GeminiTeam* ).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.4: Bard/Gemini architecture, from the DeepMind GeminiTeam (GeminiTeam)](img/B21009_01_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.4: Bard/Gemini architecture, from the DeepMind GeminiTeam (GeminiTeam)'
  prefs: []
  type: TYPE_NORMAL
- en: Gemini can deal with combinations of text, images, audio, and video inputs,
    which are represented as different colors here. Outputs can be text and images
    combined.
  prefs: []
  type: TYPE_NORMAL
- en: The transition to Gemini Ultra signifies a significant leap in Gemini’s capabilities,
    offering higher performance, greater efficiency, and a wider range of potential
    applications (Gemini). Bard/Gemini Ultra has a complex architecture that is like
    a sophisticated language processing factory, with each component playing a crucial
    role in understanding your questions and crafting the perfect response.
  prefs: []
  type: TYPE_NORMAL
- en: The key component is the transformer decoder, the brain of the operation. It
    analyzes the incoming text, dissecting each word’s meaning and its connection
    to others. It’s like a skilled translator, deciphering the message you send and
    preparing to respond fluently.
  prefs: []
  type: TYPE_NORMAL
- en: The Gemini Ultra multimodal encoder can handle more than just text. Images,
    audio, and other data types can be processed, providing a richer context for the
    decoder. This allows Gemini to interpret complex situations, such as describing
    an image you send or composing music based on your mood.
  prefs: []
  type: TYPE_NORMAL
- en: To polish the decoder’s output, pre-activation and post-activation transformers
    come into play. These additional layers refine and smoothen the response, ensuring
    it’s clear, grammatically correct, and reads like natural, human language. With
    less hallucination, the factual grounding module anchors its responses in the
    real world. Just like a reliable teacher, it ensures Gemini’s information is accurate
    and unbiased, grounding its creativity in a strong foundation of truth. Beyond
    basic understanding, Gemini Ultra also has reasoning abilities. It can answer
    complex questions, draw logical conclusions, and even solve problems.
  prefs: []
  type: TYPE_NORMAL
- en: The implementation that is Gemini also has a little link to Google to help users
    to fact-check its responses. At the bottom of the output, above the input window,
    Google enables you to double-check its response.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.5: Gemini’s Google search button to fact-check the output it gives
    you](img/B21009_01_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.5: Gemini’s Google search button to fact-check the output it gives
    you'
  prefs: []
  type: TYPE_NORMAL
- en: Click this and it says **Google search** and outputs some search results and
    a guide to what you’re seeing.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.6: Google search based on its output](img/B21009_01_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.6: Google search based on its output'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 1* *.7* shows what the highlighting means.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.7: Understanding the results of the Google search to help fact-check](img/B21009_01_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.7: Understanding the results of the Google search to help fact-check'
  prefs: []
  type: TYPE_NORMAL
- en: On your Gemini screen, you’ll see various passages highlighted in brown or green.
    The green-highlighted text has results agreeing, the brown-highlighted text doesn’t
    agree with the sources, and no highlight means not enough information to confirm.
  prefs: []
  type: TYPE_NORMAL
- en: This is just a simplified glimpse into Gemini Ultra’s architecture and functioning.
    With its massive parameter count, self-attention mechanisms, and fine-tuning capabilities,
    it’s a constantly evolving language maestro , pushing the boundaries of what LLMs
    can achieve.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Olympus
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Amazon has developed an enormous new LLM. It’s a hulking beast, dwarfing even
    OpenAI’s GPT-4 in sheer size. But this isn’t just a power contest. Olympus aims
    for something more: a significant leap in coherence, reasoning, and factual accuracy.
    Their chatbot, Metis is powered by Olympus: [https://happyfutureai.com/amazons-metis-a-new-ai-chatbot-powered-by-olympus-llm/](https://happyfutureai.com/amazons-metis-a-new-ai-chatbot-powered-by-olympus-llm/)
    .'
  prefs: []
  type: TYPE_NORMAL
- en: With no half-baked ideas, Olympus digs deep, thinks logically, and double-checks
    its facts before uttering a word. Amazon is purportedly working to reduce bias
    and misinformation. This LLM strives for high levels of wisdom and reliability.
  prefs: []
  type: TYPE_NORMAL
- en: It’s not just about bragging rights for Amazon. Olympus represents a potential
    turning point for language models.
  prefs: []
  type: TYPE_NORMAL
- en: The aim is to be able to tackle complex tasks with pinpoint accuracy, grasp
    subtle nuances of meaning, and engage in intelligent, fact-based conversations
    with other AI.
  prefs: []
  type: TYPE_NORMAL
- en: Olympus will, hopefully, be a more thoughtful companion capable of deeper understanding
    and insightful exchange.
  prefs: []
  type: TYPE_NORMAL
- en: Olympus may not be ready to join your book club just yet, but its story is worth
    watching. Hopefully, Olympus will be a needed advancement for LLMs and not hallucinate,
    only producing truth and changing what LLMs can do.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Olympus should have around two trillion parameters (weights and biases)
    ( *Life_Achritecture* ).
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Olympus is expected in the second half of 2024 but not much information
    has come out since November 2023.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have introduced many of the modern LLMs, let’s look at how they
    work, including using an example piece of text.
  prefs: []
  type: TYPE_NORMAL
- en: How Transformers work
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Moving on to the general transformers, *Figure 1* *.8* shows the structure
    of a Transformer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.8: Architecture of a Transformer: an encoder for the inputs and
    a decoder for the outputs (reproduced from Zahere)](img/B21009_01_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.8: Architecture of a Transformer: an encoder for the inputs and a
    decoder for the outputs (reproduced from Zahere)'
  prefs: []
  type: TYPE_NORMAL
- en: You can see that it has an encoder and a decoder. The encoder learns the patterns
    in the data and the decoder tries to recreate them.
  prefs: []
  type: TYPE_NORMAL
- en: The encoder has multiple neural network layers. In transformers, each layer
    uses self-attention, allowing the encoder to understand how the different parts
    of the sentence fit together and understand the context.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a quick version of the transformer process:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Encoder network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Uses multiple layers of neural networks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Each layer employs self-attention to understand relationships between sentence
    parts and context.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Creates a compressed representation of the input.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Decoder network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Utilizes the encoder’s representation for generating new outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Employs multiple layers with cross-attention for information exchange with the
    encoder.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Generates meaningful outputs such as translations, summaries, or answers based
    on input.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Encoder-decoder partnership:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Combined, they power the transformer for various tasks with high accuracy and
    flexibility.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For example, Microsoft Bing leverages GPT-4, a transformer model, to understand
    user intent and context beyond keywords for delivering relevant search results.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Beyond keywords:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bing transforms from a search engine to an AI-powered copilot using GPT-4.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: It interprets questions and requests by analyzing context and intent, not just
    keywords.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For example, instead of only providing ingredient lists, it recommends personalized
    recipes considering dietary needs and skill levels.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'From links to understanding:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bing evolves beyond finding links to comprehending user needs and delivering
    relevant, helpful information .
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next is the detailed version of the Transformer process.
  prefs: []
  type: TYPE_NORMAL
- en: How an LLM processes a piece of text
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The encoder produces a compressed representation of the input. This allows the
    decoder to not only consider its own outputs but also look back at the encoder’s
    representation, which contains a representation of the whole input sequence for
    guidance. This is used by the decoder for each step of its output generation.
  prefs: []
  type: TYPE_NORMAL
- en: The decoder uses output from the encoder to generate a new output sequence.
    Because of Transformers, modern LLMs can hold entire sentences or paragraphs in
    their attention, not just one word at a time like RNNs.
  prefs: []
  type: TYPE_NORMAL
- en: Again, this section has lots of layers but, this time, there is cross-attention.
  prefs: []
  type: TYPE_NORMAL
- en: This back-and-forth conversation between the decoder and the encoder’s compressed
    knowledge empowers the decoder to generate meaningful and relevant outputs, such
    as translating a sentence to another language, summarizing a paragraph, or answering
    a question based on the input.
  prefs: []
  type: TYPE_NORMAL
- en: Together, the encoder and decoder form the powerhouse of the transformer, enabling
    it to perform a wide range of tasks with remarkable accuracy and flexibility.
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft’s Bing search engine uses GPT-4 to deliver more relevant search results,
    understanding your intent and context beyond just keywords.
  prefs: []
  type: TYPE_NORMAL
- en: Bing has gone from a search engine to an AI-powered copilot with the help of
    GPT-4. This powerful language model acts as Bing’s brain, understanding your questions
    and requests not just through keywords, but by analyzing the context and intent.
  prefs: []
  type: TYPE_NORMAL
- en: You can, for example, ask for a recipe instead of just ingredients; GPT-4 scours
    the web, considers your dietary needs and skill level, and then presents a personalized
    selection. It’s like having a knowledgeable friend helping you navigate the vast
    ocean of information. So, Bing isn’t just about finding links anymore; it’s about
    understanding what you truly need and delivering it in a way that’s relevant and
    helpful ( [https://www.bing.com/](https://www.bing.com/) ).
  prefs: []
  type: TYPE_NORMAL
- en: 'The whole process of getting a paragraph into an LLM goes like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tokenization
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Word-to-number conversion (words given indices: 1, 2, 3, 4…)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Numbers are turned into vectors
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Contextual embedding
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Context vectors are formed
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Attention vectors are formed and fed into final blocks
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Subsequent words are predicted
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ( *ChatGPT* , *Gemini* , *Panuganty* , *Aakanksha* ).
  prefs: []
  type: TYPE_NORMAL
- en: With this framework in your subconscious, we can go through the details of the
    stages. When you pay for ChatGPT questions and answers (more for developers),
    you pay by thousands of tokens. Tokens are where the sentences are split up into
    words and punctuation or tokenized. Tokens are turned into numbers (indices) and
    those are put into vectors, as the maths happens more easily with vectors or context
    vectors. The attention layers show the model where to focus in each sentence,
    and the next word can be predicted.
  prefs: []
  type: TYPE_NORMAL
- en: This process is needed to input the words and sentences into the transformer
    model to train it and to query it to get responses.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before tokenization, the data (sentence, paragraph, etc.) would need to be
    cleaned and normalized: remove special characters, lowercase everything, and some
    other basic cleaning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of a paragraph for tokenization:'
  prefs: []
  type: TYPE_NORMAL
- en: “ *The game Fallout stands out as a distinctive and immersive gaming experience
    when compared to Fortnite. Fallout’s strength lies in its rich narrative, offering
    players a post-apocalyptic world filled with intricate storytelling, character
    development, and meaningful choices. Unlike Fortnite’s fast-paced battle royale
    format, Fallout provides a vast open-world exploration, encouraging players to
    delve into a detailed and atmospheric environment. The emphasis on role-playing
    and decision-making in Fallout adds layers of complexity, contributing to a more
    profound* *gaming engagement.* ”
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, here is that same paragraph tokenized ( *ChatGPT* ):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Sentences can be tokenized with **BertTokenizer** ( *Metzger* ):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, a word becomes an index by word-to-number conversion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Indices become vectors, as defined by pre-trained representations of words
    that come from training on the huge datasets mentioned earlier. This comes from
    Word2Vec, GloVe or FastText, ELMo, or the all-famous BERTs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**"The": [0.2, 0.8, -** **0.5, 0.3]**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**"game": [0.5, -0.7,** **0.1, 0.6]**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**"Fallout": [0.9, 0.4, -** **0.2, -0.1]**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**"stands": [-0.3, 0.6,** **0.7, -0.5]**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**"out": [-0.7, 0.2, -** **0.4, 0.9]**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**"as": [0.3, 0.1, -** **0.6, 0.4]**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The size of the vectors depends on the number of dimensions of the model. The
    preceding model implies a four-dimensional model, which is very small, just for
    this simple explanation.
  prefs: []
  type: TYPE_NORMAL
- en: The model with only two dimensions might have **"woman"** in the context of
    **"man"** or **"fast"** in the context of **"slow"** .
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we have contextual embedding: what is the environment of the word?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples of the sort of thing that would happen:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sentence 1, *The game Fallout stands out...* : Embedding might emphasize aspects
    of distinctiveness and gaming experience'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sentence 2, *Fallout’s strength lies in its rich narrative...* : Embedding
    might focus on storytelling and narrative elements'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sentence 3, *Unlike Fortnite’s fast-paced format, Fallout provides...* : Embedding
    might highlight the contrast with another game and world exploration aspects'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As vectors, that would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The: [0.12, 0.34, 0.56, 0.21, -0.05, ..., 0.90] ( 300 values)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'game: [0.78, 0.21, -0.45, 0.10, 0.83, ..., 0.68] ( 300 values)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fallout: [0.90, -0.10, 0.05, 0.75, 0.43, ..., -0.22] ( 300 values)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are 300 dimensions because that enables the model to capture rather subtle
    semantic relationships but would also require more training data and computational
    resources.
  prefs: []
  type: TYPE_NORMAL
- en: This could be done with only 50 dimensions if the dataset were small, and you
    didn’t want to spend a lot of time and money computing it all.
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT uses reinforcement learning from human feedback
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ChatGPT stands out among other LLMs due to its ability to continuously improve
    through a process called **reinforcement learning from human feedback** ( **RLHF**
    ). This means it doesn’t just learn from massive datasets of text and code but
    also incorporates direct feedback from human users.
  prefs: []
  type: TYPE_NORMAL
- en: When a new GPT model is trained (call it GPT-X for any GPT model), before being
    released to the public, users interact with GPT-X, asking questions or giving
    instructions. After receiving a response, they can express approval or disapproval
    through various methods, such as thumbs-up/down ratings or explicit feedback prompts.
    This valuable input directly affects how the GPT-X model refines its internal
    model, prioritizing responses that resonate with humans and minimizing those that
    miss the mark.
  prefs: []
  type: TYPE_NORMAL
- en: Think of it like training a puppy. Just as rewards encourage desired behaviors,
    positive feedback in RLHF reinforces helpful and accurate responses within GPT-X.
    Over time, through countless interactions and feedback loops, GPT-X fine-tunes
    its responses to be more informative, engaging, and aligned with human preferences.
    This human-in-the-loop approach sets GPT models apart, allowing them to adapt
    and learn dynamically, continuously evolving their capabilities based on real-world
    interactions.
  prefs: []
  type: TYPE_NORMAL
- en: This is how the researchers and developers attempt to make the AI ethical and
    moral, according to their understanding of human morals, which will not agree
    with everybody, but do agree with common culture, including laws.
  prefs: []
  type: TYPE_NORMAL
- en: Other people like to make sure uncensored LLMs exist that don’t encourage the
    politics of the LLM developers such as Californian tech companies.
  prefs: []
  type: TYPE_NORMAL
- en: This process should stop the AI from helping anybody to do anything violent/illegal,
    such as constructing weapons or illegally hacking into an organization’s website
    or servers.
  prefs: []
  type: TYPE_NORMAL
- en: While the specifics of RLHF implementation remain proprietary, its impact is
    evident in ChatGPT’s ability to handle diverse conversation styles, generate different
    creative text formats, and provide informative answers. As RLHF technologies advance,
    we can expect LLMs such as ChatGPT to become even more adept at understanding
    and responding to human needs, blurring the lines between machine and human communication.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are expensive
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many tech company players have been working to create and train their own LLMs
    or chatbots to ride this wave of innovation for money and control. LLMs of today,
    2024, require an enormous amount of training and this takes enormous piles of
    cash. OpenAI took funding of about $13 billion when Microsoft bought shares in
    OpenAI, and much of this was likely used on training the GPT family of LLMs on
    Microsoft’s own Azure cloud servers ( *Sanman* ).
  prefs: []
  type: TYPE_NORMAL
- en: Cash and cooling (energy) are required to train and run LLMs, so it’s a good
    thing deep learning models can be used to save energy and reduce pollution. DeepMind
    once saved Google data centers 40% of their cooling bill! They did this by developing
    a deep learning model that made suggestions for how to modify how the cooling
    systems worked. Later, the DeepMind model was set to just run the cooling systems
    directly [Hooper 2021 and DeepMind]. These Google data centers have their own
    dedicated power stations, so this is a lot of energy saved and money and pollution
    saved too!
  prefs: []
  type: TYPE_NORMAL
- en: Speaking of numbers and calculations, let’s briefly look at what classes of
    mathematics are involved in LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: A note on the mathematics of LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Getting into the mathematical center of LLMs can be a bit of work, but understanding
    their core principles reveals a lot about how the most powerful and widely used
    AIs today function. So, if you want to make these AI models and research them,
    the mathematics is very interesting:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Foundations in linear algebra** : The bedrock of LLMs lies in linear algebra,
    where matrices and vectors rule. Words are mapped to high-dimensional vectors,
    capturing their meanings and relationships within a vast semantic space. Each
    word is a point in a multi-dimensional space, with related words clustering closer
    together.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Backpropagation and optimization** : Training LLMs requires massive datasets
    and sophisticated optimization algorithms. One powerful tool is backpropagation,
    a mathematical technique that calculates the error gradient – how much each parameter
    in the model contributes to the overall deviation from the desired output. By
    iteratively adjusting these parameters based on the error gradient, the LLM learns
    and improves its predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Loss functions and metrics** : To evaluate the performance of an LLM, we
    need quantitative measures. Loss functions define how much the model’s output
    deviates from the desired outcome, while metrics such as accuracy, perplexity,
    and BLEU score assess its ability to generate fluent, contextually appropriate
    text:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BLEU score** stands for **Bilingual Evaluation Understudy score** , which
    is from translation but can be used as a way to compare AI-generated translations
    with reference translations. It can be calculated with the NLTK code library in
    Python using the **sentence_bleu()** function ( *Brownlee_BLEU* ).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Beyond basic maths** : The mathematics of LLMs extends far beyond these core
    principles. Techniques such as regularization, dropout, and gradient clipping
    help prevent overfitting and improve generalization. RNNs add memory capabilities,
    allowing the model to learn from longer sequences of data. The world of mathematics
    is constantly evolving, pushing the boundaries of what LLMs can achieve.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transformers and attention** : This mathematical architecture forms the engine
    of modern LLMs. In Transformers, the mechanism for calculating attention scores
    involves dot products between query and key vectors. While in LSTMs, each time
    step acts as both *query* and *key* , Transformers separate these roles: The query
    originates from the current token’s representation, while the keys are derived
    from the value representations of all tokens in the sequence. This distinction
    helps to compute attention scores that indicate how significant or relevant each
    token is within its context. Transformers also use values, which are also derived
    from the word embeddings of all tokens, carrying the actual information from each
    token:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s look at an example sentence, *The cat played with* *a ball* .
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In a Transformer’s attention mechanism, the following applies:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Words and their meanings are usually represented numerically using embeddings,
    such as Word2Vec or GloVe vectors
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The query would be derived from the representation of the current token; let’s
    say it’s the word *played*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The keys are calculated from the value representations of all tokens in the
    sequence, so we’d have keys for *The* , *cat* , *played* , *with* , *a* , and
    *ball*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, each query would do a dot product with every key vector
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: These dot products would then be used to calculate the attention scores
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Ultimately, this process helps highlight which words in the sequence are most
    relevant or important within context, enhancing the Transformer’s ability to understand
    text
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: While the maths might seem daunting, it’s crucial to remember that it’s just
    a tool. The true power lies in how these algorithms are woven together to create
    models capable of remarkable feats of language processing. As mathematical models
    evolve and datasets grow, LLMs promise to push the boundaries of language, blurring
    the lines betwee n human and machine communication in ever-fascinating ways [
    Llama3, Gemini].
  prefs: []
  type: TYPE_NORMAL
- en: Applications of LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The LLM revolution is reaching its virtual tentacles into every corner of life,
    from writing your college essay to generating personalized Coca-Cola ads and customer
    services. Here’s a quick peek into just 16 diverse applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '*DIYVA* : Designs stunning visuals and logos, making even the artistically
    challenged look like Picassos ( [https://diyva.life/](https://diyva.life/) ).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*LimeWire* : Conjures up unique AI-generated artwork, turning your wildest
    creative visions into reality. Start here: [https://limewire.com/studio?referrer=ml736b1k7k](https://limewire.com/studio?referrer=ml736b1k7k)
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Coca-Cola* : Creates targeted ad campaigns, crafting personalized marketing
    messages for each individual Coke-sipper ( [https://www.coca-cola.com/gb/en](https://www.coca-cola.com/gb/en)
    ).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Slack* : Transcribes meetings and automatically summarizes key points, saving
    you precious time and attention ( [https://slack.com/intl/en-gb](https://slack.com/intl/en-gb)
    ).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Octopus Energy* : Predicts your energy usage and suggests personalized plans,
    optimizing your home’s power with LLM intelligence ( [https://octopus.energy/](https://octopus.energy/)
    ).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Cheggmate* : Offers AI-powered tutoring tailored to each student’s specific
    needs, making learning more efficient and engaging ( [https://www.cheggmate.ai/](https://www.cheggmate.ai/)
    ).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Freshworks* : Automates customer service, analyzing chats and offering solutions
    before agents even blink ( [https://www.cheggmate.ai/](https://www.cheggmate.ai/)
    ).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Udacity* : Designs personalized learning paths, guiding you through the tech
    jungle with LLM-powered recommendations ( [https://www.udacity.com/](https://www.udacity.com/)
    ).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Zalando* : This European fashion retailer uses LLMs to generate personalized
    product recommendations based on user preferences and behavior ( [https://www.zalando.co.uk/](https://www.zalando.co.uk/)
    ).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Headspace* : Headspace leverages LLMs to personalize guided meditations, adapting
    practices to your mood, sleep patterns, and personal goals ( [https://www.headspace.com/](https://www.headspace.com/)
    ).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Spotify* : Spotify’s Discover Weekly playlists and other personalized recommendations
    are generated by LLMs, analyzing your listening habits and music preferences to
    curate an ever-evolving soundtrack for your life ( [https://open.spotify.com/](https://open.spotify.com/)
    ).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Peloton* : Peloton’s AI coaches utilize LLMs to deliver dynamic real-time
    feedback during workouts, tailoring prompts and challenges to your individual
    performance and fitness goals ( [https://www.onepeloton.co.uk/](https://www.onepeloton.co.uk/)
    ).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Baidu’s WenLan* : Helps Chinese businesses analyze customer reviews and personalize
    marketing campaigns; a local LLM giant ( [https://ir.baidu.com/company-overview](https://ir.baidu.com/company-overview)
    ).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*NVIDIA Megatron-Turing NLG* : Generates different creative text formats such
    as poems, code, scripts, and so on, pushing the boundaries of LLM expressiveness
    ( [https://gpt3demo.com/apps/mt-nlg-by-microsoft-and-nvidia-ai](https://gpt3demo.com/apps/mt-nlg-by-microsoft-and-nvidia-ai)
    ).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Grammarly* : This writing assistant uses LLMs to analyze your writing, offering
    real-time grammar and style suggestions for clearer, more impactful communication'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ( [https://app.grammarly.com/](https://app.grammarly.com/) ).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*DeepBrain AI:* Utilizes their own LLM, along with sophisticated animation
    and voice-sy nthesis techniques ( [https://www.deepbrain.io/aistudios?via=abtnews](https://www.deepbrain.io/aistudios?via=abtnews)
    ). ( *ForbesMarr* , *f6s* , *Gemini* .)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we covered what ChatGPT is and what LLMs in general are, the
    origins of some widely used LLMs such as BERT, the GPT family, LlaMDA, LlaMA,
    and modern LLMs such as GPT-4 and Gemini. We looked at some architecture of LLMs
    and transformers. We had a go at fully processing a sentence in the way an LLM
    model would: tokenizing, Word2Vec contextual embedding, and more. We also touched
    on the types of mathematics involved and the applications of this fantastic technology
    deployed by companies.'
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully, you now understand the nature of LLMs such as ChatGPT/Gemini; understand
    the architectures of LLMs; understand some mathematics of LLMs; and are enlightened
    about competition in the field and how to teach LLMs to others.
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 2*](B21009_02.xhtml#_idTextAnchor051) , we will look at the advantages
    of coding with L LMs, planning your LLM-powered coding, doing some coding with
    LLMs, and making it work for you.
  prefs: []
  type: TYPE_NORMAL
- en: Bibliography
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Amari* : “ *Learning Patterns and Pattern Sequences by Self-Organizing Nets
    of Threshold Elements* ”, S. I. Amari [https://ieeexplore.ieee.org/document/1672070](https://ieeexplore.ieee.org/document/1672070)
    in IEEE Transactions on Computers, vol. C-21, no. 11, pp. 1197-1206, Nov. 1972,
    doi: 10.1109/T-C.1972.223477'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'keywords: {Associative memory, brain model, concept formation, logic nets of
    threshold elements, self-organization, sequential recalling, stability of state
    transition}'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*AnswerIQ* : “ *25+ Google Bard Statistics 2024 (Usage, Traffic & Cost)* ”,
    Paul Rogers: [https://www.answeriq.com/google-bard-statistics/](https://www.answeriq.com/google-bard-statistics/)
    6th Jan 2024'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Brownlee_LLMs* : “ *What are Large Language Models* ”, Adrian Tam: [https://machinelearningmastery.com/what-are-large-language-models/](https://machinelearningmastery.com/what-are-large-language-models/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Brownlee_BLEU* : “ *A Gentle Introduction to Calculating the BLEU Score for
    Text in Python* ”, Jason Brownlee, [https://machinelearningmastery.com/calculate-bleu-score-for-text-python/](https://machinelearningmastery.com/calculate-bleu-score-for-text-python/
    )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Brush* : “ *History of the Lenz-Ising Model* ”, Stephen G. Brush, 1967, Reviews
    of Modern Physics. 39 (4): 883–893. [https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.39.883](https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.39.883)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*ChatGPT* :” *ChatGPT* ”, OpenAI, [https://chat.openai.com/](https://chat.openai.com/
    )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Chellapilla2006* : “ *High Performance Convolutional Neural Networks for Document
    Processing* ”, Kumar Chellapilla; Sid Puri; Patrice Simard (2006). In Lorette,
    Guy (ed.). Tenth International Workshop on Frontiers in Handwriting Recognition.
    Suvisoft. Archived from the original on 2020-05-18. Retrieved 2016-03-14. [https://inria.hal.science/inria-00112631/document](https://inria.hal.science/inria-00112631/document
    )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*CodeLlama* *2023* : “ *Introducing Code Llama, an AI Tool for Coding* ”, Meta,
    [https://about.fb.com/news/2023/08/code-llama-ai-for-coding/](https://about.fb.com/news/2023/08/code-llama-ai-for-coding/
    )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*DeepMind* : “ *DeepMind AI Reduces Google Data Centre Cooling Bill by 40%*
    ”, Richard Evans, Jim Gao: [https://deepmind.google/discover/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-by-40/](https://deepmind.google/discover/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-by-40/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*encord* : “ *Stephen Oladele, GPT-4o vs. Gemini 1.5 Pro vs. Claude 3 Opus:
    Multimodal AI Model* *Comparison* ”, [https://encord.com/blog/gpt-4o-vs-gemini-vs-claude-3-opus/#:~:text=Code%20Generation%20Capability,GPT%2D4o%20in%20this%20domain](https://encord.com/blog/gpt-4o-vs-gemini-vs-claude-3-opus/#:~:text=Code%20Generation%20Capability,GPT%2D4o%20in%20this%20domain
    )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*ForbesMarr* : “ *10 Amazing Real-World Examples Of How Companies Are Using
    ChatGPT In 2023* ”, Bernard Marr: [https://www.forbes.com/sites/bernardmarr/2023/05/30/10-amazing-real-world-examples-of-how-companies-are-using-chatgpt-in-2023/?sh=3fe5f9601441](https://www.forbes.com/sites/bernardmarr/2023/05/30/10-amazing-real-world-examples-of-how-companies-are-using-chatgpt-in-2023/?sh=3fe5f9601441
    )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*f62* : “ *100 top ChatGPT companies and startups in 2024* ”, f62: [https://www.f6s.com/companies/chatgpt/mo](https://www.f6s.com/companies/chatgpt/mo
    )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Fukushima1980* : “ *Neocognitron: A self-organizing neural network model for
    a mechanism of pattern recognition unaffected by shift in position* ”, Kunihiko
    Fukushima, J. Biological Cybernetics., [https://doi.org/10.1007/BF00344251](https://doi.org/10.1007/BF00344251)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*GeekCultureBERT* : “ *4 Crucial Things to Know about GPT-4: You should know
    these to use GPT-4* ”, Tirendaz AI, [https://medium.com/geekculture/an-overview-of-gpt-4-in-4-steps-867bb81b31e3](https://medium.com/geekculture/an-overview-of-gpt-4-in-4-steps-867bb81b31e3
    )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Gemini* : “ *Gemini* ”, Google Research, [https://gemini.google.com/](https://gemini.google.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*GeminiTeam* : “ *Gemini: A Family of Highly Capable Multimodal Models* ”,
    Gemini Team, Google, [https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf](https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf
    )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hochreiter1997* : “ *Long Short-Term Memory* ”, Sepp Hochreiter, Sepp; Jürgen
    Schmidhuber, Jürgen (1997-11-01). Neural Computation. 9 (8): 1735–1780. doi:10.1162/neco.1997.9.8.1735.
    PMID 9377276. S2CID 1915014. [https://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory?redirectedFrom=fulltexthttps://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory?redirectedFrom=fulltext](https://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory?redirectedFrom=fulltexthttps://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory?redirectedFrom=fulltext
    )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hooper* : “ *How to Spend $1 trillion* ”, Rowan Hooper ( 2021), [https://www.goodreads.com/en/book/show/54823535](https://www.goodreads.com/en/book/show/54823535)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*HuggingFace* : “ *describeai/gemini* ”, Hugging Face, [https://huggingface.co/describeai/gemini](https://huggingface.co/describeai/gemini)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Investors.com* : “ *OpenAI Circus Continues As Ousted Chief Executive Returns
    As Boss* ”, Patrick Seitz: [https://www.investors.com/news/technology/microsoft-stock-rises-on-sam-altman-return-to-openai/](https://www.investors.com/news/technology/microsoft-stock-rises-on-sam-altman-return-to-openai/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*LeCun1989* : “ *Backpropagation Applied to Handwritten Zip Code Recognition*
    ”, Y. LeCun; B.J..S. Boser; J.S. Denker; D. Henderson; R.E. Howard; W. Hubbard;
    L.D. Jackel (1989) Advances in Neural Information Processing Systems, 1, 323-331.
    [https://doi.org/10.1162/neco.1989.1.4.541](https://doi.org/10.1162/neco.1989.1.4.541)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Life_Architecture* : “ *Amazon Olympus (large language model due 2024H2* )”,
    Alan D. Thompson, [https://lifearchitect.ai/olympus/](https://lifearchitect.ai/olympus/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Llama3* : “ *Llama3 8b* ”, Meta, **https://llama.meta.com/llama3/**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Mandlik* : “ *How GPT-4 Image Works* ?”, Sanman Mandlik, [https://sanmancreations.medium.com/how-gpt-4-image-works-4d7a87cf4497#:~:text=GPT%2D4%20Image%3A%20A%20Fusion,a%20pioneering%20advancement%20in%20AI](https://sanmancreations.medium.com/how-gpt-4-image-works-4d7a87cf4497#:~:text=GPT%2D4%20Image%3A%20A%20Fusion,a%20pioneering%20advancement%20in%20AI)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Megatron-Turing 2022* : “ *Using DeepSpeed and Megatron to Train Megatron-Turing
    NLG 530B, A Large-Scale Generative Language Model* ”, Shaden Smith; Mostofa Patwary;
    Brandon Norick; Patrick LeGresley; Samyam Rajbhandari; Jared Casper; Zhun Liu;
    Shrimai Prabhumoye; George Zerveas; Vijay Korthikanti; Elton Zhang; Rewon Child;
    Reza Yazdani Aminabadi; Julie Bernauer; Xia Song; Mohammad Shoeybi; Yuxiong He;
    Michael Houston; Saurabh Tiwary; Bryan Catanzaro: [https://arxiv.org/abs/2201.11990](https://arxiv.org/abs/2201.11990)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Menon* : “ *Introduction to Large Language Models and the Transformer Architecture*
    ”, Pradeep Menon, [https://rpradeepmenon.medium.com/introduction-to-large-language-models-and-the-transformer-architecture-534408ed7e61](https://rpradeepmenon.medium.com/introduction-to-large-language-models-and-the-transformer-architecture-534408ed7e61)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Metzger* : “ *A Beginner’s Guide to Tokens, Vectors, and Embeddings in NLP*
    ”, Sascha Metzger, [https://medium.com/@saschametzger/what-are-tokens-vectors-and-embeddings-how-do-you-create-them-e2a3e698e037](mailto:https://medium.com/@saschametzger/what-are-tokens-vectors-and-embeddings-how-do-you-create-them-e2a3e698e037)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*MotiveX_Gemini* : “ *Is GEMINI AI The Best? - SHOCKING Power (GPT-4 HUMBLED)*
    ”, MotiveX YouTube channel, [https://youtu.be/JvA9os8Oq20?t=144](https://youtu.be/JvA9os8Oq20?t=144)
    , 6 Feb 2024'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*OpenAI_CLIP* : “ *CLIP: Connecting text and images* ”, OpenAI, [https://openai.com/index/clip/](https://openai.com/index/clip/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*OpenAI_DALL.E* : “ *DALL.E: Creating images from text* ” OpenAI, [https://openai.com/index/dall-e/](https://openai.com/index/dall-e/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*OpenAI-GPT-4o* : “ *Hello GPT-4o* ”, OpenAI, [https://openai.com/index/hello-gpt-4o/](https://openai.com/index/hello-gpt-4o/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*OpenAI_GPT4Turbo* : “ *New models and developer products announced at DevDay
    GPT-4 Turbo with 128K context and lower prices, the new Assistants API, GPT-4
    Turbo with Vision, DALL·E 3 API, and more* ”, OpenAI, [https://openai.com/blog/new-models-and-developer-products-announced-at-devday](https://openai.com/blog/new-models-and-developer-products-announced-at-devday)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*OpenAI_LP* : “ *OpenAI LP* ”, OpenAI, [https://openai.com/index/openai-lp/](https://openai.com/index/openai-lp/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*OpenAIStructure* : “ *Our Structure* ”, OpenAI, [https://openai.com/our-structure/](https://openai.com/our-structure/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*OpenLMR* : “ *OpenLLaMA: An Open Reproduction of LLaMA* ”, Xinyang (Young)
    Geng; Hao Liu; Martin, Jul, [https://github.com/openlm-research/open_llama](https://github.com/openlm-research/open_llama)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Panuganty* : “ *From Words to Vectors: Inside the LLM Transformer Architecture*
    ”, Harika Panuganty: [https://medium.com/@harikapanuganty/from-words-to-vectors-inside-the-llm-transformer-architecture-50275c354bc4](mailto:https://medium.com/@harikapanuganty/from-words-to-vectors-inside-the-llm-transformer-architecture-50275c354bc4)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Patil* : “ *Top 5 Pre-trained Word Embeddings* ”, Aakanksha Patil, [https://patil-aakanksha.medium.com/top-5-pre-trained-word-embeddings-20de114bc26](https://patil-aakanksha.medium.com/top-5-pre-trained-word-embeddings-20de114bc26)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*ProjectPro* : “ *ChatGPT vs Google BARD-Battle of the Large Language Models*
    ”, Manika, [https://www.projectpro.io/article/chatgpt-vs-google-bard/815](https://www.projectpro.io/article/chatgpt-vs-google-bard/815)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*SkillLeapAI* : “ *How to Use Google Gemini in Bard - Including new prompts*
    ”, Skill Leap AI YouTube channel, [https://www.youtube.com/watch?v=9qszKWO68wQ](https://www.youtube.com/watch?v=9qszKWO68wQ)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Vaswani* : “ *Attention Is All You Need* ”, Ashish Vaswani; Noam Shazeer;
    Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N. Gomez; Lukasz Kaiser and Illia
    Polosukhin, [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Wagh* : “ *What’s new in GPT-4: An Overview of the GPT-4 Architecture and
    Capabilities of Next-Generation AI* ”, Amol Wagh, [https://medium.com/@amol-wagh/whats-new-in-gpt-4-an-overview-of-the-gpt-4-architecture-and-capabilities-of-next-generation-ai-900c445d5ffe](mailto:https://medium.com/@amol-wagh/whats-new-in-gpt-4-an-overview-of-the-gpt-4-architecture-and-capabilities-of-next-generation-ai-900c445d5ffe)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Watson 2023* : “Op *en Llama Unleashed: Revolutionizing AI for Business &*
    *Beyond!* [https://medium.com/nextgen-tech/open-llama-unleashed-revolutionizing-ai-for-business-beyond-18de67aa0b9d](https://medium.com/nextgen-tech/open-llama-unleashed-revolutionizing-ai-for-business-beyond-18de67aa0b9d)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Wiki_Gemini* : “ *Gemini (chatbot)* ”, Wikipedia, [https://en.wikipedia.org/wiki/Gemini_(chatbot)](https://en.wikipedia.org/wiki/Gemini_(chatbot))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Wiki_GPT3* : “ *GPT-3* ”, Wikipedia, [https://en.wikipedia.org/wiki/GPT-3](https://en.wikipedia.org/wiki/GPT-3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Wiki_GPT4* : “ *GPT-4* ”, , Wikipedia, [https://en.wikipedia.org/wiki/GPT-4](https://en.wikipedia.org/wiki/GPT-4)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Wiki_llama* : (2024), “ *LLaMA* ”, Wikipedia, **https://en.wikipedia.org/wiki/LLaMA**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Wiki_LSTM* : “ *Recurrent Neural Network* ”, Wikipedia, [https://en.wikipedia.org/wiki/Recurrent_neural_network#:~:text=Long%20short%2Dterm%20memory%20(LSTM,models%20in%20certain%20speech%20applications](https://en.wikipedia.org/wiki/Recurrent_neural_network#:~:text=Long%20short%2Dterm%20memory%20(LSTM,models%20in%20certain%20speech%20applications)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Yang2023* : “ *Harnessing the Power of LLMs in Practice: A Survey on ChatGPT
    and Beyond* ”, Jingfeng Yang; Hongye Jin; Ruixiang Tang; Xiaotian Han; Qizhang
    Feng; Haoming Jiang; Bing Yin and Xia Hu, [https://arxiv.org/abs/2304.13712](https://arxiv.org/abs/2304.13712)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Zahere* : “ *How ChatGPT Works: The Architectural Details You Need to Know*
    ”, Zahiruddin Tavargere, [https://zahere.com/how-chatgpt-works-the-architectural-details-you-need-to-know](https://zahere.com/how-chatgpt-works-the-architectural-details-you-need-to-know)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
