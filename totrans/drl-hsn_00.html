<html><head></head><body>
<div id="sbo-rt-content"><section class="level2 chapterHead" id="preface">
<h1 class="mainHeading"><span id="x1-6000"/>Preface</h1>
<p>This book is on <span class="cmbx-10x-x-109">reinforcement learning </span>(<span class="cmbx-10x-x-109">RL</span>), which is a subfield of <span class="cmbx-10x-x-109">machine</span> <span class="cmbx-10x-x-109">learning </span>(<span class="cmbx-10x-x-109">ML</span>); it focuses on the general and challenging problem of learning optimal behavior in complex environments. The learning process is driven only by the reward value and observations obtained from the environment. This model is very general and can be applied to many practical situations, from playing games to optimizing complex manufacturing processes. We largely focus on <span class="cmbx-10x-x-109">deep RL </span>in this book, which is RL that leverages <span class="cmbx-10x-x-109">deep learning </span>(<span class="cmbx-10x-x-109">DL</span>) methods.</p>
<p>Due to its flexibility and generality, the field of RL is developing very quickly and attracting lots of attention, both from researchers who are trying to improve existing methods or create new methods and from practitioners interested in solving their problems in the most efficient way.</p>
<section class="level3 likesectionHead" id="why-i-wrote-this-book">
<h1 class="heading-1" id="sigil_toc_id_5"><span id="x1-7000"/>Why I wrote this book</h1>
<p>There is a lot of ongoing research activity in the RL field all around the world. New research papers are being published almost every day, and a large number of DL conferences, such as <span class="cmbx-10x-x-109">Neural Information Processing Systems</span> (<span class="cmbx-10x-x-109">NeurIPS</span>) or the <span class="cmbx-10x-x-109">International Conference on Learning Representations</span> (<span class="cmbx-10x-x-109">ICLR</span>), are dedicated to RL methods. There are also several large research groups focusing on the application of RL methods to robotics, medicine, multi-agent systems, and others.</p>
<p>However, although information about the recent research is widely available, it is too specialized and abstract to be easily understandable. Even worse is the situation surrounding the practical aspect of RL, as it is not always obvious how to make the step from an abstract method described in its mathematics-heavy form in a research paper to a working implementation solving an actual problem.</p>
<p>This makes it hard for somebody interested in the field to get a clear understanding of the methods and ideas behind papers and conference talks. There are some very good blog posts about various aspects of RL that are illustrated with working examples, but the limited format of a blog post allows authors to describe only one or two methods, without building a complete structured picture and showing how different methods are related to each other in a systematic way. This book was written as an attempt to fill this obvious gap in practical and structured information about RL methods and approaches.</p>
</section>
<section class="level3 likesectionHead" id="the-approach">
<h1 class="heading-1" id="sigil_toc_id_6"><span id="x1-8000"/>The approach</h1>
<p>A key aspect of the book is its orientation to practice. Every method is implemented for various environments, from the very trivial to the quite complex. I’ve tried to make the examples clean and easy to understand, which was made possible by the expressiveness and power of PyTorch. On the other hand, the complexity and requirements of the examples are oriented to RL hobbyists without access to very large computational resources, such as clusters of <span class="cmbx-10x-x-109">graphics processing units </span>(<span class="cmbx-10x-x-109">GPUs</span>) or very powerful workstations. This, I believe, will make the fun-filled and exciting RL domain accessible to a much wider audience than just research groups or large artificial intelligence companies. On the other hand, this is still deep RL, so access to a GPU is highly recommended, as computation speed up will make experimentations much more convenient (waiting for several weeks for a single optimization to complete is not very fun). Approximately half of the examples in the book will benefit from being run on a GPU.</p>
<p>In addition to traditional medium-sized examples of environments used in RL, such as Atari games or continuous control problems, this book contains several chapters (10, 13, 14, 19, 20, and 21) that contain larger projects, illustrating how RL methods can be applied to more complicated environments and tasks. These examples are still not full-sized, real-life projects (they would occupy a separate book on their own), but just larger problems illustrating how the RL paradigm can be applied to domains beyond the well-established benchmarks.</p>
<p>Another thing to note about the examples in Parts 1, 2, and 3 of the book is that I’ve tried to make them self-contained, with the source code shown in full. Sometimes this has led to the repetition of code pieces (for example, the training loop is very similar in most of the methods), but I believe that giving you the freedom to jump directly into the method you want to learn is more important than avoiding a few repetitions. All examples in the book are available on GitHub at <a class="url" href="https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-3E/"><span class="cmtt-10x-x-109">https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-3E/</span></a>, and you’re welcome to fork them, experiment, and contribute.</p>
<p>Besides the source code, several chapters (15, 16, 19, and 22) are accompanied by video recordings of the trained model. All these recordings are available in the following YouTube playlist: <a class="url" href="https://youtube.com/playlist?list=PLMVwuZENsfJmjPlBuFy5u7c3uStMTJYz7"><span class="cmtt-10x-x-109">https://youtube.com/playlist?list=PLMVwuZENsfJmjPlBuFy5u7c3uStMTJYz7</span></a>.</p>
</section>
<section class="level3 likesectionHead" id="who-this-book-is-for">
<h1 class="heading-1" id="sigil_toc_id_7"><span id="x1-9000"/>Who this book is for</h1>
<p>This book is ideal for machine learning engineers, software engineers, and data scientists looking to learn and apply deep RL in practice. It assumes familiarity with Python, calculus, and ML concepts. With practical examples and high-level overviews, it’s also suitable for experienced professionals looking to deepen their understanding of advanced deep RL methods and apply them across industries, such as gaming and finance.</p>
</section>
<section class="level3 likesectionHead" id="what-this-book-covers">
<h1 class="heading-1" id="sigil_toc_id_8"><span id="x1-10000"/>What this book covers</h1>
<p><span class="cmti-10x-x-109">Chapter 1, What Is Reinforcement Learning?</span>, contains an introduction to RL ideas and the main formal models.</p>
<p><span class="cmti-10x-x-109">Chapter 2, OpenAI Gym API and Gymansium</span>, introduces the practical aspects of RL, using the open source library Gym and its descendant, Gymnasium.</p>
<p><span class="cmti-10x-x-109">Chapter 3, Deep Learning with PyTorch</span>, gives you a quick overview of the PyTorch library.</p>
<p><span class="cmti-10x-x-109">Chapter 4, The Cross-Entropy Method</span>, introduces one of the simplest methods in RL to give you an impression of RL methods and problems.</p>
<p><span class="cmti-10x-x-109">Chapter 5, Tabular Learning and the Bellman Equation</span>, this chapter opens Part 2 of the book, devoted to value-based family of methods.</p>
<p><span class="cmti-10x-x-109">Chapter 6, Deep Q-Networks</span>, describes <span class="cmbx-10x-x-109">deep Q-networks </span>(<span class="cmbx-10x-x-109">DQNs</span>), an extension of the basic value-based methods, allowing us to solve complicated environments.</p>
<p><span class="cmti-10x-x-109">Chapter 7, Higher-Level RL Libraries</span>, describes the library PTAN, which we will use in the book to simplify the implementations of RL methods.</p>
<p><span class="cmti-10x-x-109">Chapter 8, DQN Extensions</span>, gives a detailed overview of a modern extension to the DQN method, to improve its stability and convergence in complex environments.</p>
<p><span class="cmti-10x-x-109">Chapter 9, Ways to Speed up RL Methods</span>, provides an overview of ways to make the execution of RL code faster.</p>
<p><span class="cmti-10x-x-109">Chapter 10, Stocks Trading Using RL</span>, is the first practical project and focuses on applying the DQN method to stock trading.</p>
<p><span class="cmti-10x-x-109">Chapter 11, Policy Gradients</span>, opens Part 3 of the book and introduces another family of RL methods that is based on direct policy optimisation.</p>
<p><span class="cmti-10x-x-109">Chapter 12, The Actor-Critic Method: A2C and A3C</span>, describes one of the most widely used policy-based method in RL.</p>
<p><span class="cmti-10x-x-109">Chapter 13, The TextWorld Environment</span>, covers the application of RL methods to interactive fiction games.</p>
<p><span class="cmti-10x-x-109">Chapter 14, Web Navigation</span>, is another long project that applies RL to web page navigation using the MiniWoB++ environment.</p>
<p><span class="cmti-10x-x-109">Chapter 15, Continuous Action Space</span>, opens the <span class="cmti-10x-x-109">advanced RL </span>part of the book and describes the specifics of environments using continuous action spaces and various methods (widely used in robotics).</p>
<p><span class="cmti-10x-x-109">Chapter 16, Trust Regions</span>, is yet another chapter about continuous action spaces describing the trust region set of methods: PPO, TRPO, ACKTR and SAC.</p>
<p><span class="cmti-10x-x-109">Chapter 17, Black-Box Optimization in RL</span>, shows another set of methods that don’t use gradients in their explicit form.</p>
<p><span class="cmti-10x-x-109">Chapter 18, Advanced Exploration</span>, covers different approaches that can be used for better exploration of the environment — a very important aspect of RL.</p>
<p><span class="cmti-10x-x-109">Chapter 19, Reinforcement Learning with Human Feedback</span>, introduces and implements recent approach to guide the process of learning by giving human feedback. This methed is widely used in training <span class="cmbx-10x-x-109">large language models</span> (<span class="cmbx-10x-x-109">LLMs</span>). In this chapter, we’ll implement RLHF pipeline from scratch and check its efficiency.</p>
<p><span class="cmti-10x-x-109">Chapter 20, AlphaGo Zero and MuZero</span>, describes the AlphaGo Zero method and its evolution into MuZero, and applies both these methods to the game Connect 4.</p>
<p><span class="cmti-10x-x-109">Chapter 21, RL in Discrete Optimization</span>, describes the application of RL methods to the domain of discrete optimization, using the Rubik’s cube as an environment.</p>
<p><span class="cmti-10x-x-109">Chapter 22, Multi-Agent RL</span>, introduces a relatively new direction of RL methods for situations with multiple agents.</p>
</section>
<section class="level3 likesectionHead" id="to-get-the-most-out-of-this-book">
<h1 class="heading-1" id="sigil_toc_id_9"><span id="x1-11000"/>To get the most out of this book</h1>
<p>This book is suitable for you if you’re using a machine with at least 32 GB of RAM. A GPU is not strictly required, but an Nvidia GPU is highly recommended. The code has been tested on Linux and macOS. For more details on the hardware and software requirements, refer to Chapter 2.</p>
<p>All the chapters in this book that describe RL methods have the same structure: in the beginning, we discuss the motivation of the method, its theoretical foundation, and the idea behind it. Then, we follow several examples of the method applied to different environments with the full source code.</p>
<p>You can use the book in different ways:</p>
<ul>
<li>
<p>To quickly become familiar with a particular method, you can read only the introductory part of the relevant chapter</p>
</li>
<li>
<p>To get a deeper understanding of the way the method is implemented, you can read the code and the explanations accompanying it</p>
</li>
<li>
<p>To gain a deeper familiarity with the method (which I beleive is the best way to learn) you can try to reimplement the method and make it work, using the provided source code as a reference point</p>
</li>
</ul>
<p>Whichever approach you choose, I hope the book will be useful for you!</p>
</section>
<section class="level3 likesectionHead" id="changes-in-the-third-edition">
<h1 class="heading-1" id="sigil_toc_id_10"><span id="x1-12000"/>Changes in the third edition</h1>
<p>In comparison to the second edition of this book (published in 2020), there are several major changes made to the book’s contents in this new edition:</p>
<ul>
<li>
<p>All the dependencies of code examples have been updated to the recent versions or replaced with better alternatives. For example, OpenAI Gym is not supported anymore, but we have the Farama Foundation Gymnasium fork. Another example is the MiniWoB++ library, which has replaced the MiniWoB and Universe environment.</p>
</li>
<li>
<p>A new chapter on RLHF has been included, and the MuZero method has been added to the chapter on AlphaGo Zero.</p>
</li>
<li>
<p>There are lots of small fixes and improvements — most of the figures have been redrawn to make them clearer and more easily understandable.</p>
</li>
</ul>
<p>To better meet book volume limitations, several chapters were rearranged, which I hope made the book more consistent and easier to read.</p>
</section>
<section class="level3 likesectionHead" id="download-the-example-code-files">
<h1 class="heading-1" id="sigil_toc_id_11"><span id="x1-13000"/>Download the example code files</h1>
<p>The code bundle for the book is hosted on GitHub at <a class="url" href="https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Third-Edition"><span class="cmtt-10x-x-109">https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Third-Edition</span></a>. We also have other code bundles from our rich catalog of books and videos available at <a class="url" href="https://github.com/PacktPublishing/"><span class="cmtt-10x-x-109">https://github.com/PacktPublishing/</span></a>. Check them out!</p>
</section>
<section class="level3 likesectionHead" id="download-the-color-images">
<h1 class="heading-1" id="sigil_toc_id_12"><span id="x1-14000"/>Download the color images</h1>
<p>We also provide a PDF file that has color images of the screenshots/diagrams used in this book. You can download it here: <a class="url" href="https://packt.link/gbp/9781835882702"><span class="cmtt-10x-x-109">https://packt.link/gbp/9781835882702</span></a>.</p>
</section>
<section class="level3 likesectionHead" id="conventions-used">
<h1 class="heading-1" id="sigil_toc_id_13"><span id="x1-15000"/>Conventions used</h1>
<p>There are a number of text conventions used throughout this book. <span class="cmtt-10x-x-109">CodeInText</span>: Indicates code words in text, database table names, folder names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles. For example: ”For the reward table, it is represented as a tuple with <span class="cmtt-10x-x-109">[State</span>, <span class="cmtt-10x-x-109">Action</span>, <span class="cmtt-10x-x-109">State] </span>and for the transition table, it is written as <span class="cmtt-10x-x-109">[State</span>, <span class="cmtt-10x-x-109">Action]</span>.”</p>
<p>A block of code is set as follows:</p>
<div class="tcolorbox" id="tcolobox-1">
<div class="tcolorbox-content">
<pre class="lstinputlisting" id="listing-1"><code>import typing as tt 
import gymnasium as gym 
from collections import defaultdict, Counter 
from torch.utils.tensorboard.writer import SummaryWriter 
 
ENV_NAME = "FrozenLake-v1" 
GAMMA = 0.9 
TEST_EPISODES = 20</code></pre>
</div>
</div>
<p>Any command-line input or output is written as follows:</p>
<pre class="lstlisting" id="listing-2"><code>&gt;&gt;&gt; e.action_space 
Discrete(2) 
&gt;&gt;&gt; e.observation_space 
Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)</code></pre>
<p><span class="cmbx-10x-x-109">Bold</span>: Indicates a new term, an important word, or words that you see on the screen. For instance, words in menus or dialog boxes appear in the text like this. For example: ”The second term is called <span class="cmbx-10x-x-109">cross-entropy</span>, which is a very common optimization objective in deep learning.” Citations are represented using a condensed author–year format within square brackets, similar to [Sut88] or [Kro+11]. You can find the details of the corresponding paper in the <span class="cmti-10x-x-109">Bibliography </span>section at the end of the book.</p>
<div class="tcolorbox infobox" id="tcolobox-2">
<div class="tcolorbox-content">
<p>Warnings or important notes appear like this.</p>
</div>
</div>
<div class="tcolorbox tipbox" id="tcolobox-3">
<div class="tcolorbox-content">
<p>Tips and tricks appear like this.</p>
</div>
</div>
</section>
<section class="level3 likesectionHead" id="get-in-touch">
<h1 class="heading-1" id="sigil_toc_id_14"><span id="x1-16000"/>Get in touch</h1>
<p>Feedback from our readers is always welcome.</p>
<p> <span class="cmbx-10x-x-109">General feedback</span>: Email <span class="url">feedback@packtpub.com </span>and mention the book’s title in the subject of your message. If you have questions about any aspect of this book, please email us at <span class="url">questions@packtpub.com</span>.</p>
<p><span class="cmbx-10x-x-109"/> Errata: Although we have taken every care to ensure the accuracy of our content, mistakes do happen. If you have found a mistake in this book, we would be grateful if you reported this to us. Please visit <a class="url" href="http://www.packtpub.com/submit-errata"><span class="url">http://www.packtpub.com/submit-errata</span></a>, click <span class="cmbx-10x-x-109">Submit Errata</span>, and fill in the form.</p>
<p> <span class="cmbx-10x-x-109">Piracy</span>: If you come across any illegal copies of our works in any form on the internet, we would be grateful if you would provide us with the location address or website name. Please contact us at <span class="url">copyright@packtpub.com </span>with a link to the material.</p>
<p> <span class="cmbx-10x-x-109">If you are interested in becoming an author</span>: If there is a topic that you have expertise in and you are interested in either writing or contributing to a book, please visit <a class="url" href="http://authors.packtpub.com"><span class="cmtt-10x-x-109">http://authors.packtpub.com</span></a>.</p>
</section><p class="eop"/>
<section class="level3 likesectionHead" id="leave-a-review">
<h1 class="heading-1" id="sigil_toc_id_15"><span id="x1-17000"/>Leave a Review!</h1>
<p class="normal">Thank you for purchasing this book from Packt Publishing—we hope you enjoy it! Your feedback is invaluable and helps us improve and grow. Once you’ve completed reading it, please take a moment to leave an <a href="https://packt.link/r/1835882714"><span class="url">Amazon review</span></a>; it will only take a minute, but it makes a big difference for readers like you.</p>
<p>Scan the QR code below to receive a free ebook of your choice.</p>
<div class="center">
<p class="center"><img alt="PIC" height="208" src="../Images/file3.png" width="208"/></p>
</div>
<p class="center"><em class="italic">https://packt.link/NzOWQ</em></p>
</section>
<section class="level3 likesectionHead" id="download-a-free-pdf-copy-of-this-book">
<p class="eop"/>
<h1 class="heading-1" id="sigil_toc_id_16"><span id="x1-18000"/>Download a free PDF copy of this book</h1>
<p>Thanks for purchasing this book!</p>
<p>Do you like to read on the go but are unable to carry your print books everywhere? Is your eBook purchase not compatible with the device of your choice?</p>
<p>Don’t worry; with every Packt book, you now get a DRM-free PDF version of that book at no cost.</p>
<p>Read anywhere, on any device. Search, copy, and paste code from your favorite technical books directly into your application.</p>
<p>The perks don’t stop there! You can get exclusive access to discounts, newsletters, and great free content in your inbox daily.</p>
<p>Follow these simple steps to get the benefits:</p>
<ol>
<li>
<div id="x1-18002x1">
<p>Scan the QR code or visit the link below:</p>
<div class="center">
<p class="center"><img alt="PIC" height="208" src="../Images/file4.png" width="208"/></p>
</div>
<div class="center">
<p class="center"><em>https://packt.link/free-ebook/9781835882702</em></p>
</div>
</div>
</li>
<li>
<div id="x1-18004x2">
<p>Submit your proof of purchase.</p>
</div>
</li>
<li>
<div id="x1-18006x3">
<p>That’s it! We’ll send your free PDF and other benefits to your email address directly.</p>
</div>
</li>
</ol>
</section>
</section>
</div>

<div id="sbo-rt-content"><h1 class="partNumber" style="padding-top:280px;">Part 1</h1>
<h1 class="partTitle" id="sigil_toc_id_426">Introduction to RL</h1>
</div></body></html>