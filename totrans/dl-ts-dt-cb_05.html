<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer024">
			<h1 id="_idParaDest-216" class="chapter-number"><a id="_idTextAnchor306"/>5</h1>
			<h1 id="_idParaDest-217"><a id="_idTextAnchor307"/>Global Forecasting Models</h1>
			<p><a id="_idTextAnchor308"/><a id="_idTextAnchor309"/>In this chapter, we explore various time series forecasting scenarios and learn how to handle them with deep learning. These scenarios include multi-step and multi-output forecasting tasks, and problems involving multiple time series. We’ll cover each of these cases, explaining how to prepare your data, train appropriate neural network models, and <span class="No-Break">validate them.</span></p>
			<p>By the end of this chapter, you should be able to build deep learning forecasting models for different time series datasets. This includes hyperparameter optimization, which is an important stage in <span class="No-Break">model development.</span></p>
			<p>This chapter will guide you through the <span class="No-Break">following recipes:</span></p>
			<ul>
				<li>Multi-step forecasting with multivariate <span class="No-Break">time series</span></li>
				<li>Multi-step and multi-output forecasting with multivariate <span class="No-Break">time series</span></li>
				<li>Preparing multiple time series for a <span class="No-Break">global model</span></li>
				<li>Training a global LSTM with multiple <span class="No-Break">time series</span></li>
				<li>Global forecasting models for seasonal <span class="No-Break">time series</span></li>
				<li>Hyperparameter optimization using <span class="No-Break">Ray Tune</span></li>
			</ul>
			<h1 id="_idParaDest-218"><a id="_idTextAnchor310"/>Technical requirements</h1>
			<p>This chapter requires the following <span class="No-Break">Python libraries:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="source-inline">numpy</strong></span><span class="No-Break"> (1.26.3)</span></li>
				<li><span class="No-Break"><strong class="source-inline">pandas</strong></span><span class="No-Break"> (2.0.3)</span></li>
				<li><span class="No-Break"><strong class="source-inline">scikit-learn</strong></span><span class="No-Break"> (1.4.0)</span></li>
				<li><span class="No-Break"><strong class="source-inline">sktime</strong></span><span class="No-Break"> (0.26.0)</span></li>
				<li><span class="No-Break"><strong class="source-inline">torch</strong></span><span class="No-Break"> (2.2.0)</span></li>
				<li><span class="No-Break"><strong class="source-inline">pytorch-forecasting</strong></span><span class="No-Break"> (1.0.0)</span></li>
				<li><span class="No-Break"><strong class="source-inline">pytorch-lightning</strong></span><span class="No-Break"> (2.1.4)</span></li>
				<li><span class="No-Break"><strong class="source-inline">gluonts</strong></span><span class="No-Break"> (0.14.2)</span></li>
				<li><span class="No-Break"><strong class="source-inline">ray</strong></span><span class="No-Break"> (2.9.2)</span></li>
			</ul>
			<p>You can install these libraries in one go <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">pip</strong></span><span class="No-Break">:</span></p>
			<pre class="console">
pip install -U pandas numpy scikit-learn sktime torch pytorch-forecasting pytorch-lightning gluonts</pre>			<p>The recipes in this chapter will follow a design philosophy based on PyTorch Lightning that provides a modular and flexible way of building and deploying PyTorch models. The code for this chapter can be found at the following GitHub <span class="No-Break">URL: </span><a href="https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook"><span class="No-Break">https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-219"><a id="_idTextAnchor311"/><a id="_idTextAnchor312"/>Multi-step forecasting with multivariate time series</h1>
			<p>So far, we’ve <a id="_idIndexMarker272"/>been working <a id="_idIndexMarker273"/>on forecasting the next value of a single variable of a time series. Forecasting the value of the next observation is referred to as one-step-ahead forecasting. In this recipe, we’ll extend the models we developed in the previous chapter for <span class="No-Break">multi-step-ahead forecasting.</span></p>
			<h2 id="_idParaDest-220"><a id="_idTextAnchor313"/>Getting ready</h2>
			<p>Multi-step ahead forecasting is the process of forecasting several observations in advance. This task is important for reducing the long-term uncertainty of <span class="No-Break">time series.</span></p>
			<p>It turns out that much of the work we did before is also applicable to multi-step forecasting settings. The <strong class="source-inline">TimeSeriesDataSet</strong> class makes it extremely simple to extend the one-step-ahead problem to the <span class="No-Break">multi-step case.</span></p>
			<p>In this recipe, we’ll set the forecasting horizon to <strong class="source-inline">7</strong> and the number of lags <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">14</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
N_LAGS = 7
HORIZON = 14</pre>			<p>In practice, this means the predictive task is to forecast the next 7 days of solar radiation based on the past 14 days <span class="No-Break">of data.</span></p>
			<h2 id="_idParaDest-221"><a id="_idTextAnchor314"/>How to do it…</h2>
			<p>For <a id="_idIndexMarker274"/>multi-step ahead forecasting <a id="_idIndexMarker275"/>problems, two things need to <span class="No-Break">be changed:</span></p>
			<ul>
				<li>One is the output dimension of the neural network model. Instead of <strong class="source-inline">1</strong> (which represents the next value), the output dimension needs to match the number of prediction steps. This is done in the <strong class="source-inline">output_dim</strong> variable of <span class="No-Break">the model.</span></li>
				<li>The prediction length of the data module needs to be set to the forecasting horizon. This is done in the <strong class="source-inline">max_prediction_length</strong> parameter of the <span class="No-Break"><strong class="source-inline">TimeSeriesDataSet</strong></span><span class="No-Break"> class.</span></li>
			</ul>
			<p>These two inputs can be passed to the data and model modules <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
datamodule = MultivariateSeriesDataModule(data=mvtseries,
    n_lags=N_LAGS,
    horizon=HORIZON,
    batch_size=32,
    test_size=0.3)
model = MultivariateLSTM(input_dim=n_vars,
    hidden_dim=32,
    num_layers=1,
    output_dim=HORIZON)</pre>			<p>Then, the training and testing of the model remain <span class="No-Break">the same:</span></p>
			<pre class="source-code">
early_stop_callback = EarlyStopping(monitor="val_loss",
    min_delta=1e-4,
    patience=10,
    verbose=False,
    mode="min")
trainer = Trainer(max_epochs=20, callbacks=[early_stop_callback])
trainer.fit(model, datamodule)
trainer.test(model=model, datamodule=datamodule)</pre>			<p>We <a id="_idIndexMarker276"/>trained the model for 20 epochs <a id="_idIndexMarker277"/>and then evaluated it in the test set, which is retrieved using the data loader defined in the <span class="No-Break">data module.</span></p>
			<h2 id="_idParaDest-222"><a id="_idTextAnchor315"/>How it works…</h2>
			<p>Traditional supervised machine learning models usually learn from a one-dimensional target variable. In forecasting problems, this variable can be, for example, the value of the time series in the next period. However, multi-step-ahead forecasting problems require the prediction of several values at each time. Deep learning models are naturally multi-output algorithms. So, they can handle several target variables with a <span class="No-Break">single model.</span></p>
			<p>Other approaches for multi-step-ahead forecasting often involve creating several models or reusing the same model for different horizons. However, a multi-output approach is preferable because it enables the capture of dependencies among different horizons. This can lead to better forecasting performance, as has been documented in articles such as the following: Taieb, Souhaib Ben, et al., <em class="italic">A review and comparison of strategies for multi-step ahead time series forecasting based on the NN5 forecasting competition</em>. Expert systems with applications 39.8 (<span class="No-Break">2012): 7067-7083</span></p>
			<h2 id="_idParaDest-223"><a id="_idTextAnchor316"/>There’s more…</h2>
			<p>There <a id="_idIndexMarker278"/>are other ways <a id="_idIndexMarker279"/>we could use a deep learning neural network for multi-step-ahead forecasting. Three other popular methods are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="source-inline">Recursive</strong>: Training a neural network for one-step-ahead forecasting and using it recursively to get <span class="No-Break">multi-step forecasts</span></li>
				<li><strong class="source-inline">Direct</strong>: Training one neural network for each <span class="No-Break">forecasting horizon</span></li>
				<li><strong class="source-inline">DirRec</strong>: Training one neural network for each forecasting horizon and feeding the previous forecast as input to the <span class="No-Break">next one</span></li>
			</ul>
			<h1 id="_idParaDest-224"><a id="_idTextAnchor317"/>Multi-step and multi-output forecasting with multivariate time series</h1>
			<p>In <a id="_idIndexMarker280"/>this recipe, we’ll extend <a id="_idIndexMarker281"/>the LSTM model to predict multiple steps of several variables of a multivariate <span class="No-Break">time series.</span></p>
			<h2 id="_idParaDest-225"><a id="_idTextAnchor318"/>Getting ready</h2>
			<p>So far, in this chapter, we have built several models to forecast the future of one particular <a id="_idIndexMarker282"/>variable, solar <a id="_idIndexMarker283"/>radiation. We used the extra variables in the time series to improve the modeling of <span class="No-Break">solar radiation.</span></p>
			<p>Yet, when working with multivariate time series, we’re often interested in forecasting several variables, not just one. A common example occurs when dealing with spatiotemporal data. A spatiotemporal dataset is a particular case of a multivariate time series where a real-world process is observed in different locations. In this type of dataset, the goal is to forecast the future values of all these locations. Again, we can leverage the fact that neural networks are multi-output algorithms to handle multiple target variables in a <span class="No-Break">single model.</span></p>
			<p>In this recipe, we’ll work with the solar radiation dataset, as in previous ones. However, our goal is to forecast the future values of three variables—solar radiation, vapor pressure, and <span class="No-Break">air temperature:</span></p>
			<pre class="source-code">
N_LAGS = 14
HORIZON = 7
TARGET = ['Incoming Solar', 'Air Temp', 'Vapor Pressure']
mvtseries = pd.read_csv('assets/daily_multivariate_timeseries.csv',
    parse_dates=['datetime'],
    index_col='datetime')</pre>			<p>Regarding <a id="_idIndexMarker284"/>data preparation, the <a id="_idIndexMarker285"/>process is similar to what we did before. The difference is that we set the target variable (<strong class="source-inline">TARGET</strong>) to the preceding list of variables instead of just solar radiation. The <strong class="source-inline">TimeSeriesDataSet</strong> class and the data module handle all the preprocessing and data sharing <span class="No-Break">for us.</span></p>
			<h2 id="_idParaDest-226"><a id="_idTextAnchor319"/>How to do it…</h2>
			<p>We <a id="_idIndexMarker286"/>start by tweaking <a id="_idIndexMarker287"/>the data module to handle multiple target variables. In the following code, we make the necessary changes. Let’s start by defining the constructor of <span class="No-Break">the module:</span></p>
			<pre class="source-code">
class MultivariateSeriesDataModule(pl.LightningDataModule):
    def __init__(
            self,
            data: pd.DataFrame,
            target_variables: List[str],
            n_lags: int,
            horizon: int,
            test_size: float = 0.2,
            batch_size: int = 16,
    ):
        super().__init__()
        self.data = data
        self.batch_size = batch_size
        self.test_size = test_size
        self.n_lags = n_lags
        self.horizon = horizon
        self.target_variables = target_variables
        self.target_scaler = {k: MinMaxScaler() 
            for k in target_variables}
        self.feature_names = [col for col in data.columns
            if col not in self.target_variables]
        self.training = None
        self.validation = None
        self.test = None
        self.predict_set = None
        self.setup()</pre>			<p>The <a id="_idIndexMarker288"/>constructor contains <a id="_idIndexMarker289"/>a new argument, <strong class="source-inline">target_variables</strong>, which <a id="_idIndexMarker290"/>we use to pass the list of target variables. Besides that, we also make <a id="_idIndexMarker291"/>a small change to the <strong class="source-inline">self.target_scaler</strong> attribute, which is now a dictionary object that contains a scaler for each target variable. Then, we build the <strong class="source-inline">setup()</strong> method <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
def setup(self, stage=None):
    self.preprocess_data()
    train_indices, val_indices, test_indices = self.split_data()
    train_df = self.data.loc
        [self.data["time_index"].isin(train_indices)]
    val_df = self.data.loc[self.data["time_index"].isin(val_indices)]
    test_df = self.data.loc
        [self.data["time_index"].isin(test_indices)]
    for c in self.target_variables:
        self.target_scaler[c].fit(train_df[[c]])
    self.scale_target(train_df, train_df.index)
    self.scale_target(val_df, val_df.index)
    self.scale_target(test_df, test_df.index)
    self.training = TimeSeriesDataSet(
        train_df,
        time_idx="time_index",
        target=self.target_variables,
        group_ids=["group_id"],
        max_encoder_length=self.n_lags,
        max_prediction_length=self.horizon,
        time_varying_unknown_reals=self.feature_names + 
            self.target_variables,
        scalers={name: MinMaxScaler() for name in self.feature_names},
    )
    self.validation = TimeSeriesDataSet.from_dataset
        (self.training, val_df)
    self.test = TimeSeriesDataSet.from_dataset(self.training, test_df)
    self.predict_set = TimeSeriesDataSet.from_dataset(
        self.training, self.data, predict=True
    )</pre>			<p>The <a id="_idIndexMarker292"/>main differences from <a id="_idIndexMarker293"/>the previous recipe <a id="_idIndexMarker294"/>are the following. We <a id="_idIndexMarker295"/>pass the list of target variables to the target input of the <strong class="source-inline">TimeSeriesDataSet</strong> class. The scaling process of the target variables is also changed to a <strong class="source-inline">for</strong> loop that iterates over each <span class="No-Break">target variable.</span></p>
			<p>We also update the model module to process multiple target variables. Let’s start with the constructor and <span class="No-Break"><strong class="source-inline">forward()</strong></span><span class="No-Break"> method:</span></p>
			<pre class="source-code">
class MultiOutputLSTM(LightningModule):
    def __init__(self, input_dim, hidden_dim, num_layers, 
        horizon, n_output):
        super().__init__()
        self.n_output = n_output
        self.horizon = horizon
        self.hidden_dim = hidden_dim
        self.input_dim = input_dim
        self.output_dim = int(self.n_output * self.horizon)
        self.lstm = nn.LSTM(input_dim, hidden_dim,
            num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_dim, self.output_dim)
    def forward(self, x):
        h0 = torch.zeros(self.lstm.num_layers, x.size(0),
            self.hidden_dim).to(self.device)
        c0 = torch.zeros(self.lstm.num_layers, x.size(0),
            self.hidden_dim).to(self.device)
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])
        return out</pre>			<p>The <strong class="source-inline">forward()</strong> method <a id="_idIndexMarker296"/>is the <a id="_idIndexMarker297"/>same as in the previous chapter. We store a few more elements in the constructor, such <a id="_idIndexMarker298"/>as the forecasting <a id="_idIndexMarker299"/>horizon (<strong class="source-inline">self.horizon</strong>), as they are necessary in the <span class="No-Break">following steps:</span></p>
			<pre class="source-code">
    def training_step(self, batch, batch_idx):
        x, y = batch
        y_pred = self(x['encoder_cont'])
        y_pred = y_pred.unsqueeze(-1).view(-1, self.horizon, 
            self.n_output)
        y_pred = [y_pred[:, :, i] for i in range(self.n_output)]
        loss = [F.mse_loss(y_pred[i], 
            y[0][i]) for i in range(self.n_output)]
        loss = torch.mean(torch.stack(loss))
        self.log('train_loss', loss)
        return loss
    def test_step(self, batch, batch_idx):
        x, y = batch
        y_pred = self(x['encoder_cont'])
        y_pred = y_pred.unsqueeze(-1).view(-1, self.horizon, 
            self.n_output)
        y_pred = [y_pred[:, :, i] for i in range(self.n_output)]
        loss = [F.mse_loss(y_pred[i],
            y[0][i]) for i in range(self.n_output)]
        loss = torch.mean(torch.stack(loss))
        self.log('test_loss', loss)
    def predict_step(self, batch, batch_idx, dataloader_idx=0):
        x, y = batch
        y_pred = self(x['encoder_cont'])
        y_pred = y_pred.unsqueeze(-1).view(-1,
            self.horizon, self.n_output)
        y_pred = [y_pred[:, :, i] for i in range(self.n_output)]
        return y_pred
    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=0.001)</pre>			<p>Let’s <a id="_idIndexMarker300"/>break down the <span class="No-Break">preceding code:</span></p>
			<ul>
				<li>We <a id="_idIndexMarker301"/>add an <strong class="source-inline">n_output</strong> parameter to the constructor, which details the number of target variables (in this <span class="No-Break">example, </span><span class="No-Break"><strong class="source-inline">3</strong></span><span class="No-Break">)</span></li>
				<li>The <a id="_idIndexMarker302"/>output dimension is set to the number of target variables times the forecasting horizon (<strong class="source-inline">self.n_output * </strong><span class="No-Break"><strong class="source-inline">self.horizon</strong></span><span class="No-Break">)</span></li>
				<li>When <a id="_idIndexMarker303"/>processing the data in the training and testing steps, the predictions are reshaped into the appropriate format (batch size, horizon, and number <span class="No-Break">of variables)</span></li>
				<li>We compute the MSE loss for each target variable, and then take the average across them <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">torch.mean(torch.stack(loss))</strong></span></li>
			</ul>
			<p>Then, the remaining processes are similar to what we did in previous recipes based on <span class="No-Break">PyTorch Lightning:</span></p>
			<pre class="source-code">
model = MultiOutputLSTM(input_dim=n_vars,
    hidden_dim=32,
    num_layers=1,
    horizon=HORIZON,
    n_vars=len(TARGET))
datamodule = MultivariateSeriesDataModule(data=mvtseries,
    n_lags=N_LAGS,
    horizon=HORIZON,
    target_variables=TARGET)
early_stop_callback = EarlyStopping(monitor="val_loss",
    min_delta=1e-4,
    patience=10,
    verbose=False,
    mode="min")
trainer = pl.Trainer(max_epochs=20, callbacks=[early_stop_callback])
trainer.fit(model, datamodule)
trainer.test(model=model, datamodule=datamodule)
forecasts = trainer.predict(model=model, datamodule=datamodule)</pre>			<h2 id="_idParaDest-227"><a id="_idTextAnchor320"/>How it works…</h2>
			<p>The <a id="_idIndexMarker304"/>modeling approach <a id="_idIndexMarker305"/>used in this recipe <a id="_idIndexMarker306"/>follows the idea of <strong class="bold">Vector Auto-Regression</strong> (<strong class="bold">VAR</strong>). VAR works by modeling the future value of the variables of a multivariate <a id="_idIndexMarker307"/>time series <a id="_idIndexMarker308"/>as a function of the past values of all these variables. Predicting multiple variables may be relevant in several scenarios, such as <span class="No-Break">spatiotemporal forecasting.</span></p>
			<p>In this recipe, we adapted the VAR principle to a deep learning context, specifically through the use of LSTM networks. Unlike traditional VAR models that linearly project future values based on past observations, our deep learning model captures nonlinear relationships and temporal dependencies across multiple time steps <span class="No-Break">and variables.</span></p>
			<p>To <a id="_idIndexMarker309"/>compute the <strong class="source-inline">loss</strong><strong class="source-inline">()</strong> function <a id="_idIndexMarker310"/>of our model—essential for training and evaluating its performance—we had to perform some changes <a id="_idIndexMarker311"/>in the <strong class="source-inline">training_step()</strong> and <strong class="source-inline">test_step()</strong> methods. After the network generates predictions, we <a id="_idIndexMarker312"/>segment the output by variable. This segmentation allows us to calculate the MSE loss for each variable separately. These individual losses are then aggregated to form a composite loss measure, which guides the optimization process of <span class="No-Break">the model.</span></p>
			<h1 id="_idParaDest-228"><a id="_idTextAnchor321"/>Preparing multiple time series for a global model</h1>
			<p>Now, it is <a id="_idIndexMarker313"/>time to move on to the type of time series problems that involve multiple time series. In this recipe, we will learn the fundamentals of global forecasting models and how they work. We’ll also explore how to prepare a dataset that contains multiple time series for forecasting. Again, we leverage the capabilities of the <strong class="source-inline">TimeSeriesDataSet</strong> and <strong class="source-inline">DataModule</strong> classes to help us <span class="No-Break">do this.</span></p>
			<h2 id="_idParaDest-229"><a id="_idTextAnchor322"/>Getting ready</h2>
			<p>So far, we’ve been working with time series problems involving a single dataset. Now, we’ll learn about global forecasting models, including <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Transitioning from local to global models</strong>: Initially, our work with time series forecasting focused on single datasets, where models predict future values based on historical data of one series. These so-called local models are tailored to specific time series, whereas global models involve handling multiple related time series and capturing relevant information <span class="No-Break">across them.</span></li>
				<li><strong class="bold">Leveraging neural networks</strong>: Neural networks excel in data-rich environments, making them ideal for global forecasting. This is particularly effective in domains such as retail, where understanding the relationships across different product sales can lead to more <span class="No-Break">accurate forecasts.</span></li>
			</ul>
			<p>We’ll learn how to build a global forecasting model using a dataset concerning transportation <a id="_idIndexMarker314"/>called <strong class="bold">NN5</strong>. This dataset was used in a previous forecasting competition and includes 111 different <span class="No-Break">time series.</span></p>
			<p>The data <a id="_idIndexMarker315"/>is available in the <strong class="source-inline">gluonts</strong> Python library and can be loaded <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
N_LAGS = 7
HORIZON = 7
from gluonts.dataset.repository.datasets import get_dataset
dataset = get_dataset('nn5_daily_without_missing', regenerate=False)</pre>			<p>Here’s a sample of five of the time series in <span class="No-Break">the dataset:</span></p>
			<div>
				<div id="_idContainer022" class="IMG---Figure">
					<img src="image/B21145_05_001.jpg" alt="Figure 5.1: Sample of the NN5 time series dataset" width="1566" height="630"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.1: Sample of the NN5 time series dataset</p>
			<p>The original source of this dataset is at the following <span class="No-Break">link: </span><a href="https://zenodo.org/records/3889750"><span class="No-Break">https://zenodo.org/records/3889750</span></a><span class="No-Break">.</span></p>
			<p>Now, let’s build a <strong class="source-inline">DataModule</strong> class to handle the data <span class="No-Break">preprocessing steps.</span></p>
			<h2 id="_idParaDest-230"><a id="_idTextAnchor323"/>How to do it…</h2>
			<p>We’ll build a <strong class="source-inline">LightningDataModule</strong> class that handles a dataset with multiple time <a id="_idIndexMarker316"/>series and passes them to a model. Here’s what it looks like starting with <span class="No-Break">the constructor:</span></p>
			<pre class="source-code">
import lightning.pytorch as pl
class GlobalDataModule(pl.LightningDataModule):
    def __init__(self,
                 data,
                 n_lags: int,
                 horizon: int,
                 test_size: float,
                 batch_size: int):
        super().__init__()
        self.data = data
        self.batch_size = batch_size
        self.test_size = test_size
        self.n_lags = n_lags
        self.horizon = horizon
        self.training = None
        self.validation = None
        self.test = None
        self.predict_set = None
        self.target_scaler = LocalScaler()</pre>			<p>Essentially, we store the necessary elements for training and using the model. This includes a <strong class="source-inline">self.target_scaler</strong> attribute based on a <span class="No-Break"><strong class="source-inline">LocalScaler</strong></span><span class="No-Break"> class.</span></p>
			<p>The main method of the <strong class="source-inline">LocalScaler</strong> class <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">transform()</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
def transform(self, df: pd.DataFrame):
    df = df.copy()
    df["value"] = LogTransformation.transform(df["value"])
    df_g = df.groupby("group_id")
    scaled_df_l = []
    for g, df_ in df_g:
        df_[["value"]] = self.scalers[g].transform(df_[["value"]])
        scaled_df_l.append(df_)
    scaled_df = pd.concat(scaled_df_l)
    scaled_df = scaled_df.sort_index()
    return scaled_df</pre>			<p>This <a id="_idIndexMarker317"/>method applies two preprocessing operations to <span class="No-Break">the dataset:</span></p>
			<ul>
				<li>A log transformation to stabilize the variance of the <span class="No-Break">time series</span></li>
				<li>Standardization of each time series in <span class="No-Break">the dataset</span></li>
			</ul>
			<p>You can extend this class to include any transformation that you need to perform on your dataset. The complete implementation of the <strong class="source-inline">LocalScaler</strong> class is available on the <span class="No-Break">GitHub repository.</span></p>
			<p>Then, we preprocess the data in the <span class="No-Break"><strong class="source-inline">setup()</strong></span><span class="No-Break"> function:</span></p>
			<pre class="source-code">
def setup(self, stage=None):
    data_list = list(self.data.train)
    data_list = [pd.Series(ts['target'],
        index=pd.date_range(start=ts['start'].to_timestamp(),
        freq=ts['start'].freq,
        periods=len(ts['target'])))
        for ts in data_list]
    tseries_df = pd.concat(data_list, axis=1)
    tseries_df['time_index'] = np.arange(tseries_df.shape[0])
    ts_df = tseries_df.melt('time_index')
    ts_df = ts_df.rename(columns={'variable': 'group_id'})
    unique_times = ts_df['time_index'].sort_values().unique()
    tr_ind, ts_ind = \
        train_test_split(unique_times,
            test_size=self.test_size,
            shuffle=False)
    tr_ind, vl_ind = \
        train_test_split(tr_ind,
            test_size=0.1,
            shuffle=False)
    training_df = ts_df.loc[ts_df['time_index'].isin(tr_ind), :]
    validation_df = ts_df.loc[ts_df['time_index'].isin(vl_ind), :]
    test_df = ts_df.loc[ts_df['time_index'].isin(ts_ind), :]
    self.target_scaler.fit(training_df)
    training_df = self.target_scaler.transform(training_df)
    validation_df = self.target_scaler.transform(validation_df)
    test_df = self.target_scaler.transform(test_df)
    self.training = TimeSeriesDataSet(
        data=training_df,
        time_idx='time_index',
        target='value',
        group_ids=['group_id'],
        max_encoder_length=self.n_lags,
        max_prediction_length=self.horizon,
        time_varying_unknown_reals=['value'],
    )
    self.validation = TimeSeriesDataSet.from_dataset
        (self.training, validation_df)
    self.test = TimeSeriesDataSet.from_dataset(self.training, test_df)
    self.predict_set = TimeSeriesDataSet.from_dataset
        (self.training, ts_df, predict=True)</pre>			<p>In the preceding code, we split the data into training, validation, testing, and prediction sets and set up the respective <strong class="source-inline">TimeSeriesDataSet</strong> instances. Finally, the data loaders are similar to what we’ve done in <span class="No-Break">previous recipes:</span></p>
			<pre class="source-code">
    def train_dataloader(self):
        return self.training.to_dataloader(batch_size=self.batch_size,
            shuffle=False)
    def val_dataloader(self):
        return self.validation.to_dataloader
            (batch_size=self.batch_size, shuffle=False)
    def test_dataloader(self):
        return self.test.to_dataloader(batch_size=self.batch_size,
            shuffle=False)
    def predict_dataloader(self):
        return self.predict_set.to_dataloader(batch_size=1,
            shuffle=False)</pre>			<p>We <a id="_idIndexMarker318"/>can call the data module <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
datamodule = GlobalDataModule(data=dataset,
    n_lags=N_LAGS,
    horizon=HORIZON,
    test_size=0.2,
    batch_size=1)</pre>			<p>Using this module, each individual series in the dataset will be processed in such a way as to use the last <strong class="source-inline">N_LAGS</strong> values to predict the next <span class="No-Break"><strong class="source-inline">HORIZON</strong></span><span class="No-Break"> observations.</span></p>
			<h2 id="_idParaDest-231"><a id="_idTextAnchor324"/>How it works…</h2>
			<p>Global methods are trained on multiple time series. The idea is that there are common patterns across the different time series. So, a neural network can use observations from these series to train <span class="No-Break">better models.</span></p>
			<p>In the <a id="_idIndexMarker319"/>preceding section, we retrieved a dataset involving several time series from the <strong class="source-inline">gluonts</strong> Python library via the <strong class="source-inline">get_dataset</strong><strong class="source-inline">()</strong> function. The process of preparing a dataset that contains multiple time series for supervised learning is similar to what we did before. The key input to the <strong class="source-inline">TimeSeriesDataSet</strong> instance is the <strong class="source-inline">group_id</strong> variable that details the entity to which each <span class="No-Break">observation belongs.</span></p>
			<p>The main work happens in the <strong class="source-inline">setup()</strong> method. First, we transform the dataset into a <strong class="source-inline">pandas</strong> DataFrame with a long format. Here’s a sample of <span class="No-Break">this data:</span></p>
			<div>
				<div id="_idContainer023" class="IMG---Figure">
					<img src="image/B21145_05_002.jpg" alt="Figure 5.2: Sample of the NN5 time series dataset in a long format" width="862" height="756"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.2: Sample of the NN5 time series dataset in a long format</p>
			<p>In this <a id="_idIndexMarker320"/>case, the <strong class="source-inline">group_id</strong> column is not constant and details which time series the observation refers to. Since each time series is univariate, there’s a single numeric variable <span class="No-Break">called </span><span class="No-Break"><strong class="source-inline">value</strong></span><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-232"><a id="_idTextAnchor325"/>Training a global LSTM with multiple time series</h1>
			<p>In the <a id="_idIndexMarker321"/>previous recipe, we learned how to prepare datasets with multiple time series for supervised learning with a global forecasting model. In this recipe, we continue this topic and describe how to train a global LSTM neural network <span class="No-Break">for forecasting.</span></p>
			<h2 id="_idParaDest-233"><a id="_idTextAnchor326"/>Getting ready</h2>
			<p>We’ll continue with the same data module we used in the <span class="No-Break">previous recipe:</span></p>
			<pre class="source-code">
N_LAGS = 7
HORIZON = 7
from gluonts.dataset.repository.datasets import get_dataset, dataset_names
dataset = get_dataset('nn5_daily_without_missing', regenerate=False)
datamodule = GlobalDataModule(data=dataset,
    n_lags=N_LAGS,
    horizon=HORIZON,
    batch_size=32,
    test_size=0.3)</pre>			<p>Let’s see <a id="_idIndexMarker322"/>how to create an LSTM module to handle a data module with multiple <span class="No-Break">time series.</span></p>
			<h2 id="_idParaDest-234"><a id="_idTextAnchor327"/>How to do it…</h2>
			<p>We create a <strong class="source-inline">LightningModule</strong> class that contains the implementation of the LSTM. First, let’s look at the class constructor and the <span class="No-Break"><strong class="source-inline">forward()</strong></span><span class="No-Break"> method:</span></p>
			<pre class="source-code">
class GlobalLSTM(pl.LightningModule):
    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, 
            batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)
    def forward(self, x):
        h0 = torch.zeros(self.lstm.num_layers, x.size(0), 
            self.hidden_dim).to(self.device)
        c0 = torch.zeros(self.lstm.num_layers, x.size(0), 
            self.hidden_dim).to(self.device)
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])
        return out</pre>			<p>The logic <a id="_idIndexMarker323"/>of the neural network is similar to what we’ve done for a dataset with a single time series. This is also true for the <span class="No-Break">remaining methods:</span></p>
			<pre class="source-code">
    def training_step(self, batch, batch_idx):
        x, y = batch
        y_pred = self(x['encoder_cont'])
        loss = F.mse_loss(y_pred, y[0])
        self.log('train_loss', loss)
        return loss
    def validation_step(self, batch, batch_idx):
        x, y = batch
        y_pred = self(x['encoder_cont'])
        loss = F.mse_loss(y_pred, y[0])
        self.log('val_loss', loss)
        return loss
    def test_step(self, batch, batch_idx):
        x, y = batch
        y_pred = self(x['encoder_cont'])
        loss = F.mse_loss(y_pred, y[0])
        self.log('test_loss', loss)
    def predict_step(self, batch, batch_idx, dataloader_idx=0):
        x, y = batch
        y_pred = self(x['encoder_cont'])
        return y_pred
    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=0.01)</pre>			<p>Next, we <a id="_idIndexMarker324"/>can call the model and train it <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
model = GlobalLSTM(input_dim=1,
    hidden_dim=32,
    num_layers=1,
    output_dim=HORIZON)
early_stop_callback = EarlyStopping(monitor="val_loss",
    min_delta=1e-4,
    patience=10,
    verbose=False,
    mode="min")
trainer = pl.Trainer(max_epochs=20, callbacks=[early_stop_callback])
trainer.fit(model, datamodule)
trainer.test(model=model, datamodule=datamodule)
forecasts = trainer.predict(model=model, datamodule=datamodule)</pre>			<p>Using the PyTorch Lightning design, the training, testing, and prediction steps are similar to what we did in other recipes based on <span class="No-Break">this framework.</span></p>
			<h2 id="_idParaDest-235"><a id="_idTextAnchor328"/>How it works…</h2>
			<p>As you can see, the <strong class="source-inline">LightningModule</strong> class that contains the LSTM is identical to the one we built for a single multivariate time series. This class only deals with the part of <a id="_idIndexMarker325"/>the model definition, so no change is necessary. The main work is done during the data preprocessing stage. So, we only need to change the <strong class="source-inline">setup()</strong> method in the data module to reflect the necessary changes, which were explained in the <span class="No-Break">previous recipe.</span></p>
			<p>We transitioned from a local LSTM model, designed for forecasting a single time series, to a global LSTM model capable of handling multiple time series simultaneously. The main difference lies in how the data is prepared and presented to the model than changes in the neural network architecture itself. Both local and global models utilize the same underlying LSTM structure, characterized by its ability to process sequences of data and predict <span class="No-Break">future values.</span></p>
			<p>In a local LSTM setup, the model’s input typically follows the structure [<strong class="source-inline">batch_size</strong>, <strong class="source-inline">sequence_length</strong>, <strong class="source-inline">num_features</strong>], with the output shaped to match the forecasting horizon, usually [<strong class="source-inline">batch_size</strong>, <strong class="source-inline">horizon</strong>]. This setup is straightforward as it deals with data from a <span class="No-Break">single series.</span></p>
			<p>Shifting to a global LSTM model, the approach to input and output configuration remains fundamentally the same in terms of dimensionality. However, the input now aggregates information across multiple time series. It increases the ability of the neural network to learn new patterns and dependencies not just within a single series but across several. Consequently, the output of a global LSTM model is designed to produce forecasts for multiple time series simultaneously, reflecting predictions across the <span class="No-Break">entire dataset.</span></p>
			<h1 id="_idParaDest-236"><a id="_idTextAnchor329"/>Global forecasting models for seasonal time series</h1>
			<p>This recipe <a id="_idIndexMarker326"/>shows how to extend a data module to include extra explanatory variables in a <strong class="source-inline">TimeSeriesDataSet</strong> class and a <strong class="source-inline">DataModule</strong> class. We’ll use a particular case about seasonal <span class="No-Break">time series.</span></p>
			<h2 id="_idParaDest-237"><a id="_idTextAnchor330"/>Getting ready</h2>
			<p>We load the dataset that we used in the <span class="No-Break">previous recipe:</span></p>
			<pre class="source-code">
N_LAGS = 7
HORIZON = 7
from gluonts.dataset.repository.datasets import get_dataset
dataset = get_dataset('nn5_daily_without_missing', regenerate=False)</pre>			<p>This dataset contains time series with a daily granularity. Here, we’ll model weekly seasonality using the <strong class="source-inline">Fourier</strong> series. Unlike what we did in the previous chapter (in the <em class="italic">Handling seasonality: seasonal dummies and Fourier series</em> recipe), we’ll learn how to include these features using the <span class="No-Break"><strong class="source-inline">TimeSeriesDataSet</strong></span><span class="No-Break"> framework.</span></p>
			<h2 id="_idParaDest-238"><a id="_idTextAnchor331"/>How to do it…</h2>
			<p>Here’s the updated <strong class="source-inline">DataModule</strong> that includes the <strong class="source-inline">Fourier</strong> series. We only describe part of the <strong class="source-inline">setup()</strong> method for brevity. The remaining methods stay the same, and you can check them in the <span class="No-Break">GitHub repository:</span></p>
			<pre class="source-code">
from sktime.transformations.series.fourier import FourierFeatures
def setup(self, stage=None):
    […]
    fourier = FourierFeatures(sp_list=[7],
        fourier_terms_list=[2],
        keep_original_columns=False)
    fourier_features = fourier.fit_transform(ts_df['index'])
    ts_df = pd.concat
        ([ts_df, fourier_features], axis=1).drop('index', axis=1)
    […]
    self.training = TimeSeriesDataSet(
        data=training_df,
        time_idx='time_index',
        target='value',
        group_ids=['group_id'],
        max_encoder_length=self.n_lags,
        max_prediction_length=self.horizon,
        time_varying_unknown_reals=['value'],
        time_varying_known_reals=['sin_7_1', 'cos_7_1',
            'sin_7_2', 'cos_7_2']
    )</pre>			<p>In <a id="_idIndexMarker327"/>the <strong class="source-inline">setup()</strong> method, we compute the <strong class="source-inline">Fourier</strong> terms using the date and time information of the dataset. This leads to four deterministic variables: <strong class="source-inline">sin_7_1</strong>,<strong class="source-inline"> cos_7_1</strong>, <strong class="source-inline">sin_7_2</strong>, and <strong class="source-inline">cos_7_2</strong>. These are <strong class="source-inline">Fourier</strong> series that we use to model seasonality. After adding them to the dataset using <strong class="source-inline">pd.concat([tseries_long, fourier_features], axis=1)</strong>, we use the <strong class="source-inline">time_varying_known_reals</strong> argument to tell that these features vary over time but in a <span class="No-Break">predictable way.</span></p>
			<p>In the LSTM, we need to update the input dimension to <strong class="source-inline">5</strong> to reflect the number of variables <a id="_idIndexMarker328"/>in the dataset (the target variable plus four <strong class="source-inline">Fourier</strong> series). This is done <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
model = GlobalLSTM(input_dim=5,
    hidden_dim=32,
    num_layers=1,
    output_dim=HORIZON)
datamodule = GlobalDataModuleSeas(data=dataset,
    n_lags=N_LAGS,
    horizon=HORIZON,
    batch_size=128,
    test_size=0.3)
early_stop_callback = EarlyStopping(monitor="val_loss",
    min_delta=1e-4,
    patience=10,
    verbose=False,
    mode="min")
trainer = pl.Trainer(max_epochs=20, callbacks=[early_stop_callback])
trainer.fit(model, datamodule)
trainer.test(model=model, datamodule=datamodule)
forecasts = trainer.predict(model=model, datamodule=datamodule)</pre>			<p>Again, the training and inference stages are similar to the previous recipe since the only differences here are in the data preprocessing stage handled by the <span class="No-Break">data module.</span></p>
			<h2 id="_idParaDest-239"><a id="_idTextAnchor332"/>How it works…</h2>
			<p>Modeling seasonality with the <strong class="source-inline">Fourier</strong> series involves enriching the dataset with extra <a id="_idIndexMarker329"/>variables derived from the Fourier transformation. This approach was implemented in the <strong class="source-inline">setup()</strong> method of the <strong class="source-inline">DataModule</strong> instance, where these variables were incorporated into the <span class="No-Break"><strong class="source-inline">TimeSeriesDataSet</strong></span><span class="No-Break"> objects.</span></p>
			<p><strong class="source-inline">Fourier</strong> series decomposition allows us to capture seasonality by breaking down complex periodic patterns into simpler, sinusoidal waves. Each component of the <strong class="source-inline">Fourier</strong> series corresponds to a different frequency, capturing different seasonal cycles within the time series data. This is particularly beneficial for neural networks for <span class="No-Break">several reasons:</span></p>
			<ul>
				<li><strong class="bold">Feature engineering</strong>: The <strong class="source-inline">Fourier</strong> series acts as automatic feature engineering, creating informative features that directly encode periodic behaviors. This can significantly improve the ability of the model to recognize and predict seasonal patterns, even in complex or noisy data. Since Fourier features are added to the input data, they can work with any neural network algorithm <span class="No-Break">or architecture.</span></li>
				<li><strong class="bold">Flexibility in modeling complex seasonality</strong>: Real-world time series often exhibit multiple seasonal patterns (e.g., daily, weekly, or yearly). <strong class="source-inline">Fourier</strong> series can model these multiple seasonality levels simultaneously, providing a more nuanced representation of the data that can be difficult to achieve with traditional seasonal <span class="No-Break">decomposition methods.</span></li>
				<li><strong class="bold">Improved generalization</strong>: By providing a clear, mathematical representation of seasonality, Fourier features help neural networks to generalize better from the observed data to unseen future periods. This reduces the risk of overfitting noise and anomalies in the data, focusing the model’s learning on the underlying <span class="No-Break">periodic trends.</span></li>
			</ul>
			<h2 id="_idParaDest-240"><a id="_idTextAnchor333"/>There’s more…</h2>
			<p>You can check the following URL to learn how to include extra categorical variables in the dataset (such as <span class="No-Break">holidays): </span><a href="https://pytorch-forecasting.readthedocs.io/en/stable/tutorials/stallion.html#Load-data"><span class="No-Break">https://pytorch-forecasting.readthedocs.io/en/stable/tutorials/stallion.html#Load-data</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-241"><a id="_idTextAnchor334"/>Hyperparameter optimization using Ray Tune</h1>
			<p>Neural networks have hyperparameters that define their structure and learning process. Hyperparameters include the learning rate or the number of hidden layers and units. Different hyperparameter values can affect the learning process and the accuracy <a id="_idIndexMarker330"/>of models. Incorrectly chosen values can result in underfitting or overfitting, which decreases the model’s performance. So, it’s important to optimize the value of hyperparameters to get the most out <a id="_idIndexMarker331"/>of deep learning models. In this recipe, we’ll explore how to do hyperparameter optimization using Ray Tune, including learning rate, regularization parameters, the number of hidden layers, and so on. The optimization of these parameters is very important to the performance of our models. More often than not, we face poor results in fitting neural network models simply due to poor selection of hyperparameters, which can lead to underfitting or overfitting <span class="No-Break">unseen data.</span></p>
			<h2 id="_idParaDest-242"><a id="_idTextAnchor335"/>Getting ready</h2>
			<p>Before we begin with hyperparameter optimization, we need to install Ray Tune, if it’s not already installed. This can be done using the <span class="No-Break">following command:</span></p>
			<pre class="console">
pip install -U 'ray[data,train,tune,serve]'</pre>			<p>We will use the same data and LSTM model <span class="No-Break">to optimize:</span></p>
			<pre class="source-code">
class GlobalDataModule(pl.LightningDataModule):
    ...
class GlobalLSTM(pl.LightningModule):
    ...
from ray.train.lightning import RayTrainReportCallback
from ray import tune
from ray.tune.schedulers import ASHAScheduler
from ray.train import RunConfig, ScalingConfig, CheckpointConfig
from ray.train.torch import TorchTrainer</pre>			<p>In the <a id="_idIndexMarker332"/>preceding code, we also made all <a id="_idIndexMarker333"/>the necessary imports for <span class="No-Break">this recipe.</span></p>
			<h2 id="_idParaDest-243"><a id="_idTextAnchor336"/>How to do it…</h2>
			<p>Let’s <a id="_idIndexMarker334"/>discuss how we can implement hyperparameter optimization using <span class="No-Break">Ray Tune:</span></p>
			<ol>
				<li><strong class="bold">Define the search space</strong>: First, define the hyperparameter space you want <span class="No-Break">to explore.</span></li>
				<li><strong class="bold">Configure Ray Tune</strong>: Initialize the Tune experiment with the desired settings, such as the number of trials, resources, and <span class="No-Break">so on.</span></li>
				<li><strong class="bold">Run the optimization</strong>: Execute the experiment by passing the training function and the defined <span class="No-Break">search space.</span></li>
				<li><strong class="bold">Analyze the results</strong>: Utilize Ray Tune’s tools to analyze the results and identify the <span class="No-Break">best hyperparameters.</span></li>
			</ol>
			<p>Let’s start by defining the <span class="No-Break">search space:</span></p>
			<pre class="source-code">
search_space = {
    "hidden_dim": tune.choice([8, 16, 32]),
    "num_layers": tune.choice([1, 2]),
}</pre>			<p>In this example, we only optimize two parameters: the number of hidden units and the number of layers in the LSTM <span class="No-Break">neural network.</span></p>
			<p>Then, we <a id="_idIndexMarker335"/>define the training cycle within <span class="No-Break">a function:</span></p>
			<pre class="source-code">
def train_tune(config_hyper):
    hidden_dim = config_hyper["hidden_dim"]
    num_layers = config_hyper["num_layers"]
    model = GlobalLSTM(input_dim=1,
        hidden_dim=hidden_dim,
        output_dim=HORIZON,
        num_layers=num_layers)
    data_module = GlobalDataModule(dataset,
        n_lags=N_LAGS,
        horizon=HORIZON,
        batch_size=128,
        test_size=0.3)
    trainer = Trainer(callbacks=[RayTrainReportCallback()])
    trainer.fit(model, data_module)</pre>			<p>After defining the training function, we pass it to a <strong class="source-inline">TorchTrainer</strong> class instance, along with the <span class="No-Break">running configuration:</span></p>
			<pre class="source-code">
scaling_config = ScalingConfig(
    num_workers=2, use_gpu=False, 
        resources_per_worker={"CPU": 1, "GPU": 0}
)
run_config = RunConfig(
    checkpoint_config=CheckpointConfig(
        num_to_keep=1,
        checkpoint_score_attribute="val_loss",
        checkpoint_score_order="min",
    ),
)
ray_trainer = TorchTrainer(
    train_tune,
    scaling_config=scaling_config,
    run_config=run_config,
)</pre>			<p>In the <strong class="source-inline">ScalingConfig</strong> instance, we configured the computational environment, specifying <a id="_idIndexMarker336"/>whether the process should run on a GPU or CPU, the number of workers allocated, and the resources per worker. Meanwhile, the <strong class="source-inline">RunConfig</strong> instance is set to define the optimization process, including the metric that should be monitored throughout <span class="No-Break">this process.</span></p>
			<p>Then, we create a <strong class="source-inline">Tuner</strong> instance that combines <span class="No-Break">this information:</span></p>
			<pre class="source-code">
scheduler = ASHAScheduler(max_t=30, grace_period=1, reduction_factor=2)
tuner = tune.Tuner(
    ray_trainer,
    param_space={"train_loop_config": search_space},
    tune_config=tune.TuneConfig(
        metric="val_loss",
        mode="min",
        num_samples=10,
        scheduler=scheduler,
    ),
)</pre>			<p>The <strong class="source-inline">Tuner</strong> instance <a id="_idIndexMarker337"/>requires a scheduler as one of its inputs. For this purpose, we utilize <strong class="source-inline">ASHAScheduler</strong>, which employs an <strong class="bold">Asynchronous Successive Halving Algorithm</strong> (<strong class="bold">ASHA</strong>) to efficiently allocate resources across various configurations. This method helps identify the most effective configuration by iteratively narrowing down the <a id="_idIndexMarker338"/>search space based on performance. Ultimately, by running this process, we can determine the <span class="No-Break">optimal configuration:</span></p>
			<pre class="source-code">
results = tuner.fit()
best_model_conf = \
    results.get_best_result(metric='val_loss', mode='min')</pre>			<p>In the preceding code, we get the configuration that minimizes the <span class="No-Break">validation loss.</span></p>
			<p>After selecting the best hyperparameters based on the validation loss, we can evaluate the model on the test set. Retrieve the model weights from the checkpoint and load the best hyperparameters from the tuning process. Then, use these parameters to load the model and evaluate it on the <span class="No-Break">test data:</span></p>
			<pre class="source-code">
path = best_model_conf.get_best_checkpoint(metric='val_loss',
    mode='min').path
config = best_model_conf.config['train_loop_config']
best_model = \
    GlobalLSTM.load_from_checkpoint(checkpoint_path=f'{path}/
        checkpoint.ckpt',
        **config)
data_module = GlobalDataModule(dataset, n_lags=7, horizon=3)
trainer = Trainer(max_epochs=30)
trainer.test(best_model, datamodule=data_module)</pre>			<p>In the preceding code, we load the model with the best configuration and test it in the test set defined in the <span class="No-Break"><strong class="source-inline">DataModule</strong></span><span class="No-Break"> class.</span></p>
			<h2 id="_idParaDest-244"><a id="_idTextAnchor337"/>How it works…</h2>
			<p>Our hyperparameter optimization process involves defining a search space, configuring <a id="_idIndexMarker339"/>and executing the optimization, and analyzing the results. The code snippets shared in this section provide a step-by-step guide to integrating Ray Tune into any machine learning workflow, allowing us to explore and find the best hyperparameters for <span class="No-Break">our model:</span></p>
			<ul>
				<li>The <strong class="source-inline">search_space</strong> dictionary defines the hyperparameter <span class="No-Break">search space</span></li>
				<li>The <strong class="source-inline">train_tune()</strong> function encapsulates the training process, including model configuration, data preparation, <span class="No-Break">and fitting</span></li>
				<li>The <strong class="source-inline">ScalingConfig</strong> class defines the computational environment for the optimization process, such as whether to run it on GPU <span class="No-Break">or CPU</span></li>
				<li>The <strong class="source-inline">RunConfig</strong> class sets up how the optimization is done, such as the metric that should be tracked during <span class="No-Break">this process</span></li>
				<li>The <strong class="source-inline">ASHAScheduler</strong> class is a scheduler that defines how to select from among different <span class="No-Break">possible configurations</span></li>
			</ul>
			<p>Ray Tune efficiently explores the hyperparameter space using various algorithms such as Random Search, Grid Search, or more advanced methods such as ASHA. It parallelizes trials to utilize available resources effectively, hence speeding up the <span class="No-Break">search process.</span></p>
			<h2 id="_idParaDest-245"><a id="_idTextAnchor338"/>There’s more…</h2>
			<p>Ray Tune offers several additional features and advantages. It can integrate with other libraries, making it compatible with popular machine learning frameworks such as PyTorch, TensorFlow, and Scikit-Learn. Moreover, it provides advanced search <a id="_idIndexMarker340"/>algorithms such as Bayesian Optimization and Population-Based Training, giving users the flexibility to experiment with different optimization strategies. Lastly, Ray Tune supports visualization tools, allowing users to utilize TensorBoard or custom tools provided by Ray to effectively visualize and analyze the hyperparameter <span class="No-Break">search process.</span></p>
		</div>
	</div>
</div>
</body></html>