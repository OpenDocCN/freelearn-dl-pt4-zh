<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Your First Classifier</h1>
                </header>
            
            <article>
                
<p>With TensorFlow now installed, we need to kick the tires. We will do so by writing our first classifier and then training and testing it from start to finish!</p>
<p>Our first classifier will be a handwriting recognizer. One of the most common datasets to train is the <strong>MNIST</strong> handwritten digits dataset. We'll be using a similar dataset called <kbd>notMNIST</kbd>, which features the first ten letters of the English alphabet.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The key parts</h1>
                </header>
            
            <article>
                
<p>There are three key parts to most machine learning classifiers, which are as follows:</p>
<ul>
<li>The training pipeline</li>
<li>The neural network setup and training outputs</li>
<li>The usage pipeline</li>
</ul>
<p>The training pipeline obtains data, stages it, cleanses it, homogenizes it, and puts it in a format acceptable to the neural network. Do not be surprised if the training pipeline takes 80% to 85% of your effort initially—this is the reality of most machine learning work. Generally, the more realistic the training data, the more time spent on the training pipeline. In enterprise settings, the training pipeline might be an ongoing effort being enhanced perpetually. This is especially true as datasets get larger.</p>
<p>The second part, the neural network setup, and training, can be quick for routine problems and can be a research-grade effort for harder problems. You may find yourself making small changes to the network setup, over and over, until you finally achieve the desired classifier accuracy. The training is the most computationally expensive part, so it takes time before you can evaluate the result of each incremental modification.</p>
<p>Once the initial setup is complete and the network is trained to a sufficient level of accuracy, we can just use it over and over. In <a href="f1a5c9c4-6076-487f-abd1-b5a6e800890f.xhtml"><span class="ChapterrefPACKT">Chapter 10</span></a>, <em>Go Live and Go Big</em>, we'll explore more advanced topics, such as continuous learning, where even usage can feed back into further training the classifier.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Obtaining training data</h1>
                </header>
            
            <article>
                
<p>Machine learning requires training data—often a lot of training data. One of the great things about machine learning is the availability of standard training datasets. These are often used to benchmark node models and configurations and provide a consistent yardstick to gauge performance against previous progress. Many of the datasets are also used in annual global competitions.</p>
<p>This chapter uses training data, which is kindly provided by Yaroslav Bulatov, a machine learning researcher.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Downloading training data</h1>
                </header>
            
            <article>
                
<p>You should start by downloading the training data from the following links:</p>
<ul>
<li><a href="http://yaroslavvb.com/upload/notMNIST/notMNIST_small.tar.gz"><span class="URLPACKT">http://yaroslavvb.com/upload/notMNIST/notMNIST_small.tar.gz</span></a></li>
<li><a href="http://yaroslavvb.com/upload/notMNIST/notMNIST_large.tar.gz"><span class="URLPACKT">http://yaroslavvb.com/upload/notMNIST/notMNIST_large.tar.gz</span></a></li>
</ul>
<p>We will download this programmatically, but we should start with a manual download just to peek at the data and structure of the archive. This will be important when we write the pipeline, as we'll need to understand the structure so we can manipulate the data.</p>
<p>The small set is ideal for peeking. You can do this via the following command line, or just use a browser to download the file with an unarchiver to extract the files (I suggest getting familiarized with the command line as all of this needs to be automated):</p>
<pre><strong>cd ~/workdir</strong>
<strong>wget http://yaroslavvb.com/upload/notMNIST/notMNIST_small.tar.gz</strong>
<strong>tar xvf notMNIST_small.tar.gz</strong></pre>
<p>The preceding command line will reveal a container folder called <kbd>notMNIST_small</kbd> with ten subfolders underneath, one for each letter of the alphabet <kbd>a</kbd> through <kbd>j</kbd>. Under each lettered folder, there are thousands of 28x28 pixel images of the letter. Additionally, an interesting thing to note is the filename of each letter image, (<kbd>QnJhbmRpbmcgSXJvbi50dGY=</kbd>), suggesting a random string that does not contain information of use.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding classes</h1>
                </header>
            
            <article>
                
<p>The classifier we're writing seeks to assign unknown images to a class. Classes can be of the following types:</p>
<ul>
<li>Feline versus canine</li>
<li>Two versus seven</li>
<li>Tumor versus normal</li>
<li>Smiling versus frowning</li>
</ul>
<p>In our case, we are considering each letter a class for a total of 10 classes. The training set will reveal 10 subfolders with thousands of images underneath each subfolder. The name of the subfolder is important as it is the label for each of the images. These details will be used by the pipeline to prepare data for TensorFlow.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Automating the training data setup</h1>
                </header>
            
            <article>
                
<p>Ideally, we will want the entire process automated. This way, we can easily run the process end to end on any computer we use without having to carry around ancillary assets. This will be important later, as we will often develop on one computer (our development machine) and deploy on a different machine (our production server).</p>
<p>I have already written the code for this chapter, as well as all the other chapters; it is available at <a href="https://github.com/mlwithtf/MLwithTF">https://github.com/mlwithtf/MLwithTF</a>. Our approach will be to rewrite it together while understanding it. Some straightforward parts, such as this, may be skipped. I recommend forking the repository and cloning a local copy for your projects:</p>
<pre><strong>cd ~/workdir</strong>
<strong>git clone https://github.com/mlwithtf/MLwithTF</strong>
<strong>cd chapter_02</strong></pre>
<p>The code for this specific section is available at— <a href="https://github.com/mlwithtf/mlwithtf/blob/master/chapter_02/download.py"><span class="URLPACKT">https://github.com/mlwithtf/mlwithtf/blob/master/chapter_02/download.py.</span></a></p>
<p>Preparing the dataset is an important part of the training process. Before we go deeper into the code, we will run <kbd>download.py</kbd> to automatically download and prepare the dataset:</p>
<pre><strong>python download.py</strong></pre>
<p>The result will look like this:</p>
<div class="mce-root CDPAlignCenter"><strong><img height="260" width="481" class=" image-border" src="assets/12200b7b-6dbd-481d-a663-03ed6c482479.png"/></strong></div>
<p>Now, we will take a look at several functions that are used in <kbd>download.py</kbd>. You can find the code in this file:</p>
<p><a href="https://github.com/mlwithtf/mlwithtf/blob/master/data_utils.py">https://github.com/mlwithtf/mlwithtf/blob/master/data_utils.py</a></p>
<p>The following <kbd>downloadFile</kbd> function will automatically download the file and validate it against an expected file size:</p>
<pre style="padding-left: 60px"> from __future__ import print_function 
 import os 
 from six.moves.urllib.request import urlretrieve 
 import datetime 
 def downloadFile(fileURL, expected_size): 
    timeStampedDir=datetime.datetime.now()<br/>     .strftime("%Y.%m.%d_%I.%M.%S") 
    os.makedirs(timeStampedDir) 
    fileNameLocal = timeStampedDir + "/" +     <br/>    fileURL.split('/')[-1] 
    print ('Attempting to download ' + fileURL) 
    print ('File will be stored in ' + fileNameLocal) 
    filename, _ = urlretrieve(fileURL, fileNameLocal) 
    statinfo = os.stat(filename) 
    if statinfo.st_size == expected_size: 
        print('Found and verified', filename) 
    else: 
        raise Exception('Could not get ' + filename) 
    return filename </pre>
<p>The function can be called as follows:</p>
<pre style="padding-left: 60px"> tst_set = <br/> downloadFile('http://yaroslavvb.com/upload/notMNIST/notMNIST_small<br/> .tar.gz', 8458043) </pre>
<p>The code to extract the contents is as follows (note that the additional import is required):</p>
<pre style="padding-left: 60px"> import os, sys, tarfile 
 from os.path import basename 
 
 def extractFile(filename): 
    timeStampedDir=datetime.datetime.now()<br/>     .strftime("%Y.%m.%d_%I.%M.%S") 
    tar = tarfile.open(filename) 
    sys.stdout.flush() 
    tar.extractall(timeStampedDir) 
    tar.close() 
    return timeStampedDir + "/" + os.listdir(timeStampedDir)[0] </pre>
<p>We call the <kbd>download</kbd> and extract methods in sequence as follows:</p>
<pre style="padding-left: 60px"> tst_src='http://yaroslavvb.com/upload/notMNIST/notMNIST_small.tar.<br/> gz' 
 tst_set = downloadFile(tst_src, 8458043) 
 print ('Test set stored in: ' + tst_set) 
 tst_files = extractFile(tst_set) 
 print ('Test file set stored in: ' + tst_files) </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Additional setup</h1>
                </header>
            
            <article>
                
<p>The next part will focus on image processing and manipulation. This requires some extra libraries you may not have. At this point, it may make sense to just install all the typical packages required in scientific computing, which can be done as follows:</p>
<pre><strong>sudo apt-get install python-numpy python-scipy python-matplotlib <br/>ipython ipython-notebook python-pandas python-sympy python-nose</strong></pre>
<p>Additionally, install the image processing library, some external matrix mathematics libraries, and underlying requirements, which can be done as follows:</p>
<pre><strong>sudo pip install ndimage</strong>
<strong>sudo apt-get install libatlas3-base-dev gcc gfortran g++</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Converting images to matrices</h1>
                </header>
            
            <article>
                
<p>Much of machine learning is just operations on matrices. We will start that process next by converting our images into a series of matrices—essentially, a 3D matrix as wide as the number of images we have.</p>
<p>Almost all matrix operations that we will perform in this chapter, and the entire book, use NumPy—the most popular scientific computing package in the Python landscape. NumPy is available at <a href="http://www.numpy.org/"><span class="URLPACKT">http://www.numpy.org/</span></a>. You should install it before running the next series of operations.</p>
<p>The following code opens images and creates the matrices of data (note the three extra imports now required):</p>
<pre style="padding-left: 60px"> import numpy as np 
 from IPython.display import display, Image 
 from scipy import ndimage 
 
 image_size = 28  # Pixel width and height. 
 pixel_depth = 255.0  # Number of levels per pixel. 
 def loadClass(folder): 
  image_files = os.listdir(folder) 
  dataset = np.ndarray(shape=(len(image_files), <br/>  image_size, <br/>   image_size), dtype=np.float32) 
  image_index = 0 
  print(folder) 
  for image in os.listdir(folder): 
    image_file = os.path.join(folder, image) 
    try: 
      image_data =  <br/>     (ndimage.imread(image_file).astype(float) -  
                    pixel_depth / 2) / pixel_depth 
      if image_data.shape != (image_size, image_size): 
        raise Exception('Unexpected image shape: %s' % <br/>     str(image_data.shape)) 
      dataset[image_index, :, :] = image_data 
      image_index += 1 
     except IOError as e: l
      print('Could not read:', image_file, ':', e, '-   <br/>      it\'s ok, <br/>       skipping.') 
     return dataset[0:image_index, :, :] </pre>
<p>We have our extracted files from the previous section. Now, we can simply run this procedure on all our extracted images, as follows:</p>
<pre style="padding-left: 60px"> classFolders = [os.path.join(tst_files, d) for d in <br/> os.listdir(tst_files) if os.path.isdir(os.path.join(tst_files, <br/> d))] 
 print (classFolders) 
 for cf in classFolders: 
    print ("\n\nExaming class folder " + cf) 
    dataset=loadClass(cf) 
    print (dataset.shape) </pre>
<p>The procedure essentially loads letters into a matrix that looks something like this:</p>
<div class="packt_figure CDPAlignCenter"><img height="415" width="497" src="assets/a75199c9-3913-41b5-9d5a-69e450dd4ee2.png"/></div>
<p>However, a peek into the matrix reveals more subtlety. Go ahead and take a look by printing an arbitrary layer on the stack (for example, <kbd>np.set_printoptions(precision=2); print (dataset[47]</kbd>). You will find a matrix not of bits, but of floating point numbers:</p>
<div class="packt_figure CDPAlignCenter"><img height="396" width="755" class=" image-border" src="assets/eed77f16-0ef9-4f7c-8cef-8a2501e7fda2.png"/></div>
<p>The images first get loaded into a matrix of values <span class="packt_screen">0</span> to <span class="packt_screen">255</span>:</p>
<div class="packt_figure CDPAlignCenter"><img height="495" width="592" src="assets/21972f82-50be-496d-b548-73f8ec47175d.png"/></div>
<p>These get scaled down to numbers between <span class="packt_screen">-0.5</span> and <span class="packt_screen">0.5,</span> we will revisit the reasons why later. We will end up with a stack of images that looks like this:</p>
<div class="packt_figure CDPAlignCenter"><img height="601" width="720" src="assets/9c07636d-2cf7-4b1b-8bbf-75c312f2bef9.png"/></div>
<p>These are all greyscale images, so we will deal with just one layer. We'll deal with color images in future chapters; in those cases, each photo will have a matrix of height three and a separate matrix for red, green, and blue.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Logical stopping points</h1>
                </header>
            
            <article>
                
<p>Downloading our training file took a long time. Even extracting all the images took a while. To avoid repeating all this, we will try to do all the work just once and then create <strong>pickle files</strong>—archives of the Python data structures.</p>
<p>The following procedure runs through each class in our training and test set and creates a separate <kbd>pickle</kbd> file for each. In future runs, we'll just begin from here:</p>
<pre style="padding-left: 60px"> def makePickle(imgSrcPath): 
    data_folders = [os.path.join(tst_files, d) for d in <br/>     os.listdir(tst_files) if os.path.isdir(os.path.join(tst_files, <br/>     d))] 
    dataset_names = [] 
    for folder in data_folders: 
        set_filename = folder + '.pickle' 
        dataset_names.append(set_filename) 
        print('Pickling %s.' % set_filename) 
        dataset = loadClass(folder) 
        try: 
            with open(set_filename, 'wb') as f: 
                pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL) 
        except Exception as e: 
            print('Unable to save data to', set_filename, ':', e) 
    return dataset_names </pre>
<p>The <kbd>Pickle</kbd> files are essentially persistable and reconstitutable dumps of dictionaries.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The machine learning briefcase</h1>
                </header>
            
            <article>
                
<p>We just created nice, clean, <kbd>pickle</kbd> files with preprocessed images to train and test our classifier. However, we've ended up with 20 <kbd>pickle</kbd> files. There are two problems with this. First, we have too many files to keep track of easily. Secondly, we've only completed part of our pipeline, where we've processed our image sets but have not prepared a TensorFlow consumable file.</p>
<p>Now we will need to create our three major sets—the training set, the validation set, and the test set. The training set will be used to nudge our classifier, while the validation set will be used to gauge progress on each iteration. The test set will be kept secret until the end of the training, at which point, it will be used to test how well we've trained the model.</p>
<p>The code to do all this is long, so we'll leave you to review the Git repository. Pay close attention to the following three functions:</p>
<pre style="padding-left: 60px"> def randomize(dataset, labels): 
    permutation = np.random.permutation(labels.shape[0]) 
    shuffled_dataset = dataset[permutation, :, :] 
    shuffled_labels = labels[permutation] 
    return shuffled_dataset, shuffled_labels  
 
 def make_arrays(nb_rows, img_size): 
    if nb_rows: 
        dataset = np.ndarray((nb_rows, img_size, img_size),   <br/> dtype=np.float32) 
        labels = np.ndarray(nb_rows, dtype=np.int32) 
    else: 
        dataset, labels = None, None 
    return dataset, labels 
 
 def merge_datasets(pickle_files, train_size, valid_size=0): 
  num_classes = len(pickle_files) 
  valid_dataset, valid_labels = make_arrays(valid_size,  
  image_size) 
  train_dataset, train_labels = make_arrays(train_size,  
  image_size) 
  vsize_per_class = valid_size // num_classes 
  tsize_per_class = train_size // num_classes 
 
  start_v, start_t = 0, 0 
  end_v, end_t = vsize_per_class, tsize_per_class 
  end_l = vsize_per_class+tsize_per_class 
  for label, pickle_file in enumerate(pickle_files): 
    try: 
      with open(pickle_file, 'rb') as f: 
        letter_set = pickle.load(f) 
        np.random.shuffle(letter_set) 
        if valid_dataset is not None: 
          valid_letter = letter_set[:vsize_per_class, :, :] 
          valid_dataset[start_v:end_v, :, :] = valid_letter 
          valid_labels[start_v:end_v] = label 
          start_v += vsize_per_class 
          end_v += vsize_per_class 
 
        train_letter = letter_set[vsize_per_class:end_l, :, :] 
        train_dataset[start_t:end_t, :, :] = train_letter 
        train_labels[start_t:end_t] = label 
        start_t += tsize_per_class 
        end_t += tsize_per_class 
    except Exception as e: 
      print('Unable to process data from', pickle_file, ':', e) 
      raise 
 
  return valid_dataset, valid_labels, train_dataset, train_labels </pre>
<p>These three complete our pipeline methods. But, we will still need to use the pipeline. To do so, we will first define our training, validation, and test sizes. You can change this, but you should keep it less than the full size available, of course:</p>
<pre style="padding-left: 30px">     train_size = 200000 
     valid_size = 10000 
     test_size = 10000 </pre>
<p>These sizes will then be used to construct merged (that is, combining all our classes) datasets. We will pass in the list of <kbd>pickle</kbd> files to source our data from and get back a vector of labels and a matrix stack of images. We will finish by shuffling our datasets, as follows:</p>
<pre style="padding-left: 60px"> valid_dataset, valid_labels, train_dataset, train_labels = <br/>  merge_datasets( 
   picklenamesTrn, train_size, valid_size) 
 _, _, test_dataset, test_labels = merge_datasets(picklenamesTst, <br/>  test_size) 
 train_dataset, train_labels = randomize(train_dataset, <br/>  train_labels) 
 test_dataset, test_labels = randomize(test_dataset, test_labels) 
 valid_dataset, valid_labels = randomize(valid_dataset, <br/>  valid_labels) </pre>
<p>We can peek into our newly-merged datasets as follows:</p>
<pre style="padding-left: 60px"> print('Training:', train_dataset.shape, train_labels.shape) 
 print('Validation:', valid_dataset.shape, valid_labels.shape) 
 print('Testing:', test_dataset.shape, test_labels.shape) </pre>
<p>Whew! That was a lot of work we do not want to repeat in the future. Luckily, we won't have to, because we'll re-pickle our three new datasets into a single, giant, <kbd>pickle</kbd> file. Going forward, all learning will skip the preceding steps and work straight off the giant <kbd>pickle</kbd>:</p>
<pre style="padding-left: 60px"> pickle_file = 'notMNIST.pickle' 
 
 try: 
   f = open(pickle_file, 'wb') 
   save = { 
      'datTrn': train_dataset, 
    'labTrn': train_labels, 
    'datVal': valid_dataset, 
    'labVal': valid_labels, 
    'datTst': test_dataset, 
    'labTst': test_labels, 
     } 
   pickle.dump(save, f, pickle.HIGHEST_PROTOCOL) 
   f.close() 
 except Exception as e: 
   print('Unable to save data to', pickle_file, ':', e) 
   raise 
 
 statinfo = os.stat(pickle_file) 
 print('Compressed pickle size:', statinfo.st_size) </pre>
<p>The ideal way to feed the matrices into TensorFlow is actually as a one-dimensional array; so, we'll reformat our 28x28 matrices into strings of 784 decimals. For that, we'll use the following <kbd>reformat</kbd> method:</p>
<pre style="padding-left: 60px"> def reformat(dataset, labels): 
   dataset = dataset.reshape((-1, image_size * <br/>    image_size)).astype(np.float32) 
   labels = (np.arange(num_labels) == <br/>    labels[:,None]).astype(np.float32) 
   return dataset, labels </pre>
<p>Our images now look like this, with a row for every image in the training, validation, and test sets:</p>
<div class="packt_figure CDPAlignCenter"><img src="assets/c59f15a1-8a00-492b-a94c-4333666cbe79.png"/></div>
<p>Finally, to open up and work with the contents of the <kbd>pickle</kbd> file, we will simply read the variable names chosen earlier and pick off the data like a hashmap:</p>
<pre style="padding-left: 60px"> with open(pickle_file, 'rb') as f: 
   pkl = pickle.load(f) 
   train_dataset, train_labels = reformat(pkl['datTrn'], <br/>    pkl['labTrn']) 
   valid_dataset, valid_labels = reformat(pkl['datVal'], <br/>    pkl['labVal']) 
   test_dataset, test_labels = reformat(pkl['datTst'], <br/>    pkl['labTst']) </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training day</h1>
                </header>
            
            <article>
                
<p>Now, we arrive at the fun part—the neural network. The complete code to train this model is available at the following link: <a href="https://github.com/mlwithtf/mlwithtf/blob/master/chapter_02/training.py"><span class="URLPACKT">https://github.com/mlwithtf/mlwithtf/blob/master/chapter_02/training.py</span></a></p>
<p>To train the model, we'll import several more modules:</p>
<pre style="padding-left: 60px"> import sys, os<br/> import tensorflow as tf<br/> import numpy as np<br/> sys.path.append(os.path.realpath('..'))<br/> import data_utils<br/> import logmanager </pre>
<p>Then, we will define a few parameters for the training process:</p>
<pre style="padding-left: 60px" class="mce-root"> batch_size = 128<br/> num_steps = 10000<br/> learning_rate = 0.3<br/> data_showing_step = 500</pre>
<p>After that, we will use the <kbd>data_utils</kbd> package to load the dataset that was downloaded in the previous section:</p>
<pre style="padding-left: 60px"> dataset, image_size, num_of_classes, num_of_channels =  <br/> data_utils.prepare_not_mnist_dataset(root_dir="..")<br/> dataset = data_utils.reformat(dataset, image_size, num_of_channels,   <br/> num_of_classes)<br/> print('Training set', dataset.train_dataset.shape,  <br/> dataset.train_labels.shape)<br/> print('Validation set', dataset.valid_dataset.shape,  <br/> dataset.valid_labels.shape)<br/> print('Test set', dataset.test_dataset.shape,  <br/> dataset.test_labels.shape)</pre>
<p>We'll start off with a fully-connected network. For now, just trust the network setup (we'll jump into the theory of setup a bit later). We will represent the neural network as a graph, called <kbd>graph</kbd> in the following code:</p>
<pre style="padding-left: 60px"> graph = tf.Graph()<br/> with graph.as_default():<br/> # Input data. For the training data, we use a placeholder that will  <br/> be fed<br/> # at run time with a training minibatch.<br/> tf_train_dataset = tf.placeholder(tf.float32,<br/> shape=(batch_size, image_size * image_size * num_of_channels))<br/> tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size,  <br/> num_of_classes))<br/> tf_valid_dataset = tf.constant(dataset.valid_dataset)<br/> tf_test_dataset = tf.constant(dataset.test_dataset)<br/> # Variables.<br/> weights = {<br/> 'fc1': tf.Variable(tf.truncated_normal([image_size * image_size *  <br/> num_of_channels, num_of_classes])),<br/> 'fc2': tf.Variable(tf.truncated_normal([num_of_classes,  <br/> num_of_classes]))<br/> }<br/> biases = {<br/> 'fc1': tf.Variable(tf.zeros([num_of_classes])),<br/> 'fc2': tf.Variable(tf.zeros([num_of_classes]))<br/> }<br/> # Training computation.<br/> logits = nn_model(tf_train_dataset, weights, biases)<br/> loss = tf.reduce_mean(<br/> tf.nn.softmax_cross_entropy_with_logits(logits=logits,  <br/> labels=tf_train_labels))<br/> # Optimizer.<br/> optimizer =  <br/> tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)<br/> # Predictions for the training, validation, and test data.<br/> train_prediction = tf.nn.softmax(logits)<br/> valid_prediction = tf.nn.softmax(nn_model(tf_valid_dataset,  <br/> weights, biases))<br/> test_prediction = tf.nn.softmax(nn_model(tf_test_dataset, weights,  <br/> biases))<br/> The most important line here is the nn_model where the neural  <br/> network is defined:<br/> def nn_model(data, weights, biases):<br/> layer_fc1 = tf.matmul(data, weights['fc1']) + biases['fc1']<br/> relu_layer = tf.nn.relu(layer_fc1)<br/> return tf.matmul(relu_layer, weights['fc2']) + biases['fc2']</pre>
<p>The <kbd>loss</kbd> function that is used to train the model is also an important factor in this process:</p>
<pre style="padding-left: 60px"> loss = tf.reduce_mean(<br/> tf.nn.softmax_cross_entropy_with_logits(logits=logits,  <br/> labels=tf_train_labels))<br/> # Optimizer.<br/> optimizer =  <br/> tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)</pre>
<p>This is the optimizer being used (Stochastic Gradient Descent) along with the <kbd>learning_rate (0.3)</kbd> and the function we're trying to minimize (softmax with cross-entropy).</p>
<p>The real action, and the most time-consuming part, lies in the next and final segment—the training loop:</p>
<div class="mce-root CDPAlignCenter"><img height="334" width="681" class=" image-border" src="assets/4ed292fc-aeb9-4402-bf14-a09bcc8a5085.png"/></div>
<p>We can run this training process using the following command in the <kbd>chapter_02</kbd> directory:</p>
<pre><strong>python training.py</strong></pre>
<p>Running the procedure produces the following output:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/feedd131-0973-44c5-9ce0-fd1f31c7ff75.png"/></div>
<p>We are running through hundreds of cycles and printing indicative results once every 500 cycles. Of course, you are welcome to modify any of these settings. The important part is to appreciate the cycle:</p>
<ul>
<li>We will cycle through the process many times.</li>
<li>Each time, we will create a mini batch of photos, which is a carve-out of the full image set.</li>
<li>Each step runs the TensorFlow session and produces a loss and a set of predictions. Each step additionally makes a prediction on the validation set.</li>
<li>At the end of the iterative cycle, we will make a final prediction on our test set, which is a secret up until now.</li>
<li>For each prediction made, we will observe our progress in the form of prediction accuracy.</li>
</ul>
<p>We did not discuss the <kbd>accuracy</kbd> method earlier. This method simply compares the predicted labels against known labels to calculate a percentage score:</p>
<pre style="padding-left: 60px"> def accuracy(predictions, labels): 
  return (100.0 * np.sum(np.argmax(predictions, 1) == <br/>   np.argmax(labels, 1)) 
          / predictions.shape[0])</pre>
<p>Just running the preceding classifier will yield accuracy in the general range of 85%. This is remarkable because we have just begun! There are much more tweaks that we can continue to make.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Saving the model for ongoing use</h1>
                </header>
            
            <article>
                
<p>To save variables from the TensorFlow session for future use, you can use the <kbd>Saver()</kbd> function, which is as follows:</p>
<pre style="padding-left: 60px"> saver = tf.train.Saver() </pre>
<p>Later, you can retrieve the state of the model and avoid tedious retraining by restoring the following checkpoint:</p>
<pre style="padding-left: 60px"> ckpt = tf.train.get_checkpoint_state(FLAGS.checkpoint_dir) 
 if ckpt and ckpt.model_checkpoint_path: 
 saver.restore(sess, ckpt.model_checkpoint_path) </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Why hide the test set?</h1>
                </header>
            
            <article>
                
<p>Notice how we did not use the test set until the last step. Why not? This is a pretty important detail to ensure that the test remains a good one. As we iterate through the training set and nudge our classifier one way or another, we can sometimes <em>wrap the classifier</em> around the images or overtrain. This happens when you learn the training set rather than learn the features inside each of the classes.</p>
<p>When we overtrain, our accuracy on the iterative rounds of the training set will look promising, but that is all false hope. Having a never-before-seen test set should introduce reality back into the process. Great accuracy on the training set followed by poor results on the test set suggests overfitting.</p>
<p>This is why we've kept a separate test set. It helps indicate the real accuracy of our classifier. This is also why you should never shuffle your dataset or intermingle the dataset with the test set.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using the classifier</h1>
                </header>
            
            <article>
                
<p>We will demonstrate the usage of the classifier with <kbd>notMNIST_small.tar.gz</kbd>, which becomes the test set. For ongoing use of the classifier, you can source your own images and run them through a similar pipeline to test, not train.</p>
<p>You can create some 28x28 images yourself and place them into the test set for evaluation. You will be pleasantly surprised!</p>
<p>The practical issue with field usage is the heterogeneity of images in the wild. You may need to find images, crop them, downscale them, or perform a dozen other transformations. This all falls into the usage pipeline, which we discussed earlier.</p>
<p>Another technique to cover larger images, such as finding a letter on a page-sized image, is to slide a small window across the large image and feed every subsection of the image through the classifier.</p>
<p>We'll be taking our models into production in future chapters but, as a preview, one common setup is to move the trained model into a server on the cloud. The façade of the system might be a smartphone app that takes photos and sends them off for classification behind the scenes. In this case, we will wrap our entire program with a web service to accept incoming classification requests and programmatically respond to them. There are dozens of popular setups and we will explore several of them in <a href="b38dd75a-b632-4e7b-b581-202500f4e001.xhtml"><span class="ChapterrefPACKT">Chapter 9</span></a>, <em>Cruise Control -Automation</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep diving into the network</h1>
                </header>
            
            <article>
                
<p>Notice how we achieved 86% accuracy. This is a great result for two hours of work, but we can do much better. Much of the future potential is in changing the neural network. Our preceding application used a <strong>fully-connected</strong> setup, where each node on a layer is connected to each node on the previous layer and looks like this:</p>
<div class="packt_figure CDPAlignCenter"><img height="369" width="334" class=" image-border" src="assets/0be54afc-3577-4dfb-b045-2e916734b1d2.png"/></div>
<p>As you will learn with more complex network setups in coming chapters, this setup is fast but not ideal. The biggest issue is the large number of parameters, which can cause overfitting of the model on the training data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Skills learned</h1>
                </header>
            
            <article>
                
<p>You should have learned these skills in the chapter:</p>
<ul>
<li>Preparing training and test data</li>
<li>Creating a training set consumable by TensorFlow</li>
<li>Setting up a basic neural network graph</li>
<li>Training the TensorFlow classifier</li>
<li>Validating the classifier</li>
<li>Piping in real-world data</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>Superb progress! We just built a handwriting classifier that would have been world class a decade ago. Also, we built an entire pipeline around the process to fully automate the training setup and execution. This means that our program can be migrated to almost any server and continue to function almost turn-key.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </body></html>