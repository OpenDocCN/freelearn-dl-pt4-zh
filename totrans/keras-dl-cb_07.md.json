["```py\nclass Model(object):\n    \"\"\"Class representing an abstract Model.\"\"\"\n\n    def __init__(self, name):\n        \"\"\"Constructor.\n\n        :param name: name of the model, used as filename.\n            string, default 'dae'\n        \"\"\"\n        self.name = name\n        self.model_path = os.path.join(Config().models_dir, self.name)\n\n        self.input_data = None\n        self.input_labels = None\n>        self.keep_prob = None\n        self.layer_nodes = []  # list of layers of the final network\n        self.train_step = None\n        self.cost = None\n\n        # tensorflow objects\n        self.tf_graph = tf.Graph()\n        self.tf_session = None\n        self.tf_saver = None\n        self.tf_merged_summaries = None\n        self.tf_summary_writer = None\n```", "```py\n[\"cross_entropy\", \"softmax_cross_entropy\", \"mse\"]\n```", "```py\nclass DeepBeliefNetwork(SupervisedModel):\n    \"\"\"Implementation of Deep Belief Network for Supervised Learning.\n\n    The interface of the class is sklearn-like.\n    \"\"\"\n\n    def __init__(\n        self, rbm_layers, name='dbn', do_pretrain=False,\n        rbm_num_epochs=[10], rbm_gibbs_k=[1],\n        rbm_gauss_visible=False, rbm_stddev=0.1, rbm_batch_size=[10],\n        rbm_learning_rate=[0.01], finetune_dropout=1,\n        finetune_loss_func='softmax_cross_entropy',\n        finetune_act_func=tf.nn.sigmoid, finetune_opt='sgd',\n        finetune_learning_rate=0.001, finetune_num_epochs=10,\n            finetune_batch_size=20, momentum=0.5):\n\n        SupervisedModel.__init__(self, name)\n\n        self.loss_func = finetune_loss_func\n        self.learning_rate = finetune_learning_rate\n        self.opt = finetune_opt\n        self.num_epochs = finetune_num_epochs\n        self.batch_size = finetune_batch_size\n        self.momentum = momentum\n        self.dropout = finetune_dropout\n\n        self.loss = Loss(self.loss_func)\n        self.trainer = Trainer(\n            finetune_opt, learning_rate=finetune_learning_rate,\n            momentum=momentum)\n\n        self.do_pretrain = do_pretrain\n        self.layers = rbm_layers\n        self.finetune_act_func = finetune_act_func\n\n        # Model parameters\n        self.encoding_w_ = []  # list of matrices of encoding weights per layer\n        self.encoding_b_ = []  # list of arrays of encoding biases per layer\n\n        self.softmax_W = None\n        self.softmax_b = None\n\n        rbm_params = {\n            'num_epochs': rbm_num_epochs, 'gibbs_k': rbm_gibbs_k,\n            'batch_size': rbm_batch_size, 'learning_rate': rbm_learning_rate}\n\n        for p in rbm_params:\n            if len(rbm_params[p]) != len(rbm_layers):\n                # The current parameter is not specified by the user,\n                # should default it for all the layers\n                rbm_params[p] = [rbm_params[p][0] for _ in rbm_layers]\n\n        self.rbms = []\n        self.rbm_graphs = []\n\n        for l, layer in enumerate(rbm_layers):\n            rbm_str = 'rbm-' + str(l+1)\n\n            if l == 0 and rbm_gauss_visible:\n                self.rbms.append(\n                    rbm.RBM(\n                        name=self.name + '-' + rbm_str,\n                        num_hidden=layer,\n                        learning_rate=rbm_params['learning_rate'][l],\n                        num_epochs=rbm_params['num_epochs'][l],\n                        batch_size=rbm_params['batch_size'][l],\n                        gibbs_sampling_steps=rbm_params['gibbs_k'][l],\n                        visible_unit_type='gauss', stddev=rbm_stddev))\n\n            else:\n                self.rbms.append(\n                    rbm.RBM(\n                        name=self.name + '-' + rbm_str,\n                        num_hidden=layer,\n                        learning_rate=rbm_params['learning_rate'][l],\n                        num_epochs=rbm_params['num_epochs'][l],\n                        batch_size=rbm_params['batch_size'][l],\n                        gibbs_sampling_steps=rbm_params['gibbs_k'][l]))\n\n            self.rbm_graphs.append(tf.Graph())\n\n```", "```py\nfor l, layer in enumerate(rbm_layers):\n            rbm_str = 'rbm-' + str(l+1)\n\n            if l == 0 and rbm_gauss_visible:\n                self.rbms.append(\n                    rbm.RBM(\n                        name=self.name + '-' + rbm_str,\n                        num_hidden=layer,\n                        learning_rate=rbm_params['learning_rate'][l],\n                        num_epochs=rbm_params['num_epochs'][l],\n                        batch_size=rbm_params['batch_size'][l],\n                        gibbs_sampling_steps=rbm_params['gibbs_k'][l],\n                        visible_unit_type='gauss', stddev=rbm_stddev))\n\n            else:\n                self.rbms.append(\n                    rbm.RBM(\n                        name=self.name + '-' + rbm_str,\n                        num_hidden=layer,\n                        learning_rate=rbm_params['learning_rate'][l],\n                        num_epochs=rbm_params['num_epochs'][l],\n                        batch_size=rbm_params['batch_size'][l],\n                        gibbs_sampling_steps=rbm_params['gibbs_k'][l]))\n```", "```py\nclass RBM(UnsupervisedModel):\n    \"\"\"Restricted Boltzmann Machine implementation using TensorFlow.\n\n    The interface of the class is sklearn-like.\n    \"\"\"\n\n    def __init__(\n        self, num_hidden, visible_unit_type='bin',\n        name='rbm', loss_func='mse', learning_rate=0.01,\n        regcoef=5e-4, regtype='none', gibbs_sampling_steps=1,\n            batch_size=10, num_epochs=10, stddev=0.1):\n        \"\"\"Constructor.\n\n        :param num_hidden: number of hidden units\n        :param loss_function: type of loss function\n        :param visible_unit_type: type of the visible units (bin or gauss)\n        :param gibbs_sampling_steps: optional, default 1\n        :param stddev: default 0.1\\. Ignored if visible_unit_type is not 'gauss'\n        \"\"\"\n        UnsupervisedModel.__init__(self, name)\n\n        self.loss_func = loss_func\n        self.learning_rate = learning_rate\n        self.num_epochs = num_epochs\n        self.batch_size = batch_size\n        self.regtype = regtype\n        self.regcoef = regcoef\n\n        self.loss = Loss(self.loss_func)\n\n        self.num_hidden = num_hidden\n        self.visible_unit_type = visible_unit_type\n        self.gibbs_sampling_steps = gibbs_sampling_steps\n        self.stddev = stddev\n\n        self.W = None\n        self.bh_ = None\n        self.bv_ = None\n\n        self.w_upd8 = None\n        self.bh_upd8 = None\n        self.bv_upd8 = None\n\n        self.cost = None\n\n        self.input_data = None\n        self.hrand = None\n        self.vrand = None\n```", "```py\nself.rbm_graphs.append(tf.Graph())\n```", "```py\nclass RBM(UnsupervisedModel):\n...\n  def pretrain(self, train_set, validation_set=None):\n    \"\"\"Perform Unsupervised pretraining of the DBN.\"\"\"\n    self.do_pretrain = True\n\n    def set_params_func(rbmmachine, rbmgraph):\n    params = rbmmachine.get_parameters(graph=rbmgraph)\n     self.encoding_w_.append(params['W'])\n     self.encoding_b_.append(params['bh_'])\n\n    return SupervisedModel.pretrain_procedure(\n     self, self.rbms, self.rbm_graphs, set_params_func=set_params_func,\n     train_set=train_set, validation_set=validation_set)\n```", "```py\ndef pretrain_procedure(self, layer_objs, layer_graphs, set_params_func,\n train_set, validation_set=None):\n   next_train = train_set\n   next_valid = validation_set\n\n   for l, layer_obj in enumerate(layer_objs):\n     print('Training layer {}...'.format(l + 1))\n     next_train, next_valid = self._pretrain_layer_and_gen_feed(\n       layer_obj, set_params_func, next_train, next_valid,\n       layer_graphs[l])\n\n   return next_train, next_valid\n```", "```py\ndef _pretrain_layer_and_gen_feed(self, layer_obj, set_params_func,\n                                 train_set, validation_set, graph):\n    layer_obj.fit(train_set, train_set,\n                  validation_set, validation_set, graph=graph)\n\n    with graph.as_default():\n        set_params_func(layer_obj, graph)\n\n        next_train = layer_obj.transform(train_set, graph=graph)\n        if validation_set is not None:\n            next_valid = layer_obj.transform(validation_set, graph=graph)\n        else:\n            next_valid = None\n\n    return next_train, next_valid\n```", "```py\ndef fit(self, train_X, train_Y, val_X=None, val_Y=None, graph=None):\n\n    if len(train_Y.shape) != 1:\n        num_classes = train_Y.shape[1]\n    else:\n        raise Exception(\"Please convert the labels with one-hot encoding.\")\n\n    g = graph if graph is not None else self.tf_graph\n\n    with g.as_default():\n        # Build model\n        self.build_model(train_X.shape[1], num_classes)\n        with tf.Session() as self.tf_session:\n            # Initialize tf stuff\n            summary_objs = tf_utils.init_tf_ops(self.tf_session)\n            self.tf_merged_summaries = summary_objs[0]\n            self.tf_summary_writer = summary_objs[1]\n            self.tf_saver = summary_objs[2]\n            # Train model\n            self._train_model(train_X, train_Y, val_X, val_Y)\n            # Save model\n            self.tf_saver.save(self.tf_session, self.model_path)\n```", "```py\ndef predict(self, test_X):\n    with self.tf_graph.as_default():\n        with tf.Session() as self.tf_session:\n            self.tf_saver.restore(self.tf_session, self.model_path)\n            feed = {\n                self.input_data: test_X,\n                self.keep_prob: 1\n            }\n            return self.mod_y.eval(feed)\n```", "```py\ndef score(self, test_X, test_Y):\n  ...\n```", "```py\ndef score(self, test_X, test_Y):\n    with self.tf_graph.as_default():\n        with tf.Session() as self.tf_session:\n            self.tf_saver.restore(self.tf_session, self.model_path)\n            feed = {\n                self.input_data: test_X,\n                self.input_labels: test_Y,\n                self.keep_prob: 1\n            }\n            return self.accuracy.eval(feed)\n```", "```py\nimport tensorflow as tf\nfrom common.models.boltzmann import dbn\nfrom common.utils import datasets, utilities\n```", "```py\ntrainX, trainY, validX, validY, testX, testY = \n     datasets.load_mnist_dataset(mode='supervised')\n```", "```py\ndef load_mnist_dataset(mode='supervised', one_hot=True):\n   mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=one_hot)\n   # Training set\n   trX = mnist.train.images\n   trY = mnist.train.labels\n   # Validation set\n   vlX = mnist.validation.images\n   vlY = mnist.validation.labels\n   # Test set\n   teX = mnist.test.images\n   teY = mnist.test.labels\n   if mode == 'supervised':\n     return trX, trY, vlX, vlY, teX, teY\n   elif mode == 'unsupervised':\n     return trX, vlX, teX\n```", "```py\n\n finetune_act_func = tf.nn.relu\n rbm_layers = [256]\n do_pretrain = True\n name = 'dbn'\n rbm_layers = [256]\n finetune_act_func ='relu'\n do_pretrain = True\n rbm_learning_rate = [0.001]\n rbm_num_epochs = [1]\n rbm_gibbs_k= [1]\n rbm_stddev= 0.1\n rbm_gauss_visible= False\n momentum= 0.5\n rbm_batch_size= [32]\n finetune_learning_rate = 0.01\n finetune_num_epochs = 1\n finetune_batch_size = 32\n finetune_opt = 'momentum'\n finetune_loss_func = 'softmax_cross_entropy'\n finetune_dropout = 1\n finetune_act_func = tf.nn.sigmoid\n```", "```py\nsrbm = dbn.DeepBeliefNetwork(\n    name=name, do_pretrain=do_pretrain,\n    rbm_layers=rbm_layers,\n    finetune_act_func=finetune_act_func,\n    rbm_learning_rate=rbm_learning_rate,\n    rbm_num_epochs=rbm_num_epochs, rbm_gibbs_k = rbm_gibbs_k,\n    rbm_gauss_visible=rbm_gauss_visible, rbm_stddev=rbm_stddev,\n    momentum=momentum, rbm_batch_size=rbm_batch_size, \n    finetune_learning_rate=finetune_learning_rate,\n    finetune_num_epochs=finetune_num_epochs, \n    finetune_batch_size=finetune_batch_size,\n    finetune_opt=finetune_opt, finetune_loss_func=finetune_loss_func,\n    finetune_dropout=finetune_dropout\n    )\n\nprint(do_pretrain)\nif do_pretrain:\n    srbm.pretrain(trainX, validX)\n\n# finetuning\nprint('Start deep belief net finetuning...')\nsrbm.fit(trainX, trainY, validX, validY)\n\n# Test the model\nprint('Test set accuracy: {}'.format(srbm.score(testX, testY)))\n```", "```py\nReconstruction loss: 0.156712: 100%|██████████| 5/5 [00:49&lt;00:00, 9.99s/it]\nStart deep belief net finetuning...\nTensorboard logs dir for this run is /home/ubuntu/.yadlt/logs/run53\nAccuracy: 0.0868: 100%|██████████| 1/1 [00:04&lt;00:00, 4.09s/it]\nTest set accuracy: 0.0868000015616\n```", "```py\nReconstruction loss: 0.120337: 100%|██████████| 20/20 [03:07<00:00, 8.79s/it]\nStart deep belief net finetuning...\nTensorboard logs dir for this run is /home/ubuntu/.yadlt/logs/run80\nAccuracy: 0.105: 100%|██████████| 1/1 [00:04<00:00, 4.16s/it]\nTest set accuracy: 0.10339999944\n```", "```py\nReconstruction loss: 0.104798: 100%|██████████| 40/40 [06:20<00:00, 9.18s/it]\nStart deep belief net finetuning...\nTensorboard logs dir for this run is /home/ubuntu/.yadlt/logs/run82\nAccuracy: 0.075: 100%|██████████| 1/1 [00:04<00:00, 4.08s/it]\nTest set accuracy: 0.0773999989033\nAs can be seen the accuracy again came down so the optimal number of iterations peaks somewhere between 20 and 40\n```", "```py\nReconstruction loss: 0.128517: 100%|██████████| 5/5 [01:32&lt;00:00, 19.25s/it]\nStart deep belief net finetuning...\nTensorboard logs dir for this run is /home/ubuntu/.yadlt/logs/run55\nAccuracy: 0.0758: 100%|██████████| 1/1 [00:06&lt;00:00, 6.40s/it]\nTest set accuracy: 0.0689999982715\n```", "```py\nReconstruction loss: 0.180337: 100%|██████████| 5/5 [00:32&lt;00:00, 6.44s/it]\n Start deep belief net finetuning...\n Tensorboard logs dir for this run is /home/ubuntu/.yadlt/logs/run57\n Accuracy: 0.0698: 100%|██████████| 1/1 [00:03&lt;00:00, 3.16s/it]\n Test set accuracy: 0.0763999968767\n```", "```py\nname = 'dbn'\nrbm_layers = [256, 256]\nfinetune_act_func ='relu'\ndo_pretrain = True\nrbm_learning_rate = [0.001, 0.001]\nrbm_num_epochs = [5, 5]\nrbm_gibbs_k= [1, 1]\nrbm_stddev= 0.1\nrbm_gauss_visible= False\nmomentum= 0.5\nrbm_batch_size= [32, 32]\nfinetune_learning_rate = 0.01\nfinetune_num_epochs = 1\nfinetune_batch_size = 32\nfinetune_opt = 'momentum'\nfinetune_loss_func = 'softmax_cross_entropy'\nfinetune_dropout = 1\nfinetune_act_func = tf.nn.sigmoid\n```", "```py\nsrbm = dbn.DeepBeliefNetwork(\n name=name, do_pretrain=do_pretrain,\n rbm_layers=rbm_layers,\n finetune_act_func=finetune_act_func, rbm_learning_rate=rbm_learning_rate,\n rbm_num_epochs=rbm_num_epochs, rbm_gibbs_k = rbm_gibbs_k,\n rbm_gauss_visible=rbm_gauss_visible, rbm_stddev=rbm_stddev,\n momentum=momentum, rbm_batch_size=rbm_batch_size, finetune_learning_rate=finetune_learning_rate,\n finetune_num_epochs=finetune_num_epochs, finetune_batch_size=finetune_batch_size,\n finetune_opt=finetune_opt, finetune_loss_func=finetune_loss_func,\n finetune_dropout=finetune_dropout\n )\n```", "```py\n\n if do_pretrain:\n   srbm.pretrain(trainX, validX)\n```", "```py\n# \nfinetuning\n print('Start deep belief net finetuning...')\n srbm.fit(trainX, trainY, validX, validY)\n```", "```py\nprint('Test set accuracy: {}'.format(srbm.score(testX, testY)))\n```", "```py\nReconstruction loss: 0.156286: 100%|██████████| 5/5 [01:03&lt;00:00, 13.04s/it]\nTraining layer 2...\nTensorboard logs dir for this run is /home/ubuntu/.yadlt/logs/run73\nReconstruction loss: 0.127524: 100%|██████████| 5/5 [00:23&lt;00:00, 4.87s/it]\nStart deep belief net finetuning...\nTensorboard logs dir for this run is /home/ubuntu/.yadlt/logs/run74\nAccuracy: 0.1496: 100%|██████████| 1/1 [00:05&lt;00:00, 5.53s/it]\nTest set accuracy: 0.140300005674\n```", "```py\nimport tensorflow as tf\nimport numpy as np\nimport cPickle as pickle\n\nfrom common.models.boltzmann import dbn\nfrom common.utils import datasets, utilities\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\npickle_file = '../notMNIST.pickle'\n\nimage_size = 28\nnum_of_labels = 10\n\nRELU = 'RELU'\nRELU6 = 'RELU6'\nCRELU = 'CRELU'\nSIGMOID = 'SIGMOID'\nELU = 'ELU'\nSOFTPLUS = 'SOFTPLUS'\n\n```", "```py\nif __name__ == '__main__':\n    utilities.random_seed_np_tf(-1)\n    with open(pickle_file, 'rb') as f:\n        save = pickle.load(f)\n        training_dataset = save['train_dataset']\n        training_labels = save['train_labels']\n        validation_dataset = save['valid_dataset']\n        validation_labels = save['valid_labels']\n        test_dataset = save['test_dataset']\n        test_labels = save['test_labels']\n        del save  # hint to help gc free up memory\n        print 'Training set', training_dataset.shape, training_labels.shape\n        print 'Validation set', validation_dataset.shape, validation_labels.shape\n        print 'Test set', test_dataset.shape, test_labels.shape\n\n    train_dataset, train_labels = reformat(training_dataset, training_labels)\n    valid_dataset, valid_labels = reformat(validation_dataset, validation_labels)\n    test_dataset, test_labels = reformat(test_dataset, test_labels)\n\n    #trainX, trainY, validX, validY, testX, testY = datasets.load_mnist_dataset(mode='supervised')\n    trainX = train_dataset\n    trainY = train_labels\n\n    validX = valid_dataset\n    validY = valid_labels\n    testX = test_dataset\n    testY = test_labels\n\n    finetune_act_func = tf.nn.relu\n    rbm_layers = [256]\n    do_pretrain = True\n\n    name = 'dbn'\n    rbm_layers = [256]\n    finetune_act_func ='relu'\n    do_pretrain = True\n\n    rbm_learning_rate = [0.001]\n\n    rbm_num_epochs = [1]\n    rbm_gibbs_k= [1]\n    rbm_stddev= 0.1\n    rbm_gauss_visible= False\n    momentum= 0.5\n    rbm_batch_size= [32]\n    finetune_learning_rate = 0.01\n    finetune_num_epochs = 1\n    finetune_batch_size = 32\n    finetune_opt = 'momentum'\n    finetune_loss_func = 'softmax_cross_entropy'\n\n    finetune_dropout = 1\n    finetune_act_func = tf.nn.sigmoid\n\n    srbm = dbn.DeepBeliefNetwork(\n        name=name, do_pretrain=do_pretrain,\n        rbm_layers=rbm_layers,\n        finetune_act_func=finetune_act_func, rbm_learning_rate=rbm_learning_rate,\n        rbm_num_epochs=rbm_num_epochs, rbm_gibbs_k = rbm_gibbs_k,\n        rbm_gauss_visible=rbm_gauss_visible, rbm_stddev=rbm_stddev,\n        momentum=momentum, rbm_batch_size=rbm_batch_size, finetune_learning_rate=finetune_learning_rate,\n        finetune_num_epochs=finetune_num_epochs, finetune_batch_size=finetune_batch_size,\n        finetune_opt=finetune_opt, finetune_loss_func=finetune_loss_func,\n        finetune_dropout=finetune_dropout\n    )\n\n    if do_pretrain:\n        srbm.pretrain(trainX, validX)\n\n    # finetuning\n    print('Start deep belief net finetuning...')\n    srbm.fit(trainX, trainY, validX, validY)\n\n    # Test the model\n    print('Test set accuracy: {}'.format(srbm.score(testX, testY)))\n```", "```py\nReconstruction loss: 0.546223: 100%|██████████| 1/1 [00:00&lt;00:00, 5.51it/s]\nStart deep belief net finetuning...\nTensorboard logs dir for this run is /home/ubuntu/.yadlt/logs/run76\nAccuracy: 0.126: 100%|██████████| 1/1 [00:00&lt;00:00, 8.83it/s]\nTest set accuracy: 0.180000007153\n```"]