- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep Learning for Time Series Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ll tackle **time series classification** (**TSC**) problems
    using deep learning. As the name implies, TSC is a classification task involving
    time series data. The dataset contains several time series, and each of these
    has an associated categorical label. This problem is similar to a standard classification
    task, but the input explanatory variables are time series. We’ll explore how to
    approach this problem using different approaches. Besides using the **K-nearest
    neighbors** model to tackle this task, we’ll also develop different neural networks,
    such as a **residual neural network** (**ResNet**) and a convolutional neural
    network.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you’ll be able to set up a TSC task using a PyTorch
    Lightning data module and solve it with different models. You’ll also learn how
    to use the `sktime` Python library to solve this problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter contains the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Tackling TSC with K-nearest neighbors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a `DataModule` class for TSC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convolutional neural networks for TSC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ResNets for TSC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tackling TSC problems with `sktime`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We’ll focus on the PyTorch Lightning ecosystem to build deep learning models.
    Besides that, we’ll also use scikit-learn to create a baseline. Overall, the list
    of libraries used in the package is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: scikit-learn (1.3.2)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pandas` (2.1.3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NumPy (1.26.2)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Torch (2.1.1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch Lightning (2.1.2)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sktime` (0.24.1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keras-self-attention` (0.51.0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As an example, we’ll use the `Car` dataset from the repository available at
    the following link: [https://www.timeseriesclassification.com](https://www.timeseriesclassification.com).
    You can learn more about the dataset in the following work:'
  prefs: []
  type: TYPE_NORMAL
- en: Thakoor, Ninad, and Jean Gao. *Shape classifier based on generalized probabilistic
    descent method with hidden Markov descriptor*. Tenth IEEE **International Conference
    on Computer Vision** (**ICCV**’05) Volume 1\. Vol. 1\. IEEE, 2005.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code and datasets used in this chapter can be found at the following GitHub
    URL: [https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook](https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook).'
  prefs: []
  type: TYPE_NORMAL
- en: Tackling TSC with K-nearest neighbors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we’ll show you how to tackle TSC tasks using a popular method
    called K-nearest neighbors. The goal of this recipe is to show you how standard
    machine-learning models can be used to solve this problem.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, let’s start by loading the data using `pandas`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The dataset is already split into a training and testing set, so we read them
    separately. Now, let’s see how to build a K-nearest neighbor model using this
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here, we describe the steps necessary for building a time series classifier
    using scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by splitting the target variable from the explanatory variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The first column of each dataset (index `0`) contains the target variable, which
    we assign to the `y_train` and `y_test` objects for the training and testing sets,
    respectively. The `X_train` and `X_test` objects contain the input explanatory
    time series for the corresponding datasets.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'This particular dataset contains four different classes. Here’s what the distribution
    looks like:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.1: Distribution of the four classes in the dataset](img/B21145_08_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.1: Distribution of the four classes in the dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'Afterward, we need to normalize the time series. We accomplish this using the
    `MinMaxScaler` method from scikit-learn, which brings all values into a range
    between `0` and `1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, we fit the scaler using the training set, and then use
    it to transform the data in both datasets.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, we’re ready to create a K-nearest neighbors classification model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, we create a `KNeighborsTimeSeriesClassifier` instance
    that implements K-nearest neighbors and fits it using the training set. Then,
    we apply this model to the testing set by calling the `predict``()` method.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following figure shows the confusion matrix concerning the predictions
    of the K-nearest neighbor model:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.2: Confusion matrix for the predictions of the K-nearest neighbor
    model](img/B21145_08_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.2: Confusion matrix for the predictions of the K-nearest neighbor
    model'
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TSC problems are like standard classification tasks where the input explanatory
    variables are time series. So, the process for tackling the problem is similar.
    After splitting the explanatory variables (`X`) from the target variable (`y`),
    we prepare the explanatory variables using operators such as normalization functions.
    Then, we can use any classifier to solve this task. In this recipe, we use the
    K-nearest neighbor model, which is well-known for being a simple yet effective
    approach for this task.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the normalization step using `MinMaxScaler` is important to bring
    all observations into a common value range.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `sktime` Python library provides several methods for tackling TSC problems.
    Here’s the link to the documentation: [https://www.sktime.net/en/stable/examples/02_classification.html](https://www.sktime.net/en/stable/examples/02_classification.html).'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we used the K-nearest neighbors model using default parameters.
    For example, we used the Minkowski metric, which may not be the best one. For
    time series, distance metrics such as dynamic time warping are usually better
    approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Building a DataModule class for TSC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we return to the PyTorch Lightning framework. We’ll build a
    `DataModule` class to encapsulate the data preprocessing and the passing of observations
    to models.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s load the dataset from the previous recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Next, we’ll build a `DataModule` class to handle this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous chapters, we used `TimeSeriesDataSet` from PyTorch Forecasting
    to handle the data preparation for us. This class managed several steps. These
    include normalization and transformation of the data for supervised learning.
    However, in TSC, an observation uses the entire time series as input:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll start creating a simpler variant of `TimeSeriesDataSet` to handle the
    passing of observations to the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `__getitem__``()` method is used internally to get observations from the
    dataset and pass them to the model, while the `__len__``()` method outputs the
    size of the dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Then, we’re ready to build our `LightningDataModule` class. Here’s the constructor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`TSCDataModule` contains the `self.train`, `self.validation`, and `self.test`
    dataset attributes that will be filled with the `setup``()` method. Besides that,
    the constructor also sets up the normalization method based on `MinMaxScaler`
    and the one-hot encoder called `OneHotEncoder`. We use the one-hot encoder to
    transform the target variable into a set of binary variables. This process is
    necessary for the training of neural networks.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Then, the `setup``()` method is implemented as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, we use the encoder to transform the target variable and
    do the same using the normalization method with the explanatory variables. Then,
    we create a validation set based on the training instances. We also cast the data
    objects as torch data structures using the `torch.tensor()` method. Finally, we
    create the `TSCDataset` instances based on the training, validation, and testing
    sets.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We create the data loader methods using the `DataLoader` class directly on
    the respective dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, here’s an example of how to get an observation using this data module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the next recipe, we’ll learn how to build a classification model using this
    data module.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We created a `DataModule` class tailored for TSC classification problems. We
    encapsulated the data logic within the `setup``()` method, thus enabling us to
    use the PyTorch Lightning ecosystem to build deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, TSC problems do not involve autoregression. So, we created a simple
    variant of the `TimeSeriesDataSet` class to handle the process of passing the
    data to models.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional neural networks for TSC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we’ll walk you through building a convolutional neural network
    to tackle TSC problems. We’ll use the `DataModule` class created in the previous
    recipe to do this.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We start again by importing the dataset used in the previous recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We also create an instance of the `TSCDataModule` data module we defined in
    the previous recipe. Let’s see how to create a convolutional neural network classifier
    to handle this task.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here, we will walk you through the steps of building a convolutional neural
    network for TSC problems using PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by creating the neural network based on PyTorch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then wrap this model within a `LightningModule` class from PyTorch Lightning
    called `TSCCnnModel`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This module also contains the usual training, validation, and testing steps.
    These are implemented as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: These steps are similar to those developed for forecasting problems, but in
    this case, we use cross entropy as the `loss``()` function.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We are now ready to train the model, which we do with a `Trainer` instance
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, we set the output dimension to `4` when creating an instance
    of the `TSCCnnModel` class, which represents the number of classes in the dataset.
    We also set up an early stopping callback to drive the training process of the
    network.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Convolutional neural networks have been successfully applied to TSC problems.
    In this recipe, we explored developing a classifier based on PyTorch Lightning’s
    `LightningModule`. We create a `ConvolutionalTSC` class that extends the `nn.Module`
    class. In the constructor of this class, we define the layers of the network:
    three convolutional layers (`conv1`, `conv2`, and `conv3`), and two densely connected
    layers (`fc1` and `fc2`). The `forward()` method details how these layers are
    composed together. Then, the convolutional layers stack on top of each other,
    and a max pooling operation (`MaxPool1d`) is applied after each convolution. Finally,
    we stack two densely connected layers, where the last one is the output layer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows the confusion matrix for the convolution neural
    network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3: Confusion matrix for the convolutional neural network](img/B21145_08_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.3: Confusion matrix for the convolutional neural network'
  prefs: []
  type: TYPE_NORMAL
- en: The results obtained with the neural network are better than those when using
    the K-nearest neighbor model.
  prefs: []
  type: TYPE_NORMAL
- en: The workflow of this model follows the same logic as other recipes based on
    PyTorch Lightning. The main difference to take into account is that we’re dealing
    with a classification problem. So, we need to set a loss function that deals with
    this problem. Cross-entropy is the usual go-to function to train neural networks
    for classification tasks. The output dimension of the neural network is also set
    according to the number of classes. Essentially, there’s an output unit for each
    class in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: ResNets for TSC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe shows you how to train a ResNet for TSC tasks. ResNets are a type
    of deep neural network architecture widely used in computer vision problems, such
    as image classification or object detection. Here, you’ll learn how to use them
    for modeling time series data.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ll continue with the same dataset and data module as in the previous recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Let’s see how to build a ResNet and train it with PyTorch Lightning.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we describe the process of creating a ResNet for TSC tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by creating a ResNet using `nn.Module` from the `torch` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we wrap `ResidualNeuralNetworkModel` with `pl.LightningModule`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The training, validation, and testing steps are implemented identically to
    in the previous recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that we transform the predictions and the actual values into a `torch.FloatTensor`
    structure for computing the `loss``()` function. We set cross-entropy as the `loss``()`
    function, which is typically used in classification. In the testing stage, we
    also evaluate the accuracy of the model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, here’s the workflow for training and testing the model based on PyTorch
    Lightning’s `Trainer`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Essentially, after defining the model, the training and testing process with
    PyTorch Lightning’s `Trainer` is similar to that shown in the previous recipe.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'ResNets have shown promising performance in TSC problems. The idea is to learn
    the difference (residual) between the original input and a transformed output
    obtained from a convolutional layer. In the preceding code, we create a neural
    network that takes as input three parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`in_channels`: The number of input channels, which is equal to 1 because our
    time series are univariate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`out_channels`: The number of output channels from each residual block'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_classes`: The number of classes in the dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The layers of the neural network are composed of three residual blocks named
    `ResNNBlock`. Residual blocks are the cornerstone of ResNets and are designed
    to solve the vanishing gradient problem. You can check the implementation of the
    residual blocks named `ResNNBlock` at the following URL: [https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook](https://github.com/PacktPublishing/Deep-Learning-for-Time-Series-Data-Cookbook).
    The output from the residual blocks is passed on to a linear layer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows the confusion matrix for the ResNet:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4: Confusion matrix for the ResNet](img/B21145_08_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.4: Confusion matrix for the ResNet'
  prefs: []
  type: TYPE_NORMAL
- en: Like in the previous recipe, we wrap the implementation of the ResNet with the
    PyTorch Lightning framework. In the testing stage of this recipe, we also include
    the accuracy metric, which tells us the percentage of cases the model gets right.
    This metric is commonly used in TSC problems, though it might not be very informative
    for datasets with an imbalanced target distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Tackling TSC problems with sktime
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explore an alternative approach to PyTorch for TSC problems,
    which is `sktime`. `sktime` is a Python library devoted to time series modeling,
    which includes several neural network models for TSC.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can install `sktime` using `pip`. You’ll also need the `keras-self-attention`
    library, which includes self-attention methods necessary for running some of the
    methods in `sktime`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The trailing `dl` tag in squared brackets when installing `sktime` means you
    want to include the optional deep learning models available in the library.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we’ll use an example dataset available in `sktime`. We’ll load
    it in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the name implies, the `sktime` library follows a design pattern similar to
    scikit-learn. So, our approach to building a deep learning model using `sktime`
    will be similar to the workflow described in the *Tackling TSC with K-nearest*
    *neighbors* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by loading the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we load a dataset concerning energy demand in Italy.
    You can check the following link for more information about this dataset: [https://www.timeseriesclassification.com/description.php?Dataset=ItalyPowerDemand](https://www.timeseriesclassification.com/description.php?Dataset=ItalyPowerDemand).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The data was originally used in the following work:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Keogh, Eamonn, et al. *Intelligent icons: Integrating lite-weight data mining
    and visualization into GUI operating systems*. Sixth **International Conference
    on Data** Mining (**ICDM**’06). IEEE, 2006.'
  prefs: []
  type: TYPE_NORMAL
- en: We use `load_italy_power_demand` to load the train and test sets as `numpy`
    data structures.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s see how to build different types of neural networks using this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Fully connected neural network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We start by training a fully connected neural network. The configuration of
    this network, including `loss``()` and `activation``()` functions, is passed as
    arguments to the `FCNClassifier` instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The training and inference steps are done using the `fit``()` and `predict``()`
    methods, respectively. If you’re used to scikit-learn methods, this approach should
    be familiar to you.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional neural network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we learned in the *Convolutional neural networks for TSC* recipe, convolutional
    models can be an effective approach to classifying time series. Here’s the implementation
    available on `sktime` with `CNNClassifier`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also set other parameters concerning convolutions, such as `avg_pool_size`
    or `n_conv_layers`. Check the documentation for a complete list of parameters
    at the following link: [https://www.sktime.net/en/stable/api_reference/auto_generated/sktime.classification.deep_learning.CNNClassifier.html](https://www.sktime.net/en/stable/api_reference/auto_generated/sktime.classification.deep_learning.CNNClassifier.html).'
  prefs: []
  type: TYPE_NORMAL
- en: LSTM-FCN neural network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Recurrent neural networks can also be useful for this problem. Here’s a combination
    of an LSTM with a fully connected layer that is available in `LSTMFCNClassifier`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This method also includes an attention mechanism that improves classification
    accuracy significantly.
  prefs: []
  type: TYPE_NORMAL
- en: TapNet model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `sktime`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This model can manage a low-dimensional space (small number of features), and
    work well under a semi-supervised setting – that is, when there’s a large number
    of unlabeled observations available.
  prefs: []
  type: TYPE_NORMAL
- en: InceptionTime model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`InceptionTime` is a state-of-the-art deep learning method for TSC problems.
    In practice, `InceptionTime` is an ensemble of deep convolutional neural networks
    that is inspired by the inception architecture created for computer vision tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The model also includes optional residual connections, which, in the preceding
    code, we use by setting the `use_residual` parameter to `True`.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can use standard classification metrics to evaluate the performance of TSC
    models. Here’s how to compute the accuracy of the models we trained in this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5: Accuracy of the models](img/B21145_08_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.5: Accuracy of the models'
  prefs: []
  type: TYPE_NORMAL
- en: Overall, `InceptionTime` appears to be the best approach for this particular
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we use the `sktime` Python library to build deep-learning models
    for TSC. While you can use PyTorch as we’ve shown in the other recipes of this
    chapter, `sktime` provides an extensive toolkit for tackling TSC tasks. Since
    `sktime` follows the philosophy of scikit-learn, most of the work is done using
    the `fit``()` and `predict``()` methods of the respective class.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can check the documentation of `sktime` for other models, including some
    that are not based on deep learning. Here’s the link: [https://www.sktime.net/en/stable/users.html](https://www.sktime.net/en/stable/users.html).'
  prefs: []
  type: TYPE_NORMAL
- en: In most of the models we used in this recipe, we set the parameters to their
    default values. But, you can create a validation set and optimize the configuration
    of the models for better performance.
  prefs: []
  type: TYPE_NORMAL
