<html><head></head><body><div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Research in Neural Networks</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">In this chapter, we will look at some of the active research areas in neural networks. The following problems are analyzed from basic research areas to complex real-life problems:</p>
<ul class="calibre20">
<li class="calibre21">Overfitting in neural networks</li>
<li class="calibre21">Large-scale video processing with a neural network</li>
<li class="calibre21"><span class="calibre5">Named entity recognition using a twisted neural network</span></li>
<li class="calibre21">Bidirectional recurrent neural networks</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Avoiding overfitting in neural networks</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">Let's understand the constituents of overfitting and how to avoid it in neural networks. Nitesh Srivastava, Geoffrey Hinton, et al. published a paper, <a href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2">https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf</a>, in 2014, which shows cases on how to avoid overfitting.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Problem statement</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">Deep neural networks contain nonlinear hidden layers, and this makes them expressive models that can learn very complicated relationships between inputs and outputs. However, these complicated relationships will be the result of sampling noise. These complicated relationships might not exist in test data, leading to overfitting. Many techniques and methods have been developed to reduce this noise. These include stopping the training as soon as performance on a validation set starts getting worse, introducing weight penalties such as L1 and L2 regularization, and soft weight sharing (Nowlan and Hinton, 1992).</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Solution</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">Dropout is a technique that addresses performance issues of some of the other techniques such as averaging across multiple models. It also prevents overfitting and provides a way to combine exponentially many different neural network architectures efficiently. The term dropout means dropping out units (hidden and visible) in a neural network. By dropping a unit out, it means removing it from the network and its incoming and outgoing connections, as shown in the following figure.</p>
<p class="calibre4">The choice of which units to be dropped is usually random. In a simple case, each unit is retained with a probability p independent of other units. The technique to choose p can be a validation set or can be set at 0.5; this value is close to optimal for a wide range of networks and tasks.</p>
<p class="calibre4">For the input units, however, the optimal probability of retention is usually closer to 1 than to 0.5.</p>
<div class="mce-root"><img src="Images/a1c5b947-8244-4601-9ba1-2c34e27a6638.png" width="855" height="464" class="calibre182"/></div>
<p class="calibre4">Dropout neural net model:</p>
<ul class="calibre20">
<li class="calibre21">A standard neural network with two hidden layers</li>
<li class="calibre21">A thinned neural net produced by applying dropout to the network on the left; crossed units have been dropped</li>
</ul>
<p class="calibre4">Example of how Dropout can be applied in TensorFlow</p>
<pre class="calibre26">cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)<br class="calibre2"/>cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=0.5)<br class="calibre2"/>cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers, state_is_tuple=True)</pre>
<p class="calibre4">As can be seen above a Dropout of 0.5 is applied to the <kbd class="calibre18">LSTMCell</kbd>, where <kbd class="calibre18">output_keep_prob</kbd>: unit Tensor or float between 0 and 1, output keep probability; if it is constant and 1, no output dropout will be added.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Results</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">Let's look at how the dropout strategy affects the accuracy of the model:</p>
<div class="mce-root"> <img src="Images/3933cd47-acda-4760-89ac-019dfaf1d9c8.png" width="601" height="456" class="calibre183"/></div>
<p class="calibre4">As can be seen, the classification error decreases significantly with dropout.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Large-scale video processing with neural networks</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">In this paper, <a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42455.pdf" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2">https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42455.pdf</a>, the authors explore how CNNs could be used for large-scale video classification. In this use case, the neural networks have access to not only the appearance information in single, static images, but also the complex temporal evolution of the image. There are several challenges in extending and applying CNNs in this setting.</p>
<p class="calibre4">There are very few (or none) video classification benchmarks that match the scale and variety of existing image datasets as videos are significantly more challenging to collect, annotate, and store. To obtain sufficient amount of data needed to train our CNN architectures, authors collected a new Sports-1M dataset. This dataset contains 1 million videos (from YouTube) belonging to a taxonomy of 487 classes of sports. Sports-1M is also available to the research community to support future work in this area.</p>
<p class="calibre4">In this work, the authors treat every video as a bag of short, fixed-sized clips. Each clip contains several contiguous frames in time, hence the connectivity of the network can be extended in a time dimension to learning spatio-temporal features. The authors describe three broad connectivity pattern categories (<strong class="calibre7">Early Fusion</strong>, <strong class="calibre7">Late Fusion</strong>, and <strong class="calibre7">Slow Fusion</strong>). Afterward, we will look at a multiresolution architecture to address the computational efficiency.</p>
<p class="calibre4">The following figure explains various techniques for fusion:</p>
<div class="mce-root"><img src="Images/529a83c9-f956-4c19-905b-cacb2c394ba2.png" width="662" height="266" class="calibre184"/></div>
<div class="mce-root3">Various fusion techniques to combine frames separated by time</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Resolution improvements</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">The authors used a multiresolution architecture that aimed to strike a compromise by having two separate streams of processing (called Fovea and Context Streams) over two spatial resolutions (see the following figure). A 178 × 178 frame video clip is the input to the network:</p>
<div class="mce-root"><img src="Images/a7822995-bf74-4bbe-8995-a213dd0c66f6.png" width="665" height="397" class="calibre185"/></div>
<div class="mce-root3">Multiresolution CNN</div>
<p class="calibre4">The <strong class="calibre7">context stream</strong> receives the downsampled frames at half the original spatial resolution (89 × 89 pixels). The <strong class="calibre7">fovea stream</strong> receives the center 89 × 89 region at the original resolution. In this way, the total input dimensionality is halved.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Feature histogram baselines</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">In addition to comparing CNN architectures with each other, the authors also report the accuracy of a feature-based approach. A standard bag-of-words pipeline was used to extract several types of features at all frames of the videos, followed by discretizing them using k-means vector quantization and accumulating words into histograms with spatial pyramid encoding and soft quantization.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Quantitative results</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">Sports-1M dataset test set results (200,000 videos and 4,000,000 clips) are summarized in the following table. The approach of multiple networks consistently and significantly outperforms the feature-based baseline. The feature-based approach computes visual words densely over the duration of the video and produces predictions that are based on the complete video-level feature vector, while the authors' networks only see 20 randomly sampled clips individually:</p>
<div class="mce-root"><img src="Images/4133e9d9-37f1-44fd-826e-a4e6890ac9d2.png" width="1018" height="376" class="calibre186"/></div>
<div class="mce-root3">Results on the 200,000 videos of the Sports-1M test set. Hit@k values indicate the fraction of test samples that<br class="calibre2"/>
contained at least one of the ground truth labels in the top k predictions.</div>
<p class="cdpalignleft1">The approach taken with network topology learns well despite label noise; the training videos are subject to some incorrect annotations and even the correctly-labeled videos often contain a large amount of artifacts such as text, effects, cuts, and logos, none of which we attempted to filter out explicitly.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Named entity recognition using a twisted neural network</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">In this paper, <a href="http://www.cs.cmu.edu/~leili/pubs/lu-baylearn2015-twinet.pdf" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2">http://www.cs.cmu.edu/~leili/pubs/lu-baylearn2015-twinet.pdf,</a> the authors look at the problem of recognizing entities in natural language. This is often the first step in question answering, conversations, and a host of other NLP use cases. For a sequence of text tokens, a named entity recognizer identifies chunks of tokens that belong to a predefined category of persons and organizations.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Example of a named entity recognition</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">  IOB  tagging system is one of the convention for NER.</p>
<p class="calibre4">The<span class="calibre14"> </span><strong class="calibre7">IOB Tagging</strong><span class="calibre14"> </span>system contains tags of the form:</p>
<ul class="calibre20">
<li class="calibre21"><kbd class="calibre18">B-{CHUNK_TYPE}</kbd>: for the word in the<span class="calibre5"> </span><strong class="calibre3">B</strong>eginning chunk</li>
<li class="calibre21"><kbd class="calibre18">I-{CHUNK_TYPE}</kbd>: for words<span class="calibre5"> </span><strong class="calibre3">I</strong>nside the chunk</li>
<li class="calibre21"><kbd class="calibre18">O</kbd>: <strong class="calibre3">O</strong>utside any chunk</li>
<li class="calibre21"><kbd class="calibre18">B-PERSON</kbd> : Person Entity</li>
<li class="calibre21"><kbd class="calibre18">B-GPE</kbd> : <span class="calibre5">Geopolitical Entity</span></li>
</ul>
<p class="calibre4"><span class="calibre14"><br class="calibre25"/>
Th following text shows example of a names entities in a sentence:<br class="calibre25"/></span></p>
<pre class="calibre26">John    has lived in Britain  for    14        years   . <br class="calibre2"/>B-PERSON O    O    O B-GPE     O B-CARDINAL I-CARDINAL O</pre>
<p class="calibre4">However, it is quite challenging due to two reasons:</p>
<ul class="calibre20">
<li class="calibre21">Entity databases are often incomplete (given the number of new organizations being established)</li>
<li class="calibre21">The same phrase can refer to a different entity (or none entity) depending on the context</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Defining Twinet</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">Twisting RNNs (Twinet) use two parallel branches. Each branch is composed of a recurrent network layer, a nonlinear perceptron layer, and a reversed recurrent network layer. Branches are <em class="calibre17">twisted</em>: the order of the layers is reversed in the second branch. The output of all the recurrent layers is collected toward the end.</p>
<p class="calibre4">To recap, a <strong class="calibre7">recurrent neural network</strong> (<strong class="calibre7">RNN</strong>) takes a sequence of input vectors <em class="calibre17">x1..T</em>, and recurrently computes hidden states (also called <strong class="calibre7">output labels</strong>):</p>
<p class="calibre48"><em class="calibre17">h<sub class="calibre36">t</sub> = σ(U · x<sub class="calibre36">t</sub> + W · ht−1)</em></p>
<p class="calibre4">where,</p>
<ul class="calibre20">
<li class="calibre21"><em class="calibre29">t</em> is 1..T</li>
<li class="calibre21"><em class="calibre29">x<sub class="calibre36">t</sub></em> is the external signal</li>
<li class="calibre21"><em class="calibre29">W</em> are the weights</li>
<li class="calibre21"><em class="calibre29">h<sub class="calibre36">t-1</sub></em> is the hidden layer weights for time step <em class="calibre29">t-1</em></li>
<li class="calibre21"><em class="calibre29">h<sub class="calibre36">t</sub></em> weights being calculated for time step <em class="calibre29">t</em></li>
<li class="calibre21"><em class="calibre29">U</em> is tan<em class="calibre29">h</em> layer which helps in creating weights for time step <em class="calibre29">t</em></li>
</ul>
<p class="calibre4"><em class="calibre17">σ(·)</em> is a nonlinear activation function. In the experiments that the authors used, we used <strong class="calibre7">rectified linear units</strong> (<strong class="calibre7">RELU</strong>).</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Results</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">Twinet was compared to Stanford NER and Illinois NER and the results were quite favorable. <span class="calibre14">Here  NER stands for60;</span> (<strong class="calibre7">Named Entity Recognizer</strong>).</p>
<div class="mce-root"><img src="Images/e70dfc38-aaec-400c-bd50-ceeb3c45a3c7.png" width="472" height="167" class="calibre187"/></div>
<p class="calibre4">As can be seen from the preceding figure, the Precision-Recall as well as the F1 scores are all higher.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Bidirectional RNNs</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">In this section, we will look at a new neural network topology that is gaining momentum in the area of NLP.</p>
<p class="calibre4">Schuster and Paliwal have introduced <strong class="calibre7">Bidirectional Recurrent Neural Networks</strong> (<strong class="calibre7">BRNN</strong>) in 1997. BRNNs help increase the amount of input information available to the network. <strong class="calibre7">Multilayer perceptrons</strong> (<strong class="calibre7">MLPs</strong>) and <strong class="calibre7">time delay neural networks</strong> (<strong class="calibre7">TDNNs</strong>) are known to have limitations on the input data flexibility. RNNs also require their input data to be fixed. More advanced topologies like RNNs also have restrictions as the future input information cannot be predicted from the current state. BRNNs, on the contrary, do not need their input data to be fixed. Their future input information is reachable from the current state. The idea of BRNNs is to connect two hidden layers of opposite directions to the same output. With this structure, the output layer is able to get information from past and future states.</p>
<p class="calibre4">BRNNs are useful when the context of the input is needed. As an example, in handwriting recognition, the performance can be enhanced by knowledge of the letters located before and after the current letter.</p>
<div class="mce-root"><img src="Images/807e2fd3-ea0e-4cc3-9554-d7dd4e059fa5.png" width="1425" height="941" class="calibre188"/></div>
<div class="mce-root3">This depicts a Bidirectional RNN</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">BRNN on TIMIT dataset</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">In this section, we will look at how a BRNN provides higher accuracy results on the TIMIT Dataset for phoneme text classification.</p>
<p class="calibre4">TIMIT <span class="calibre14">is a corpus of phonemically and lexically transcribed speeches of American English speakers of different sexes and dialects. Each transcribed element has been delineated in time.</span> TIMIT <span class="calibre14">was designed to further acoustic-phonetic knowledge and automatic speech recognition systems:</span></p>
<div class="mce-root"><img src="Images/037a0b0a-62a7-4c7a-a18b-d76acaec0920.png" width="692" height="685" class="calibre189"/></div>
<p class="calibre4">As can be seen from the preceding figure, the BRNN gives higher percent frames accurately as compared to MLP, both for the training set and testing set. BLSTM gives even higher accuracy.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">In this chapter, we addressed some of the areas where research has been done on improving accuracy and avoiding overfitting. We also looked at some of the newer areas such as video classification. While it is outside the scope of this book to cover all the research areas in detail, we sincerely advise you to explore the research websites of Google, Facebook, and Baidu, in addition to Tier 1 ACM and IEEE conferences, to skim through new research being done.</p>
<p class="calibre4"/>


            </article>

            
        </section>
    </div>



  </body></html>