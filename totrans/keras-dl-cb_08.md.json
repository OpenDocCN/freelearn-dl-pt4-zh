["```py\ny = f(x;θ) = σ(Wx+b)y = f(x;θ) =σ(Wx+b)\n```", "```py\nz = g(y;θ′) = g(f(x;θ);θ′) = σ(W′y+b′)\n```", "```py\n\nfrom tensorflow.examples.tutorials.mnist import input_data\nimport matplotlib.pyplot as plt\n\nmnist = input_data.read_data_sets('MNIST_data', one_hot = True)\n\nclass OriginalImages:\n\n    def __init__(self):\n        pass\n\n    def main(self):\n        X_train, X_test = self.standard_scale(mnist.train.images, mnist.test.images)\n\n        original_imgs = X_test[:100]\n        plt.figure(1, figsize=(10, 10))\n\n        for i in range(0, 100):\n            im = original_imgs[i].reshape((28, 28))\n            ax = plt.subplot(10, 10, i + 1)\n            for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n                label.set_fontsize(8)\n\n            plt.imshow(im, cmap=\"gray\", clim=(0.0, 1.0))\n        plt.suptitle(' Original Images', fontsize=15, y=0.95)\n        plt.savefig('figures/original_images.png')\n        plt.show()\n\ndef main():\n    auto = OriginalImages()\n    auto.main()\n\nif __name__ == '__main__':\n    main()\n```", "```py\n self.num_input = num_input\n self.num_hidden = num_hidden\n self.transfer = transfer_function\n network_weights = self._initialize_weights()\n self.weights = network_weights\n```", "```py\ndef _initialize_weights(self):\n    weights = dict()\n    weights['w1'] = tf.get_variable(\"w1\", shape=[self.num_input, self.num_hidden],\n                      initializer=tf.contrib.layers.xavier_initializer())\n    weights['b1'] = tf.Variable(tf.zeros([self.num_hidden], dtype=tf.float32))\n    weights['w2'] = tf.Variable(tf.zeros([self.num_hidden, self.num_input],\n      dtype=tf.float32))\n    weights['b2'] = tf.Variable(tf.zeros([self.num_input], dtype=tf.float32))\n    return weights\n```", "```py\n self.x_var = tf.placeholder(tf.float32, [None, self.num_input])\n self.hidden_layer = self.transfer(tf.add(tf.matmul(self.x_var, \n   self.weights['w1']), self.weights['b1']))\n self.reconstruction = tf.add(tf.matmul(self.hidden_layer, \n   self.weights['w2']), self.weights['b2'])\n```", "```py\nThis is followed by the cost function and the Optimizer\n# cost function\nself.cost = 0.5 * tf.reduce_sum(\n   tf.pow(tf.subtract(self.reconstruction, self.x_var), 2.0))\nself.optimizer = optimizer.minimize(self.cost)\n```", "```py\ninitializer = tf.global_variables_initializer()\nself.session = tf.Session()\nself.session.run(initializer)\n```", "```py\nimport tensorflow as tf\n\nclass AutoEncoder:\n\n    def __init__(self, num_input, num_hidden, \n      transfer_function=tf.nn.softplus, \n      optimizer = tf.train.AdamOptimizer()):\n        self.num_input = num_input\n        self.num_hidden = num_hidden\n        self.transfer = transfer_function\n\n        network_weights = self._initialize_weights()\n        self.weights = network_weights\n\n        # model for reconstruction of the image\n        self.x_var = tf.placeholder(tf.float32, [None, self.num_input])\n        self.hidden_layer = self.transfer(tf.add(tf.matmul(self.x_var, \n          self.weights['w1']), self.weights['b1']))\n        self.reconstruction = tf.add(tf.matmul(self.hidden_layer, \n          self.weights['w2']), self.weights['b2'])\n\n        # cost function\n        self.cost = \n          0.5 * tf.reduce_sum(tf.pow(tf.subtract(self.reconstruction,\n          self.x_var), 2.0))\n        self.optimizer = optimizer.minimize(self.cost)\n\n        initializer = tf.global_variables_initializer()\n        self.session = tf.Session()\n        self.session.run(initializer)\n\n    def _initialize_weights(self):\n        weights = dict()\n        weights['w1'] = tf.get_variable(\"w1\", \n                          shape=[self.num_input, \n                          self.num_hidden],\n                          initializer=\n                            tf.contrib.layers.xavier_initializer())\n        weights['b1'] = tf.Variable(tf.zeros([self.num_hidden], \n                                    dtype=tf.float32))\n        weights['w2'] = tf.Variable(\n          tf.zeros([self.num_hidden, self.num_input],\n          dtype=tf.float32))\n        weights['b2'] = tf.Variable(tf.zeros(\n                          [self.num_input], dtype=tf.float32))\n        return weights\n\n    def partial_fit(self, X):\n        cost, opt = self.session.run((self.cost, self.optimizer), \n          feed_dict={self.x_var: X})\n        return cost\n\n    def calculate_total_cost(self, X):\n        return self.session.run(self.cost, feed_dict = {self.x_var: X})\n\n    def transform(self, X):\n        return self.session.run(self.hidden_layer, \n          feed_dict={self.x_var: X})\n\n    def generate(self, hidden = None):\n        if hidden is None:\n            hidden = self.session.run(\n              tf.random_normal([1, self.num_hidden]))\n        return self.session.run(self.reconstruction, \n               feed_dict={self.hidden_layer: hidden})\n\n    def reconstruct(self, X):\n        return self.session.run(self.reconstruction, \n                                feed_dict={self.x_var: X})\n\n    def get_weights(self):\n        return self.session.run(self.weights['w1'])\n\n    def get_biases(self):\n        return self.session.run(self.weights['b1'])\n```", "```py\nX_train, X_test = self.standard_scale(mnist.train.images, mnist.test.images).\n```", "```py\n n_samples = int(mnist.train.num_examples)\n training_epochs = 5\n batch_size = 128\n display_step = 1\n\n autoencoder = AutoEncoder(n_input = 784,\n   n_hidden = 200,\n   transfer_function = tf.nn.softplus,\n   optimizer = tf.train.AdamOptimizer(learning_rate = 0.001))\n```", "```py\nfor epoch in range(training_epochs):\n    avg_cost = 0.\n    total_batch = int(n_samples / batch_size)\n    # Loop over all batches\n    for i in range(total_batch):\n        batch_xs = self.get_random_block_from_data(X_train, batch_size)\n\n        # Fit training using batch data\n        cost = autoencoder.partial_fit(batch_xs)\n        # Compute average loss\n        avg_cost += cost / n_samples * batch_size\n\n    # Display logs per epoch step\n    if epoch % display_step == 0:\n        print(\"Epoch:\", '%04d' % (epoch + 1), \"cost=\", \"{:.9f}\".format(avg_cost))\n```", "```py\nprint(\"Total cost: \" + str(autoencoder.calc_total_cost(X_test)))\n```", "```py\n('Epoch:', '0001', 'cost=', '20432.278386364')\n('Epoch:', '0002', 'cost=', '13542.435997727')\n('Epoch:', '0003', 'cost=', '10630.662196023')\n('Epoch:', '0004', 'cost=', '10717.897946591')\n('Epoch:', '0005', 'cost=', '9354.191921023')\nTotal cost: 824850.0\n```", "```py\nprint(\"Total cost: \" + str(autoencoder.calc_total_cost(X_test)))\nwts = autoencoder.getWeights()\ndim = math.ceil(math.sqrt(autoencoder.n_hidden))\nplt.figure(1, figsize=(dim, dim))\n\nfor i in range(0, autoencoder.n_hidden):\n    im = wts.flatten()[i::autoencoder.n_hidden].reshape((28, 28))\n    ax = plt.subplot(dim, dim, i + 1)\n    for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n        label.set_fontname('Arial')\n        label.set_fontsize(8)\n    # plt.title('Feature Weights ' + str(i))\n    plt.imshow(im, cmap=\"gray\", clim=(-1.0, 1.0))\nplt.suptitle('Basic AutoEncoder Weights', fontsize=15, y=0.95)\n#plt.title(\"Test Title\", y=1.05)\nplt.savefig('figures/basic_autoencoder_weights.png')\nplt.show()\n```", "```py\npredicted_imgs = autoencoder.reconstruct(X_test[:100])\n\nplt.figure(1, figsize=(10, 10))\n\nfor i in range(0, 100):\n    im = predicted_imgs[i].reshape((28, 28))\n    ax = plt.subplot(10, 10, i + 1)\n    for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n                label.set_fontname('Arial')\n                label.set_fontsize(8)\n\n    plt.imshow(im, cmap=\"gray\", clim=(0.0, 1.0))\nplt.suptitle('Basic AutoEncoder Images', fontsize=15, y=0.95)\nplt.savefig('figures/basic_autoencoder_images.png')\nplt.show()\n```", "```py\nimport numpy as np\n\nimport sklearn.preprocessing as prep\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\nfrom autencoder_models.auto_encoder import AutoEncoder\nimport math\nimport matplotlib.pyplot as plt\n\nmnist = input_data.read_data_sets('MNIST_data', one_hot = True)\n\nclass BasicAutoEncoder:\n\n    def __init__(self):\n        pass\n\n    def standard_scale(self,X_train, X_test):\n        preprocessor = prep.StandardScaler().fit(X_train)\n        X_train = preprocessor.transform(X_train)\n        X_test = preprocessor.transform(X_test)\n        return X_train, X_test\n\n    def get_random_block_from_data(self,data, batch_size):\n        start_index = np.random.randint(0, len(data) - batch_size)\n        return data[start_index:(start_index + batch_size)]\n\n    def main(self):\n        X_train, X_test = self.standard_scale(mnist.train.images, mnist.test.images)\n\n        n_samples = int(mnist.train.num_examples)\n        training_epochs = 5\n        batch_size = 128\n        display_step = 1\n\n        autoencoder = AutoEncoder(n_input = 784,\n                                  n_hidden = 200,\n                                  transfer_function = tf.nn.softplus,\n                                  optimizer = tf.train.AdamOptimizer(\n                                    learning_rate = 0.001))\n\n        for epoch in range(training_epochs):\n            avg_cost = 0.\n            total_batch = int(n_samples / batch_size)\n            # Loop over all batches\n            for i in range(total_batch):\n                batch_xs = self.get_random_block_from_data(X_train, batch_size)\n\n                # Fit training using batch data\n                cost = autoencoder.partial_fit(batch_xs)\n                # Compute average loss\n                avg_cost += cost / n_samples * batch_size\n\n            # Display logs per epoch step\n            if epoch % display_step == 0:\n                print(\"Epoch:\", '%04d' % (epoch + 1), \"cost=\", \"{:.9f}\".format(avg_cost))\n\n        print(\"Total cost: \" + str(autoencoder.calc_total_cost(X_test)))\n\n        wts = autoencoder.getWeights()\n        dim = math.ceil(math.sqrt(autoencoder.n_hidden))\n        plt.figure(1, figsize=(dim, dim))\n\n        for i in range(0, autoencoder.n_hidden):\n            im = wts.flatten()[i::autoencoder.n_hidden].reshape((28, 28))\n            ax = plt.subplot(dim, dim, i + 1)\n            for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n                label.set_fontname('Arial')\n                label.set_fontsize(8)\n            # plt.title('Feature Weights ' + str(i))\n            plt.imshow(im, cmap=\"gray\", clim=(-1.0, 1.0))\n        plt.suptitle('Basic AutoEncoder Weights', fontsize=15, y=0.95)\n        #plt.title(\"Test Title\", y=1.05)\n        plt.savefig('figures/basic_autoencoder_weights.png')\n        plt.show()\n\n        predicted_imgs = autoencoder.reconstruct(X_test[:100])\n\n        plt.figure(1, figsize=(10, 10))\n\n        for i in range(0, 100):\n            im = predicted_imgs[i].reshape((28, 28))\n            ax = plt.subplot(10, 10, i + 1)\n            for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n                        label.set_fontname('Arial')\n                        label.set_fontsize(8)\n\n            plt.imshow(im, cmap=\"gray\", clim=(0.0, 1.0))\n        plt.suptitle('Basic AutoEncoder Images', fontsize=15, y=0.95)\n        plt.savefig('figures/basic_autoencoder_images.png')\n        plt.show()\n\ndef main():\n    auto = BasicAutoEncoder()\n    auto.main()\n\nif __name__ == '__main__':\n    main()\n```", "```py\ndef __init__(self, num_input, num_hidden, \n                 transfer_function=tf.nn.sigmoid, \n                 optimizer=tf.train.AdamOptimizer(),\n                 scale=0.1):\n```", "```py\n        self.num_input = num_input\n        self.num_hidden = num_hidden\n        self.transfer = transfer_function\n        self.scale = tf.placeholder(tf.float32)\n        self.training_scale = scale\n        n_weights = self._initialize_weights()\n        self.weights = n_weights\n```", "```py\nself.x = tf.placeholder(tf.float32, [None, self.num_input])\nself.hidden_layer = self.transfer(\n            tf.add(tf.matmul(\n                      self.x + scale * tf.random_normal((n_input,)),\n                      self.weights['w1']),\n                      self.weights['b1']))\nself.reconstruction = tf.add(\n                        tf.matmul(self.hidden_layer, self.weights['w2']),    \n                        self.weights['b2'])\n```", "```py\nself.cost = 0.5 * tf.reduce_sum(tf.pow(tf.subtract(\n                           self.reconstruction, self.x), 2.0))\nself.optimizer = optimizer.minimize(self.cost)\n```", "```py\ninit = tf.global_variables_initializer()\nself.session = tf.Session()\nself.session.run(init)\n\n```", "```py\nmnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n\ndef get_random_block_from_data(data, batch_size):\n    start_index = np.random.randint(0, len(data) - batch_size)\n    return data[start_index:(start_index + batch_size)]\n\nX_train = mnist.train.images\nX_test = mnist.test.images\n\n```", "```py\nn_samples = int(mnist.train.num_examples)\ntraining_epochs = 2\nbatch_size = 128\ndisplay_step = 1\n```", "```py\nautoencoder = AdditiveGaussianNoiseAutoEncoder(n_input=784,\n                                               n_hidden=200,\n                                               transfer_function=tf.nn.sigmoid,\n                                               optimizer=tf.train.AdamOptimizer(learning_rate=0.001),\n                                               scale=0.01)\n```", "```py\nfor epoch in range(training_epochs):\n    avg_cost = 0.\n    total_batch = int(n_samples / batch_size)\n    # Loop over all batches\n    for i in range(total_batch):\n        batch_xs = get_random_block_from_data(X_train, batch_size)\n\n        # Fit training using batch data\n        cost = autoencoder.partial_fit(batch_xs)\n        # Compute average loss\n        avg_cost += cost / n_samples * batch_size\n\n    # Display logs per epoch step\n    if epoch % display_step == 0:\n        print(\"Epoch:\", '%04d' % (epoch + 1), \"cost=\", avg_cost)\n\nprint(\"Total cost: \" + str(autoencoder.calc_total_cost(X_test)))\n```", "```py\n('Epoch:', '0001', 'cost=', 1759.873304261363)\n('Epoch:', '0002', 'cost=', 686.85984829545475)\n('Epoch:', '0003', 'cost=', 460.52834446022746)\n('Epoch:', '0004', 'cost=', 355.10590241477308)\n('Epoch:', '0005', 'cost=', 297.99104825994351)\n```", "```py\nTotal cost: 21755.4\n```", "```py\nwts = autoencoder.get_weights()\ndim = math.ceil(math.sqrt(autoencoder.num_hidden))\nplt.figure(1, figsize=(dim, dim))\nfor i in range(0, autoencoder.num_hidden):\n    im = wts.flatten()[i::autoencoder.num_hidden].reshape((28, 28))\n    ax = plt.subplot(dim, dim, i + 1)\n    for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n        label.set_fontsize(8)\n    #plt.title('Feature Weights ' + str(i))\n\n    plt.imshow(im, cmap=\"gray\", clim=(-1.0, 1.0))\nplt.suptitle('Additive Gaussian Noise AutoEncoder Weights', fontsize=15, y=0.95)\nplt.savefig('figures/additive_gaussian_weights.png')\nplt.show()\n```", "```py\npredicted_imgs = autoencoder.reconstruct(X_test[:100])\n\n# plot the reconstructed images\nplt.figure(1, figsize=(10, 10))\nplt.title('Autoencoded Images')\nfor i in range(0, 100):\n    im = predicted_imgs[i].reshape((28, 28))\n    ax = plt.subplot(10, 10, i + 1)\n    for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n        label.set_fontname('Arial')\n        label.set_fontsize(8)\n\n    plt.imshow(im, cmap=\"gray\", clim=(0.0, 1.0))\nplt.suptitle('Additive Gaussian Noise AutoEncoder Images', fontsize=15, y=0.95)\nplt.savefig('figures/additive_gaussian_images.png')\nplt.show()\n```", "```py\nimport numpy as np\nimport tensorflow as tf\ndef xavier_init(fan_in, fan_out, constant = 1):\n    low = -constant * np.sqrt(6.0 / (fan_in + fan_out))\n    high = constant * np.sqrt(6.0 / (fan_in + fan_out))\n    return tf.random_uniform((fan_in, fan_out), minval = low, maxval = high, dtype = tf.float32)\n\nclass AdditiveGaussianNoiseAutoEncoder(object):\n    def __init__(self, num_input, num_hidden, \n                 transfer_function=tf.nn.sigmoid, \n                 optimizer=tf.train.AdamOptimizer(),\n                 scale=0.1):\n        self.num_input = num_input\n        self.num_hidden = num_hidden\n        self.transfer = transfer_function\n        self.scale = tf.placeholder(tf.float32)\n        self.training_scale = scale\n        n_weights = self._initialize_weights()\n        self.weights = n_weights\n      # model\n</span>        self.x = tf.placeholder(tf.float32, [None, self.num_input])\n        self.hidden_layer = self.transfer(\n            tf.add(tf.matmul(\n                      self.x + scale * tf.random_normal((n_input,)),\n                      self.weights['w1']),\n                      self.weights['b1']))\n        self.reconstruction = tf.add(\n                                tf.matmul(self.hidden_layer, self.weights['w2']),    \n                                self.weights['b2'])\n\n        # cost\n        self.cost = 0.5 * tf.reduce_sum(tf.pow(tf.subtract(\n                           self.reconstruction, self.x), 2.0))\n\n        self.optimizer = optimizer.minimize(self.cost)\n\n        init = tf.global_variables_initializer()\n        self.session = tf.Session()\n        self.session.run(init)\n\n    def _initialize_weights(self):\n        weights = dict()\n        weights['w1'] = tf.Variable(xavier_init(self.num_input, self.num_hidden))\n        weights['b1'] = tf.Variable(tf.zeros([self.num_hidden], dtype=tf.float32))\n        weights['w2'] = tf.Variable(tf.zeros([self.num_hidden, self.num_input],\n          dtype=tf.float32))\n        weights['b2'] = tf.Variable(tf.zeros([self.num_input], dtype=tf.float32))\n        return weights\n\n    def partial_fit(self, X):\n        cost, opt = self.session.run((self.cost, self.optimizer), \n            feed_dict={self.x: X,self.scale: self.training_scale})\n        return cost\n\n    def kl_divergence(self, p, p_hat):\n        return tf.reduce_mean(\n          p * tf.log(p) - p * tf.log(p_hat) + (1 - p) * tf.log(1 - p) - (1 - p) *        tf.log(1 - p_hat))\n\n    def calculate_total_cost(self, X):\n        return self.session.run(self.cost, feed_dict={self.x: X,\n                                                      self.scale: self.training_scale\n                                                      })\n\n    def transform(self, X):\n        return self.session.run(\n          self.hidden_layer, \n          feed_dict = {self.x: X, self.scale: self.training_scale})\n\n    def generate_value(self, _hidden=None):\n        if _hidden is None:\n            _hidden = np.random.normal(size=self.weights[\"b1\"])\n        return self.session.run(self.reconstruction, \n            feed_dict={self.hidden_layer: _hidden})\n\n    def reconstruct(self, X):\n        return self.session.run(self.reconstruction, \n          feed_dict={self.x: X,self.scale: self.training_scale })\n\n    def get_weights(self):\n        return self.session.run(self.weights['w1'])\n\n    def get_biases(self):\n        return self.session.run(self.weights['b1'])\n```", "```py\ndef kl_divergence(self, p, p_hat):\n    return tf.reduce_mean(\n           p*(tf.log(p)/tf.log(p_hat)) + \n           (1-p)*(tf.log(1-p)/tf.log(1-p_hat)))\n```", "```py\nself.cost = 0.5 * tf.reduce_sum(\n  tf.pow(tf.subtract(self.reconstruction, self.x), 2.0)) + \n    self.sparse_reg * self.kl_divergence(self.sparsity_level, self.hidden_layer)\n```", "```py\nclass SparseAutoencoder(object):\n    def __init__(self, num_input, num_hidden, \n                 transfer_function=tf.nn.softplus, \n                 optimizer=tf.train.AdamOptimizer(),\n                 scale=0.1):\n        self.num_input = num_input\n        self.num_hidden = num_hidden\n        self.transfer = transfer_function\n        self.scale = tf.placeholder(tf.float32)\n        self.training_scale = scale\n        network_weights = self._initialize_weights()\n        self.weights = network_weights\n        self.sparsity_level = np.repeat([0.05], \n           self.num_hidden).astype(np.float32)\n        self.sparse_reg = 0.0\n\n        # model\n        self.x = tf.placeholder(tf.float32, [None, self.num_input])\n        self.hidden_layer = self.transfer(tf.add(tf.matmul(\n            self.x + scale * tf.random_normal((num_input,)), \n                                               self.weights['w1']),\n                                               self.weights['b1']))\n        self.reconstruction = tf.add(tf.matmul(self.hidden_layer, \n            self.weights['w2']), self.weights['b2'])\n\n        # cost\n        self.cost = 0.5 * tf.reduce_sum(\n            tf.pow(tf.subtract(self.reconstruction, self.x), 2.0)) + \n            self.sparse_reg * self.kl_divergence(\n                self.sparsity_level, self.hidden_layer)\n\n        self.optimizer = optimizer.minimize(self.cost)\n\n        init = tf.global_variables_initializer()\n        self.session = tf.Session()\n        self.session.run(init)\n\n    def _initialize_weights(self):\n        all_weights = dict()\n        all_weights['w1'] = tf.Variable(xavier_init(self.num_input, \n           self.num_hidden))\n        all_weights['b1'] = tf.Variable(tf.zeros([self.num_hidden], \n           dtype = tf.float32))\n        all_weights['w2'] = tf.Variable(tf.zeros([self.num_hidden, \n                            self.num_input], \n                            dtype = tf.float32))\n        all_weights['b2'] = tf.Variable(tf.zeros([self.num_input], \n            dtype = tf.float32))\n        return all_weights\n\n    def partial_fit(self, X):\n        cost, opt = self.session.run((self.cost, self.optimizer), \n                    feed_dict = {self.x: X,\n                                 self.scale: self.training_scale})\n        return cost\n\n     def kl_divergence(self, p, p_hat):\n         return tf.reduce_mean(p*(tf.log(p)/tf.log(p_hat)) + \n             (1-p)*(tf.log(1-p)/tf.log(1-p_hat)))\n\n     def calculate_total_cost(self, X):\n         return self.session.run(self.cost, feed_dict = {\n             self.x: X,\n             self.scale: self.training_scale\n         })\n\n     def transform(self, X):\n         return self.session.run(self.hidden_layer, \n             feed_dict = {self.x: X, self.scale: self.training_scale})\n\n     def generate(self, hidden = None):\n         if hidden is None:\n             hidden = np.random.normal(size = self.weights[\"b1\"])\n             return self.session.run(self.reconstruction, \n                 feed_dict = {self.hidden_layer: hidden})\n\n     def reconstruct(self, X):\n         return self.session.run(self.reconstruction, \n             feed_dict = {self.x: X, self.scale: self.training_scale})\n\n     def get_weights(self):\n         return self.session.run(self.weights['w1'])\n\n     def get_biases(self):\n         return self.session.run(self.weights['b1'])\n\n```", "```py\nclass SparseAutoEncoderExample:\n    def main(self):\n        mnist = input_data.read_data_sets('MNIST_data', one_hot = True)\n\n        def get_random_block_from_data(data, batch_size):\n            start_index = np.random.randint(0, len(data) - batch_size)\n            return data[start_index:(start_index + batch_size)]\n\n        X_train = mnist.train.images\n        X_test = mnist.test.images\n\n        n_samples = int(mnist.train.num_examples)\n        training_epochs = 5\n        batch_size = 128\n        display_step = 1\n\n        autoencoder =SparseAutoencoder(num_input=784,\n                                       num_hidden = 200,\n                                       transfer_function = tf.nn.sigmoid,\n                                       optimizer = tf.train.AdamOptimizer(\n                                           learning_rate = 0.001),\n                                       scale = 0.01)\n\n        for epoch in range(training_epochs):\n            avg_cost = 0.\n            total_batch = int(n_samples / batch_size)\n            # Loop over all batches\n            for i in range(total_batch):\n                batch_xs = get_random_block_from_data(X_train, batch_size)\n\n                # Fit training using batch data\n                cost = autoencoder.partial_fit(batch_xs)\n                # Compute average loss\n                avg_cost += cost / n_samples * batch_size\n\n            # Display logs per epoch step\n            if epoch % display_step == 0:\n                print(\"Epoch:\", '%04d' % (epoch + 1), \"cost=\", avg_cost)\n\n        print(\"Total cost: \" + \n            str(autoencoder.calculate_total_cost(X_test)))\n\n        # input weights\n        wts = autoencoder.get_weights()\n        dim = math.ceil(math.sqrt(autoencoder.num_hidden))\n        plt.figure(1, figsize=(dim, dim))\n        for i in range(0, autoencoder.num_hidden):\n            im = wts.flatten()[i::autoencoder.num_hidden].reshape((28, 28))\n            ax = plt.subplot(dim, dim, i + 1)\n            for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n                label.set_fontsize(6)\n            plt.subplot(dim, dim, i+1)\n            plt.imshow(im, cmap=\"gray\", clim=(-1.0, 1.0))\n        plt.suptitle('Sparse AutoEncoder Weights', fontsize=15, y=0.95)\n        plt.savefig('figures/sparse_autoencoder_weights.png')\n        plt.show()\n\n        predicted_imgs = autoencoder.reconstruct(X_test[:100])\n\n        # plot the reconstructed images\n        plt.figure(1, figsize=(10, 10))\n        plt.title('Sparse Autoencoded Images')\n        for i in range(0,100):\n            im = predicted_imgs[i].reshape((28,28))\n            ax = plt.subplot(10, 10, i + 1)\n            for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n                label.set_fontsize(6)\n\n            plt.subplot(10, 10, i+1)\n            plt.imshow(im, cmap=\"gray\", clim=(0.0, 1.0))\n        plt.suptitle('Sparse AutoEncoder Images', fontsize=15, y=0.95)\n        plt.savefig('figures/sparse_autoencoder_images.png')\n        plt.show()\n\ndef main():\n    auto = SparseAutoEncoderExample()\n    auto.main()\n\nif __name__ == '__main__':\n    main()\n```", "```py\n('Epoch:', '0001', 'cost=', 1697.039439488638)\n('Epoch:', '0002', 'cost=', 667.23002088068188)\n('Epoch:', '0003', 'cost=', 450.02947024147767)\n('Epoch:', '0004', 'cost=', 351.54360497159115)\n('Epoch:', '0005', 'cost=', 293.73473448153396)\nTotal cost: 21025.2\n```"]